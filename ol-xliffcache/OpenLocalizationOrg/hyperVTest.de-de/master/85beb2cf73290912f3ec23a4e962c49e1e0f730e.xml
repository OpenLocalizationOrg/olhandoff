{
  "nodes": [
    {
      "pos": [
        28,
        121
      ],
      "content": "Use Script Action to install Apache Spark on Linux-based HDInsight (Hadoop) | Microsoft Azure"
    },
    {
      "pos": [
        141,
        363
      ],
      "content": "Learn how to install Spark on a Linux-based HDInsight cluster using Script Actions. Script Actions allow you to customize the cluster during creation, by changing cluster configuration or installing services and utilities.",
      "nodes": [
        {
          "content": "Learn how to install Spark on a Linux-based HDInsight cluster using Script Actions.",
          "pos": [
            0,
            83
          ]
        },
        {
          "content": "Script Actions allow you to customize the cluster during creation, by changing cluster configuration or installing services and utilities.",
          "pos": [
            84,
            222
          ]
        }
      ]
    },
    {
      "pos": [
        681,
        731
      ],
      "content": "Install and use Spark on HDInsight Hadoop clusters"
    },
    {
      "pos": [
        733,
        1112
      ],
      "content": "In this document, you will learn how to install Spark by using Script Action. Script Action lets you run scripts to customize a cluster, only when the cluster is being created. For more information, see <bpt id=\"p1\">[</bpt>Customize HDInsight cluster using Script Action<ept id=\"p1\">][hdinsight-cluster-customize]</ept>. Once you have installed Spark, you'll also learn how to run a Spark query on HDInsight clusters.",
      "nodes": [
        {
          "content": "In this document, you will learn how to install Spark by using Script Action.",
          "pos": [
            0,
            77
          ]
        },
        {
          "content": "Script Action lets you run scripts to customize a cluster, only when the cluster is being created.",
          "pos": [
            78,
            176
          ]
        },
        {
          "content": "For more information, see <bpt id=\"p1\">[</bpt>Customize HDInsight cluster using Script Action<ept id=\"p1\">][hdinsight-cluster-customize]</ept>.",
          "pos": [
            177,
            320
          ]
        },
        {
          "content": "Once you have installed Spark, you'll also learn how to run a Spark query on HDInsight clusters.",
          "pos": [
            321,
            417
          ]
        }
      ]
    },
    {
      "pos": [
        1116,
        1579
      ],
      "content": "<ph id=\"ph2\">[AZURE.NOTE]</ph><ph id=\"ph3\"/> HDInsight also provides Spark as a cluster type, which means you can now directly provision a Spark cluster without modifying a Hadoop cluster. However, this is limited to Windows-based clusters currently. Using the Spark cluster type, you get a Windows-based HDInsight version 3.2 cluster with Spark version 1.3.1. For more information, see <bpt id=\"p2\">[</bpt>Get Started with Apache Spark on HDInsight<ept id=\"p2\">](hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql.md)</ept>.",
      "nodes": [
        {
          "content": "<ph id=\"ph2\">[AZURE.NOTE]</ph><ph id=\"ph3\"/> HDInsight also provides Spark as a cluster type, which means you can now directly provision a Spark cluster without modifying a Hadoop cluster.",
          "pos": [
            0,
            188
          ]
        },
        {
          "content": "However, this is limited to Windows-based clusters currently.",
          "pos": [
            189,
            250
          ]
        },
        {
          "content": "Using the Spark cluster type, you get a Windows-based HDInsight version 3.2 cluster with Spark version 1.3.1.",
          "pos": [
            251,
            360
          ]
        },
        {
          "content": "For more information, see <bpt id=\"p2\">[</bpt>Get Started with Apache Spark on HDInsight<ept id=\"p2\">](hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql.md)</ept>.",
          "pos": [
            361,
            533
          ]
        }
      ]
    },
    {
      "pos": [
        1605,
        1619
      ],
      "content": "What is Spark?"
    },
    {
      "pos": [
        1694,
        1706
      ],
      "content": "Apache Spark"
    },
    {
      "pos": [
        1711,
        1987
      ],
      "content": "is an open-source parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications. Spark's in-memory computation capabilities make it a good choice for iterative algorithms in machine learning and graph computations.",
      "nodes": [
        {
          "content": "is an open-source parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications.",
          "pos": [
            0,
            142
          ]
        },
        {
          "content": "Spark's in-memory computation capabilities make it a good choice for iterative algorithms in machine learning and graph computations.",
          "pos": [
            143,
            276
          ]
        }
      ]
    },
    {
      "pos": [
        1989,
        2321
      ],
      "content": "Spark can also be used to perform conventional disk-based data processing. Spark improves the traditional MapReduce framework by avoiding writes to disk in the intermediate stages. Also, Spark is compatible with the Hadoop Distributed File System (HDFS) and Azure Blob storage so the existing data can easily be processed via Spark.",
      "nodes": [
        {
          "content": "Spark can also be used to perform conventional disk-based data processing.",
          "pos": [
            0,
            74
          ]
        },
        {
          "content": "Spark improves the traditional MapReduce framework by avoiding writes to disk in the intermediate stages.",
          "pos": [
            75,
            180
          ]
        },
        {
          "content": "Also, Spark is compatible with the Hadoop Distributed File System (HDFS) and Azure Blob storage so the existing data can easily be processed via Spark.",
          "pos": [
            181,
            332
          ]
        }
      ]
    },
    {
      "pos": [
        2324,
        2415
      ],
      "content": "This topic provides instructions on how to customize an HDInsight cluster to install Spark."
    },
    {
      "pos": [
        2441,
        2478
      ],
      "content": "Which version of Spark can I install?"
    },
    {
      "pos": [
        2480,
        2607
      ],
      "content": "In this topic, we use a Script Action custom script to install Spark on an HDInsight cluster. This script installs Spark 1.5.1.",
      "nodes": [
        {
          "content": "In this topic, we use a Script Action custom script to install Spark on an HDInsight cluster.",
          "pos": [
            0,
            93
          ]
        },
        {
          "content": "This script installs Spark 1.5.1.",
          "pos": [
            94,
            127
          ]
        }
      ]
    },
    {
      "pos": [
        2609,
        2697
      ],
      "content": "You can modify this script or create your own script to install other versions of Spark."
    },
    {
      "pos": [
        2702,
        2722
      ],
      "content": "What the script does"
    },
    {
      "pos": [
        2724,
        2795
      ],
      "content": "This script installs Spark version 1.5.1 into <ph id=\"ph4\">`/usr/hdp/current/spark`</ph>."
    },
    {
      "pos": [
        2799,
        3013
      ],
      "content": "<ph id=\"ph5\">[AZURE.WARNING]</ph><ph id=\"ph6\"/> You may discover that some Spark 1.3.1 binaries are installed by default on your HDInsight cluster. These should not be used, and will be removed from the HDInsight cluster image in a future update.",
      "nodes": [
        {
          "content": "<ph id=\"ph5\">[AZURE.WARNING]</ph><ph id=\"ph6\"/> You may discover that some Spark 1.3.1 binaries are installed by default on your HDInsight cluster.",
          "pos": [
            0,
            147
          ]
        },
        {
          "content": "These should not be used, and will be removed from the HDInsight cluster image in a future update.",
          "pos": [
            148,
            246
          ]
        }
      ]
    },
    {
      "pos": [
        3040,
        3074
      ],
      "content": "Install Spark using Script Actions"
    },
    {
      "pos": [
        3076,
        3501
      ],
      "content": "A sample script to install Spark on an HDInsight cluster is available from a read-only Azure storage blob at <bpt id=\"p3\">[</bpt>https://hdiconfigactions.blob.core.windows.net/linuxsparkconfigactionv02/spark-installer-v02.sh<ept id=\"p3\">](https://hdiconfigactions.blob.core.windows.net/linuxsparkconfigactionv02/spark-installer-v02.sh)</ept>. This section provides instructions on how to use the sample script while creating the cluster by using the Azure portal.",
      "nodes": [
        {
          "content": "A sample script to install Spark on an HDInsight cluster is available from a read-only Azure storage blob at <bpt id=\"p3\">[</bpt>https://hdiconfigactions.blob.core.windows.net/linuxsparkconfigactionv02/spark-installer-v02.sh<ept id=\"p3\">](https://hdiconfigactions.blob.core.windows.net/linuxsparkconfigactionv02/spark-installer-v02.sh)</ept>.",
          "pos": [
            0,
            342
          ]
        },
        {
          "content": "This section provides instructions on how to use the sample script while creating the cluster by using the Azure portal.",
          "pos": [
            343,
            463
          ]
        }
      ]
    },
    {
      "pos": [
        3506,
        3763
      ],
      "content": "<ph id=\"ph7\">[AZURE.NOTE]</ph><ph id=\"ph8\"/> You can also use Azure PowerShell or the HDInsight .NET SDK to create a cluster using this script. For more information on using these methods, see <bpt id=\"p4\">[</bpt>Customize HDInsight clusters with Script Actions<ept id=\"p4\">](hdinsight-hadoop-customize-cluster-linux.md)</ept>.",
      "nodes": [
        {
          "content": "<ph id=\"ph7\">[AZURE.NOTE]</ph><ph id=\"ph8\"/> You can also use Azure PowerShell or the HDInsight .NET SDK to create a cluster using this script.",
          "pos": [
            0,
            143
          ]
        },
        {
          "content": "For more information on using these methods, see <bpt id=\"p4\">[</bpt>Customize HDInsight clusters with Script Actions<ept id=\"p4\">](hdinsight-hadoop-customize-cluster-linux.md)</ept>.",
          "pos": [
            144,
            327
          ]
        }
      ]
    },
    {
      "pos": [
        3768,
        3935
      ],
      "content": "Start creating a cluster by using the steps in <bpt id=\"p5\">[</bpt>Create Linux-based HDInsight clusters<ept id=\"p5\">](hdinsight-hadoop-create-linux-clusters-portal.md)</ept>, but do not complete creation."
    },
    {
      "pos": [
        3940,
        4042
      ],
      "content": "On the <bpt id=\"p6\">**</bpt>Optional Configuration<ept id=\"p6\">**</ept><ph id=\"ph9\"/> blade, select <bpt id=\"p7\">**</bpt>Script Actions<ept id=\"p7\">**</ept>, and provide the information below:"
    },
    {
      "pos": [
        4050,
        4104
      ],
      "content": "<bpt id=\"p8\">__</bpt>NAME<ept id=\"p8\">__</ept>: Enter a friendly name for the script action."
    },
    {
      "pos": [
        4111,
        4222
      ],
      "content": "<bpt id=\"p9\">__</bpt>SCRIPT URI<ept id=\"p9\">__</ept>: https://hdiconfigactions.blob.core.windows.net/linuxsparkconfigactionv02/spark-installer-v02.sh"
    },
    {
      "pos": [
        4229,
        4256
      ],
      "content": "<bpt id=\"p10\">__</bpt>HEAD<ept id=\"p10\">__</ept>: Check this option"
    },
    {
      "pos": [
        4263,
        4294
      ],
      "content": "<bpt id=\"p11\">__</bpt>WORKER<ept id=\"p11\">__</ept>: Uncheck this option"
    },
    {
      "pos": [
        4301,
        4335
      ],
      "content": "<bpt id=\"p12\">__</bpt>ZOOKEEPER<ept id=\"p12\">__</ept>: Uncheck this option"
    },
    {
      "pos": [
        4342,
        4380
      ],
      "content": "<bpt id=\"p13\">__</bpt>PARAMETERS<ept id=\"p13\">__</ept>: Leave this field blank"
    },
    {
      "pos": [
        4392,
        4515
      ],
      "content": "<ph id=\"ph10\">[AZURE.NOTE]</ph><ph id=\"ph11\"/> The example Spark script only installs components on the head nodes, so the other node types can be unchecked."
    },
    {
      "pos": [
        4520,
        4750
      ],
      "content": "At the bottom of the <bpt id=\"p14\">**</bpt>Script Actions<ept id=\"p14\">**</ept>, use the <bpt id=\"p15\">**</bpt>Select<ept id=\"p15\">**</ept><ph id=\"ph12\"/> button to save the configuration. Finally, use the <bpt id=\"p16\">**</bpt>Select<ept id=\"p16\">**</ept><ph id=\"ph13\"/> button at the bottom of the <bpt id=\"p17\">**</bpt>Optional Configuration<ept id=\"p17\">**</ept><ph id=\"ph14\"/> blade to save the optional configuration information.",
      "nodes": [
        {
          "content": "At the bottom of the <bpt id=\"p14\">**</bpt>Script Actions<ept id=\"p14\">**</ept>, use the <bpt id=\"p15\">**</bpt>Select<ept id=\"p15\">**</ept><ph id=\"ph12\"/> button to save the configuration.",
          "pos": [
            0,
            188
          ]
        },
        {
          "content": "Finally, use the <bpt id=\"p16\">**</bpt>Select<ept id=\"p16\">**</ept><ph id=\"ph13\"/> button at the bottom of the <bpt id=\"p17\">**</bpt>Optional Configuration<ept id=\"p17\">**</ept><ph id=\"ph14\"/> blade to save the optional configuration information.",
          "pos": [
            189,
            435
          ]
        }
      ]
    },
    {
      "pos": [
        4755,
        4890
      ],
      "content": "Continue provisining the cluster as described in <bpt id=\"p18\">[</bpt>Create Linux-based HDInsight clusters<ept id=\"p18\">](hdinsight-provision-linux-clusters.md#portal)</ept>."
    },
    {
      "pos": [
        4918,
        4950
      ],
      "content": "How do I use Spark in HDInsight?"
    },
    {
      "pos": [
        4952,
        5163
      ],
      "content": "Spark provides APIs in Scala, Python, and Java. You can also use the interactive Spark shell to run Spark queries. Once your cluster has finished creation, use the following to connect to your HDInsight cluster:",
      "nodes": [
        {
          "content": "Spark provides APIs in Scala, Python, and Java.",
          "pos": [
            0,
            47
          ]
        },
        {
          "content": "You can also use the interactive Spark shell to run Spark queries.",
          "pos": [
            48,
            114
          ]
        },
        {
          "content": "Once your cluster has finished creation, use the following to connect to your HDInsight cluster:",
          "pos": [
            115,
            211
          ]
        }
      ]
    },
    {
      "pos": [
        5222,
        5290
      ],
      "content": "For more information on using SSH with HDInsight, see the following:"
    },
    {
      "pos": [
        5294,
        5406
      ],
      "content": "<bpt id=\"p19\">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X<ept id=\"p19\">](hdinsight-hadoop-linux-use-ssh-unix.md)</ept>"
    },
    {
      "pos": [
        5410,
        5512
      ],
      "content": "<bpt id=\"p20\">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Windows<ept id=\"p20\">](hdinsight-hadoop-linux-use-ssh-windows.md)</ept>"
    },
    {
      "pos": [
        5514,
        5591
      ],
      "content": "Once connected, use the following sections for specific steps on using Spark:"
    },
    {
      "pos": [
        5595,
        5658
      ],
      "content": "<bpt id=\"p21\">[</bpt>Using the Spark shell to run interactive queries<ept id=\"p21\">](#sparkshell)</ept>"
    },
    {
      "pos": [
        5661,
        5720
      ],
      "content": "<bpt id=\"p22\">[</bpt>Using the Spark shell to run Spark SQL queries<ept id=\"p22\">](#sparksql)</ept>"
    },
    {
      "pos": [
        5724,
        5771
      ],
      "content": "<bpt id=\"p23\">[</bpt>Using a standalone Scala program<ept id=\"p23\">](#standalone)</ept>"
    },
    {
      "pos": [
        5773,
        5776
      ],
      "content": "###"
    },
    {
      "pos": [
        5801,
        5849
      ],
      "content": "Using the Spark shell to run interactive queries"
    },
    {
      "pos": [
        5854,
        5905
      ],
      "content": "Run the following command to start the Spark shell:"
    },
    {
      "pos": [
        5974,
        6040
      ],
      "content": "After the command finishes running, you should get a Scala prompt:"
    },
    {
      "pos": [
        6062,
        6301
      ],
      "content": "On the Scala prompt, enter the Spark query shown below. This query counts the occurrence of each word in the davinci.txt file that is available at the /example/data/gutenberg/ location on the Azure Blob storage associated with the cluster.",
      "nodes": [
        {
          "content": "On the Scala prompt, enter the Spark query shown below.",
          "pos": [
            0,
            55
          ]
        },
        {
          "content": "This query counts the occurrence of each word in the davinci.txt file that is available at the /example/data/gutenberg/ location on the Azure Blob storage associated with the cluster.",
          "pos": [
            56,
            239
          ]
        }
      ]
    },
    {
      "pos": [
        6520,
        6561
      ],
      "content": "The output should resemble the following:"
    },
    {
      "pos": [
        6646,
        6680
      ],
      "content": "Enter :q to exit the Scala prompt."
    },
    {
      "pos": [
        6694,
        6697
      ],
      "content": "###"
    },
    {
      "pos": [
        6720,
        6766
      ],
      "content": "Using the Spark shell to run Spark SQL queries"
    },
    {
      "pos": [
        6768,
        7091
      ],
      "content": "Spark SQL allows you to use Spark to run relational queries expressed in Structured Query Language (SQL), HiveQL, or Scala. In this section, we look at using Spark to run a Hive query on a sample Hive table. The Hive table used in this section (called <bpt id=\"p24\">**</bpt>hivesampletable<ept id=\"p24\">**</ept>) is available by default when you create a cluster.",
      "nodes": [
        {
          "content": "Spark SQL allows you to use Spark to run relational queries expressed in Structured Query Language (SQL), HiveQL, or Scala.",
          "pos": [
            0,
            123
          ]
        },
        {
          "content": "In this section, we look at using Spark to run a Hive query on a sample Hive table.",
          "pos": [
            124,
            207
          ]
        },
        {
          "content": "The Hive table used in this section (called <bpt id=\"p24\">**</bpt>hivesampletable<ept id=\"p24\">**</ept>) is available by default when you create a cluster.",
          "pos": [
            208,
            363
          ]
        }
      ]
    },
    {
      "pos": [
        7096,
        7147
      ],
      "content": "Run the following command to start the Spark shell:"
    },
    {
      "pos": [
        7216,
        7282
      ],
      "content": "After the command finishes running, you should get a Scala prompt:"
    },
    {
      "pos": [
        7304,
        7405
      ],
      "content": "On the Scala prompt, set the Hive context. This is required to work with Hive queries by using Spark.",
      "nodes": [
        {
          "content": "On the Scala prompt, set the Hive context.",
          "pos": [
            0,
            42
          ]
        },
        {
          "content": "This is required to work with Hive queries by using Spark.",
          "pos": [
            43,
            101
          ]
        }
      ]
    },
    {
      "pos": [
        7486,
        7595
      ],
      "content": "<ph id=\"ph15\">[AZURE.NOTE]</ph>  <ph id=\"ph16\">`sc`</ph><ph id=\"ph17\"/> in this statement is the default Spark context that is set when you start the Spark shell."
    },
    {
      "pos": [
        7600,
        7784
      ],
      "content": "Run a Hive query by using the Hive context and print the output to the console. The query retrieves data on devices of a specific make and limits the number of records retrieved to 20.",
      "nodes": [
        {
          "content": "Run a Hive query by using the Hive context and print the output to the console.",
          "pos": [
            0,
            79
          ]
        },
        {
          "content": "The query retrieves data on devices of a specific make and limits the number of records retrieved to 20.",
          "pos": [
            80,
            184
          ]
        }
      ]
    },
    {
      "pos": [
        7916,
        7960
      ],
      "content": "You should see an output like the following:"
    },
    {
      "pos": [
        8239,
        8273
      ],
      "content": "Enter :q to exit the Scala prompt."
    },
    {
      "pos": [
        8316,
        8348
      ],
      "content": "Using a standalone Scala program"
    },
    {
      "pos": [
        8350,
        8530
      ],
      "content": "In this section, you will create a Scala application that counts the number of lines containing the letters 'a' and 'b' in a sample data file (/example/data/gutenberg/davinci.txt.)"
    },
    {
      "pos": [
        8536,
        8595
      ],
      "content": "Use the following commands to install the Scala Build Tool:"
    },
    {
      "pos": [
        8772,
        8812
      ],
      "content": "When prompted, select <bpt id=\"p25\">__</bpt>Y<ept id=\"p25\">__</ept><ph id=\"ph18\"/> to continue."
    },
    {
      "pos": [
        8817,
        8870
      ],
      "content": "Create the directory structure for the Scala project:"
    },
    {
      "pos": [
        8931,
        9033
      ],
      "content": "Create a new file named <bpt id=\"p26\">__</bpt>simple.sbt<ept id=\"p26\">__</ept>, which contains the configuration information for this project."
    },
    {
      "pos": [
        9098,
        9144
      ],
      "content": "Use the following as the contents of the file:"
    },
    {
      "pos": [
        9331,
        9400
      ],
      "content": "<ph id=\"ph19\">[AZURE.NOTE]</ph><ph id=\"ph20\"/> Make sure you retain the empty lines between each entry."
    },
    {
      "pos": [
        9410,
        9468
      ],
      "content": "Use <bpt id=\"p27\">__</bpt>Ctrl+X<ept id=\"p27\">__</ept>, then <bpt id=\"p28\">__</bpt>Y<ept id=\"p28\">__</ept><ph id=\"ph21\"/> and <bpt id=\"p29\">__</bpt>Enter<ept id=\"p29\">__</ept><ph id=\"ph22\"/> to save the file."
    },
    {
      "pos": [
        9473,
        9597
      ],
      "content": "Use the following command to create a new file named <bpt id=\"p30\">__</bpt>SimpleApp.scala<ept id=\"p30\">__</ept><ph id=\"ph23\"/> in the <bpt id=\"p31\">__</bpt>SimpleScalaApp/src/main/scala<ept id=\"p31\">__</ept><ph id=\"ph24\"/> directory:"
    },
    {
      "pos": [
        9648,
        9694
      ],
      "content": "Use the following as the contents of the file:"
    },
    {
      "pos": [
        10496,
        10555
      ],
      "content": "Use <bpt id=\"p32\">__</bpt>Ctrl+X<ept id=\"p32\">__</ept>, then <bpt id=\"p33\">__</bpt>Y<ept id=\"p33\">__</ept>, and <bpt id=\"p34\">__</bpt>Enter<ept id=\"p34\">__</ept><ph id=\"ph25\"/> to save the file."
    },
    {
      "pos": [
        10560,
        10678
      ],
      "content": "From the <bpt id=\"p35\">__</bpt>SimpleScalaApp<ept id=\"p35\">__</ept><ph id=\"ph26\"/> directory, use the following command to build the application, and store it in a jar file:"
    },
    {
      "pos": [
        10705,
        10848
      ],
      "content": "Once the application is compiled, you will see a <bpt id=\"p36\">**</bpt>simpleapp_2.10-1.0.jar<ept id=\"p36\">**</ept><ph id=\"ph27\"/> file created in the __SimpleScalaApp/target/scala-2.10** directory."
    },
    {
      "pos": [
        10853,
        10914
      ],
      "content": "Use the following command to run the SimpleApp.scala program:"
    },
    {
      "pos": [
        11044,
        11118
      ],
      "content": "When the program finishes running, the output is displayed on the console."
    },
    {
      "pos": [
        11173,
        11183
      ],
      "content": "Next steps"
    },
    {
      "pos": [
        11187,
        11406
      ],
      "content": "<bpt id=\"p37\">[</bpt>Install and use Hue on HDInsight clusters<ept id=\"p37\">](hdinsight-hadoop-hue-linux.md)</ept>. Hue is a web UI that makes it easy to create, run and save Pig and Hive jobs, as well as browse the default storage for your HDInsight cluster.",
      "nodes": [
        {
          "content": "<bpt id=\"p37\">[</bpt>Install and use Hue on HDInsight clusters<ept id=\"p37\">](hdinsight-hadoop-hue-linux.md)</ept>.",
          "pos": [
            0,
            115
          ]
        },
        {
          "content": "Hue is a web UI that makes it easy to create, run and save Pig and Hive jobs, as well as browse the default storage for your HDInsight cluster.",
          "pos": [
            116,
            259
          ]
        }
      ]
    },
    {
      "pos": [
        11410,
        11853
      ],
      "content": "<bpt id=\"p38\">[</bpt>Install R on HDInsight clusters<ept id=\"p38\">][hdinsight-install-r]</ept><ph id=\"ph28\"/> provides instructions on how to use cluster customization to install and use R on HDInsight Hadoop clusters. R is an open-source language and environment for statistical computing. It provides hundreds of built-in statistical functions and its own programming language that combines aspects of functional and object-oriented programming. It also provides extensive graphical capabilities.",
      "nodes": [
        {
          "content": "<bpt id=\"p38\">[</bpt>Install R on HDInsight clusters<ept id=\"p38\">][hdinsight-install-r]</ept><ph id=\"ph28\"/> provides instructions on how to use cluster customization to install and use R on HDInsight Hadoop clusters.",
          "pos": [
            0,
            218
          ]
        },
        {
          "content": "R is an open-source language and environment for statistical computing.",
          "pos": [
            219,
            290
          ]
        },
        {
          "content": "It provides hundreds of built-in statistical functions and its own programming language that combines aspects of functional and object-oriented programming.",
          "pos": [
            291,
            447
          ]
        },
        {
          "content": "It also provides extensive graphical capabilities.",
          "pos": [
            448,
            498
          ]
        }
      ]
    },
    {
      "pos": [
        11857,
        12113
      ],
      "content": "<bpt id=\"p39\">[</bpt>Install Giraph on HDInsight clusters<ept id=\"p39\">](hdinsight-hadoop-giraph-install-linux.md)</ept>. Use cluster customization to install Giraph on HDInsight Hadoop clusters. Giraph allows you to perform graph processing by using Hadoop, and can be used with Azure HDInsight.",
      "nodes": [
        {
          "content": "<bpt id=\"p39\">[</bpt>Install Giraph on HDInsight clusters<ept id=\"p39\">](hdinsight-hadoop-giraph-install-linux.md)</ept>.",
          "pos": [
            0,
            121
          ]
        },
        {
          "content": "Use cluster customization to install Giraph on HDInsight Hadoop clusters.",
          "pos": [
            122,
            195
          ]
        },
        {
          "content": "Giraph allows you to perform graph processing by using Hadoop, and can be used with Azure HDInsight.",
          "pos": [
            196,
            296
          ]
        }
      ]
    },
    {
      "pos": [
        12117,
        12336
      ],
      "content": "<bpt id=\"p40\">[</bpt>Install Solr on HDInsight clusters<ept id=\"p40\">](hdinsight-hadoop-solr-install-linux.md)</ept>. Use cluster customization to install Solr on HDInsight Hadoop clusters. Solr allows you to perform powerful search operations on data stored.",
      "nodes": [
        {
          "content": "<bpt id=\"p40\">[</bpt>Install Solr on HDInsight clusters<ept id=\"p40\">](hdinsight-hadoop-solr-install-linux.md)</ept>.",
          "pos": [
            0,
            117
          ]
        },
        {
          "content": "Use cluster customization to install Solr on HDInsight Hadoop clusters.",
          "pos": [
            118,
            189
          ]
        },
        {
          "content": "Solr allows you to perform powerful search operations on data stored.",
          "pos": [
            190,
            259
          ]
        }
      ]
    },
    {
      "pos": [
        12340,
        12551
      ],
      "content": "<bpt id=\"p41\">[</bpt>Install Hue on HDInsight clusters<ept id=\"p41\">](hdinsight-hadoop-hue-linux.md)</ept>. Use cluster customization to install Hue on HDInsight Hadoop clusters. Hue is a set of Web applications used to interact with a Hadoop cluster.",
      "nodes": [
        {
          "content": "<bpt id=\"p41\">[</bpt>Install Hue on HDInsight clusters<ept id=\"p41\">](hdinsight-hadoop-hue-linux.md)</ept>.",
          "pos": [
            0,
            107
          ]
        },
        {
          "content": "Use cluster customization to install Hue on HDInsight Hadoop clusters.",
          "pos": [
            108,
            178
          ]
        },
        {
          "content": "Hue is a set of Web applications used to interact with a Hadoop cluster.",
          "pos": [
            179,
            251
          ]
        }
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Use Script Action to install Apache Spark on Linux-based HDInsight (Hadoop) | Microsoft Azure\" \n    description=\"Learn how to install Spark on a Linux-based HDInsight cluster using Script Actions. Script Actions allow you to customize the cluster during creation, by changing cluster configuration or installing services and utilities.\" \n    services=\"hdinsight\" \n    documentationCenter=\"\" \n    authors=\"Blackmist\" \n    manager=\"paulettm\" \n    editor=\"cgronlun\"/>\n\n<tags \n    ms.service=\"hdinsight\" \n    ms.workload=\"big-data\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"02/05/2016\" \n    ms.author=\"larryfr\"/>\n\n# Install and use Spark on HDInsight Hadoop clusters\n\nIn this document, you will learn how to install Spark by using Script Action. Script Action lets you run scripts to customize a cluster, only when the cluster is being created. For more information, see [Customize HDInsight cluster using Script Action][hdinsight-cluster-customize]. Once you have installed Spark, you'll also learn how to run a Spark query on HDInsight clusters.\n\n> [AZURE.NOTE] HDInsight also provides Spark as a cluster type, which means you can now directly provision a Spark cluster without modifying a Hadoop cluster. However, this is limited to Windows-based clusters currently. Using the Spark cluster type, you get a Windows-based HDInsight version 3.2 cluster with Spark version 1.3.1. For more information, see [Get Started with Apache Spark on HDInsight](hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql.md).\n\n## <a name=\"whatis\"></a>What is Spark?\n\n<a href=\"http://spark.apache.org/docs/latest/index.html\" target=\"_blank\">Apache Spark</a> is an open-source parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications. Spark's in-memory computation capabilities make it a good choice for iterative algorithms in machine learning and graph computations.\n\nSpark can also be used to perform conventional disk-based data processing. Spark improves the traditional MapReduce framework by avoiding writes to disk in the intermediate stages. Also, Spark is compatible with the Hadoop Distributed File System (HDFS) and Azure Blob storage so the existing data can easily be processed via Spark. \n\nThis topic provides instructions on how to customize an HDInsight cluster to install Spark.\n\n## <a name=\"whatis\"></a>Which version of Spark can I install?\n\nIn this topic, we use a Script Action custom script to install Spark on an HDInsight cluster. This script installs Spark 1.5.1.\n\nYou can modify this script or create your own script to install other versions of Spark.\n\n## What the script does\n\nThis script installs Spark version 1.5.1 into `/usr/hdp/current/spark`.\n\n> [AZURE.WARNING] You may discover that some Spark 1.3.1 binaries are installed by default on your HDInsight cluster. These should not be used, and will be removed from the HDInsight cluster image in a future update.\n\n## <a name=\"install\"></a>Install Spark using Script Actions\n\nA sample script to install Spark on an HDInsight cluster is available from a read-only Azure storage blob at [https://hdiconfigactions.blob.core.windows.net/linuxsparkconfigactionv02/spark-installer-v02.sh](https://hdiconfigactions.blob.core.windows.net/linuxsparkconfigactionv02/spark-installer-v02.sh). This section provides instructions on how to use the sample script while creating the cluster by using the Azure portal. \n\n> [AZURE.NOTE] You can also use Azure PowerShell or the HDInsight .NET SDK to create a cluster using this script. For more information on using these methods, see [Customize HDInsight clusters with Script Actions](hdinsight-hadoop-customize-cluster-linux.md).\n\n1. Start creating a cluster by using the steps in [Create Linux-based HDInsight clusters](hdinsight-hadoop-create-linux-clusters-portal.md), but do not complete creation.\n\n2. On the **Optional Configuration** blade, select **Script Actions**, and provide the information below:\n\n    * __NAME__: Enter a friendly name for the script action.\n    * __SCRIPT URI__: https://hdiconfigactions.blob.core.windows.net/linuxsparkconfigactionv02/spark-installer-v02.sh\n    * __HEAD__: Check this option\n    * __WORKER__: Uncheck this option\n    * __ZOOKEEPER__: Uncheck this option\n    * __PARAMETERS__: Leave this field blank\n    \n    > [AZURE.NOTE] The example Spark script only installs components on the head nodes, so the other node types can be unchecked.\n\n3. At the bottom of the **Script Actions**, use the **Select** button to save the configuration. Finally, use the **Select** button at the bottom of the **Optional Configuration** blade to save the optional configuration information.\n\n4. Continue provisining the cluster as described in [Create Linux-based HDInsight clusters](hdinsight-provision-linux-clusters.md#portal).\n\n## <a name=\"usespark\"></a>How do I use Spark in HDInsight?\n\nSpark provides APIs in Scala, Python, and Java. You can also use the interactive Spark shell to run Spark queries. Once your cluster has finished creation, use the following to connect to your HDInsight cluster:\n\n    ssh USERNAME@CLUSTERNAME-ssh.azurehdinsight.net\n    \nFor more information on using SSH with HDInsight, see the following:\n\n* [Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X](hdinsight-hadoop-linux-use-ssh-unix.md)\n\n* [Use SSH with Linux-based Hadoop on HDInsight from Windows](hdinsight-hadoop-linux-use-ssh-windows.md)\n\nOnce connected, use the following sections for specific steps on using Spark:\n\n- [Using the Spark shell to run interactive queries](#sparkshell)\n- [Using the Spark shell to run Spark SQL queries](#sparksql) \n- [Using a standalone Scala program](#standalone)\n\n###<a name=\"sparkshell\"></a>Using the Spark shell to run interactive queries\n\n1. Run the following command to start the Spark shell:\n\n         /usr/hdp/current/spark/bin/spark-shell --master yarn\n\n    After the command finishes running, you should get a Scala prompt:\n\n         scala>\n\n5. On the Scala prompt, enter the Spark query shown below. This query counts the occurrence of each word in the davinci.txt file that is available at the /example/data/gutenberg/ location on the Azure Blob storage associated with the cluster.\n\n        val file = sc.textFile(\"/example/data/gutenberg/davinci.txt\")\n        val counts = file.flatMap(line => line.split(\" \")).map(word => (word, 1)).reduceByKey(_ + _)\n        counts.toArray().foreach(println)\n\n6. The output should resemble the following:\n\n        (felt,,1)\n        (axle-tree,1)\n        (deals,9)\n        (virtuous,4)\n\n7. Enter :q to exit the Scala prompt.\n\n        :q\n\n###<a name=\"sparksql\"></a>Using the Spark shell to run Spark SQL queries\n\nSpark SQL allows you to use Spark to run relational queries expressed in Structured Query Language (SQL), HiveQL, or Scala. In this section, we look at using Spark to run a Hive query on a sample Hive table. The Hive table used in this section (called **hivesampletable**) is available by default when you create a cluster.\n\n1. Run the following command to start the Spark shell:\n\n         /usr/hdp/current/spark/bin/spark-shell --master yarn\n\n    After the command finishes running, you should get a Scala prompt:\n\n         scala>\n\n2. On the Scala prompt, set the Hive context. This is required to work with Hive queries by using Spark.\n\n        val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)\n\n    > [AZURE.NOTE]  `sc` in this statement is the default Spark context that is set when you start the Spark shell.\n\n5. Run a Hive query by using the Hive context and print the output to the console. The query retrieves data on devices of a specific make and limits the number of records retrieved to 20.\n\n        hiveContext.sql(\"\"\"SELECT * FROM hivesampletable WHERE devicemake LIKE \"HTC%\" LIMIT 20\"\"\").collect().foreach(println)\n\n6. You should see an output like the following:\n\n        [820,11:35:17,en-US,Android,HTC,Inspire 4G,Louisiana,UnitedStates, 2.7383836,0,1]\n        [1055,17:24:08,en-US,Android,HTC,Incredible,Ohio,United States,18.0894738,0,0]\n        [1067,03:42:29,en-US,Windows Phone,HTC,HD7,District Of Columbia,United States,null,0,0]\n\n7. Enter :q to exit the Scala prompt.\n\n        :q\n\n### <a name=\"standalone\"></a>Using a standalone Scala program\n\nIn this section, you will create a Scala application that counts the number of lines containing the letters 'a' and 'b' in a sample data file (/example/data/gutenberg/davinci.txt.) \n\n1. Use the following commands to install the Scala Build Tool:\n\n        echo \"deb http://dl.bintray.com/sbt/debian /\" | sudo tee -a /etc/apt/sources.list.d/sbt.list\n        sudo apt-get update\n        sudo apt-get install sbt\n        \n    When prompted, select __Y__ to continue.\n\n2. Create the directory structure for the Scala project:\n\n        mkdir -p SimpleScalaApp/src/main/scala\n        \n3. Create a new file named __simple.sbt__, which contains the configuration information for this project.\n\n        cd SimpleScalaApp\n        nano simple.sbt\n        \n    Use the following as the contents of the file:\n\n        name := \"SimpleApp\"\n    \n        version := \"1.0\"\n    \n        scalaVersion := \"2.10.4\"\n    \n        libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"1.2.0\"\n\n\n    > [AZURE.NOTE] Make sure you retain the empty lines between each entry.\n    \n    Use __Ctrl+X__, then __Y__ and __Enter__ to save the file.\n\n4. Use the following command to create a new file named __SimpleApp.scala__ in the __SimpleScalaApp/src/main/scala__ directory:\n\n        nano src/main/scala/SimpleApp.scala\n\n    Use the following as the contents of the file:\n\n        /* SimpleApp.scala */\n        import org.apache.spark.SparkContext\n        import org.apache.spark.SparkContext._\n        import org.apache.spark.SparkConf\n        \n        object SimpleApp {\n          def main(args: Array[String]) {\n            val logFile = \"/example/data/gutenberg/davinci.txt\"         //Location of the sample data file on Azure Blob storage\n            val conf = new SparkConf().setAppName(\"SimpleApplication\")\n            val sc = new SparkContext(conf)\n            val logData = sc.textFile(logFile, 2).cache()\n            val numAs = logData.filter(line => line.contains(\"a\")).count()\n            val numBs = logData.filter(line => line.contains(\"b\")).count()\n            println(\"Lines with a: %s, Lines with b: %s\".format(numAs, numBs))\n          }\n        }\n\n    Use __Ctrl+X__, then __Y__, and __Enter__ to save the file.\n\n5. From the __SimpleScalaApp__ directory, use the following command to build the application, and store it in a jar file:\n\n        sbt package\n\n    Once the application is compiled, you will see a **simpleapp_2.10-1.0.jar** file created in the __SimpleScalaApp/target/scala-2.10** directory.\n\n6. Use the following command to run the SimpleApp.scala program:\n\n\n        /usr/hdp/current/spark/bin/spark-submit --class \"SimpleApp\" --master yarn target/scala-2.10/simpleapp_2.10-1.0.jar\n\n4. When the program finishes running, the output is displayed on the console.\n\n        Lines with a: 21374, Lines with b: 11430\n\n## Next steps\n\n- [Install and use Hue on HDInsight clusters](hdinsight-hadoop-hue-linux.md). Hue is a web UI that makes it easy to create, run and save Pig and Hive jobs, as well as browse the default storage for your HDInsight cluster.\n\n- [Install R on HDInsight clusters][hdinsight-install-r] provides instructions on how to use cluster customization to install and use R on HDInsight Hadoop clusters. R is an open-source language and environment for statistical computing. It provides hundreds of built-in statistical functions and its own programming language that combines aspects of functional and object-oriented programming. It also provides extensive graphical capabilities.\n\n- [Install Giraph on HDInsight clusters](hdinsight-hadoop-giraph-install-linux.md). Use cluster customization to install Giraph on HDInsight Hadoop clusters. Giraph allows you to perform graph processing by using Hadoop, and can be used with Azure HDInsight.\n\n- [Install Solr on HDInsight clusters](hdinsight-hadoop-solr-install-linux.md). Use cluster customization to install Solr on HDInsight Hadoop clusters. Solr allows you to perform powerful search operations on data stored.\n\n- [Install Hue on HDInsight clusters](hdinsight-hadoop-hue-linux.md). Use cluster customization to install Hue on HDInsight Hadoop clusters. Hue is a set of Web applications used to interact with a Hadoop cluster.\n\n\n\n[hdinsight-install-r]: hdinsight-hadoop-r-scripts-linux.md\n[hdinsight-cluster-customize]: hdinsight-hadoop-customize-cluster-linux.md\n \n"
}