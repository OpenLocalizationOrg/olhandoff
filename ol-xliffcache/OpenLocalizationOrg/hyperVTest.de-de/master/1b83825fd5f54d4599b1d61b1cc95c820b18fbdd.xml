{
  "nodes": [
    {
      "pos": [
        28,
        123
      ],
      "content": "Use Azure Event Hubs with Apache Spark in HDInsight to process streaming data | Microsoft Azure"
    },
    {
      "pos": [
        143,
        281
      ],
      "content": "Step-by-step instructions on how to send a data stream to Azure Event Hub and then receive those events in Spark using a Zeppelin notebook"
    },
    {
      "pos": [
        622,
        716
      ],
      "content": "Spark Streaming: Process events from Azure Event Hubs with Apache Spark on HDInsight (Windows)"
    },
    {
      "pos": [
        720,
        1015
      ],
      "content": "<ph id=\"ph2\">[AZURE.NOTE]</ph><ph id=\"ph3\"/> HDInsight now provides Spark clusters on Linux. For information on how to run a streaming application on HDInsight Spark Linux clusters, see <bpt id=\"p1\">[</bpt>Spark Streaming: Process events from Azure Event Hubs with Apache Spark on HDInsight (Linux)<ept id=\"p1\">](hdinsight-apache-spark-eventhub-streaming.md)</ept>.",
      "nodes": [
        {
          "content": "<ph id=\"ph2\">[AZURE.NOTE]</ph><ph id=\"ph3\"/> HDInsight now provides Spark clusters on Linux.",
          "pos": [
            0,
            92
          ]
        },
        {
          "content": "For information on how to run a streaming application on HDInsight Spark Linux clusters, see <bpt id=\"p1\">[</bpt>Spark Streaming: Process events from Azure Event Hubs with Apache Spark on HDInsight (Linux)<ept id=\"p1\">](hdinsight-apache-spark-eventhub-streaming.md)</ept>.",
          "pos": [
            93,
            365
          ]
        }
      ]
    },
    {
      "pos": [
        1017,
        1328
      ],
      "content": "Spark Streaming extends the core Spark API to build scalable, high-throughput, fault-tolerant stream processing applications. Data can be ingested from many sources. In this article we use Event Hubs to ingest data. Event Hubs is a highly scalable ingestion system that can intake millions of events per second.",
      "nodes": [
        {
          "content": "Spark Streaming extends the core Spark API to build scalable, high-throughput, fault-tolerant stream processing applications.",
          "pos": [
            0,
            125
          ]
        },
        {
          "content": "Data can be ingested from many sources.",
          "pos": [
            126,
            165
          ]
        },
        {
          "content": "In this article we use Event Hubs to ingest data.",
          "pos": [
            166,
            215
          ]
        },
        {
          "content": "Event Hubs is a highly scalable ingestion system that can intake millions of events per second.",
          "pos": [
            216,
            311
          ]
        }
      ]
    },
    {
      "pos": [
        1331,
        1574
      ],
      "content": "In this tutorial, you will learn how to create an Azure Event Hub, how to ingest messages into an Event Hub using a console application in C#, and to retrieve them in parallel using a Zeppelin notebook configured for Apache Spark in HDInsight."
    },
    {
      "pos": [
        1578,
        1892
      ],
      "content": "<ph id=\"ph4\">[AZURE.NOTE]</ph><ph id=\"ph5\"/> To follow the instructions in this article, you will have to use both versions of the Azure portal. To create an Event Hub you will use the <bpt id=\"p2\">[</bpt>Azure portal<ept id=\"p2\">](https://manage.windowsazure.com)</ept>. To work with the HDInsight Spark cluster, you will use the <bpt id=\"p3\">[</bpt>Azure Preview Portal<ept id=\"p3\">](https://ms.portal.azure.com/)</ept>.",
      "nodes": [
        {
          "content": "<ph id=\"ph4\">[AZURE.NOTE]</ph><ph id=\"ph5\"/> To follow the instructions in this article, you will have to use both versions of the Azure portal.",
          "pos": [
            0,
            144
          ]
        },
        {
          "content": "To create an Event Hub you will use the <bpt id=\"p2\">[</bpt>Azure portal<ept id=\"p2\">](https://manage.windowsazure.com)</ept>.",
          "pos": [
            145,
            271
          ]
        },
        {
          "content": "To work with the HDInsight Spark cluster, you will use the <bpt id=\"p3\">[</bpt>Azure Preview Portal<ept id=\"p3\">](https://ms.portal.azure.com/)</ept>.",
          "pos": [
            272,
            422
          ]
        }
      ]
    },
    {
      "pos": [
        1896,
        1914
      ],
      "content": "<bpt id=\"p4\">**</bpt>Prerequisites:<ept id=\"p4\">**</ept>"
    },
    {
      "pos": [
        1916,
        1944
      ],
      "content": "You must have the following:"
    },
    {
      "pos": [
        1948,
        2102
      ],
      "content": "An Azure subscription. See <bpt id=\"p5\">[</bpt>Get Azure free trial<ept id=\"p5\">](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/)</ept>.",
      "nodes": [
        {
          "content": "An Azure subscription.",
          "pos": [
            0,
            22
          ]
        },
        {
          "content": "See <bpt id=\"p5\">[</bpt>Get Azure free trial<ept id=\"p5\">](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/)</ept>.",
          "pos": [
            23,
            192
          ]
        }
      ]
    },
    {
      "pos": [
        2105,
        2248
      ],
      "content": "An Apache Spark cluster. For instructions, see <bpt id=\"p6\">[</bpt>Create Apache Spark clusters in Azure HDInsight<ept id=\"p6\">](hdinsight-apache-spark-provision-clusters.md)</ept>.",
      "nodes": [
        {
          "content": "An Apache Spark cluster.",
          "pos": [
            0,
            24
          ]
        },
        {
          "content": "For instructions, see <bpt id=\"p6\">[</bpt>Create Apache Spark clusters in Azure HDInsight<ept id=\"p6\">](hdinsight-apache-spark-provision-clusters.md)</ept>.",
          "pos": [
            25,
            181
          ]
        }
      ]
    },
    {
      "pos": [
        2251,
        2325
      ],
      "content": "An <bpt id=\"p7\">[</bpt>Azure Event Hub<ept id=\"p7\">](../event-hubs/event-hubs-csharp-ephcs-getstarted.md)</ept>."
    },
    {
      "pos": [
        2328,
        2473
      ],
      "content": "A workstation with Microsoft Visual Studio 2013. For instructions, see <bpt id=\"p8\">[</bpt>Install Visual Studio<ept id=\"p8\">](https://msdn.microsoft.com/library/e2h7fzkw.aspx)</ept>.",
      "nodes": [
        {
          "content": "A workstation with Microsoft Visual Studio 2013.",
          "pos": [
            0,
            48
          ]
        },
        {
          "content": "For instructions, see <bpt id=\"p8\">[</bpt>Install Visual Studio<ept id=\"p8\">](https://msdn.microsoft.com/library/e2h7fzkw.aspx)</ept>.",
          "pos": [
            49,
            183
          ]
        }
      ]
    },
    {
      "pos": [
        2475,
        2477
      ],
      "content": "##"
    },
    {
      "pos": [
        2506,
        2528
      ],
      "content": "Create Azure Event Hub"
    },
    {
      "pos": [
        2533,
        2660
      ],
      "content": "From the <bpt id=\"p9\">[</bpt>Azure portal<ept id=\"p9\">](https://manage.windowsazure.com)</ept>, select <bpt id=\"p10\">**</bpt>NEW<ept id=\"p10\">**</ept><ph id=\"ph6\"/> &gt; <bpt id=\"p11\">**</bpt>Service Bus<ept id=\"p11\">**</ept><ph id=\"ph7\"/> &gt; <bpt id=\"p12\">**</bpt>Event Hub<ept id=\"p12\">**</ept><ph id=\"ph8\"/> &gt; <bpt id=\"p13\">**</bpt>Custom Create<ept id=\"p13\">**</ept>."
    },
    {
      "pos": [
        2665,
        2863
      ],
      "content": "On the <bpt id=\"p14\">**</bpt>Add a new Event Hub<ept id=\"p14\">**</ept><ph id=\"ph9\"/> screen, enter an <bpt id=\"p15\">**</bpt>Event Hub Name<ept id=\"p15\">**</ept>, select the <bpt id=\"p16\">**</bpt>Region<ept id=\"p16\">**</ept><ph id=\"ph10\"/> to create the hub in, and create a new namespace or select an existing one. Click the <bpt id=\"p17\">**</bpt>Arrow<ept id=\"p17\">**</ept><ph id=\"ph11\"/> to continue.",
      "nodes": [
        {
          "content": "On the <bpt id=\"p14\">**</bpt>Add a new Event Hub<ept id=\"p14\">**</ept><ph id=\"ph9\"/> screen, enter an <bpt id=\"p15\">**</bpt>Event Hub Name<ept id=\"p15\">**</ept>, select the <bpt id=\"p16\">**</bpt>Region<ept id=\"p16\">**</ept><ph id=\"ph10\"/> to create the hub in, and create a new namespace or select an existing one.",
          "pos": [
            0,
            314
          ]
        },
        {
          "content": "Click the <bpt id=\"p17\">**</bpt>Arrow<ept id=\"p17\">**</ept><ph id=\"ph11\"/> to continue.",
          "pos": [
            315,
            402
          ]
        }
      ]
    },
    {
      "pos": [
        2869,
        3027
      ],
      "content": "<ph id=\"ph12\">![</ph>wizard page 1<ph id=\"ph13\">](./media/hdinsight-apache-spark-csharp-apache-zeppelin-eventhub-streaming/hdispark.streaming.create.event.hub.png \"Create an Azure Event Hub\")</ph>"
    },
    {
      "pos": [
        3035,
        3158
      ],
      "content": "<ph id=\"ph14\">[AZURE.NOTE]</ph><ph id=\"ph15\"/> You should select the same <bpt id=\"p18\">**</bpt>Location<ept id=\"p18\">**</ept><ph id=\"ph16\"/> as your Apache Spark cluster in HDInsight to reduce latency and costs."
    },
    {
      "pos": [
        3163,
        3437
      ],
      "content": "On the <bpt id=\"p19\">**</bpt>Configure Event Hub<ept id=\"p19\">**</ept><ph id=\"ph17\"/> screen, enter the <bpt id=\"p20\">**</bpt>Partition count<ept id=\"p20\">**</ept><ph id=\"ph18\"/> and <bpt id=\"p21\">**</bpt>Message Retention<ept id=\"p21\">**</ept><ph id=\"ph19\"/> values, and then click the check mark. For this example, use a partition count of 10 and a message retention of 1. Note the partition count because you will need this value later.",
      "nodes": [
        {
          "content": "On the <bpt id=\"p19\">**</bpt>Configure Event Hub<ept id=\"p19\">**</ept><ph id=\"ph17\"/> screen, enter the <bpt id=\"p20\">**</bpt>Partition count<ept id=\"p20\">**</ept><ph id=\"ph18\"/> and <bpt id=\"p21\">**</bpt>Message Retention<ept id=\"p21\">**</ept><ph id=\"ph19\"/> values, and then click the check mark.",
          "pos": [
            0,
            298
          ]
        },
        {
          "content": "For this example, use a partition count of 10 and a message retention of 1.",
          "pos": [
            299,
            374
          ]
        },
        {
          "content": "Note the partition count because you will need this value later.",
          "pos": [
            375,
            439
          ]
        }
      ]
    },
    {
      "pos": [
        3443,
        3632
      ],
      "content": "<ph id=\"ph20\">![</ph>wizard page 2<ph id=\"ph21\">](./media/hdinsight-apache-spark-csharp-apache-zeppelin-eventhub-streaming/hdispark.streaming.create.event.hub2.png \"Specify partition size and retention days for Event Hub\")</ph>"
    },
    {
      "pos": [
        3637,
        3750
      ],
      "content": "Click the Event Hub that you created, click <bpt id=\"p22\">**</bpt>Configure<ept id=\"p22\">**</ept>, and then create two access policies for the event hub."
    },
    {
      "pos": [
        3756,
        3924
      ],
      "leadings": [
        "",
        "   ",
        "   ",
        "   ",
        "   "
      ],
      "content": "<ph id=\"ph22\">&lt;table&gt;</ph><ph id=\"ph23\">\n &lt;tr&gt;</ph><ph id=\"ph24\">&lt;th&gt;</ph>Name<ph id=\"ph25\">&lt;/th&gt;</ph><ph id=\"ph26\">&lt;th&gt;</ph>Permissions<ph id=\"ph27\">&lt;/th&gt;</ph><ph id=\"ph28\">&lt;/tr&gt;</ph><ph id=\"ph29\">\n &lt;tr&gt;</ph><ph id=\"ph30\">&lt;td&gt;</ph>mysendpolicy<ph id=\"ph31\">&lt;/td&gt;</ph><ph id=\"ph32\">&lt;td&gt;</ph>Send<ph id=\"ph33\">&lt;/td&gt;</ph><ph id=\"ph34\">&lt;/tr&gt;</ph><ph id=\"ph35\">\n &lt;tr&gt;</ph><ph id=\"ph36\">&lt;td&gt;</ph>myreceivepolicy<ph id=\"ph37\">&lt;/td&gt;</ph><ph id=\"ph38\">&lt;td&gt;</ph>Listen<ph id=\"ph39\">&lt;/td&gt;</ph><ph id=\"ph40\">&lt;/tr&gt;</ph><ph id=\"ph41\">\n &lt;/table&gt;</ph>"
    },
    {
      "pos": [
        3930,
        4152
      ],
      "content": "After You create the permissions, select the <bpt id=\"p23\">**</bpt>Save<ept id=\"p23\">**</ept><ph id=\"ph42\"/> icon at the bottom of the page. This creates the shared access policies that will be used to send (<bpt id=\"p24\">**</bpt>mysendpolicy<ept id=\"p24\">**</ept>) and listen (<bpt id=\"p25\">**</bpt>myreceivepolicy<ept id=\"p25\">**</ept>) to this Event Hub.",
      "nodes": [
        {
          "content": "After You create the permissions, select the <bpt id=\"p23\">**</bpt>Save<ept id=\"p23\">**</ept><ph id=\"ph42\"/> icon at the bottom of the page.",
          "pos": [
            0,
            140
          ]
        },
        {
          "content": "This creates the shared access policies that will be used to send (<bpt id=\"p24\">**</bpt>mysendpolicy<ept id=\"p24\">**</ept>) and listen (<bpt id=\"p25\">**</bpt>myreceivepolicy<ept id=\"p25\">**</ept>) to this Event Hub.",
          "pos": [
            141,
            357
          ]
        }
      ]
    },
    {
      "pos": [
        4158,
        4313
      ],
      "content": "<ph id=\"ph43\">![</ph>policies<ph id=\"ph44\">](./media/hdinsight-apache-spark-csharp-apache-zeppelin-eventhub-streaming/hdispark.streaming.event.hub.policies.png \"Create Event Hub policies\")</ph>"
    },
    {
      "pos": [
        4323,
        4452
      ],
      "content": "On the same page, take a note of the policy keys generated for the two policies. Save these keys because they will be used later.",
      "nodes": [
        {
          "content": "On the same page, take a note of the policy keys generated for the two policies.",
          "pos": [
            0,
            80
          ]
        },
        {
          "content": "Save these keys because they will be used later.",
          "pos": [
            81,
            129
          ]
        }
      ]
    },
    {
      "pos": [
        4458,
        4610
      ],
      "content": "<ph id=\"ph45\">![</ph>policy keys<ph id=\"ph46\">](./media/hdinsight-apache-spark-csharp-apache-zeppelin-eventhub-streaming/hdispark.streaming.event.hub.policy.keys.png \"Save policy keys\")</ph>"
    },
    {
      "pos": [
        4615,
        4776
      ],
      "content": "On the <bpt id=\"p26\">**</bpt>Dashboard<ept id=\"p26\">**</ept><ph id=\"ph47\"/> page, click <bpt id=\"p27\">**</bpt>Connection Information<ept id=\"p27\">**</ept><ph id=\"ph48\"/> from the bottom to retrieve and save the connection strings for the Event Hub using the two policies."
    },
    {
      "pos": [
        4782,
        4962
      ],
      "content": "<ph id=\"ph49\">![</ph>policy keys<ph id=\"ph50\">](./media/hdinsight-apache-spark-csharp-apache-zeppelin-eventhub-streaming/hdispark.streaming.event.hub.policy.connection.strings.png \"Save policy connection strings\")</ph>"
    },
    {
      "pos": [
        5096,
        5098
      ],
      "content": "##"
    },
    {
      "pos": [
        5128,
        5181
      ],
      "content": "Receive messages in Spark on HDInsight using Zeppelin"
    },
    {
      "pos": [
        5183,
        5347
      ],
      "content": "In this section, you create a <bpt id=\"p28\">[</bpt>Zeppelin<ept id=\"p28\">](https://zeppelin.incubator.apache.org)</ept><ph id=\"ph52\"/> notebook to receive messages from the Event Hub into the Spark cluster in HDInsight."
    },
    {
      "pos": [
        5353,
        5411
      ],
      "content": "Allocating resources to Zeppelin for streaming application"
    },
    {
      "pos": [
        5413,
        5510
      ],
      "content": "You must make the following considerations while creating a streaming application using Zeppelin:"
    },
    {
      "pos": [
        5514,
        5912
      ],
      "content": "<bpt id=\"p29\">**</bpt>Event hub partitions and cores allocated to Zeppelin<ept id=\"p29\">**</ept>. In the previous steps, you created an Event Hub with some partitions. In the Zeppelin streaming application you create below, you will specify the same number of partitions again. To successfully stream the data from Event Hub using Zeppelin, the number of cores you allocate to Zeppelin must be twice the number of partitions in Event Hub.",
      "nodes": [
        {
          "content": "<bpt id=\"p29\">**</bpt>Event hub partitions and cores allocated to Zeppelin<ept id=\"p29\">**</ept>.",
          "pos": [
            0,
            97
          ]
        },
        {
          "content": "In the previous steps, you created an Event Hub with some partitions.",
          "pos": [
            98,
            167
          ]
        },
        {
          "content": "In the Zeppelin streaming application you create below, you will specify the same number of partitions again.",
          "pos": [
            168,
            277
          ]
        },
        {
          "content": "To successfully stream the data from Event Hub using Zeppelin, the number of cores you allocate to Zeppelin must be twice the number of partitions in Event Hub.",
          "pos": [
            278,
            438
          ]
        }
      ]
    },
    {
      "pos": [
        5915,
        6320
      ],
      "content": "<bpt id=\"p30\">**</bpt>Minimum number of cores to be allocated to Zeppelin<ept id=\"p30\">**</ept>. In your streaming application that you create below, you create a temporary table where you store the messages that are streamed by your application. You then use a Spark SQL statement to read messages from this temporary table. To successfully run the Spark SQL statement, you must make sure that you at least have two cores allocated to Zeppelin.",
      "nodes": [
        {
          "content": "<bpt id=\"p30\">**</bpt>Minimum number of cores to be allocated to Zeppelin<ept id=\"p30\">**</ept>.",
          "pos": [
            0,
            96
          ]
        },
        {
          "content": "In your streaming application that you create below, you create a temporary table where you store the messages that are streamed by your application.",
          "pos": [
            97,
            246
          ]
        },
        {
          "content": "You then use a Spark SQL statement to read messages from this temporary table.",
          "pos": [
            247,
            325
          ]
        },
        {
          "content": "To successfully run the Spark SQL statement, you must make sure that you at least have two cores allocated to Zeppelin.",
          "pos": [
            326,
            445
          ]
        }
      ]
    },
    {
      "pos": [
        6322,
        6386
      ],
      "content": "If you combine the two requirements above, this is what you get:"
    },
    {
      "pos": [
        6390,
        6453
      ],
      "content": "The minimum number of cores you must allocate to Zeppelin is 2."
    },
    {
      "pos": [
        6456,
        6545
      ],
      "content": "The number of allocated cores must always be twice the number of partitions in Event Hub."
    },
    {
      "pos": [
        6548,
        6726
      ],
      "content": "For instructions on how to allocate resources in a Spark cluster, see <bpt id=\"p31\">[</bpt>Manage resources for the Apache Spark cluster in HDInsight<ept id=\"p31\">](hdinsight-apache-spark-resource-manager-v1.md)</ept>."
    },
    {
      "pos": [
        6732,
        6777
      ],
      "content": "Create a streaming application using Zeppelin"
    },
    {
      "pos": [
        6782,
        7023
      ],
      "content": "From the <bpt id=\"p32\">[</bpt>Azure Preview Portal<ept id=\"p32\">](https://portal.azure.com/)</ept>, from the startboard, click the tile for your Spark cluster (if you pinned it to the startboard). You can also navigate to your cluster under <bpt id=\"p33\">**</bpt>Browse All<ept id=\"p33\">**</ept><ph id=\"ph53\"/> &gt; <bpt id=\"p34\">**</bpt>HDInsight Clusters<ept id=\"p34\">**</ept>.",
      "nodes": [
        {
          "content": "From the <bpt id=\"p32\">[</bpt>Azure Preview Portal<ept id=\"p32\">](https://portal.azure.com/)</ept>, from the startboard, click the tile for your Spark cluster (if you pinned it to the startboard).",
          "pos": [
            0,
            196
          ]
        },
        {
          "content": "You can also navigate to your cluster under <bpt id=\"p33\">**</bpt>Browse All<ept id=\"p33\">**</ept><ph id=\"ph53\"/> &gt; <bpt id=\"p34\">**</bpt>HDInsight Clusters<ept id=\"p34\">**</ept>.",
          "pos": [
            197,
            379
          ]
        }
      ]
    },
    {
      "pos": [
        7031,
        7217
      ],
      "content": "From the Spark cluster blade, click <bpt id=\"p35\">**</bpt>Quick Links<ept id=\"p35\">**</ept>, and then from the <bpt id=\"p36\">**</bpt>Cluster Dashboard<ept id=\"p36\">**</ept><ph id=\"ph54\"/> blade, click <bpt id=\"p37\">**</bpt>Zeppelin Notebook<ept id=\"p37\">**</ept>. If prompted, enter the admin credentials for the cluster.",
      "nodes": [
        {
          "content": "From the Spark cluster blade, click <bpt id=\"p35\">**</bpt>Quick Links<ept id=\"p35\">**</ept>, and then from the <bpt id=\"p36\">**</bpt>Cluster Dashboard<ept id=\"p36\">**</ept><ph id=\"ph54\"/> blade, click <bpt id=\"p37\">**</bpt>Zeppelin Notebook<ept id=\"p37\">**</ept>.",
          "pos": [
            0,
            263
          ]
        },
        {
          "content": "If prompted, enter the admin credentials for the cluster.",
          "pos": [
            264,
            321
          ]
        }
      ]
    },
    {
      "pos": [
        7225,
        7396
      ],
      "content": "<ph id=\"ph55\">[AZURE.NOTE]</ph><ph id=\"ph56\"/> You may also reach the Zeppelin Notebook for your cluster by opening the following URL in your browser. Replace <bpt id=\"p38\">__</bpt>CLUSTERNAME<ept id=\"p38\">__</ept><ph id=\"ph57\"/> with the name of your cluster:",
      "nodes": [
        {
          "content": "<ph id=\"ph55\">[AZURE.NOTE]</ph><ph id=\"ph56\"/> You may also reach the Zeppelin Notebook for your cluster by opening the following URL in your browser.",
          "pos": [
            0,
            150
          ]
        },
        {
          "content": "Replace <bpt id=\"p38\">__</bpt>CLUSTERNAME<ept id=\"p38\">__</ept><ph id=\"ph57\"/> with the name of your cluster:",
          "pos": [
            151,
            260
          ]
        }
      ]
    },
    {
      "pos": [
        7463,
        7578
      ],
      "content": "Create a new notebook. From the header pane, click <bpt id=\"p39\">**</bpt>Notebook<ept id=\"p39\">**</ept>, and from the drop-down, click <bpt id=\"p40\">**</bpt>Create New Note<ept id=\"p40\">**</ept>.",
      "nodes": [
        {
          "content": "Create a new notebook.",
          "pos": [
            0,
            22
          ]
        },
        {
          "content": "From the header pane, click <bpt id=\"p39\">**</bpt>Notebook<ept id=\"p39\">**</ept>, and from the drop-down, click <bpt id=\"p40\">**</bpt>Create New Note<ept id=\"p40\">**</ept>.",
          "pos": [
            23,
            195
          ]
        }
      ]
    },
    {
      "pos": [
        7584,
        7751
      ],
      "content": "<ph id=\"ph59\">![</ph>Create a new Zeppelin notebook<ph id=\"ph60\">](./media/hdinsight-apache-spark-csharp-apache-zeppelin-eventhub-streaming/hdispark.createnewnote.png \"Create a new Zeppelin notebook\")</ph>"
    },
    {
      "pos": [
        7757,
        7908
      ],
      "content": "On the same page, under the <bpt id=\"p41\">**</bpt>Notebook<ept id=\"p41\">**</ept><ph id=\"ph61\"/> heading, you should see a new notebook with the name starting with <bpt id=\"p42\">**</bpt>Note XXXXXXXXX<ept id=\"p42\">**</ept>. Click the new notebook.",
      "nodes": [
        {
          "content": "On the same page, under the <bpt id=\"p41\">**</bpt>Notebook<ept id=\"p41\">**</ept><ph id=\"ph61\"/> heading, you should see a new notebook with the name starting with <bpt id=\"p42\">**</bpt>Note XXXXXXXXX<ept id=\"p42\">**</ept>.",
          "pos": [
            0,
            222
          ]
        },
        {
          "content": "Click the new notebook.",
          "pos": [
            223,
            246
          ]
        }
      ]
    },
    {
      "pos": [
        7913,
        8148
      ],
      "content": "On the web page for the new notebook, click the heading, and change the name of the notebook if you want to. Press ENTER to save the name change. Also, make sure the notebook header shows a <bpt id=\"p43\">**</bpt>Connected<ept id=\"p43\">**</ept><ph id=\"ph62\"/> status in the top-right corner.",
      "nodes": [
        {
          "content": "On the web page for the new notebook, click the heading, and change the name of the notebook if you want to.",
          "pos": [
            0,
            108
          ]
        },
        {
          "content": "Press ENTER to save the name change.",
          "pos": [
            109,
            145
          ]
        },
        {
          "content": "Also, make sure the notebook header shows a <bpt id=\"p43\">**</bpt>Connected<ept id=\"p43\">**</ept><ph id=\"ph62\"/> status in the top-right corner.",
          "pos": [
            146,
            290
          ]
        }
      ]
    },
    {
      "pos": [
        8154,
        8313
      ],
      "content": "<ph id=\"ph63\">![</ph>Zeppelin notebook status<ph id=\"ph64\">](./media/hdinsight-apache-spark-csharp-apache-zeppelin-eventhub-streaming/hdispark.newnote.connected.png \"Zeppelin notebook status\")</ph>"
    },
    {
      "pos": [
        8318,
        8720
      ],
      "content": "In the empty paragraph that is created by default in the new notebook, paste the following snippet and replace the placeholders to use your event hub configuration. In this snippet, you receive the stream from Event Hub and register the stream into a temporary table, called <bpt id=\"p44\">**</bpt>mytemptable<ept id=\"p44\">**</ept>. In the next section, we will start the sender application. You can then read the data directly from the table.",
      "nodes": [
        {
          "content": "In the empty paragraph that is created by default in the new notebook, paste the following snippet and replace the placeholders to use your event hub configuration.",
          "pos": [
            0,
            164
          ]
        },
        {
          "content": "In this snippet, you receive the stream from Event Hub and register the stream into a temporary table, called <bpt id=\"p44\">**</bpt>mytemptable<ept id=\"p44\">**</ept>.",
          "pos": [
            165,
            331
          ]
        },
        {
          "content": "In the next section, we will start the sender application.",
          "pos": [
            332,
            390
          ]
        },
        {
          "content": "You can then read the data directly from the table.",
          "pos": [
            391,
            442
          ]
        }
      ]
    },
    {
      "pos": [
        8728,
        9135
      ],
      "content": "<ph id=\"ph65\">[AZURE.NOTE]</ph><ph id=\"ph66\"/> In the snippet below, <bpt id=\"p45\">**</bpt>eventhubs.checkpoint.dir<ept id=\"p45\">**</ept><ph id=\"ph67\"/> must be set to a directory in your default storage container. If the directory does not exist, the streamig application creates it. You can either specify the full path to the directory like \"<bpt id=\"p46\">**</bpt>wasb://container@storageaccount.blob.core.windows.net/mycheckpointdir/<ept id=\"p46\">**</ept>\" or just the relative path to the directory, such as \"<bpt id=\"p47\">**</bpt>/mycheckpointdir<ept id=\"p47\">**</ept>\".",
      "nodes": [
        {
          "content": "<ph id=\"ph65\">[AZURE.NOTE]</ph><ph id=\"ph66\"/> In the snippet below, <bpt id=\"p45\">**</bpt>eventhubs.checkpoint.dir<ept id=\"p45\">**</ept><ph id=\"ph67\"/> must be set to a directory in your default storage container.",
          "pos": [
            0,
            214
          ]
        },
        {
          "content": "If the directory does not exist, the streamig application creates it.",
          "pos": [
            215,
            284
          ]
        },
        {
          "content": "You can either specify the full path to the directory like \"<bpt id=\"p46\">**</bpt>wasb://container@storageaccount.blob.core.windows.net/mycheckpointdir/<ept id=\"p46\">**</ept>\" or just the relative path to the directory, such as \"<bpt id=\"p47\">**</bpt>/mycheckpointdir<ept id=\"p47\">**</ept>\".",
          "pos": [
            285,
            576
          ]
        }
      ]
    },
    {
      "pos": [
        10245,
        10247
      ],
      "content": "##"
    },
    {
      "pos": [
        10269,
        10289
      ],
      "content": "Run the applications"
    },
    {
      "pos": [
        10294,
        10429
      ],
      "content": "From the Zeppelin notebook, run the paragraph with the snippet. Press <bpt id=\"p48\">**</bpt>SHIFT + ENTER<ept id=\"p48\">**</ept><ph id=\"ph68\"/> or the <bpt id=\"p49\">**</bpt>Play<ept id=\"p49\">**</ept><ph id=\"ph69\"/> button at the top-right corner.",
      "nodes": [
        {
          "content": "From the Zeppelin notebook, run the paragraph with the snippet.",
          "pos": [
            0,
            63
          ]
        },
        {
          "content": "Press <bpt id=\"p48\">**</bpt>SHIFT + ENTER<ept id=\"p48\">**</ept><ph id=\"ph68\"/> or the <bpt id=\"p49\">**</bpt>Play<ept id=\"p49\">**</ept><ph id=\"ph69\"/> button at the top-right corner.",
          "pos": [
            64,
            245
          ]
        }
      ]
    },
    {
      "pos": [
        10435,
        10642
      ],
      "content": "The status on the right-corner of the paragraph should progress from READY, PENDING, RUNNING to FINISHED. The output will show up in the bottom of the same paragraph. The screenshot looks like the following:",
      "nodes": [
        {
          "content": "The status on the right-corner of the paragraph should progress from READY, PENDING, RUNNING to FINISHED.",
          "pos": [
            0,
            105
          ]
        },
        {
          "content": "The output will show up in the bottom of the same paragraph.",
          "pos": [
            106,
            166
          ]
        },
        {
          "content": "The screenshot looks like the following:",
          "pos": [
            167,
            207
          ]
        }
      ]
    },
    {
      "pos": [
        10648,
        10823
      ],
      "content": "<ph id=\"ph70\">![</ph>Output of the snippet<ph id=\"ph71\">](./media/hdinsight-apache-spark-csharp-apache-zeppelin-eventhub-streaming/hdispark.streaming.event.hub.zeppelin.code.output.png \"Output of the snipet\")</ph>"
    },
    {
      "pos": [
        10828,
        10940
      ],
      "content": "Run the <bpt id=\"p50\">**</bpt>Sender<ept id=\"p50\">**</ept><ph id=\"ph72\"/> project and press <bpt id=\"p51\">**</bpt>Enter<ept id=\"p51\">**</ept><ph id=\"ph73\"/> in the console window to start sending messages to the Event Hub."
    },
    {
      "pos": [
        10945,
        11060
      ],
      "content": "From the Zeppelin notebook, in a new paragraph, enter the following snippet to read the messages received in Spark."
    },
    {
      "pos": [
        11124,
        11204
      ],
      "content": "The following screen capture shows the messages received in the <bpt id=\"p52\">**</bpt>mytemptable<ept id=\"p52\">**</ept>."
    },
    {
      "pos": [
        11210,
        11408
      ],
      "content": "<ph id=\"ph74\">![</ph>Receive the messages in Zeppelin<ph id=\"ph75\">](./media/hdinsight-apache-spark-csharp-apache-zeppelin-eventhub-streaming/hdispark.streaming.event.hub.zeppelin.output.png \"Receive messages in Zeppelin notebook\")</ph>"
    },
    {
      "pos": [
        11413,
        11563
      ],
      "content": "Restart the Spark SQL interpreter to exit the application. Click the <bpt id=\"p53\">**</bpt>Interpreter<ept id=\"p53\">**</ept><ph id=\"ph76\"/> tab at the top, and for the Spark interpreter, click <bpt id=\"p54\">**</bpt>Restart<ept id=\"p54\">**</ept>.",
      "nodes": [
        {
          "content": "Restart the Spark SQL interpreter to exit the application.",
          "pos": [
            0,
            58
          ]
        },
        {
          "content": "Click the <bpt id=\"p53\">**</bpt>Interpreter<ept id=\"p53\">**</ept><ph id=\"ph76\"/> tab at the top, and for the Spark interpreter, click <bpt id=\"p54\">**</bpt>Restart<ept id=\"p54\">**</ept>.",
          "pos": [
            59,
            245
          ]
        }
      ]
    },
    {
      "pos": [
        11569,
        11753
      ],
      "content": "<ph id=\"ph77\">![</ph>Restart the Zeppelin intepreter<ph id=\"ph78\">](./media/hdinsight-apache-spark-csharp-apache-zeppelin-eventhub-streaming/hdispark.zeppelin.restart.interpreter.png \"Restart the Zeppelin intepreter\")</ph>"
    },
    {
      "pos": [
        11755,
        11757
      ],
      "content": "##"
    },
    {
      "pos": [
        11788,
        11840
      ],
      "content": "Run the streaming application with high availability"
    },
    {
      "pos": [
        11842,
        12098
      ],
      "content": "Using Zeppelin to receive streaming data into Spark cluster on HDInsight is a good approach to prototype your application. However, to run your streaming application in a production setup with high-availability and resilience, you need to do the following:",
      "nodes": [
        {
          "content": "Using Zeppelin to receive streaming data into Spark cluster on HDInsight is a good approach to prototype your application.",
          "pos": [
            0,
            122
          ]
        },
        {
          "content": "However, to run your streaming application in a production setup with high-availability and resilience, you need to do the following:",
          "pos": [
            123,
            256
          ]
        }
      ]
    },
    {
      "pos": [
        12103,
        12183
      ],
      "content": "Copy over the dependency jar to the storage account associated with the cluster."
    },
    {
      "pos": [
        12187,
        12217
      ],
      "content": "Build a streaming application."
    },
    {
      "pos": [
        12221,
        12307
      ],
      "content": "RDP into the cluster and copy over the application jar to the headnode of the cluster."
    },
    {
      "pos": [
        12311,
        12376
      ],
      "content": "RDP into the cluster and run the application on the cluster node."
    },
    {
      "pos": [
        12378,
        12600
      ],
      "content": "Instructions on how to perform these steps and a sample streaming application can be downloaded from GitHub at <bpt id=\"p55\">[</bpt>https://github.com/hdinsight/hdinsight-spark-examples<ept id=\"p55\">](https://github.com/hdinsight/hdinsight-spark-examples)</ept>."
    },
    {
      "pos": [
        12604,
        12606
      ],
      "content": "##"
    },
    {
      "pos": [
        12628,
        12636
      ],
      "content": "See also"
    },
    {
      "pos": [
        12641,
        12723
      ],
      "content": "<bpt id=\"p56\">[</bpt>Overview: Apache Spark on Azure HDInsight<ept id=\"p56\">](hdinsight-apache-spark-overview-v1.md)</ept>"
    },
    {
      "pos": [
        12726,
        12880
      ],
      "content": "<bpt id=\"p57\">[</bpt>Quick Start: create Apache Spark on HDInsight and run interactive queries using Spark SQL<ept id=\"p57\">](hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql.md)</ept>"
    },
    {
      "pos": [
        12883,
        13014
      ],
      "content": "<bpt id=\"p58\">[</bpt>Use Spark in HDInsight for building machine learning applications<ept id=\"p58\">](hdinsight-apache-spark-ipython-notebook-machine-learning-v1.md)</ept>"
    },
    {
      "pos": [
        13017,
        13134
      ],
      "content": "<bpt id=\"p59\">[</bpt>Perform interactive data analysis using Spark in HDInsight with BI tools<ept id=\"p59\">](hdinsight-apache-spark-use-bi-tools-v1.md)</ept>"
    },
    {
      "pos": [
        13137,
        13250
      ],
      "content": "<bpt id=\"p60\">[</bpt>Manage resources for the Apache Spark cluster in Azure HDInsight<ept id=\"p60\">](hdinsight-apache-spark-resource-manager-v1.md)</ept>"
    }
  ],
  "content": "<properties \n    pageTitle=\"Use Azure Event Hubs with Apache Spark in HDInsight to process streaming data | Microsoft Azure\" \n    description=\"Step-by-step instructions on how to send a data stream to Azure Event Hub and then receive those events in Spark using a Zeppelin notebook\" \n    services=\"hdinsight\" \n    documentationCenter=\"\" \n    authors=\"nitinme\" \n    manager=\"paulettm\" \n    editor=\"cgronlun\"\n    tags=\"azure-portal\"/>\n\n<tags \n    ms.service=\"hdinsight\" \n    ms.workload=\"big-data\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"12/08/2015\" \n    ms.author=\"nitinme\"/>\n\n\n# Spark Streaming: Process events from Azure Event Hubs with Apache Spark on HDInsight (Windows)\n\n> [AZURE.NOTE] HDInsight now provides Spark clusters on Linux. For information on how to run a streaming application on HDInsight Spark Linux clusters, see [Spark Streaming: Process events from Azure Event Hubs with Apache Spark on HDInsight (Linux)](hdinsight-apache-spark-eventhub-streaming.md).\n\nSpark Streaming extends the core Spark API to build scalable, high-throughput, fault-tolerant stream processing applications. Data can be ingested from many sources. In this article we use Event Hubs to ingest data. Event Hubs is a highly scalable ingestion system that can intake millions of events per second. \n\nIn this tutorial, you will learn how to create an Azure Event Hub, how to ingest messages into an Event Hub using a console application in C#, and to retrieve them in parallel using a Zeppelin notebook configured for Apache Spark in HDInsight.\n\n> [AZURE.NOTE] To follow the instructions in this article, you will have to use both versions of the Azure portal. To create an Event Hub you will use the [Azure portal](https://manage.windowsazure.com). To work with the HDInsight Spark cluster, you will use the [Azure Preview Portal](https://ms.portal.azure.com/).  \n\n**Prerequisites:**\n\nYou must have the following:\n\n- An Azure subscription. See [Get Azure free trial](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/).\n- An Apache Spark cluster. For instructions, see [Create Apache Spark clusters in Azure HDInsight](hdinsight-apache-spark-provision-clusters.md).\n- An [Azure Event Hub](../event-hubs/event-hubs-csharp-ephcs-getstarted.md).\n- A workstation with Microsoft Visual Studio 2013. For instructions, see [Install Visual Studio](https://msdn.microsoft.com/library/e2h7fzkw.aspx).\n\n##<a name=\"createeventhub\"></a>Create Azure Event Hub\n\n1. From the [Azure portal](https://manage.windowsazure.com), select **NEW** > **Service Bus** > **Event Hub** > **Custom Create**.\n\n2. On the **Add a new Event Hub** screen, enter an **Event Hub Name**, select the **Region** to create the hub in, and create a new namespace or select an existing one. Click the **Arrow** to continue.\n\n    ![wizard page 1](./media/hdinsight-apache-spark-csharp-apache-zeppelin-eventhub-streaming/hdispark.streaming.create.event.hub.png \"Create an Azure Event Hub\")\n\n    > [AZURE.NOTE] You should select the same **Location** as your Apache Spark cluster in HDInsight to reduce latency and costs.\n\n3. On the **Configure Event Hub** screen, enter the **Partition count** and **Message Retention** values, and then click the check mark. For this example, use a partition count of 10 and a message retention of 1. Note the partition count because you will need this value later.\n\n    ![wizard page 2](./media/hdinsight-apache-spark-csharp-apache-zeppelin-eventhub-streaming/hdispark.streaming.create.event.hub2.png \"Specify partition size and retention days for Event Hub\")\n\n4. Click the Event Hub that you created, click **Configure**, and then create two access policies for the event hub.\n\n    <table>\n    <tr><th>Name</th><th>Permissions</th></tr>\n    <tr><td>mysendpolicy</td><td>Send</td></tr>\n    <tr><td>myreceivepolicy</td><td>Listen</td></tr>\n    </table>\n\n    After You create the permissions, select the **Save** icon at the bottom of the page. This creates the shared access policies that will be used to send (**mysendpolicy**) and listen (**myreceivepolicy**) to this Event Hub.\n\n    ![policies](./media/hdinsight-apache-spark-csharp-apache-zeppelin-eventhub-streaming/hdispark.streaming.event.hub.policies.png \"Create Event Hub policies\")\n\n    \n5. On the same page, take a note of the policy keys generated for the two policies. Save these keys because they will be used later.\n\n    ![policy keys](./media/hdinsight-apache-spark-csharp-apache-zeppelin-eventhub-streaming/hdispark.streaming.event.hub.policy.keys.png \"Save policy keys\")\n\n6. On the **Dashboard** page, click **Connection Information** from the bottom to retrieve and save the connection strings for the Event Hub using the two policies.\n\n    ![policy keys](./media/hdinsight-apache-spark-csharp-apache-zeppelin-eventhub-streaming/hdispark.streaming.event.hub.policy.connection.strings.png \"Save policy connection strings\")\n\n[AZURE.INCLUDE [service-bus-event-hubs-get-started-send-csharp](../../includes/service-bus-event-hubs-get-started-send-csharp.md)]\n\n##<a name=\"receivezeppelin\"></a>Receive messages in Spark on HDInsight using Zeppelin\n\nIn this section, you create a [Zeppelin](https://zeppelin.incubator.apache.org) notebook to receive messages from the Event Hub into the Spark cluster in HDInsight.\n\n### Allocating resources to Zeppelin for streaming application\n\nYou must make the following considerations while creating a streaming application using Zeppelin:\n\n* **Event hub partitions and cores allocated to Zeppelin**. In the previous steps, you created an Event Hub with some partitions. In the Zeppelin streaming application you create below, you will specify the same number of partitions again. To successfully stream the data from Event Hub using Zeppelin, the number of cores you allocate to Zeppelin must be twice the number of partitions in Event Hub.\n* **Minimum number of cores to be allocated to Zeppelin**. In your streaming application that you create below, you create a temporary table where you store the messages that are streamed by your application. You then use a Spark SQL statement to read messages from this temporary table. To successfully run the Spark SQL statement, you must make sure that you at least have two cores allocated to Zeppelin.\n\nIf you combine the two requirements above, this is what you get:\n\n* The minimum number of cores you must allocate to Zeppelin is 2.\n* The number of allocated cores must always be twice the number of partitions in Event Hub. \n\nFor instructions on how to allocate resources in a Spark cluster, see [Manage resources for the Apache Spark cluster in HDInsight](hdinsight-apache-spark-resource-manager-v1.md).\n\n### Create a streaming application using Zeppelin\n\n1. From the [Azure Preview Portal](https://portal.azure.com/), from the startboard, click the tile for your Spark cluster (if you pinned it to the startboard). You can also navigate to your cluster under **Browse All** > **HDInsight Clusters**.   \n\n2. From the Spark cluster blade, click **Quick Links**, and then from the **Cluster Dashboard** blade, click **Zeppelin Notebook**. If prompted, enter the admin credentials for the cluster.\n\n    > [AZURE.NOTE] You may also reach the Zeppelin Notebook for your cluster by opening the following URL in your browser. Replace __CLUSTERNAME__ with the name of your cluster:\n    >\n    > `https://CLUSTERNAME.azurehdinsight.net/zeppelin`\n\n2. Create a new notebook. From the header pane, click **Notebook**, and from the drop-down, click **Create New Note**.\n\n    ![Create a new Zeppelin notebook](./media/hdinsight-apache-spark-csharp-apache-zeppelin-eventhub-streaming/hdispark.createnewnote.png \"Create a new Zeppelin notebook\")\n\n    On the same page, under the **Notebook** heading, you should see a new notebook with the name starting with **Note XXXXXXXXX**. Click the new notebook.\n\n3. On the web page for the new notebook, click the heading, and change the name of the notebook if you want to. Press ENTER to save the name change. Also, make sure the notebook header shows a **Connected** status in the top-right corner.\n\n    ![Zeppelin notebook status](./media/hdinsight-apache-spark-csharp-apache-zeppelin-eventhub-streaming/hdispark.newnote.connected.png \"Zeppelin notebook status\")\n\n4. In the empty paragraph that is created by default in the new notebook, paste the following snippet and replace the placeholders to use your event hub configuration. In this snippet, you receive the stream from Event Hub and register the stream into a temporary table, called **mytemptable**. In the next section, we will start the sender application. You can then read the data directly from the table.\n\n    > [AZURE.NOTE] In the snippet below, **eventhubs.checkpoint.dir** must be set to a directory in your default storage container. If the directory does not exist, the streamig application creates it. You can either specify the full path to the directory like \"**wasb://container@storageaccount.blob.core.windows.net/mycheckpointdir/**\" or just the relative path to the directory, such as \"**/mycheckpointdir**\".\n\n        import org.apache.spark.streaming.{Seconds, StreamingContext}\n        import org.apache.spark.streaming.eventhubs.EventHubsUtils\n        import sqlContext.implicits._\n        \n        val ehParams = Map[String, String](\n          \"eventhubs.policyname\" -> \"<name of policy with listen permissions>\",\n          \"eventhubs.policykey\" -> \"<key of policy with listen permissions>\",\n          \"eventhubs.namespace\" -> \"<service bus namespace>\",\n          \"eventhubs.name\" -> \"<event hub in the service bus namespace>\",\n          \"eventhubs.partition.count\" -> \"1\",\n          \"eventhubs.consumergroup\" -> \"$default\",\n          \"eventhubs.checkpoint.dir\" -> \"/<check point directory in your storage account>\",\n          \"eventhubs.checkpoint.interval\" -> \"10\"\n        )\n        \n        val ssc =  new StreamingContext(sc, Seconds(10))\n        val stream = EventHubsUtils.createUnionStream(ssc, ehParams)\n        \n        case class Message(msg: String)\n        stream.map(msg=>Message(new String(msg))).foreachRDD(rdd=>rdd.toDF().registerTempTable(\"mytemptable\"))\n\n        stream.print\n        ssc.start\n\n\n##<a name=\"runapps\"></a>Run the applications\n\n1. From the Zeppelin notebook, run the paragraph with the snippet. Press **SHIFT + ENTER** or the **Play** button at the top-right corner.\n\n    The status on the right-corner of the paragraph should progress from READY, PENDING, RUNNING to FINISHED. The output will show up in the bottom of the same paragraph. The screenshot looks like the following:\n\n    ![Output of the snippet](./media/hdinsight-apache-spark-csharp-apache-zeppelin-eventhub-streaming/hdispark.streaming.event.hub.zeppelin.code.output.png \"Output of the snipet\")\n\n2. Run the **Sender** project and press **Enter** in the console window to start sending messages to the Event Hub.\n\n3. From the Zeppelin notebook, in a new paragraph, enter the following snippet to read the messages received in Spark.\n\n        %sql \n        select * from mytemptable limit 10\n\n    The following screen capture shows the messages received in the **mytemptable**.\n\n    ![Receive the messages in Zeppelin](./media/hdinsight-apache-spark-csharp-apache-zeppelin-eventhub-streaming/hdispark.streaming.event.hub.zeppelin.output.png \"Receive messages in Zeppelin notebook\")\n\n4. Restart the Spark SQL interpreter to exit the application. Click the **Interpreter** tab at the top, and for the Spark interpreter, click **Restart**.\n\n    ![Restart the Zeppelin intepreter](./media/hdinsight-apache-spark-csharp-apache-zeppelin-eventhub-streaming/hdispark.zeppelin.restart.interpreter.png \"Restart the Zeppelin intepreter\")\n\n##<a name=\"sparkstreamingha\"></a>Run the streaming application with high availability\n\nUsing Zeppelin to receive streaming data into Spark cluster on HDInsight is a good approach to prototype your application. However, to run your streaming application in a production setup with high-availability and resilience, you need to do the following:\n\n1. Copy over the dependency jar to the storage account associated with the cluster.\n2. Build a streaming application.\n3. RDP into the cluster and copy over the application jar to the headnode of the cluster.\n3. RDP into the cluster and run the application on the cluster node.\n\nInstructions on how to perform these steps and a sample streaming application can be downloaded from GitHub at [https://github.com/hdinsight/hdinsight-spark-examples](https://github.com/hdinsight/hdinsight-spark-examples). \n\n\n##<a name=\"seealso\"></a>See also\n\n\n* [Overview: Apache Spark on Azure HDInsight](hdinsight-apache-spark-overview-v1.md)\n* [Quick Start: create Apache Spark on HDInsight and run interactive queries using Spark SQL](hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql.md)\n* [Use Spark in HDInsight for building machine learning applications](hdinsight-apache-spark-ipython-notebook-machine-learning-v1.md)\n* [Perform interactive data analysis using Spark in HDInsight with BI tools](hdinsight-apache-spark-use-bi-tools-v1.md)\n* [Manage resources for the Apache Spark cluster in Azure HDInsight](hdinsight-apache-spark-resource-manager-v1.md)\n\n\n[hdinsight-versions]: hdinsight-component-versioning.md\n[hdinsight-upload-data]: hdinsight-upload-data.md\n[hdinsight-storage]: hdinsight-hadoop-use-blob-storage.md\n\n[azure-purchase-options]: http://azure.microsoft.com/pricing/purchase-options/\n[azure-member-offers]: http://azure.microsoft.com/pricing/member-offers/\n[azure-free-trial]: http://azure.microsoft.com/pricing/free-trial/\n[azure-management-portal]: https://manage.windowsazure.com/\n[azure-create-storageaccount]: storage-create-storage-account.md \n"
}