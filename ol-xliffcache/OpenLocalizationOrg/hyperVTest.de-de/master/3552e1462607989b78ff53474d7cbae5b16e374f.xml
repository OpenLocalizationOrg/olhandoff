{
  "nodes": [
    {
      "pos": [
        27,
        111
      ],
      "content": "Move data from an on-premise SQL Server to SQL Azure with Azure Data Factory | Azure"
    },
    {
      "pos": [
        130,
        284
      ],
      "content": "Set up an ADF pipeline that composes two data migration activities that together move data on a daily basis between databases on-premise and in the cloud."
    },
    {
      "pos": [
        646,
        722
      ],
      "content": "Move data from an on-premise SQL server to SQL Azure with Azure Data Factory"
    },
    {
      "pos": [
        724,
        879
      ],
      "content": "This topic shows how to move data from an on-premise SQL Server Database to a SQL Azure Database via Azure Blob Storage using the Azure Data Factory (ADF)."
    },
    {
      "pos": [
        881,
        1067
      ],
      "content": "The <bpt id=\"p1\">**</bpt>menu<ept id=\"p1\">**</ept><ph id=\"ph2\"/> below links to topics that describe how to ingest data into other target environments where the data can be stored and processed during the Cortana Analytics Process (CAPS)."
    },
    {
      "pos": [
        1180,
        1249
      ],
      "content": "Introduction: What is ADF and when should it be used to migrate data?"
    },
    {
      "pos": [
        1251,
        1683
      ],
      "content": "Azure Data Factory is a fully managed cloud-based data integration service that orchestrates and automates the movement and transformation of data. The key concept in the ADF model is pipeline. A pipelines is a logical groupings of Activities, each of which defines the actions to perform on the data contained in Datasets. Linked services are used to define the information needed for Data Factory to connect to the data resources.",
      "nodes": [
        {
          "content": "Azure Data Factory is a fully managed cloud-based data integration service that orchestrates and automates the movement and transformation of data.",
          "pos": [
            0,
            147
          ]
        },
        {
          "content": "The key concept in the ADF model is pipeline.",
          "pos": [
            148,
            193
          ]
        },
        {
          "content": "A pipelines is a logical groupings of Activities, each of which defines the actions to perform on the data contained in Datasets.",
          "pos": [
            194,
            323
          ]
        },
        {
          "content": "Linked services are used to define the information needed for Data Factory to connect to the data resources.",
          "pos": [
            324,
            432
          ]
        }
      ]
    },
    {
      "pos": [
        1685,
        2126
      ],
      "content": "With ADF, existing data processing services can be composed into data pipelines that are highly available and managed in the cloud. These data pipelines can be scheduled to ingest, prepare, transform, analyze, and publish data, and ADF will manage and orchestrate all of the complex data and processing dependencies. Solutions can be quickly built and deployed in the cloud, connecting a growing number of on-premises and cloud data sources.",
      "nodes": [
        {
          "content": "With ADF, existing data processing services can be composed into data pipelines that are highly available and managed in the cloud.",
          "pos": [
            0,
            131
          ]
        },
        {
          "content": "These data pipelines can be scheduled to ingest, prepare, transform, analyze, and publish data, and ADF will manage and orchestrate all of the complex data and processing dependencies.",
          "pos": [
            132,
            316
          ]
        },
        {
          "content": "Solutions can be quickly built and deployed in the cloud, connecting a growing number of on-premises and cloud data sources.",
          "pos": [
            317,
            441
          ]
        }
      ]
    },
    {
      "pos": [
        2128,
        2724
      ],
      "content": "Consider using ADF when data needs to be continually migrated in a hybrid scenario that accesses both on-premise and cloud resources, and when the data is transacted or needs to be modified or have business logic added to it in the course of being migrated. ADF allows for the scheduling and monitoring of jobs using simple JSON scripts that manage the movement of data on a periodic basis. ADF also has other capabilities such as support for complex operations. For more information on ADF, see the documentation at <bpt id=\"p2\">[</bpt>Azure Data Factory (ADF)<ept id=\"p2\">](https://azure.microsoft.com/services/data-factory/)</ept>.",
      "nodes": [
        {
          "content": "Consider using ADF when data needs to be continually migrated in a hybrid scenario that accesses both on-premise and cloud resources, and when the data is transacted or needs to be modified or have business logic added to it in the course of being migrated.",
          "pos": [
            0,
            257
          ]
        },
        {
          "content": "ADF allows for the scheduling and monitoring of jobs using simple JSON scripts that manage the movement of data on a periodic basis.",
          "pos": [
            258,
            390
          ]
        },
        {
          "content": "ADF also has other capabilities such as support for complex operations.",
          "pos": [
            391,
            462
          ]
        },
        {
          "content": "For more information on ADF, see the documentation at <bpt id=\"p2\">[</bpt>Azure Data Factory (ADF)<ept id=\"p2\">](https://azure.microsoft.com/services/data-factory/)</ept>.",
          "pos": [
            463,
            634
          ]
        }
      ]
    },
    {
      "pos": [
        2752,
        2764
      ],
      "content": "The Scenario"
    },
    {
      "pos": [
        2766,
        2975
      ],
      "content": "We set up an ADF pipeline that composes two data migration activities that together move data on a daily basis between an on-premise SQL database and an Azure SQL Database in the cloud. The two activities are:",
      "nodes": [
        {
          "content": "We set up an ADF pipeline that composes two data migration activities that together move data on a daily basis between an on-premise SQL database and an Azure SQL Database in the cloud.",
          "pos": [
            0,
            185
          ]
        },
        {
          "content": "The two activities are:",
          "pos": [
            186,
            209
          ]
        }
      ]
    },
    {
      "pos": [
        2979,
        3060
      ],
      "content": "copy data from an on-premise SQL Server database to an Azure Blob Storage account"
    },
    {
      "pos": [
        3063,
        3134
      ],
      "content": "copy data from the Azure Blob Storage account to an Azure SQL Database."
    },
    {
      "pos": [
        3136,
        3429
      ],
      "content": "<bpt id=\"p3\">**</bpt>Reference<ept id=\"p3\">**</ept>: The steps shown here have been adapted from the more detailed tutorial <bpt id=\"p4\">[</bpt>Enable your pipelines to work with on-premises data<ept id=\"p4\">](data-factory-use-onpremises-datasources.md)</ept><ph id=\"ph4\"/> provided by the ADF team and references to the relevant sections of that topic are provided when appropriate."
    },
    {
      "pos": [
        3457,
        3470
      ],
      "content": "Prerequisites"
    },
    {
      "pos": [
        3471,
        3502
      ],
      "content": "This tutorial assumes you have:"
    },
    {
      "pos": [
        3506,
        3652
      ],
      "content": "An <bpt id=\"p5\">**</bpt>Azure subscription<ept id=\"p5\">**</ept>. If you do not have a subscription, you can sign up for a <bpt id=\"p6\">[</bpt>free trial<ept id=\"p6\">](https://azure.microsoft.com/pricing/free-trial/)</ept>.",
      "nodes": [
        {
          "content": "An <bpt id=\"p5\">**</bpt>Azure subscription<ept id=\"p5\">**</ept>.",
          "pos": [
            0,
            64
          ]
        },
        {
          "content": "If you do not have a subscription, you can sign up for a <bpt id=\"p6\">[</bpt>free trial<ept id=\"p6\">](https://azure.microsoft.com/pricing/free-trial/)</ept>.",
          "pos": [
            65,
            222
          ]
        }
      ]
    },
    {
      "pos": [
        3655,
        4154
      ],
      "content": "An <bpt id=\"p7\">**</bpt>Azure storage account<ept id=\"p7\">**</ept>. You will use an Azure storage account for storing the data in this tutorial. If you don't have an Azure storage account, see the <bpt id=\"p8\">[</bpt>Create a storage account<ept id=\"p8\">](storage-create-storage-account.md#create-a-storage-account)</ept><ph id=\"ph5\"/> article. After you have created the storage account, you will need to obtain the account key used to access the storage. See <bpt id=\"p9\">[</bpt>View, copy and regenerate storage access keys<ept id=\"p9\">](storage-create-storage-account.md#view-copy-and-regenerate-storage-access-keys)</ept>.",
      "nodes": [
        {
          "content": "An <bpt id=\"p7\">**</bpt>Azure storage account<ept id=\"p7\">**</ept>.",
          "pos": [
            0,
            67
          ]
        },
        {
          "content": "You will use an Azure storage account for storing the data in this tutorial.",
          "pos": [
            68,
            144
          ]
        },
        {
          "content": "If you don't have an Azure storage account, see the <bpt id=\"p8\">[</bpt>Create a storage account<ept id=\"p8\">](storage-create-storage-account.md#create-a-storage-account)</ept><ph id=\"ph5\"/> article.",
          "pos": [
            145,
            344
          ]
        },
        {
          "content": "After you have created the storage account, you will need to obtain the account key used to access the storage.",
          "pos": [
            345,
            456
          ]
        },
        {
          "content": "See <bpt id=\"p9\">[</bpt>View, copy and regenerate storage access keys<ept id=\"p9\">](storage-create-storage-account.md#view-copy-and-regenerate-storage-access-keys)</ept>.",
          "pos": [
            457,
            627
          ]
        }
      ]
    },
    {
      "pos": [
        4157,
        4397
      ],
      "content": "Access to an <bpt id=\"p10\">**</bpt>Azure SQL Database<ept id=\"p10\">**</ept>. If you must setup an Azure SQL Database, <bpt id=\"p11\">[</bpt>Getting Started with Microsoft Azure SQL Database <ept id=\"p11\">](sql-database-get-started.md)</ept><ph id=\"ph6\"/> provides information on how to provision a new instance of a Azure SQL Database.",
      "nodes": [
        {
          "content": "Access to an <bpt id=\"p10\">**</bpt>Azure SQL Database<ept id=\"p10\">**</ept>.",
          "pos": [
            0,
            76
          ]
        },
        {
          "content": "If you must setup an Azure SQL Database, <bpt id=\"p11\">[</bpt>Getting Started with Microsoft Azure SQL Database <ept id=\"p11\">](sql-database-get-started.md)</ept><ph id=\"ph6\"/> provides information on how to provision a new instance of a Azure SQL Database.",
          "pos": [
            77,
            334
          ]
        }
      ]
    },
    {
      "pos": [
        4400,
        4558
      ],
      "content": "Installed and configured <bpt id=\"p12\">**</bpt>Azure PowerShell<ept id=\"p12\">**</ept><ph id=\"ph7\"/> locally. For instructions, see <bpt id=\"p13\">[</bpt>How to install and configure Azure PowerShell<ept id=\"p13\">](powershell-install-configure.md)</ept>.",
      "nodes": [
        {
          "content": "Installed and configured <bpt id=\"p12\">**</bpt>Azure PowerShell<ept id=\"p12\">**</ept><ph id=\"ph7\"/> locally.",
          "pos": [
            0,
            108
          ]
        },
        {
          "content": "For instructions, see <bpt id=\"p13\">[</bpt>How to install and configure Azure PowerShell<ept id=\"p13\">](powershell-install-configure.md)</ept>.",
          "pos": [
            109,
            252
          ]
        }
      ]
    },
    {
      "pos": [
        4562,
        4644
      ],
      "content": "<ph id=\"ph8\">[AZURE.NOTE]</ph><ph id=\"ph9\"/> This procedure uses the <bpt id=\"p14\">[</bpt>Azure Portal<ept id=\"p14\">](https://ms.portal.azure.com/)</ept>."
    },
    {
      "pos": [
        4646,
        4648
      ],
      "content": "##"
    },
    {
      "pos": [
        4675,
        4720
      ],
      "content": "Upload the data to your on-premise SQL Server"
    },
    {
      "pos": [
        4722,
        5287
      ],
      "content": "We use the <bpt id=\"p15\">[</bpt>NYC Taxi dataset<ept id=\"p15\">](http://chriswhong.com/open-data/foil_nyc_taxi/)</ept><ph id=\"ph10\"/> to demonstrate the migration process. The NYC Taxi dataset is available, as noted that post, on Azure blob storage <bpt id=\"p16\">[</bpt>NYC Taxi Data<ept id=\"p16\">](http://www.andresmh.com/nyctaxitrips/)</ept>. The data has two files, the trip_data.csv file which contains trip details and the  trip_far.csv file which contains details of the fare paid for each trip. A sample and description of these files are provided in <bpt id=\"p17\">[</bpt>NYC Taxi Trips Dataset Description<ept id=\"p17\">](machine-learning-data-science-process-sql-walkthrough.md#dataset)</ept>.",
      "nodes": [
        {
          "content": "We use the <bpt id=\"p15\">[</bpt>NYC Taxi dataset<ept id=\"p15\">](http://chriswhong.com/open-data/foil_nyc_taxi/)</ept><ph id=\"ph10\"/> to demonstrate the migration process.",
          "pos": [
            0,
            170
          ]
        },
        {
          "content": "The NYC Taxi dataset is available, as noted that post, on Azure blob storage <bpt id=\"p16\">[</bpt>NYC Taxi Data<ept id=\"p16\">](http://www.andresmh.com/nyctaxitrips/)</ept>.",
          "pos": [
            171,
            343
          ]
        },
        {
          "content": "The data has two files, the trip_data.csv file which contains trip details and the  trip_far.csv file which contains details of the fare paid for each trip.",
          "pos": [
            344,
            500
          ]
        },
        {
          "content": "A sample and description of these files are provided in <bpt id=\"p17\">[</bpt>NYC Taxi Trips Dataset Description<ept id=\"p17\">](machine-learning-data-science-process-sql-walkthrough.md#dataset)</ept>.",
          "pos": [
            501,
            700
          ]
        }
      ]
    },
    {
      "pos": [
        5290,
        5787
      ],
      "content": "You can either adapt the procedure provided here to a set of your own data or follow the steps as described by using the NYC Taxi dataset. To upload the NYC Taxi dataset into your on-premise SQL Server database, follow the procedure outlined in <bpt id=\"p18\">[</bpt>Bulk Import Data into SQL Server Database<ept id=\"p18\">](machine-learning-data-science-process-sql-walkthrough.md#dbload)</ept>. These instructions are for a SQL Server on an Azure Virtual Machine, but the procedure for uploading to the on-premise SQL Server is the same.",
      "nodes": [
        {
          "content": "You can either adapt the procedure provided here to a set of your own data or follow the steps as described by using the NYC Taxi dataset.",
          "pos": [
            0,
            138
          ]
        },
        {
          "content": "To upload the NYC Taxi dataset into your on-premise SQL Server database, follow the procedure outlined in <bpt id=\"p18\">[</bpt>Bulk Import Data into SQL Server Database<ept id=\"p18\">](machine-learning-data-science-process-sql-walkthrough.md#dbload)</ept>.",
          "pos": [
            139,
            394
          ]
        },
        {
          "content": "These instructions are for a SQL Server on an Azure Virtual Machine, but the procedure for uploading to the on-premise SQL Server is the same.",
          "pos": [
            395,
            537
          ]
        }
      ]
    },
    {
      "pos": [
        5789,
        5791
      ],
      "content": "##"
    },
    {
      "pos": [
        5817,
        5845
      ],
      "content": "Create an Azure Data Factory"
    },
    {
      "pos": [
        5847,
        6191
      ],
      "content": "The instructions for creating a new Azure Data Factory and a resource group in the <bpt id=\"p19\">[</bpt>Azure Portal<ept id=\"p19\">](https://ms.portal.azure.com/)</ept><ph id=\"ph11\"/> are provided <bpt id=\"p20\">[</bpt>Create an Azure Data Factory<ept id=\"p20\">](data-factory-build-your-first-pipeline-using-editor.md#step-1-creating-the-data-factory)</ept>. Name the new ADF instance <bpt id=\"p21\">*</bpt>adfdsp<ept id=\"p21\">*</ept><ph id=\"ph12\"/> and name the resource group created <bpt id=\"p22\">*</bpt>adfdsprg<ept id=\"p22\">*</ept>.",
      "nodes": [
        {
          "content": "The instructions for creating a new Azure Data Factory and a resource group in the <bpt id=\"p19\">[</bpt>Azure Portal<ept id=\"p19\">](https://ms.portal.azure.com/)</ept><ph id=\"ph11\"/> are provided <bpt id=\"p20\">[</bpt>Create an Azure Data Factory<ept id=\"p20\">](data-factory-build-your-first-pipeline-using-editor.md#step-1-creating-the-data-factory)</ept>.",
          "pos": [
            0,
            356
          ]
        },
        {
          "content": "Name the new ADF instance <bpt id=\"p21\">*</bpt>adfdsp<ept id=\"p21\">*</ept><ph id=\"ph12\"/> and name the resource group created <bpt id=\"p22\">*</bpt>adfdsprg<ept id=\"p22\">*</ept>.",
          "pos": [
            357,
            534
          ]
        }
      ]
    },
    {
      "pos": [
        6196,
        6248
      ],
      "content": "Install and configure up the Data Management Gateway"
    },
    {
      "pos": [
        6250,
        6757
      ],
      "content": "To enable your pipelines in an Azure data factory to work with an on-premise SQL Server, you need to add it as a linked service to the data factory. To create a Linked Service for the on-premise SQL Server, you must first download and install Microsoft Data Management Gateway onto the on-premise computer and configure the linked service for the on-premises data source to use the gateway. The Data Management Gateway serializes and deserializes the source and sink data on the computer where it is hosted.",
      "nodes": [
        {
          "content": "To enable your pipelines in an Azure data factory to work with an on-premise SQL Server, you need to add it as a linked service to the data factory.",
          "pos": [
            0,
            148
          ]
        },
        {
          "content": "To create a Linked Service for the on-premise SQL Server, you must first download and install Microsoft Data Management Gateway onto the on-premise computer and configure the linked service for the on-premises data source to use the gateway.",
          "pos": [
            149,
            390
          ]
        },
        {
          "content": "The Data Management Gateway serializes and deserializes the source and sink data on the computer where it is hosted.",
          "pos": [
            391,
            507
          ]
        }
      ]
    },
    {
      "pos": [
        6759,
        6924
      ],
      "content": "For set-up instructions and details on Data Management Gateway, see <bpt id=\"p23\">[</bpt>Enable your pipelines to work with on-premises data<ept id=\"p23\">](data-factory-use-onpremises-datasources.md)</ept>"
    },
    {
      "pos": [
        6962,
        7017
      ],
      "content": "Create linked services to connect to the data resources"
    },
    {
      "pos": [
        7019,
        7291
      ],
      "content": "A linked service defines the information needed for Azure Data Factory to connect to a data resource. The step-by-step procedure for creating linked services is provided in <bpt id=\"p24\">[</bpt>Create linked services<ept id=\"p24\">](data-factory-use-onpremises-datasources.md#step-2-create-linked-services)</ept>.",
      "nodes": [
        {
          "content": "A linked service defines the information needed for Azure Data Factory to connect to a data resource.",
          "pos": [
            0,
            101
          ]
        },
        {
          "content": "The step-by-step procedure for creating linked services is provided in <bpt id=\"p24\">[</bpt>Create linked services<ept id=\"p24\">](data-factory-use-onpremises-datasources.md#step-2-create-linked-services)</ept>.",
          "pos": [
            102,
            312
          ]
        }
      ]
    },
    {
      "pos": [
        7293,
        7371
      ],
      "content": "We have three resources in this scenario for which linked services are needed."
    },
    {
      "pos": [
        7376,
        7450
      ],
      "content": "<bpt id=\"p25\">[</bpt>Linked service for on-premise SQL Server<ept id=\"p25\">](#adf-linked-service-onprem-sql)</ept>"
    },
    {
      "pos": [
        7454,
        7525
      ],
      "content": "<bpt id=\"p26\">[</bpt>Linked service for Azure Blob Storage<ept id=\"p26\">](#adf-linked-service-blob-store)</ept>"
    },
    {
      "pos": [
        7529,
        7599
      ],
      "content": "<bpt id=\"p27\">[</bpt>Linked service for Azure SQL database<ept id=\"p27\">](#adf-linked-service-azure-sql)</ept>"
    },
    {
      "pos": [
        7602,
        8099
      ],
      "content": "###<ph id=\"ph13\">&lt;a name=\"adf-linked-service-onprem-sql\"&gt;</ph><ph id=\"ph14\">&lt;/a&gt;</ph>Linked service for on-premise SQL Server database\nTo create the linked service for the on-premise SQL Server, click on the <bpt id=\"p28\">**</bpt>Data Store<ept id=\"p28\">**</ept><ph id=\"ph15\"/> in the ADF landing page on Azure Classic Portal, select <bpt id=\"p29\">*</bpt>SQL<ept id=\"p29\">*</ept><ph id=\"ph16\"/> and enter the credentials for the <bpt id=\"p30\">*</bpt>username<ept id=\"p30\">*</ept><ph id=\"ph17\"/> and <bpt id=\"p31\">*</bpt>password<ept id=\"p31\">*</ept><ph id=\"ph18\"/> for the on-premise SQL Server. You need to enter the servername as a <bpt id=\"p32\">**</bpt>fully qualified servername backslash instance name (servername\\instancename)<ept id=\"p32\">**</ept>. Name the linked service <bpt id=\"p33\">*</bpt>adfonpremsql<ept id=\"p33\">*</ept>.",
      "nodes": [
        {
          "content": "<ph id=\"ph13\">&lt;a name=\"adf-linked-service-onprem-sql\"&gt;</ph><ph id=\"ph14\">&lt;/a&gt;</ph>Linked service for on-premise SQL Server database\nTo create the linked service for the on-premise SQL Server, click on the <bpt id=\"p28\">**</bpt>Data Store<ept id=\"p28\">**</ept><ph id=\"ph15\"/> in the ADF landing page on Azure Classic Portal, select <bpt id=\"p29\">*</bpt>SQL<ept id=\"p29\">*</ept><ph id=\"ph16\"/> and enter the credentials for the <bpt id=\"p30\">*</bpt>username<ept id=\"p30\">*</ept><ph id=\"ph17\"/> and <bpt id=\"p31\">*</bpt>password<ept id=\"p31\">*</ept><ph id=\"ph18\"/> for the on-premise SQL Server.",
          "pos": [
            3,
            607
          ]
        },
        {
          "content": "You need to enter the servername as a <bpt id=\"p32\">**</bpt>fully qualified servername backslash instance name (servername\\instancename)<ept id=\"p32\">**</ept>.",
          "pos": [
            608,
            767
          ]
        },
        {
          "content": "Name the linked service <bpt id=\"p33\">*</bpt>adfonpremsql<ept id=\"p33\">*</ept>.",
          "pos": [
            768,
            847
          ]
        }
      ]
    },
    {
      "pos": [
        8101,
        8440
      ],
      "content": "###<ph id=\"ph19\">&lt;a name=\"adf-linked-service-blob-store\"&gt;</ph><ph id=\"ph20\">&lt;/a&gt;</ph>Linked service for Blob\nTo create the linked service for the Azure Blob Storage account, click on the <bpt id=\"p34\">**</bpt>Data Store<ept id=\"p34\">**</ept><ph id=\"ph21\"/> in the ADF landing page on Azure Classic Portal, select <bpt id=\"p35\">*</bpt>Azure Storage Account<ept id=\"p35\">*</ept><ph id=\"ph22\"/> and enter the Azure Blob Storage account key and container name. Name the link service <bpt id=\"p36\">*</bpt>adfds<ept id=\"p36\">*</ept>.",
      "nodes": [
        {
          "content": "<ph id=\"ph19\">&lt;a name=\"adf-linked-service-blob-store\"&gt;</ph><ph id=\"ph20\">&lt;/a&gt;</ph>Linked service for Blob\nTo create the linked service for the Azure Blob Storage account, click on the <bpt id=\"p34\">**</bpt>Data Store<ept id=\"p34\">**</ept><ph id=\"ph21\"/> in the ADF landing page on Azure Classic Portal, select <bpt id=\"p35\">*</bpt>Azure Storage Account<ept id=\"p35\">*</ept><ph id=\"ph22\"/> and enter the Azure Blob Storage account key and container name.",
          "pos": [
            3,
            468
          ]
        },
        {
          "content": "Name the link service <bpt id=\"p36\">*</bpt>adfds<ept id=\"p36\">*</ept>.",
          "pos": [
            469,
            539
          ]
        }
      ]
    },
    {
      "pos": [
        8442,
        8821
      ],
      "content": "###<ph id=\"ph23\">&lt;a name=\"adf-linked-service-azure-sql\"&gt;</ph><ph id=\"ph24\">&lt;/a&gt;</ph>Linked service for Azure SQL database\nTo create the linked service for the Azure SQL Database, click on the <bpt id=\"p37\">**</bpt>Data Store<ept id=\"p37\">**</ept><ph id=\"ph25\"/> in the ADF landing page on Azure Classic Portal, select <bpt id=\"p38\">*</bpt>Azure SQL<ept id=\"p38\">*</ept><ph id=\"ph26\"/> and enter the credentials for the <bpt id=\"p39\">*</bpt>username<ept id=\"p39\">*</ept><ph id=\"ph27\"/> and <bpt id=\"p40\">*</bpt>password<ept id=\"p40\">*</ept><ph id=\"ph28\"/> for the Azure SQL Database. The <bpt id=\"p41\">*</bpt>username<ept id=\"p41\">*</ept><ph id=\"ph29\"/> must be specified as <bpt id=\"p42\">*</bpt>user@servername<ept id=\"p42\">*</ept>.",
      "nodes": [
        {
          "content": "<ph id=\"ph23\">&lt;a name=\"adf-linked-service-azure-sql\"&gt;</ph><ph id=\"ph24\">&lt;/a&gt;</ph>Linked service for Azure SQL database\nTo create the linked service for the Azure SQL Database, click on the <bpt id=\"p37\">**</bpt>Data Store<ept id=\"p37\">**</ept><ph id=\"ph25\"/> in the ADF landing page on Azure Classic Portal, select <bpt id=\"p38\">*</bpt>Azure SQL<ept id=\"p38\">*</ept><ph id=\"ph26\"/> and enter the credentials for the <bpt id=\"p39\">*</bpt>username<ept id=\"p39\">*</ept><ph id=\"ph27\"/> and <bpt id=\"p40\">*</bpt>password<ept id=\"p40\">*</ept><ph id=\"ph28\"/> for the Azure SQL Database.",
          "pos": [
            3,
            594
          ]
        },
        {
          "content": "The <bpt id=\"p41\">*</bpt>username<ept id=\"p41\">*</ept><ph id=\"ph29\"/> must be specified as <bpt id=\"p42\">*</bpt>user@servername<ept id=\"p42\">*</ept>.",
          "pos": [
            595,
            744
          ]
        }
      ]
    },
    {
      "pos": [
        8827,
        8829
      ],
      "content": "##"
    },
    {
      "pos": [
        8854,
        8916
      ],
      "content": "Define and create tables to specify how to access the datasets"
    },
    {
      "pos": [
        8918,
        9190
      ],
      "content": "Create tables that specify the structure, location and availability of the datasets with the following script-based procedures. JSON files are used to define the tables. For more information on the structure of these files, see <bpt id=\"p43\">[</bpt>Datasets<ept id=\"p43\">](data-factory-create-datasets.md)</ept>.",
      "nodes": [
        {
          "content": "Create tables that specify the structure, location and availability of the datasets with the following script-based procedures.",
          "pos": [
            0,
            127
          ]
        },
        {
          "content": "JSON files are used to define the tables.",
          "pos": [
            128,
            169
          ]
        },
        {
          "content": "For more information on the structure of these files, see <bpt id=\"p43\">[</bpt>Datasets<ept id=\"p43\">](data-factory-create-datasets.md)</ept>.",
          "pos": [
            170,
            312
          ]
        }
      ]
    },
    {
      "pos": [
        9194,
        9568
      ],
      "content": "<ph id=\"ph30\">[AZURE.NOTE]</ph><ph id=\"ph31\"/>  You should execute the <ph id=\"ph32\">`Add-AzureAccount`</ph><ph id=\"ph33\"/> cmdlet prior to executing the <bpt id=\"p44\">[</bpt>New-AzureDataFactoryTable<ept id=\"p44\">](https://msdn.microsoft.com/library/azure/dn835096.aspx)</ept><ph id=\"ph34\"/> cmdlet to confirm that the right Azure subscription is selected for the command execution. For documentation of this cmdlet, see <bpt id=\"p45\">[</bpt>Add-AzureAccount<ept id=\"p45\">](https://msdn.microsoft.com/library/azure/dn790372.aspx)</ept>.",
      "nodes": [
        {
          "content": "<ph id=\"ph30\">[AZURE.NOTE]</ph><ph id=\"ph31\"/>  You should execute the <ph id=\"ph32\">`Add-AzureAccount`</ph><ph id=\"ph33\"/> cmdlet prior to executing the <bpt id=\"p44\">[</bpt>New-AzureDataFactoryTable<ept id=\"p44\">](https://msdn.microsoft.com/library/azure/dn835096.aspx)</ept><ph id=\"ph34\"/> cmdlet to confirm that the right Azure subscription is selected for the command execution.",
          "pos": [
            0,
            383
          ]
        },
        {
          "content": "For documentation of this cmdlet, see <bpt id=\"p45\">[</bpt>Add-AzureAccount<ept id=\"p45\">](https://msdn.microsoft.com/library/azure/dn790372.aspx)</ept>.",
          "pos": [
            384,
            537
          ]
        }
      ]
    },
    {
      "pos": [
        9570,
        9635
      ],
      "content": "The JSON-based definitions in the tables use the following names:"
    },
    {
      "pos": [
        9639,
        9704
      ],
      "content": "the <bpt id=\"p46\">**</bpt>table name<ept id=\"p46\">**</ept><ph id=\"ph35\"/> in the on-premise SQL server is <bpt id=\"p47\">*</bpt>nyctaxi_data<ept id=\"p47\">*</ept>"
    },
    {
      "pos": [
        9707,
        9782
      ],
      "content": "the <bpt id=\"p48\">**</bpt>container name<ept id=\"p48\">**</ept><ph id=\"ph36\"/> in the Azure Blob Storage account is <bpt id=\"p49\">*</bpt>containername<ept id=\"p49\">*</ept>"
    },
    {
      "pos": [
        9786,
        9843
      ],
      "content": "Three table definitions are needed for this ADF pipeline:"
    },
    {
      "pos": [
        9848,
        9893
      ],
      "content": "<bpt id=\"p50\">[</bpt>SQL on-premise Table<ept id=\"p50\">](#adf-table-onprem-sql)</ept>"
    },
    {
      "pos": [
        9897,
        9933
      ],
      "content": "<bpt id=\"p51\">[</bpt>Blob Table <ept id=\"p51\">](#adf-table-blob-store)</ept>"
    },
    {
      "pos": [
        9937,
        9976
      ],
      "content": "<bpt id=\"p52\">[</bpt>SQL Azure Table<ept id=\"p52\">](#adf-table-azure-sql)</ept>"
    },
    {
      "pos": [
        9980,
        10282
      ],
      "content": "<ph id=\"ph37\">[AZURE.NOTE]</ph><ph id=\"ph38\"/>  The following procedures use Azure PowerShell to define and create the ADF activities. But these tasks can also be accomplished using the Azure Portal. For details, see <bpt id=\"p53\">[</bpt>Create input and output datasets<ept id=\"p53\">](data-factory-use-onpremises-datasources.md#step-3-create-input-and-output-datasets)</ept>.",
      "nodes": [
        {
          "content": "<ph id=\"ph37\">[AZURE.NOTE]</ph><ph id=\"ph38\"/>  The following procedures use Azure PowerShell to define and create the ADF activities.",
          "pos": [
            0,
            134
          ]
        },
        {
          "content": "But these tasks can also be accomplished using the Azure Portal.",
          "pos": [
            135,
            199
          ]
        },
        {
          "content": "For details, see <bpt id=\"p53\">[</bpt>Create input and output datasets<ept id=\"p53\">](data-factory-use-onpremises-datasources.md#step-3-create-input-and-output-datasets)</ept>.",
          "pos": [
            200,
            376
          ]
        }
      ]
    },
    {
      "pos": [
        10284,
        10287
      ],
      "content": "###"
    },
    {
      "pos": [
        10322,
        10342
      ],
      "content": "SQL on-premise Table"
    },
    {
      "pos": [
        10344,
        10435
      ],
      "content": "The table definition for the on-premise SQL Server is specified in the following JSON file."
    },
    {
      "pos": [
        11103,
        11310
      ],
      "content": "Note that the column names were not included here, you can sub-select on the column names by including them here (for details please check the <bpt id=\"p54\">[</bpt>ADF documentation<ept id=\"p54\">](data-factory-data-movement-activities.md )</ept>)."
    },
    {
      "pos": [
        11312,
        11547
      ],
      "content": "Copy the JSON definition of the table into a file called <bpt id=\"p55\">*</bpt>onpremtabledef.json<ept id=\"p55\">*</ept><ph id=\"ph39\"/> file and save it to a known location (here assumed to be <bpt id=\"p56\">*</bpt>C:\\temp\\onpremtabledef.json<ept id=\"p56\">*</ept>). Create the table in ADF with the following Azure PowerShell cmdlet.",
      "nodes": [
        {
          "content": "Copy the JSON definition of the table into a file called <bpt id=\"p55\">*</bpt>onpremtabledef.json<ept id=\"p55\">*</ept><ph id=\"ph39\"/> file and save it to a known location (here assumed to be <bpt id=\"p56\">*</bpt>C:\\temp\\onpremtabledef.json<ept id=\"p56\">*</ept>).",
          "pos": [
            0,
            262
          ]
        },
        {
          "content": "Create the table in ADF with the following Azure PowerShell cmdlet.",
          "pos": [
            263,
            330
          ]
        }
      ]
    },
    {
      "pos": [
        11667,
        11670
      ],
      "content": "###"
    },
    {
      "pos": [
        11705,
        11715
      ],
      "content": "Blob Table"
    },
    {
      "pos": [
        11716,
        11850
      ],
      "content": "Definition for the table for the output blob location is in the following (this maps the ingested data from on-premise to Azure blob):"
    },
    {
      "pos": [
        12443,
        12686
      ],
      "content": "Copy the JSON definition of the table into a file called <bpt id=\"p57\">*</bpt>bloboutputtabledef.json<ept id=\"p57\">*</ept><ph id=\"ph40\"/> file and save it to a known location (here assumed to be <bpt id=\"p58\">*</bpt>C:\\temp\\bloboutputtabledef.json<ept id=\"p58\">*</ept>). Create the table in ADF with the following Azure PowerShell cmdlet.",
      "nodes": [
        {
          "content": "Copy the JSON definition of the table into a file called <bpt id=\"p57\">*</bpt>bloboutputtabledef.json<ept id=\"p57\">*</ept><ph id=\"ph40\"/> file and save it to a known location (here assumed to be <bpt id=\"p58\">*</bpt>C:\\temp\\bloboutputtabledef.json<ept id=\"p58\">*</ept>).",
          "pos": [
            0,
            270
          ]
        },
        {
          "content": "Create the table in ADF with the following Azure PowerShell cmdlet.",
          "pos": [
            271,
            338
          ]
        }
      ]
    },
    {
      "pos": [
        12811,
        12814
      ],
      "content": "###"
    },
    {
      "pos": [
        12847,
        12862
      ],
      "content": "SQL Azure Table"
    },
    {
      "pos": [
        12863,
        12982
      ],
      "content": "Definition for the table for the SQL Azure output is in the following (this schema maps the data coming from the blob):"
    },
    {
      "pos": [
        13610,
        13843
      ],
      "content": "Copy the JSON definition of the table into a file called <bpt id=\"p59\">*</bpt>AzureSqlTable.json<ept id=\"p59\">*</ept><ph id=\"ph41\"/> file and save it to a known location (here assumed to be <bpt id=\"p60\">*</bpt>C:\\temp\\AzureSqlTable.json<ept id=\"p60\">*</ept>). Create the table in ADF with the following Azure PowerShell cmdlet.",
      "nodes": [
        {
          "content": "Copy the JSON definition of the table into a file called <bpt id=\"p59\">*</bpt>AzureSqlTable.json<ept id=\"p59\">*</ept><ph id=\"ph41\"/> file and save it to a known location (here assumed to be <bpt id=\"p60\">*</bpt>C:\\temp\\AzureSqlTable.json<ept id=\"p60\">*</ept>).",
          "pos": [
            0,
            260
          ]
        },
        {
          "content": "Create the table in ADF with the following Azure PowerShell cmdlet.",
          "pos": [
            261,
            328
          ]
        }
      ]
    },
    {
      "pos": [
        13963,
        13965
      ],
      "content": "##"
    },
    {
      "pos": [
        13992,
        14022
      ],
      "content": "Define and create the pipeline"
    },
    {
      "pos": [
        14024,
        14197
      ],
      "content": "Specify the activities that belong to the pipeline and create the pipeline with the following script-based procedures. A JSON file is used to define the pipeline properties.",
      "nodes": [
        {
          "content": "Specify the activities that belong to the pipeline and create the pipeline with the following script-based procedures.",
          "pos": [
            0,
            118
          ]
        },
        {
          "content": "A JSON file is used to define the pipeline properties.",
          "pos": [
            119,
            173
          ]
        }
      ]
    },
    {
      "pos": [
        14201,
        14273
      ],
      "content": "The script assumes that the <bpt id=\"p61\">**</bpt>pipeline name<ept id=\"p61\">**</ept><ph id=\"ph42\"/> is <bpt id=\"p62\">*</bpt>AMLDSProcessPipeline<ept id=\"p62\">*</ept>."
    },
    {
      "pos": [
        14276,
        14419
      ],
      "content": "Also note that we set the periodicity of the pipeline to be executed on daily basis and use the default execution time for the job (12 am UTC)."
    },
    {
      "pos": [
        14423,
        14723
      ],
      "content": "<ph id=\"ph43\">[AZURE.NOTE]</ph><ph id=\"ph44\"/>  The following procedures use Azure PowerShell to define and create the ADF pipeline. But this task can also be accomplished using the Azure Portal. For details, see <bpt id=\"p63\">[</bpt>Create and run a pipeline<ept id=\"p63\">](../data-factory/data-factory-use-onpremises-datasources.md#step-4-create-and-run-a-pipeline)</ept>.",
      "nodes": [
        {
          "content": "<ph id=\"ph43\">[AZURE.NOTE]</ph><ph id=\"ph44\"/>  The following procedures use Azure PowerShell to define and create the ADF pipeline.",
          "pos": [
            0,
            132
          ]
        },
        {
          "content": "But this task can also be accomplished using the Azure Portal.",
          "pos": [
            133,
            195
          ]
        },
        {
          "content": "For details, see <bpt id=\"p63\">[</bpt>Create and run a pipeline<ept id=\"p63\">](../data-factory/data-factory-use-onpremises-datasources.md#step-4-create-and-run-a-pipeline)</ept>.",
          "pos": [
            196,
            374
          ]
        }
      ]
    },
    {
      "pos": [
        14725,
        14829
      ],
      "content": "Using the table definitions provided above, the pipeline definition for the ADF is specified as follows:"
    },
    {
      "pos": [
        17601,
        17837
      ],
      "content": "Copy this JSON definition of the pipeline into a file called <bpt id=\"p64\">*</bpt>pipelinedef.json<ept id=\"p64\">*</ept><ph id=\"ph45\"/> file and save it to a known location (here assumed to be <bpt id=\"p65\">*</bpt>C:\\temp\\pipelinedef.json<ept id=\"p65\">*</ept>). Create the pipeline in ADF with the following Azure PowerShell cmdlet.",
      "nodes": [
        {
          "content": "Copy this JSON definition of the pipeline into a file called <bpt id=\"p64\">*</bpt>pipelinedef.json<ept id=\"p64\">*</ept><ph id=\"ph45\"/> file and save it to a known location (here assumed to be <bpt id=\"p65\">*</bpt>C:\\temp\\pipelinedef.json<ept id=\"p65\">*</ept>).",
          "pos": [
            0,
            260
          ]
        },
        {
          "content": "Create the pipeline in ADF with the following Azure PowerShell cmdlet.",
          "pos": [
            261,
            331
          ]
        }
      ]
    },
    {
      "pos": [
        17957,
        18086
      ],
      "content": "Confirm that you can see the pipeline on the ADF in the Azure Classic Portal show up as following (when you click on the diagram)"
    },
    {
      "pos": [
        18161,
        18163
      ],
      "content": "##"
    },
    {
      "pos": [
        18196,
        18214
      ],
      "content": "Start the Pipeline"
    },
    {
      "pos": [
        18215,
        18271
      ],
      "content": "The pipeline can now be run using the following command:"
    },
    {
      "pos": [
        18446,
        18578
      ],
      "content": "The <bpt id=\"p66\">*</bpt>startdate<ept id=\"p66\">*</ept><ph id=\"ph47\"/> and <bpt id=\"p67\">*</bpt>enddate<ept id=\"p67\">*</ept><ph id=\"ph48\"/> parameter values need to be replaced with the actual dates between which you want the pipeline to run."
    },
    {
      "pos": [
        18580,
        18708
      ],
      "content": "Once the pipeline executes, you should be able to see the data show up in the container selected for the blob, one file per day."
    },
    {
      "pos": [
        18710,
        18959
      ],
      "content": "Note that we have not leveraged the functionality provided by ADF to pipe data incrementally. For more details on how to do this and other capabilities provided by ADF, see the <bpt id=\"p68\">[</bpt>ADF documentation<ept id=\"p68\">](https://azure.microsoft.com/services/data-factory/)</ept>.",
      "nodes": [
        {
          "content": "Note that we have not leveraged the functionality provided by ADF to pipe data incrementally.",
          "pos": [
            0,
            93
          ]
        },
        {
          "content": "For more details on how to do this and other capabilities provided by ADF, see the <bpt id=\"p68\">[</bpt>ADF documentation<ept id=\"p68\">](https://azure.microsoft.com/services/data-factory/)</ept>.",
          "pos": [
            94,
            289
          ]
        }
      ]
    }
  ],
  "content": "<properties\n    pageTitle=\"Move data from an on-premise SQL Server to SQL Azure with Azure Data Factory | Azure\"\n    description=\"Set up an ADF pipeline that composes two data migration activities that together move data on a daily basis between databases on-premise and in the cloud.\"\n    services=\"machine-learning\"\n    documentationCenter=\"\"\n    authors=\"fashah\"\n    manager=\"jacob.spoelstra\"\n    editor=\"\"\n    videoId=\"\"\n    scriptId=\"\" />\n\n<tags\n    ms.service=\"machine-learning\"\n    ms.workload=\"data-services\"\n    ms.tgt_pltfrm=\"na\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\"\n    ms.date=\"02/08/2016\"\n    ms.author=\"fashah;bradsev\" />\n\n\n# Move data from an on-premise SQL server to SQL Azure with Azure Data Factory\n\nThis topic shows how to move data from an on-premise SQL Server Database to a SQL Azure Database via Azure Blob Storage using the Azure Data Factory (ADF).\n\nThe **menu** below links to topics that describe how to ingest data into other target environments where the data can be stored and processed during the Cortana Analytics Process (CAPS).\n\n[AZURE.INCLUDE [cap-ingest-data-selector](../../includes/cap-ingest-data-selector.md)]\n\n## <a name=\"intro\"></a>Introduction: What is ADF and when should it be used to migrate data?\n\nAzure Data Factory is a fully managed cloud-based data integration service that orchestrates and automates the movement and transformation of data. The key concept in the ADF model is pipeline. A pipelines is a logical groupings of Activities, each of which defines the actions to perform on the data contained in Datasets. Linked services are used to define the information needed for Data Factory to connect to the data resources.\n\nWith ADF, existing data processing services can be composed into data pipelines that are highly available and managed in the cloud. These data pipelines can be scheduled to ingest, prepare, transform, analyze, and publish data, and ADF will manage and orchestrate all of the complex data and processing dependencies. Solutions can be quickly built and deployed in the cloud, connecting a growing number of on-premises and cloud data sources.\n\nConsider using ADF when data needs to be continually migrated in a hybrid scenario that accesses both on-premise and cloud resources, and when the data is transacted or needs to be modified or have business logic added to it in the course of being migrated. ADF allows for the scheduling and monitoring of jobs using simple JSON scripts that manage the movement of data on a periodic basis. ADF also has other capabilities such as support for complex operations. For more information on ADF, see the documentation at [Azure Data Factory (ADF)](https://azure.microsoft.com/services/data-factory/).\n\n## <a name=\"scenario\"></a>The Scenario\n\nWe set up an ADF pipeline that composes two data migration activities that together move data on a daily basis between an on-premise SQL database and an Azure SQL Database in the cloud. The two activities are:\n\n* copy data from an on-premise SQL Server database to an Azure Blob Storage account\n* copy data from the Azure Blob Storage account to an Azure SQL Database.\n\n**Reference**: The steps shown here have been adapted from the more detailed tutorial [Enable your pipelines to work with on-premises data](data-factory-use-onpremises-datasources.md) provided by the ADF team and references to the relevant sections of that topic are provided when appropriate.\n\n\n## <a name=\"prereqs\"></a>Prerequisites\nThis tutorial assumes you have:\n\n* An **Azure subscription**. If you do not have a subscription, you can sign up for a [free trial](https://azure.microsoft.com/pricing/free-trial/).\n* An **Azure storage account**. You will use an Azure storage account for storing the data in this tutorial. If you don't have an Azure storage account, see the [Create a storage account](storage-create-storage-account.md#create-a-storage-account) article. After you have created the storage account, you will need to obtain the account key used to access the storage. See [View, copy and regenerate storage access keys](storage-create-storage-account.md#view-copy-and-regenerate-storage-access-keys).\n* Access to an **Azure SQL Database**. If you must setup an Azure SQL Database, [Getting Started with Microsoft Azure SQL Database ](sql-database-get-started.md) provides information on how to provision a new instance of a Azure SQL Database.\n* Installed and configured **Azure PowerShell** locally. For instructions, see [How to install and configure Azure PowerShell](powershell-install-configure.md).\n\n> [AZURE.NOTE] This procedure uses the [Azure Portal](https://ms.portal.azure.com/).\n\n##<a name=\"upload-data\"></a> Upload the data to your on-premise SQL Server\n\nWe use the [NYC Taxi dataset](http://chriswhong.com/open-data/foil_nyc_taxi/) to demonstrate the migration process. The NYC Taxi dataset is available, as noted that post, on Azure blob storage [NYC Taxi Data](http://www.andresmh.com/nyctaxitrips/). The data has two files, the trip_data.csv file which contains trip details and the  trip_far.csv file which contains details of the fare paid for each trip. A sample and description of these files are provided in [NYC Taxi Trips Dataset Description](machine-learning-data-science-process-sql-walkthrough.md#dataset).\n\n\nYou can either adapt the procedure provided here to a set of your own data or follow the steps as described by using the NYC Taxi dataset. To upload the NYC Taxi dataset into your on-premise SQL Server database, follow the procedure outlined in [Bulk Import Data into SQL Server Database](machine-learning-data-science-process-sql-walkthrough.md#dbload). These instructions are for a SQL Server on an Azure Virtual Machine, but the procedure for uploading to the on-premise SQL Server is the same.\n\n##<a name=\"create-adf\"></a> Create an Azure Data Factory\n\nThe instructions for creating a new Azure Data Factory and a resource group in the [Azure Portal](https://ms.portal.azure.com/) are provided [Create an Azure Data Factory](data-factory-build-your-first-pipeline-using-editor.md#step-1-creating-the-data-factory). Name the new ADF instance *adfdsp* and name the resource group created *adfdsprg*.\n\n## Install and configure up the Data Management Gateway\n\nTo enable your pipelines in an Azure data factory to work with an on-premise SQL Server, you need to add it as a linked service to the data factory. To create a Linked Service for the on-premise SQL Server, you must first download and install Microsoft Data Management Gateway onto the on-premise computer and configure the linked service for the on-premises data source to use the gateway. The Data Management Gateway serializes and deserializes the source and sink data on the computer where it is hosted.\n\nFor set-up instructions and details on Data Management Gateway, see [Enable your pipelines to work with on-premises data](data-factory-use-onpremises-datasources.md)\n\n\n## <a name=\"adflinkedservices\"></a>Create linked services to connect to the data resources\n\nA linked service defines the information needed for Azure Data Factory to connect to a data resource. The step-by-step procedure for creating linked services is provided in [Create linked services](data-factory-use-onpremises-datasources.md#step-2-create-linked-services).\n\nWe have three resources in this scenario for which linked services are needed.\n\n1. [Linked service for on-premise SQL Server](#adf-linked-service-onprem-sql)\n2. [Linked service for Azure Blob Storage](#adf-linked-service-blob-store)\n3. [Linked service for Azure SQL database](#adf-linked-service-azure-sql)\n\n\n###<a name=\"adf-linked-service-onprem-sql\"></a>Linked service for on-premise SQL Server database\nTo create the linked service for the on-premise SQL Server, click on the **Data Store** in the ADF landing page on Azure Classic Portal, select *SQL* and enter the credentials for the *username* and *password* for the on-premise SQL Server. You need to enter the servername as a **fully qualified servername backslash instance name (servername\\instancename)**. Name the linked service *adfonpremsql*.\n\n###<a name=\"adf-linked-service-blob-store\"></a>Linked service for Blob\nTo create the linked service for the Azure Blob Storage account, click on the **Data Store** in the ADF landing page on Azure Classic Portal, select *Azure Storage Account* and enter the Azure Blob Storage account key and container name. Name the link service *adfds*.\n\n###<a name=\"adf-linked-service-azure-sql\"></a>Linked service for Azure SQL database\nTo create the linked service for the Azure SQL Database, click on the **Data Store** in the ADF landing page on Azure Classic Portal, select *Azure SQL* and enter the credentials for the *username* and *password* for the Azure SQL Database. The *username* must be specified as *user@servername*.   \n\n\n##<a name=\"adf-tables\"></a>Define and create tables to specify how to access the datasets\n\nCreate tables that specify the structure, location and availability of the datasets with the following script-based procedures. JSON files are used to define the tables. For more information on the structure of these files, see [Datasets](data-factory-create-datasets.md).\n\n> [AZURE.NOTE]  You should execute the `Add-AzureAccount` cmdlet prior to executing the [New-AzureDataFactoryTable](https://msdn.microsoft.com/library/azure/dn835096.aspx) cmdlet to confirm that the right Azure subscription is selected for the command execution. For documentation of this cmdlet, see [Add-AzureAccount](https://msdn.microsoft.com/library/azure/dn790372.aspx).\n\nThe JSON-based definitions in the tables use the following names:\n\n* the **table name** in the on-premise SQL server is *nyctaxi_data*\n* the **container name** in the Azure Blob Storage account is *containername*  \n\nThree table definitions are needed for this ADF pipeline:\n\n1. [SQL on-premise Table](#adf-table-onprem-sql)\n2. [Blob Table ](#adf-table-blob-store)\n3. [SQL Azure Table](#adf-table-azure-sql)\n\n> [AZURE.NOTE]  The following procedures use Azure PowerShell to define and create the ADF activities. But these tasks can also be accomplished using the Azure Portal. For details, see [Create input and output datasets](data-factory-use-onpremises-datasources.md#step-3-create-input-and-output-datasets).\n\n###<a name=\"adf-table-onprem-sql\"></a>SQL on-premise Table\n\nThe table definition for the on-premise SQL Server is specified in the following JSON file.\n\n        {\n            \"name\": \"OnPremSQLTable\",\n            \"properties\":\n            {\n                \"location\":\n                {\n                \"type\": \"OnPremisesSqlServerTableLocation\",\n                \"tableName\": \"nyctaxi_data\",\n                \"linkedServiceName\": \"adfonpremsql\"\n                },\n                \"availability\":\n                {\n                \"frequency\": \"Day\",\n                \"interval\": 1,   \n                \"waitOnExternal\":\n                {\n                \"retryInterval\": \"00:01:00\",\n                \"retryTimeout\": \"00:10:00\",\n                \"maximumRetry\": 3\n                }\n\n                }\n            }\n        }\nNote that the column names were not included here, you can sub-select on the column names by including them here (for details please check the [ADF documentation](data-factory-data-movement-activities.md )).\n\nCopy the JSON definition of the table into a file called *onpremtabledef.json* file and save it to a known location (here assumed to be *C:\\temp\\onpremtabledef.json*). Create the table in ADF with the following Azure PowerShell cmdlet.\n\n    New-AzureDataFactoryTable -ResourceGroupName ADFdsprg -DataFactoryName ADFdsp File C:\\temp\\onpremtabledef.json\n\n\n###<a name=\"adf-table-blob-store\"></a>Blob Table\nDefinition for the table for the output blob location is in the following (this maps the ingested data from on-premise to Azure blob):\n\n        {\n            \"name\": \"OutputBlobTable\",\n            \"properties\":\n            {\n                \"location\":\n                {\n                \"type\": \"AzureBlobLocation\",\n                \"folderPath\": \"containername\",\n                \"format\":\n                {\n                \"type\": \"TextFormat\",\n                \"columnDelimiter\": \"\\t\"\n                },\n                \"linkedServiceName\": \"adfds\"\n                },\n                \"availability\":\n                {\n                \"frequency\": \"Day\",\n                \"interval\": 1\n                }\n            }\n        }\n\nCopy the JSON definition of the table into a file called *bloboutputtabledef.json* file and save it to a known location (here assumed to be *C:\\temp\\bloboutputtabledef.json*). Create the table in ADF with the following Azure PowerShell cmdlet.\n\n    New-AzureDataFactoryTable -ResourceGroupName adfdsprg -DataFactoryName adfdsp -File C:\\temp\\bloboutputtabledef.json  \n\n###<a name=\"adf-table-azure-sq\"></a>SQL Azure Table\nDefinition for the table for the SQL Azure output is in the following (this schema maps the data coming from the blob):\n\n    {\n        \"name\": \"OutputSQLAzureTable\",\n        \"properties\":\n        {\n            \"structure\":\n            [\n                { \"name\": \"column1\", type\": \"String\"},\n                { \"name\": \"column2\", type\": \"String\"}                \n            ],\n            \"location\":\n            {\n                \"type\": \"AzureSqlTableLocation\",\n                \"tableName\": \"your_db_name\",\n                \"linkedServiceName\": \"adfdssqlazure_linked_servicename\"\n            },\n            \"availability\":\n            {\n                \"frequency\": \"Day\",\n                \"interval\": 1            \n            }\n        }\n    }\n\nCopy the JSON definition of the table into a file called *AzureSqlTable.json* file and save it to a known location (here assumed to be *C:\\temp\\AzureSqlTable.json*). Create the table in ADF with the following Azure PowerShell cmdlet.\n\n    New-AzureDataFactoryTable -ResourceGroupName adfdsprg -DataFactoryName adfdsp -File C:\\temp\\AzureSqlTable.json  \n\n##<a name=\"adf-pipeline\"></a>Define and create the pipeline\n\nSpecify the activities that belong to the pipeline and create the pipeline with the following script-based procedures. A JSON file is used to define the pipeline properties.\n\n* The script assumes that the **pipeline name** is *AMLDSProcessPipeline*.\n* Also note that we set the periodicity of the pipeline to be executed on daily basis and use the default execution time for the job (12 am UTC).\n\n> [AZURE.NOTE]  The following procedures use Azure PowerShell to define and create the ADF pipeline. But this task can also be accomplished using the Azure Portal. For details, see [Create and run a pipeline](../data-factory/data-factory-use-onpremises-datasources.md#step-4-create-and-run-a-pipeline).\n\nUsing the table definitions provided above, the pipeline definition for the ADF is specified as follows:\n\n        {\n            \"name\": \"AMLDSProcessPipeline\",\n            \"properties\":\n            {\n                \"description\" : \"This pipeline has one Copy activity that copies data from an on-premise SQL to Azure blob\",\n                 \"activities\":\n                [\n                    {\n                        \"name\": \"CopyFromSQLtoBlob\",\n                        \"description\": \"Copy data from on-premise SQL server to blob\",     \n                        \"type\": \"CopyActivity\",\n                        \"inputs\": [ {\"name\": \"OnPremSQLTable\"} ],\n                        \"outputs\": [ {\"name\": \"OutputBlobTable\"} ],\n                        \"transformation\":\n                        {\n                            \"source\":\n                            {                               \n                                \"type\": \"SqlSource\",\n                                \"sqlReaderQuery\": \"select * from nyctaxi_data\"\n                            },\n                            \"sink\":\n                            {\n                                \"type\": \"BlobSink\"\n                            }   \n                        },\n                        \"Policy\":\n                        {\n                            \"concurrency\": 3,\n                            \"executionPriorityOrder\": \"NewestFirst\",\n                            \"style\": \"StartOfInterval\",\n                            \"retry\": 0,\n                            \"timeout\": \"01:00:00\"\n                        }       \n\n                     },\n\n                    {\n                        \"name\": \"CopyFromBlobtoSQLAzure\",\n                        \"description\": \"Push data to Sql Azure\",        \n                        \"type\": \"CopyActivity\",\n                        \"inputs\": [ {\"name\": \"OutputBlobTable\"} ],\n                        \"outputs\": [ {\"name\": \"OutputSQLAzureTable\"} ],\n                        \"transformation\":\n                        {\n                            \"source\":\n                            {                               \n                                \"type\": \"BlobSource\"\n                            },\n                            \"sink\":\n                            {\n                                \"type\": \"SqlSink\",\n                                \"WriteBatchTimeout\": \"00:5:00\",             \n                            }           \n                        },\n                        \"Policy\":\n                        {\n                            \"concurrency\": 3,\n                            \"executionPriorityOrder\": \"NewestFirst\",\n                            \"style\": \"StartOfInterval\",\n                            \"retry\": 2,\n                            \"timeout\": \"02:00:00\"\n                        }\n                     }\n                ]\n            }\n        }\n\nCopy this JSON definition of the pipeline into a file called *pipelinedef.json* file and save it to a known location (here assumed to be *C:\\temp\\pipelinedef.json*). Create the pipeline in ADF with the following Azure PowerShell cmdlet.\n\n    New-AzureDataFactoryPipeline  -ResourceGroupName adfdsprg -DataFactoryName adfdsp -File C:\\temp\\pipelinedef.json\n\nConfirm that you can see the pipeline on the ADF in the Azure Classic Portal show up as following (when you click on the diagram)\n\n![](media/machine-learning-data-science-move-sql-azure-adf/DJP1kji.png)\n\n##<a name=\"adf-pipeline-start\"></a>Start the Pipeline\nThe pipeline can now be run using the following command:\n\n    Set-AzureDataFactoryPipelineActivePeriod -ResourceGroupName ADFdsprg -DataFactoryName ADFdsp -StartDateTime startdateZ EndDateTime enddateZ Name AMLDSProcessPipeline\n\nThe *startdate* and *enddate* parameter values need to be replaced with the actual dates between which you want the pipeline to run.\n\nOnce the pipeline executes, you should be able to see the data show up in the container selected for the blob, one file per day.\n\nNote that we have not leveraged the functionality provided by ADF to pipe data incrementally. For more details on how to do this and other capabilities provided by ADF, see the [ADF documentation](https://azure.microsoft.com/services/data-factory/).\n"
}