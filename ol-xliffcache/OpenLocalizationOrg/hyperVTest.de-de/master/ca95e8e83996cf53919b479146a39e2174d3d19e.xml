{
  "nodes": [
    {
      "pos": [
        27,
        74
      ],
      "content": "Use Hadoop Oozie in HDInsight | Microsoft Azure"
    },
    {
      "pos": [
        93,
        207
      ],
      "content": "Use Hadoop Oozie in HDInsight, a big data service. Learn how to define an Oozie workflow, and submit an Oozie job.",
      "nodes": [
        {
          "content": "Use Hadoop Oozie in HDInsight, a big data service.",
          "pos": [
            0,
            50
          ]
        },
        {
          "content": "Learn how to define an Oozie workflow, and submit an Oozie job.",
          "pos": [
            51,
            114
          ]
        }
      ]
    },
    {
      "pos": [
        532,
        595
      ],
      "content": "Use Oozie with Hadoop to define and run a workflow in HDInsight"
    },
    {
      "pos": [
        675,
        994
      ],
      "content": "Learn how to use Apache Oozie to define a workflow and run the workflow on HDInsight. To learn about the Oozie coordinator, see <bpt id=\"p1\">[</bpt>Use time-based Hadoop Oozie Coordinator with HDInsight<ept id=\"p1\">][hdinsight-oozie-coordinator-time]</ept>. To learn Azure Data Factory, see <bpt id=\"p2\">[</bpt>Use Pig and Hive with Data Factory<ept id=\"p2\">][azure-data-factory-pig-hive]</ept>.",
      "nodes": [
        {
          "content": "Learn how to use Apache Oozie to define a workflow and run the workflow on HDInsight.",
          "pos": [
            0,
            85
          ]
        },
        {
          "content": "To learn about the Oozie coordinator, see <bpt id=\"p1\">[</bpt>Use time-based Hadoop Oozie Coordinator with HDInsight<ept id=\"p1\">][hdinsight-oozie-coordinator-time]</ept>.",
          "pos": [
            86,
            257
          ]
        },
        {
          "content": "To learn Azure Data Factory, see <bpt id=\"p2\">[</bpt>Use Pig and Hive with Data Factory<ept id=\"p2\">][azure-data-factory-pig-hive]</ept>.",
          "pos": [
            258,
            395
          ]
        }
      ]
    },
    {
      "pos": [
        996,
        1306
      ],
      "content": "Apache Oozie is a workflow/coordination system that manages Hadoop jobs. It is integrated with the Hadoop stack, and it supports Hadoop jobs for Apache MapReduce, Apache Pig, Apache Hive, and Apache Sqoop. It can also be used to schedule jobs that are specific to a system, like Java programs or shell scripts.",
      "nodes": [
        {
          "content": "Apache Oozie is a workflow/coordination system that manages Hadoop jobs.",
          "pos": [
            0,
            72
          ]
        },
        {
          "content": "It is integrated with the Hadoop stack, and it supports Hadoop jobs for Apache MapReduce, Apache Pig, Apache Hive, and Apache Sqoop.",
          "pos": [
            73,
            205
          ]
        },
        {
          "content": "It can also be used to schedule jobs that are specific to a system, like Java programs or shell scripts.",
          "pos": [
            206,
            310
          ]
        }
      ]
    },
    {
      "pos": [
        1308,
        1408
      ],
      "content": "The workflow you will implement by following the instructions in this tutorial contains two actions:"
    },
    {
      "pos": [
        1410,
        1451
      ],
      "content": "<ph id=\"ph3\">![</ph>Workflow diagram<ph id=\"ph4\">][img-workflow-diagram]</ph>"
    },
    {
      "pos": [
        1456,
        1685
      ],
      "content": "A Hive action runs a HiveQL script to count the occurrences of each log-level type in a log4j file. Each log4j file consists of a line of fields that contains a [LOG LEVEL] field that shows the type and the severity, for example:",
      "nodes": [
        {
          "content": "A Hive action runs a HiveQL script to count the occurrences of each log-level type in a log4j file.",
          "pos": [
            0,
            99
          ]
        },
        {
          "content": "Each log4j file consists of a line of fields that contains a [LOG LEVEL] field that shows the type and the severity, for example:",
          "pos": [
            100,
            229
          ]
        }
      ]
    },
    {
      "pos": [
        1942,
        1979
      ],
      "content": "The Hive script output is similar to:"
    },
    {
      "pos": [
        2099,
        2182
      ],
      "content": "For more information about Hive, see <bpt id=\"p3\">[</bpt>Use Hive with HDInsight<ept id=\"p3\">][hdinsight-use-hive]</ept>."
    },
    {
      "pos": [
        2188,
        2359
      ],
      "content": "A Sqoop action exports the HiveQL output to a table in an Azure SQL database. For more information about Sqoop, see <bpt id=\"p4\">[</bpt>Use Hadoop Sqoop with HDInsight<ept id=\"p4\">][hdinsight-use-sqoop]</ept>.",
      "nodes": [
        {
          "content": "A Sqoop action exports the HiveQL output to a table in an Azure SQL database.",
          "pos": [
            0,
            77
          ]
        },
        {
          "content": "For more information about Sqoop, see <bpt id=\"p4\">[</bpt>Use Hadoop Sqoop with HDInsight<ept id=\"p4\">][hdinsight-use-sqoop]</ept>.",
          "pos": [
            78,
            209
          ]
        }
      ]
    },
    {
      "pos": [
        2363,
        2519
      ],
      "content": "<ph id=\"ph5\">[AZURE.NOTE]</ph><ph id=\"ph6\"/> For supported Oozie versions on HDInsight clusters, see <bpt id=\"p5\">[</bpt>What's new in the Hadoop cluster versions provided by HDInsight?<ept id=\"p5\">][hdinsight-versions]</ept>."
    },
    {
      "pos": [
        2524,
        2537
      ],
      "content": "Prerequisites"
    },
    {
      "pos": [
        2539,
        2599
      ],
      "content": "Before you begin this tutorial, you must have the following:"
    },
    {
      "pos": [
        2603,
        2966
      ],
      "content": "<bpt id=\"p6\">**</bpt>A workstation with Azure PowerShell<ept id=\"p6\">**</ept>. See <bpt id=\"p7\">[</bpt>Install and use Azure PowerShell<ept id=\"p7\">](https://azure.microsoft.com/documentation/videos/install-and-use-azure-powershell/)</ept>. To execute Windows PowerShell scripts, you must run as an administrator and set the execution policy to <bpt id=\"p8\">*</bpt>RemoteSigned<ept id=\"p8\">*</ept>. For more information, see <bpt id=\"p9\">[</bpt>Run Windows PowerShell scripts<ept id=\"p9\">][powershell-script]</ept>.",
      "nodes": [
        {
          "content": "<bpt id=\"p6\">**</bpt>A workstation with Azure PowerShell<ept id=\"p6\">**</ept>.",
          "pos": [
            0,
            78
          ]
        },
        {
          "content": "See <bpt id=\"p7\">[</bpt>Install and use Azure PowerShell<ept id=\"p7\">](https://azure.microsoft.com/documentation/videos/install-and-use-azure-powershell/)</ept>.",
          "pos": [
            79,
            240
          ]
        },
        {
          "content": "To execute Windows PowerShell scripts, you must run as an administrator and set the execution policy to <bpt id=\"p8\">*</bpt>RemoteSigned<ept id=\"p8\">*</ept>.",
          "pos": [
            241,
            398
          ]
        },
        {
          "content": "For more information, see <bpt id=\"p9\">[</bpt>Run Windows PowerShell scripts<ept id=\"p9\">][powershell-script]</ept>.",
          "pos": [
            399,
            515
          ]
        }
      ]
    },
    {
      "pos": [
        2970,
        3021
      ],
      "content": "Define Oozie workflow and the related HiveQL script"
    },
    {
      "pos": [
        3023,
        3223
      ],
      "content": "Oozie workflows definitions are written in hPDL (a XML Process Definition Language). The default workflow file name is <bpt id=\"p10\">*</bpt>workflow.xml<ept id=\"p10\">*</ept>. The following is the workflow file you will use in this tutorial.",
      "nodes": [
        {
          "content": "Oozie workflows definitions are written in hPDL (a XML Process Definition Language).",
          "pos": [
            0,
            84
          ]
        },
        {
          "content": "The default workflow file name is <bpt id=\"p10\">*</bpt>workflow.xml<ept id=\"p10\">*</ept>.",
          "pos": [
            85,
            174
          ]
        },
        {
          "content": "The following is the workflow file you will use in this tutorial.",
          "pos": [
            175,
            240
          ]
        }
      ]
    },
    {
      "pos": [
        5251,
        5407
      ],
      "content": "There are two actions defined in the workflow. The start-to action is <bpt id=\"p11\">*</bpt>RunHiveScript<ept id=\"p11\">*</ept>. If the action runs successfully, the next action is <bpt id=\"p12\">*</bpt>RunSqoopExport<ept id=\"p12\">*</ept>.",
      "nodes": [
        {
          "content": "There are two actions defined in the workflow.",
          "pos": [
            0,
            46
          ]
        },
        {
          "content": "The start-to action is <bpt id=\"p11\">*</bpt>RunHiveScript<ept id=\"p11\">*</ept>.",
          "pos": [
            47,
            126
          ]
        },
        {
          "content": "If the action runs successfully, the next action is <bpt id=\"p12\">*</bpt>RunSqoopExport<ept id=\"p12\">*</ept>.",
          "pos": [
            127,
            236
          ]
        }
      ]
    },
    {
      "pos": [
        5409,
        5553
      ],
      "content": "The RunHiveScript has several variables. You will pass the values when you submit the Oozie job from your workstation by using Azure PowerShell.",
      "nodes": [
        {
          "content": "The RunHiveScript has several variables.",
          "pos": [
            0,
            40
          ]
        },
        {
          "content": "You will pass the values when you submit the Oozie job from your workstation by using Azure PowerShell.",
          "pos": [
            41,
            144
          ]
        }
      ]
    },
    {
      "pos": [
        5575,
        5576
      ],
      "content": "\n"
    },
    {
      "pos": [
        5584,
        5602
      ],
      "content": "Workflow variables"
    },
    {
      "pos": [
        5611,
        5622
      ],
      "content": "Description"
    },
    {
      "pos": [
        5632,
        5633
      ],
      "content": "\n"
    },
    {
      "pos": [
        5641,
        5654
      ],
      "content": "${jobTracker}"
    },
    {
      "pos": [
        5663,
        5712
      ],
      "content": "Specifies the URL of the Hadoop job tracker. Use ",
      "nodes": [
        {
          "content": "Specifies the URL of the Hadoop job tracker.",
          "pos": [
            0,
            44
          ]
        },
        {
          "content": "Use",
          "pos": [
            45,
            48
          ]
        }
      ]
    },
    {
      "pos": [
        5720,
        5739
      ],
      "content": "jobtrackerhost:9010"
    },
    {
      "pos": [
        5749,
        5782
      ],
      "content": "in HDInsight version 3.0 and 2.1."
    },
    {
      "pos": [
        5792,
        5793
      ],
      "content": "\n"
    },
    {
      "pos": [
        5801,
        5812
      ],
      "content": "${nameNode}"
    },
    {
      "pos": [
        5821,
        5914
      ],
      "content": "Specifies the URL of the Hadoop name node. Use the default file system address, for example, ",
      "nodes": [
        {
          "content": "Specifies the URL of the Hadoop name node.",
          "pos": [
            0,
            42
          ]
        },
        {
          "content": "Use the default file system address, for example,",
          "pos": [
            43,
            92
          ]
        }
      ]
    },
    {
      "pos": [
        5917,
        5994
      ],
      "content": "wasb://&amp;lt;containerName&amp;gt;@&amp;lt;storageAccountName&amp;gt;.blob.core.windows.net"
    },
    {
      "pos": [
        5998,
        5999
      ],
      "content": "."
    },
    {
      "pos": [
        6009,
        6010
      ],
      "content": "\n"
    },
    {
      "pos": [
        6018,
        6030
      ],
      "content": "${queueName}"
    },
    {
      "pos": [
        6039,
        6107
      ],
      "content": "Specifies the queue name that the job will be submitted to. Use the ",
      "nodes": [
        {
          "content": "Specifies the queue name that the job will be submitted to.",
          "pos": [
            0,
            59
          ]
        },
        {
          "content": "Use the",
          "pos": [
            60,
            67
          ]
        }
      ]
    },
    {
      "pos": [
        6115,
        6122
      ],
      "content": "default"
    },
    {
      "pos": [
        6131,
        6132
      ],
      "content": "."
    },
    {
      "pos": [
        6142,
        6143
      ],
      "content": "\n"
    },
    {
      "pos": [
        6173,
        6174
      ],
      "content": "\n"
    },
    {
      "pos": [
        6182,
        6202
      ],
      "content": "Hive action variable"
    },
    {
      "pos": [
        6211,
        6222
      ],
      "content": "Description"
    },
    {
      "pos": [
        6232,
        6233
      ],
      "content": "\n"
    },
    {
      "pos": [
        6241,
        6258
      ],
      "content": "${hiveDataFolder}"
    },
    {
      "pos": [
        6267,
        6332
      ],
      "content": "Specifies the source directory for the Hive Create Table command."
    },
    {
      "pos": [
        6342,
        6343
      ],
      "content": "\n"
    },
    {
      "pos": [
        6351,
        6370
      ],
      "content": "${hiveOutputFolder}"
    },
    {
      "pos": [
        6379,
        6442
      ],
      "content": "Specifies the output folder for the INSERT OVERWRITE statement."
    },
    {
      "pos": [
        6452,
        6453
      ],
      "content": "\n"
    },
    {
      "pos": [
        6461,
        6477
      ],
      "content": "${hiveTableName}"
    },
    {
      "pos": [
        6486,
        6560
      ],
      "content": "Specifies the name of the Hive table that references the log4j data files."
    },
    {
      "pos": [
        6570,
        6571
      ],
      "content": "\n"
    },
    {
      "pos": [
        6601,
        6602
      ],
      "content": "\n"
    },
    {
      "pos": [
        6610,
        6631
      ],
      "content": "Sqoop action variable"
    },
    {
      "pos": [
        6640,
        6651
      ],
      "content": "Description"
    },
    {
      "pos": [
        6661,
        6662
      ],
      "content": "\n"
    },
    {
      "pos": [
        6670,
        6700
      ],
      "content": "${sqlDatabaseConnectionString}"
    },
    {
      "pos": [
        6709,
        6760
      ],
      "content": "Specifies the Azure SQL database connection string."
    },
    {
      "pos": [
        6770,
        6771
      ],
      "content": "\n"
    },
    {
      "pos": [
        6779,
        6802
      ],
      "content": "${sqlDatabaseTableName}"
    },
    {
      "pos": [
        6811,
        6885
      ],
      "content": "Specifies the Azure SQL database table where the data will be exported to."
    },
    {
      "pos": [
        6895,
        6896
      ],
      "content": "\n"
    },
    {
      "pos": [
        6904,
        6923
      ],
      "content": "${hiveOutputFolder}"
    },
    {
      "pos": [
        6932,
        7059
      ],
      "content": "Specifies the output folder for the Hive INSERT OVERWRITE statement. This is the same folder for the Sqoop export (export-dir).",
      "nodes": [
        {
          "content": "Specifies the output folder for the Hive INSERT OVERWRITE statement.",
          "pos": [
            0,
            68
          ]
        },
        {
          "content": "This is the same folder for the Sqoop export (export-dir).",
          "pos": [
            69,
            127
          ]
        }
      ]
    },
    {
      "pos": [
        7069,
        7070
      ],
      "content": "\n"
    },
    {
      "pos": [
        7080,
        7317
      ],
      "content": "For more information about Oozie workflow and using workflow actions, see <bpt id=\"p13\">[</bpt>Apache Oozie 4.0 documentation<ept id=\"p13\">][apache-oozie-400]</ept><ph id=\"ph7\"/> (for HDInsight version 3.0) or <bpt id=\"p14\">[</bpt>Apache Oozie 3.3.2 documentation<ept id=\"p14\">][apache-oozie-332]</ept><ph id=\"ph8\"/> (for HDInsight version 2.1)."
    },
    {
      "pos": [
        7320,
        7430
      ],
      "content": "The Hive action in the workflow calls a HiveQL script file. This script file contains three HiveQL statements:",
      "nodes": [
        {
          "content": "The Hive action in the workflow calls a HiveQL script file.",
          "pos": [
            0,
            59
          ]
        },
        {
          "content": "This script file contains three HiveQL statements:",
          "pos": [
            60,
            110
          ]
        }
      ]
    },
    {
      "pos": [
        7825,
        7896
      ],
      "content": "<bpt id=\"p15\">**</bpt>The DROP TABLE statement<ept id=\"p15\">**</ept><ph id=\"ph9\"/> deletes the log4j Hive table if it exists."
    },
    {
      "pos": [
        7900,
        8229
      ],
      "content": "<bpt id=\"p16\">**</bpt>The CREATE TABLE statement<ept id=\"p16\">**</ept><ph id=\"ph10\"/> creates a log4j Hive external table that points to the location of the log4j log file. The field delimiter is \",\". The default line delimiter is \"\\n\". A Hive external table is used to avoid the data file being removed from the original location if you want to run the Oozie workflow multiple times.",
      "nodes": [
        {
          "content": "<bpt id=\"p16\">**</bpt>The CREATE TABLE statement<ept id=\"p16\">**</ept><ph id=\"ph10\"/> creates a log4j Hive external table that points to the location of the log4j log file.",
          "pos": [
            0,
            172
          ]
        },
        {
          "content": "The field delimiter is \",\".",
          "pos": [
            173,
            200
          ]
        },
        {
          "content": "The default line delimiter is \"\\n\".",
          "pos": [
            201,
            236
          ]
        },
        {
          "content": "A Hive external table is used to avoid the data file being removed from the original location if you want to run the Oozie workflow multiple times.",
          "pos": [
            237,
            384
          ]
        }
      ]
    },
    {
      "pos": [
        8233,
        8389
      ],
      "content": "<bpt id=\"p17\">**</bpt>The INSERT OVERWRITE statement<ept id=\"p17\">**</ept><ph id=\"ph11\"/> counts the occurrences of each log-level type from the log4j Hive table, and saves the output to a blob in Azure Storage."
    },
    {
      "pos": [
        8392,
        8437
      ],
      "content": "There are three variables used in the script:"
    },
    {
      "pos": [
        8441,
        8457
      ],
      "content": "${hiveTableName}"
    },
    {
      "pos": [
        8460,
        8477
      ],
      "content": "${hiveDataFolder}"
    },
    {
      "pos": [
        8480,
        8499
      ],
      "content": "${hiveOutputFolder}"
    },
    {
      "pos": [
        8501,
        8616
      ],
      "content": "The workflow definition file (workflow.xml in this tutorial) passes these values to this HiveQL script at run time."
    },
    {
      "pos": [
        8618,
        8804
      ],
      "content": "Both the workflow file and the HiveQL file are stored in a blob container.  The PowerShell script you will use later in this tutorial will copy both files to the default Storage account.",
      "nodes": [
        {
          "content": "Both the workflow file and the HiveQL file are stored in a blob container.",
          "pos": [
            0,
            74
          ]
        },
        {
          "content": "The PowerShell script you will use later in this tutorial will copy both files to the default Storage account.",
          "pos": [
            76,
            186
          ]
        }
      ]
    },
    {
      "pos": [
        8809,
        8843
      ],
      "content": "Submit Oozie jobs using PowerShell"
    },
    {
      "pos": [
        8845,
        9274
      ],
      "content": "Azure PowerShell currently doesn't provide any cmdlets for defining Oozie jobs. You can use the <bpt id=\"p18\">**</bpt>Invoke-RestMethod<ept id=\"p18\">**</ept><ph id=\"ph12\"/> cmdlet to invoke Oozie web services. The Oozie web services API is a HTTP REST JSON API. For more information about the Oozie web services API, see <bpt id=\"p19\">[</bpt>Apache Oozie 4.0 documentation<ept id=\"p19\">][apache-oozie-400]</ept><ph id=\"ph13\"/> (for HDInsight version 3.0) or <bpt id=\"p20\">[</bpt>Apache Oozie 3.3.2 documentation<ept id=\"p20\">][apache-oozie-332]</ept><ph id=\"ph14\"/> (for HDInsight version 2.1).",
      "nodes": [
        {
          "content": "Azure PowerShell currently doesn't provide any cmdlets for defining Oozie jobs.",
          "pos": [
            0,
            79
          ]
        },
        {
          "content": "You can use the <bpt id=\"p18\">**</bpt>Invoke-RestMethod<ept id=\"p18\">**</ept><ph id=\"ph12\"/> cmdlet to invoke Oozie web services.",
          "pos": [
            80,
            209
          ]
        },
        {
          "content": "The Oozie web services API is a HTTP REST JSON API.",
          "pos": [
            210,
            261
          ]
        },
        {
          "content": "For more information about the Oozie web services API, see <bpt id=\"p19\">[</bpt>Apache Oozie 4.0 documentation<ept id=\"p19\">][apache-oozie-400]</ept><ph id=\"ph13\"/> (for HDInsight version 3.0) or <bpt id=\"p20\">[</bpt>Apache Oozie 3.3.2 documentation<ept id=\"p20\">][apache-oozie-332]</ept><ph id=\"ph14\"/> (for HDInsight version 2.1).",
          "pos": [
            262,
            594
          ]
        }
      ]
    },
    {
      "pos": [
        9276,
        9343
      ],
      "content": "The PowerShell script in this section performs the following steps:"
    },
    {
      "pos": [
        9348,
        9365
      ],
      "content": "Connect to Azure."
    },
    {
      "pos": [
        9369,
        9519
      ],
      "content": "Create an Azure resource group. For more information, see <bpt id=\"p21\">[</bpt>Use Azure PowerShell with Azure Resource Manager<ept id=\"p21\">](../powershell-azure-resource-manager.md)</ept>.",
      "nodes": [
        {
          "content": "Create an Azure resource group.",
          "pos": [
            0,
            31
          ]
        },
        {
          "content": "For more information, see <bpt id=\"p21\">[</bpt>Use Azure PowerShell with Azure Resource Manager<ept id=\"p21\">](../powershell-azure-resource-manager.md)</ept>.",
          "pos": [
            32,
            190
          ]
        }
      ]
    },
    {
      "pos": [
        9523,
        9650
      ],
      "content": "Create an Azure SQL Database server, an Azure SQL database, and two tables. These are used by the Sqoop action in the workflow.",
      "nodes": [
        {
          "content": "Create an Azure SQL Database server, an Azure SQL database, and two tables.",
          "pos": [
            0,
            75
          ]
        },
        {
          "content": "These are used by the Sqoop action in the workflow.",
          "pos": [
            76,
            127
          ]
        }
      ]
    },
    {
      "pos": [
        9656,
        9690
      ],
      "content": "The table name is <bpt id=\"p22\">*</bpt>log4jLogCount<ept id=\"p22\">*</ept>."
    },
    {
      "pos": [
        9695,
        9746
      ],
      "content": "Create an HDInsight cluster used to run Oozie jobs."
    },
    {
      "pos": [
        9752,
        9825
      ],
      "content": "To examine the cluster, you can use the Azure portal or Azure PowerShell."
    },
    {
      "pos": [
        9830,
        9913
      ],
      "content": "Copy the oozie workflow file and the HiveQL script file to the default file system."
    },
    {
      "pos": [
        9919,
        9968
      ],
      "content": "Both files are stored in a public Blob container."
    },
    {
      "pos": [
        9980,
        10077
      ],
      "content": "Copy the HiveQL script (useoozie.hql) to Azure Storage (wasb:///tutorials/useoozie/useoozie.hql)."
    },
    {
      "pos": [
        10084,
        10145
      ],
      "content": "Copy workflow.xml to wasb:///tutorials/useoozie/workflow.xml."
    },
    {
      "pos": [
        10152,
        10244
      ],
      "content": "Copy the data file (/example/data/sample.log) to wasb:///tutorials/useoozie/data/sample.log."
    },
    {
      "pos": [
        10254,
        10274
      ],
      "content": "Submit an Oozie job."
    },
    {
      "pos": [
        10280,
        10384
      ],
      "content": "To examine the OOzie job results, use Visual Studio or other tools to connect to the Azure SQL Database."
    },
    {
      "pos": [
        10386,
        10508
      ],
      "content": "Here is the script.  You can run the script from Windows PowerShell ISE. You only need to configure the first 7 variables.",
      "nodes": [
        {
          "content": "Here is the script.",
          "pos": [
            0,
            19
          ]
        },
        {
          "content": "You can run the script from Windows PowerShell ISE.",
          "pos": [
            21,
            72
          ]
        },
        {
          "content": "You only need to configure the first 7 variables.",
          "pos": [
            73,
            122
          ]
        }
      ]
    },
    {
      "pos": [
        25538,
        25564
      ],
      "content": "<bpt id=\"p23\">**</bpt>To re-run the tutorial<ept id=\"p23\">**</ept>"
    },
    {
      "pos": [
        25566,
        25620
      ],
      "content": "To re-run the workflow, you must delete the following:"
    },
    {
      "pos": [
        25624,
        25651
      ],
      "content": "The Hive script output file"
    },
    {
      "pos": [
        25654,
        25690
      ],
      "content": "The data in the log4jLogsCount table"
    },
    {
      "pos": [
        25692,
        25744
      ],
      "content": "Here is a sample PowerShell script that you can use:"
    },
    {
      "pos": [
        27332,
        27342
      ],
      "content": "Next steps"
    },
    {
      "pos": [
        27343,
        27496
      ],
      "content": "In this tutorial, you learned how to define an Oozie workflow and how to run an Oozie job by using PowerShell. To learn more, see the following articles:",
      "nodes": [
        {
          "content": "In this tutorial, you learned how to define an Oozie workflow and how to run an Oozie job by using PowerShell.",
          "pos": [
            0,
            110
          ]
        },
        {
          "content": "To learn more, see the following articles:",
          "pos": [
            111,
            153
          ]
        }
      ]
    },
    {
      "pos": [
        27500,
        27583
      ],
      "content": "<bpt id=\"p24\">[</bpt>Use time-based Oozie Coordinator with HDInsight<ept id=\"p24\">][hdinsight-oozie-coordinator-time]</ept>"
    },
    {
      "pos": [
        27586,
        27688
      ],
      "content": "<bpt id=\"p25\">[</bpt>Get started using Hadoop with Hive in HDInsight to analyze mobile handset use<ept id=\"p25\">][hdinsight-get-started]</ept>"
    },
    {
      "pos": [
        27691,
        27764
      ],
      "content": "<bpt id=\"p26\">[</bpt>Get started with the HDInsight Emulator<ept id=\"p26\">][hdinsight-get-started-emulator]</ept>"
    },
    {
      "pos": [
        27767,
        27825
      ],
      "content": "<bpt id=\"p27\">[</bpt>Use Azure Blob storage with HDInsight<ept id=\"p27\">][hdinsight-storage]</ept>"
    },
    {
      "pos": [
        27828,
        27895
      ],
      "content": "<bpt id=\"p28\">[</bpt>Administer HDInsight using PowerShell<ept id=\"p28\">][hdinsight-admin-powershell]</ept>"
    },
    {
      "pos": [
        27898,
        27963
      ],
      "content": "<bpt id=\"p29\">[</bpt>Upload data for Hadoop jobs in HDInsight<ept id=\"p29\">][hdinsight-upload-data]</ept>"
    },
    {
      "pos": [
        27966,
        28023
      ],
      "content": "<bpt id=\"p30\">[</bpt>Use Sqoop with Hadoop in HDInsight<ept id=\"p30\">][hdinsight-use-sqoop]</ept>"
    },
    {
      "pos": [
        28026,
        28081
      ],
      "content": "<bpt id=\"p31\">[</bpt>Use Hive with Hadoop on HDInsight<ept id=\"p31\">][hdinsight-use-hive]</ept>"
    },
    {
      "pos": [
        28084,
        28137
      ],
      "content": "<bpt id=\"p32\">[</bpt>Use Pig with Hadoop on HDInsight<ept id=\"p32\">][hdinsight-use-pig]</ept>"
    },
    {
      "pos": [
        28140,
        28222
      ],
      "content": "<bpt id=\"p33\">[</bpt>Develop C# Hadoop streaming jobs for HDInsight<ept id=\"p33\">][hdinsight-develop-streaming-jobs]</ept>"
    },
    {
      "pos": [
        28225,
        28301
      ],
      "content": "<bpt id=\"p34\">[</bpt>Develop Java MapReduce programs for HDInsight<ept id=\"p34\">][hdinsight-develop-mapreduce]</ept>"
    }
  ],
  "content": "<properties\n    pageTitle=\"Use Hadoop Oozie in HDInsight | Microsoft Azure\"\n    description=\"Use Hadoop Oozie in HDInsight, a big data service. Learn how to define an Oozie workflow, and submit an Oozie job.\"\n    services=\"hdinsight\"\n    documentationCenter=\"\"\n    tags=\"azure-portal\"\n    authors=\"mumian\"\n    manager=\"paulettm\"\n    editor=\"cgronlun\"/>\n\n<tags\n    ms.service=\"hdinsight\"\n    ms.workload=\"big-data\"\n    ms.tgt_pltfrm=\"na\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\"\n    ms.date=\"12/02/2015\"\n    ms.author=\"jgao\"/>\n\n\n# Use Oozie with Hadoop to define and run a workflow in HDInsight\n\n[AZURE.INCLUDE [oozie-selector](../../includes/hdinsight-oozie-selector.md)]\n\nLearn how to use Apache Oozie to define a workflow and run the workflow on HDInsight. To learn about the Oozie coordinator, see [Use time-based Hadoop Oozie Coordinator with HDInsight][hdinsight-oozie-coordinator-time]. To learn Azure Data Factory, see [Use Pig and Hive with Data Factory][azure-data-factory-pig-hive].\n\nApache Oozie is a workflow/coordination system that manages Hadoop jobs. It is integrated with the Hadoop stack, and it supports Hadoop jobs for Apache MapReduce, Apache Pig, Apache Hive, and Apache Sqoop. It can also be used to schedule jobs that are specific to a system, like Java programs or shell scripts.\n\nThe workflow you will implement by following the instructions in this tutorial contains two actions:\n\n![Workflow diagram][img-workflow-diagram]\n\n1. A Hive action runs a HiveQL script to count the occurrences of each log-level type in a log4j file. Each log4j file consists of a line of fields that contains a [LOG LEVEL] field that shows the type and the severity, for example:\n\n        2012-02-03 18:35:34 SampleClass6 [INFO] everything normal for id 577725851\n        2012-02-03 18:35:34 SampleClass4 [FATAL] system problem at id 1991281254\n        2012-02-03 18:35:34 SampleClass3 [DEBUG] detail for id 1304807656\n        ...\n\n    The Hive script output is similar to:\n\n        [DEBUG] 434\n        [ERROR] 3\n        [FATAL] 1\n        [INFO]  96\n        [TRACE] 816\n        [WARN]  4\n\n    For more information about Hive, see [Use Hive with HDInsight][hdinsight-use-hive].\n\n2.  A Sqoop action exports the HiveQL output to a table in an Azure SQL database. For more information about Sqoop, see [Use Hadoop Sqoop with HDInsight][hdinsight-use-sqoop].\n\n> [AZURE.NOTE] For supported Oozie versions on HDInsight clusters, see [What's new in the Hadoop cluster versions provided by HDInsight?][hdinsight-versions].\n\n###Prerequisites\n\nBefore you begin this tutorial, you must have the following:\n\n- **A workstation with Azure PowerShell**. See [Install and use Azure PowerShell](https://azure.microsoft.com/documentation/videos/install-and-use-azure-powershell/). To execute Windows PowerShell scripts, you must run as an administrator and set the execution policy to *RemoteSigned*. For more information, see [Run Windows PowerShell scripts][powershell-script].\n\n##Define Oozie workflow and the related HiveQL script\n\nOozie workflows definitions are written in hPDL (a XML Process Definition Language). The default workflow file name is *workflow.xml*. The following is the workflow file you will use in this tutorial.\n\n    <workflow-app name=\"useooziewf\" xmlns=\"uri:oozie:workflow:0.2\">\n        <start to = \"RunHiveScript\"/>\n\n        <action name=\"RunHiveScript\">\n            <hive xmlns=\"uri:oozie:hive-action:0.2\">\n                <job-tracker>${jobTracker}</job-tracker>\n                <name-node>${nameNode}</name-node>\n                <configuration>\n                    <property>\n                        <name>mapred.job.queue.name</name>\n                        <value>${queueName}</value>\n                    </property>\n                </configuration>\n                <script>${hiveScript}</script>\n                <param>hiveTableName=${hiveTableName}</param>\n                <param>hiveDataFolder=${hiveDataFolder}</param>\n                <param>hiveOutputFolder=${hiveOutputFolder}</param>\n            </hive>\n            <ok to=\"RunSqoopExport\"/>\n            <error to=\"fail\"/>\n        </action>\n\n        <action name=\"RunSqoopExport\">\n            <sqoop xmlns=\"uri:oozie:sqoop-action:0.2\">\n                <job-tracker>${jobTracker}</job-tracker>\n                <name-node>${nameNode}</name-node>\n                <configuration>\n                    <property>\n                        <name>mapred.compress.map.output</name>\n                        <value>true</value>\n                    </property>\n                </configuration>\n            <arg>export</arg>\n            <arg>--connect</arg>\n            <arg>${sqlDatabaseConnectionString}</arg>\n            <arg>--table</arg>\n            <arg>${sqlDatabaseTableName}</arg>\n            <arg>--export-dir</arg>\n            <arg>${hiveOutputFolder}</arg>\n            <arg>-m</arg>\n            <arg>1</arg>\n            <arg>--input-fields-terminated-by</arg>\n            <arg>\"\\001\"</arg>\n            </sqoop>\n            <ok to=\"end\"/>\n            <error to=\"fail\"/>\n        </action>\n\n        <kill name=\"fail\">\n            <message>Job failed, error message[${wf:errorMessage(wf:lastErrorNode())}] </message>\n        </kill>\n\n        <end name=\"end\"/>\n    </workflow-app>\n\nThere are two actions defined in the workflow. The start-to action is *RunHiveScript*. If the action runs successfully, the next action is *RunSqoopExport*.\n\nThe RunHiveScript has several variables. You will pass the values when you submit the Oozie job from your workstation by using Azure PowerShell.\n\n<table border = \"1\">\n<tr><th>Workflow variables</th><th>Description</th></tr>\n<tr><td>${jobTracker}</td><td>Specifies the URL of the Hadoop job tracker. Use <strong>jobtrackerhost:9010</strong> in HDInsight version 3.0 and 2.1.</td></tr>\n<tr><td>${nameNode}</td><td>Specifies the URL of the Hadoop name node. Use the default file system address, for example, <i>wasb://&lt;containerName&gt;@&lt;storageAccountName&gt;.blob.core.windows.net</i>.</td></tr>\n<tr><td>${queueName}</td><td>Specifies the queue name that the job will be submitted to. Use the <strong>default</strong>.</td></tr>\n</table>\n\n<table border = \"1\">\n<tr><th>Hive action variable</th><th>Description</th></tr>\n<tr><td>${hiveDataFolder}</td><td>Specifies the source directory for the Hive Create Table command.</td></tr>\n<tr><td>${hiveOutputFolder}</td><td>Specifies the output folder for the INSERT OVERWRITE statement.</td></tr>\n<tr><td>${hiveTableName}</td><td>Specifies the name of the Hive table that references the log4j data files.</td></tr>\n</table>\n\n<table border = \"1\">\n<tr><th>Sqoop action variable</th><th>Description</th></tr>\n<tr><td>${sqlDatabaseConnectionString}</td><td>Specifies the Azure SQL database connection string.</td></tr>\n<tr><td>${sqlDatabaseTableName}</td><td>Specifies the Azure SQL database table where the data will be exported to.</td></tr>\n<tr><td>${hiveOutputFolder}</td><td>Specifies the output folder for the Hive INSERT OVERWRITE statement. This is the same folder for the Sqoop export (export-dir).</td></tr>\n</table>\n\nFor more information about Oozie workflow and using workflow actions, see [Apache Oozie 4.0 documentation][apache-oozie-400] (for HDInsight version 3.0) or [Apache Oozie 3.3.2 documentation][apache-oozie-332] (for HDInsight version 2.1).\n\n\nThe Hive action in the workflow calls a HiveQL script file. This script file contains three HiveQL statements:\n\n    DROP TABLE ${hiveTableName};\n    CREATE EXTERNAL TABLE ${hiveTableName}(t1 string, t2 string, t3 string, t4 string, t5 string, t6 string, t7 string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ' ' STORED AS TEXTFILE LOCATION '${hiveDataFolder}';\n    INSERT OVERWRITE DIRECTORY '${hiveOutputFolder}' SELECT t4 AS sev, COUNT(*) AS cnt FROM ${hiveTableName} WHERE t4 LIKE '[%' GROUP BY t4;\n\n1. **The DROP TABLE statement** deletes the log4j Hive table if it exists.\n2. **The CREATE TABLE statement** creates a log4j Hive external table that points to the location of the log4j log file. The field delimiter is \",\". The default line delimiter is \"\\n\". A Hive external table is used to avoid the data file being removed from the original location if you want to run the Oozie workflow multiple times.\n3. **The INSERT OVERWRITE statement** counts the occurrences of each log-level type from the log4j Hive table, and saves the output to a blob in Azure Storage.\n\n\nThere are three variables used in the script:\n\n- ${hiveTableName}\n- ${hiveDataFolder}\n- ${hiveOutputFolder}\n\nThe workflow definition file (workflow.xml in this tutorial) passes these values to this HiveQL script at run time.\n\nBoth the workflow file and the HiveQL file are stored in a blob container.  The PowerShell script you will use later in this tutorial will copy both files to the default Storage account. \n\n##Submit Oozie jobs using PowerShell\n\nAzure PowerShell currently doesn't provide any cmdlets for defining Oozie jobs. You can use the **Invoke-RestMethod** cmdlet to invoke Oozie web services. The Oozie web services API is a HTTP REST JSON API. For more information about the Oozie web services API, see [Apache Oozie 4.0 documentation][apache-oozie-400] (for HDInsight version 3.0) or [Apache Oozie 3.3.2 documentation][apache-oozie-332] (for HDInsight version 2.1).\n\nThe PowerShell script in this section performs the following steps:\n\n1. Connect to Azure.\n2. Create an Azure resource group. For more information, see [Use Azure PowerShell with Azure Resource Manager](../powershell-azure-resource-manager.md).\n3. Create an Azure SQL Database server, an Azure SQL database, and two tables. These are used by the Sqoop action in the workflow.\n\n    The table name is *log4jLogCount*.\n\n4. Create an HDInsight cluster used to run Oozie jobs.\n\n    To examine the cluster, you can use the Azure portal or Azure PowerShell.\n\n5. Copy the oozie workflow file and the HiveQL script file to the default file system.\n\n    Both files are stored in a public Blob container.\n    \n    - Copy the HiveQL script (useoozie.hql) to Azure Storage (wasb:///tutorials/useoozie/useoozie.hql).\n    - Copy workflow.xml to wasb:///tutorials/useoozie/workflow.xml.\n    - Copy the data file (/example/data/sample.log) to wasb:///tutorials/useoozie/data/sample.log.\n     \n6. Submit an Oozie job.\n\n    To examine the OOzie job results, use Visual Studio or other tools to connect to the Azure SQL Database.\n\nHere is the script.  You can run the script from Windows PowerShell ISE. You only need to configure the first 7 variables.\n\n    #region - provide the following values\n    \n    $subscriptionID = \"<Enter your Azure subscription ID>\"\n    \n    # SQL Database server login credentials used for creating and connecting\n    $sqlDatabaseLogin = \"<Enter SQL Database Login Name>\"\n    $sqlDatabasePassword = \"<Enter SQL Database Login Password>\"\n    \n    # HDInsight cluster HTTP user credential used for creating and connectin\n    $httpUserName = \"admin\"  # The default name is \"admin\"\n    $httpPassword = \"<Enter HDInsight Cluster HTTP User Password>\"\n    \n    # Used for creating Azure service names\n    $nameToken = \"<Enter an Alias>\"\n    $namePrefix = $nameToken.ToLower() + (Get-Date -Format \"MMdd\")\n    #endregion\n    \n    #region - variables\n    \n    # Resource group variables\n    $resourceGroupName = $namePrefix + \"rg\"\n    $location = \"East US 2\" # used by all Azure services defined in this tutorial\n    \n    # SQL database varialbes\n    $sqlDatabaseServerName = $namePrefix + \"sqldbserver\"\n    $sqlDatabaseName = $namePrefix + \"sqldb\"\n    $sqlDatabaseConnectionString = \"Data Source=$sqlDatabaseServerName.database.windows.net;Initial Catalog=$sqlDatabaseName;User ID=$sqlDatabaseLogin;Password=$sqlDatabasePassword;Encrypt=true;Trusted_Connection=false;\"\n    $sqlDatabaseMaxSizeGB = 10\n    \n    # Used for retrieving external IP address and creating firewall rules\n    $ipAddressRestService = \"http://bot.whatismyipaddress.com\"\n    $fireWallRuleName = \"UseSqoop\"\n    \n    # HDInsight variables\n    $hdinsightClusterName = $namePrefix + \"hdi\"\n    $defaultStorageAccountName = $namePrefix + \"store\"\n    $defaultBlobContainerName = $hdinsightClusterName\n    #endregion\n    \n    # Treat all errors as terminating\n    $ErrorActionPreference = \"Stop\"\n    \n    #region - Connect to Azure subscription\n    Write-Host \"`nConnecting to your Azure subscription ...\" -ForegroundColor Green\n    try{Get-AzureRmContext}\n    catch{\n        Login-AzureRmAccount\n        Select-AzureRmSubscription -SubscriptionId $subscriptionID\n    }\n    #endregion\n    \n    #region - Create Azure resouce group\n    Write-Host \"`nCreating an Azure resource group ...\" -ForegroundColor Green\n    try{\n        Get-AzureRmResourceGroup -Name $resourceGroupName\n    }\n    catch{\n        New-AzureRmResourceGroup -Name $resourceGroupName -Location $location\n    }\n    #endregion\n    \n    #region - Create Azure SQL database server\n    Write-Host \"`nCreating an Azure SQL Database server ...\" -ForegroundColor Green\n    try{\n        Get-AzureRmSqlServer -ServerName $sqlDatabaseServerName -ResourceGroupName $resourceGroupName}\n    catch{\n        Write-Host \"`nCreating SQL Database server ...\"  -ForegroundColor Green\n    \n        $sqlDatabasePW = ConvertTo-SecureString -String $sqlDatabasePassword -AsPlainText -Force\n        $sqlLoginCredentials = New-Object System.Management.Automation.PSCredential($sqlDatabaseLogin,$sqlDatabasePW)\n    \n        $sqlDatabaseServerName = (New-AzureRmSqlServer `\n                                    -ResourceGroupName $resourceGroupName `\n                                    -ServerName $sqlDatabaseServerName `\n                                    -SqlAdministratorCredentials $sqlLoginCredentials `\n                                    -Location $location).ServerName\n        Write-Host \"`tThe new SQL database server name is $sqlDatabaseServerName.\" -ForegroundColor Cyan\n    \n        Write-Host \"`nCreating firewall rule, $fireWallRuleName ...\" -ForegroundColor Green\n        $workstationIPAddress = Invoke-RestMethod $ipAddressRestService\n        New-AzureRmSqlServerFirewallRule `\n            -ResourceGroupName $resourceGroupName `\n            -ServerName $sqlDatabaseServerName `\n            -FirewallRuleName \"$fireWallRuleName-workstation\" `\n            -StartIpAddress $workstationIPAddress `\n            -EndIpAddress $workstationIPAddress\n    \n        #To allow other Azure services to access the server add a firewall rule and set both the StartIpAddress and EndIpAddress to 0.0.0.0. \n        #Note that this allows Azure traffic from any Azure subscription to access the server.\n        New-AzureRmSqlServerFirewallRule `\n            -ResourceGroupName $resourceGroupName `\n            -ServerName $sqlDatabaseServerName `\n            -FirewallRuleName \"$fireWallRuleName-Azureservices\" `\n            -StartIpAddress \"0.0.0.0\" `\n            -EndIpAddress \"0.0.0.0\"\n    }\n    #endregion\n    \n    #region - Create and validate Azure SQL database\n    Write-Host \"`nCreating SQL Database, $sqlDatabaseName ...\"  -ForegroundColor Green\n    \n    try {\n        Get-AzureRmSqlDatabase `\n            -ResourceGroupName $resourceGroupName `\n            -ServerName $sqlDatabaseServerName `\n            -DatabaseName $sqlDatabaseName\n    }\n    catch {\n        New-AzureRMSqlDatabase `\n            -ResourceGroupName $resourceGroupName `\n            -ServerName $sqlDatabaseServerName `\n            -DatabaseName $sqlDatabaseName `\n            -Edition \"Standard\" `\n            -RequestedServiceObjectiveName \"S1\"\n    }\n    #endregion\n    \n    #region - Create SQL database tables\n    Write-Host \"Creating the log4jlogs table  ...\" -ForegroundColor Green\n    \n    $sqlDatabaseTableName = \"log4jLogsCount\"\n    $cmdCreateLog4jCountTable = \" CREATE TABLE [dbo].[$sqlDatabaseTableName](\n            [Level] [nvarchar](10) NOT NULL,\n            [Total] float,\n        CONSTRAINT [PK_$sqlDatabaseTableName] PRIMARY KEY CLUSTERED\n        (\n        [Level] ASC\n        )\n        )\"\n    \n    $conn = New-Object System.Data.SqlClient.SqlConnection\n    $conn.ConnectionString = $sqlDatabaseConnectionString\n    $conn.Open()\n    \n    # Create the log4jlogs table and index\n    $cmd = New-Object System.Data.SqlClient.SqlCommand\n    $cmd.Connection = $conn\n    $cmd.CommandText = $cmdCreateLog4jCountTable\n    $cmd.ExecuteNonQuery()\n    \n    $conn.close()\n    #endregion\n    \n    #region - Create HDInsight cluster\n    \n    Write-Host \"Creating the HDInsight cluster and the dependent services ...\" -ForegroundColor Green\n    \n    # Create the default storage account\n    New-AzureRmStorageAccount `\n        -ResourceGroupName $resourceGroupName `\n        -Name $defaultStorageAccountName `\n        -Location $location `\n        -Type Standard_LRS\n    \n    # Create the default Blob container\n    $defaultStorageAccountKey = Get-AzureRmStorageAccountKey `\n                                    -ResourceGroupName $resourceGroupName `\n                                    -Name $defaultStorageAccountName |  %{ $_.Key1 }\n    $defaultStorageAccountContext = New-AzureStorageContext `\n                                        -StorageAccountName $defaultStorageAccountName `\n                                        -StorageAccountKey $defaultStorageAccountKey \n    New-AzureStorageContainer `\n        -Name $defaultBlobContainerName `\n        -Context $defaultStorageAccountContext \n    \n    # Create the HDInsight cluster\n    $pw = ConvertTo-SecureString -String $httpPassword -AsPlainText -Force\n    $httpCredential = New-Object System.Management.Automation.PSCredential($httpUserName,$pw)\n    \n    New-AzureRmHDInsightCluster `\n        -ResourceGroupName $resourceGroupName `\n        -ClusterName $HDInsightClusterName `\n        -Location $location `\n        -ClusterType Hadoop `\n        -OSType Windows `\n        -ClusterSizeInNodes 2 `\n        -HttpCredential $httpCredential `\n        -DefaultStorageAccountName \"$defaultStorageAccountName.blob.core.windows.net\" `\n        -DefaultStorageAccountKey $defaultStorageAccountKey `\n        -DefaultStorageContainer $defaultBlobContainerName \n    \n    # Validate the cluster\n    Get-AzureRmHDInsightCluster -ClusterName $hdinsightClusterName\n    #endregion\n    \n    #region - copy Oozie workflow and HiveQL files\n    \n    Write-Host \"Copy workflow definition and HiveQL script file ...\" -ForegroundColor Green\n    \n    # Both files are stored in a public Blob\n    $publicBlobContext = New-AzureStorageContext -StorageAccountName \"hditutorialdata\" -Anonymous\n    \n    # WASB folder for storing the Oozie tutorial files.\n    $destFolder = \"tutorials/useoozie\"  # Do NOT use the long path here\n    \n    Start-CopyAzureStorageBlob `\n        -Context $publicBlobContext `\n        -SrcContainer \"useoozie\" `\n        -SrcBlob \"useooziewf.hql\"  `\n        -DestContext $defaultStorageAccountContext `\n        -DestContainer $defaultBlobContainerName `\n        -DestBlob \"$destFolder/useooziewf.hql\" `\n        -Force\n    \n    Start-CopyAzureStorageBlob `\n        -Context $publicBlobContext `\n        -SrcContainer \"useoozie\" `\n        -SrcBlob \"workflow.xml\"  `\n        -DestContext $defaultStorageAccountContext `\n        -DestContainer $defaultBlobContainerName `\n        -DestBlob \"$destFolder/workflow.xml\" `\n        -Force\n    \n    #validate the copy\n    Get-AzureStorageBlob `\n        -Context $defaultStorageAccountContext `\n        -Container $defaultBlobContainerName `\n        -Blob $destFolder/workflow.xml\n    \n    Get-AzureStorageBlob `\n        -Context $defaultStorageAccountContext `\n        -Container $defaultBlobContainerName `\n        -Blob $destFolder/useooziewf.hql\n    \n    #endregion\n    \n    #region - copy the sample.log file\n    \n    Write-Host \"Make a copy of the sample.log file ... \" -ForegroundColor Green\n    \n    Start-CopyAzureStorageBlob `\n        -Context $defaultStorageAccountContext `\n        -SrcContainer $defaultBlobContainerName `\n        -SrcBlob \"example/data/sample.log\"  `\n        -DestContext $defaultStorageAccountContext `\n        -DestContainer $defaultBlobContainerName `\n        -destBlob \"$destFolder/data/sample.log\" \n    \n    #validate the copy\n    Get-AzureStorageBlob `\n        -Context $defaultStorageAccountContext `\n        -Container $defaultBlobContainerName `\n        -Blob $destFolder/data/sample.log\n    \n    #endregion\n    \n    #region - submit Oozie job\n    \n    $storageUri=\"wasb://$defaultBlobContainerName@$defaultStorageAccountName.blob.core.windows.net\"\n    \n    $oozieJobName = $namePrefix + \"OozieJob\"\n    \n    #Oozie WF variables\n    $oozieWFPath=\"$storageUri/tutorials/useoozie\"  # The default name is workflow.xml. And you don't need to specify the file name.\n    $waitTimeBetweenOozieJobStatusCheck=10\n    \n    #Hive action variables\n    $hiveScript = \"$storageUri/tutorials/useoozie/useooziewf.hql\"\n    $hiveTableName = \"log4jlogs\"\n    $hiveDataFolder = \"$storageUri/tutorials/useoozie/data\"\n    $hiveOutputFolder = \"$storageUri/tutorials/useoozie/output\"\n    \n    #Sqoop action variables\n    $sqlDatabaseConnectionString = \"jdbc:sqlserver://$sqlDatabaseServerName.database.windows.net;user=$sqlDatabaseLogin@$sqlDatabaseServerName;password=$sqlDatabasePassword;database=$sqlDatabaseName\"\n    \n    $OoziePayload =  @\"\n    <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n    <configuration>\n    \n    <property>\n        <name>nameNode</name>\n        <value>$storageUrI</value>\n    </property>\n    \n    <property>\n        <name>jobTracker</name>\n        <value>jobtrackerhost:9010</value>\n    </property>\n    \n    <property>\n        <name>queueName</name>\n        <value>default</value>\n    </property>\n    \n    <property>\n        <name>oozie.use.system.libpath</name>\n        <value>true</value>\n    </property>\n    \n    <property>\n        <name>hiveScript</name>\n        <value>$hiveScript</value>\n    </property>\n    \n    <property>\n        <name>hiveTableName</name>\n        <value>$hiveTableName</value>\n    </property>\n    \n    <property>\n        <name>hiveDataFolder</name>\n        <value>$hiveDataFolder</value>\n    </property>\n    \n    <property>\n        <name>hiveOutputFolder</name>\n        <value>$hiveOutputFolder</value>\n    </property>\n    \n    <property>\n        <name>sqlDatabaseConnectionString</name>\n        <value>&quot;$sqlDatabaseConnectionString&quot;</value>\n    </property>\n    \n    <property>\n        <name>sqlDatabaseTableName</name>\n        <value>$SQLDatabaseTableName</value>\n    </property>\n    \n    <property>\n        <name>user.name</name>\n        <value>$httpUserName</value>\n    </property>\n    \n    <property>\n        <name>oozie.wf.application.path</name>\n        <value>$oozieWFPath</value>\n    </property>\n    \n    </configuration>\n    \"@\n    \n    Write-Host \"Checking Oozie server status...\" -ForegroundColor Green\n    $clusterUriStatus = \"https://$hdinsightClusterName.azurehdinsight.net:443/oozie/v2/admin/status\"\n    $response = Invoke-RestMethod -Method Get -Uri $clusterUriStatus -Credential $httpCredential -OutVariable $OozieServerStatus\n    \n    $jsonResponse = ConvertFrom-Json (ConvertTo-Json -InputObject $response)\n    $oozieServerSatus = $jsonResponse[0].(\"systemMode\")\n    Write-Host \"Oozie server status is $oozieServerSatus.\"\n    \n    # create Oozie job\n    Write-Host \"Sending the following Payload to the cluster:\" -ForegroundColor Green\n    Write-Host \"`n--------`n$OoziePayload`n--------\"\n    $clusterUriCreateJob = \"https://$hdinsightClusterName.azurehdinsight.net:443/oozie/v2/jobs\"\n    $response = Invoke-RestMethod -Method Post -Uri $clusterUriCreateJob -Credential $httpCredential -Body $OoziePayload -ContentType \"application/xml\" -OutVariable $OozieJobName #-debug\n    \n    $jsonResponse = ConvertFrom-Json (ConvertTo-Json -InputObject $response)\n    $oozieJobId = $jsonResponse[0].(\"id\")\n    Write-Host \"Oozie job id is $oozieJobId...\"\n    \n    # start Oozie job\n    Write-Host \"Starting the Oozie job $oozieJobId...\" -ForegroundColor Green\n    $clusterUriStartJob = \"https://$hdinsightClusterName.azurehdinsight.net:443/oozie/v2/job/\" + $oozieJobId + \"?action=start\"\n    $response = Invoke-RestMethod -Method Put -Uri $clusterUriStartJob -Credential $httpCredential | Format-Table -HideTableHeaders #-debug\n    \n    # get job status\n    Write-Host \"Sleeping for $waitTimeBetweenOozieJobStatusCheck seconds until the job metadata is populated in the Oozie metastore...\" -ForegroundColor Green\n    Start-Sleep -Seconds $waitTimeBetweenOozieJobStatusCheck\n    \n    Write-Host \"Getting job status and waiting for the job to complete...\" -ForegroundColor Green\n    $clusterUriGetJobStatus = \"https://$hdinsightClusterName.azurehdinsight.net:443/oozie/v2/job/\" + $oozieJobId + \"?show=info\"\n    $response = Invoke-RestMethod -Method Get -Uri $clusterUriGetJobStatus -Credential $httpCredential\n    $jsonResponse = ConvertFrom-Json (ConvertTo-Json -InputObject $response)\n    $JobStatus = $jsonResponse[0].(\"status\")\n    \n    while($JobStatus -notmatch \"SUCCEEDED|KILLED\")\n    {\n        Write-Host \"$(Get-Date -format 'G'): $oozieJobId is in $JobStatus state...waiting $waitTimeBetweenOozieJobStatusCheck seconds for the job to complete...\"\n        Start-Sleep -Seconds $waitTimeBetweenOozieJobStatusCheck\n        $response = Invoke-RestMethod -Method Get -Uri $clusterUriGetJobStatus -Credential $httpCredential\n        $jsonResponse = ConvertFrom-Json (ConvertTo-Json -InputObject $response)\n        $JobStatus = $jsonResponse[0].(\"status\")\n        $jobStatus\n    }\n    \n    Write-Host \"$(Get-Date -format 'G'): $oozieJobId is in $JobStatus state!\" -ForegroundColor Green\n    \n    #endregion\n\n\n**To re-run the tutorial**\n\nTo re-run the workflow, you must delete the following:\n\n- The Hive script output file\n- The data in the log4jLogsCount table\n\nHere is a sample PowerShell script that you can use:\n\n    $resourceGroupName = \"<AzureResourceGroupName>\"\n    \n    $defaultStorageAccountName = \"<AzureStorageAccountName>\"\n    $defaultBlobContainerName = \"<ContainerName>\"\n\n    #SQL database variables\n    $sqlDatabaseServerName = \"<SQLDatabaseServerName>\"\n    $sqlDatabaseLogin = \"<SQLDatabaseLoginName>\"\n    $sqlDatabasePassword = \"<SQLDatabaseLoginPassword>\"\n    $sqlDatabaseName = \"<SQLDatabaseName>\"\n    $sqlDatabaseTableName = \"log4jLogsCount\"\n\n    Write-host \"Delete the Hive script output file ...\" -ForegroundColor Green\n    $defaultStorageAccountKey = Get-AzureRmStorageAccountKey `\n                                -ResourceGroupName $resourceGroupName `\n                                -Name $defaultStorageAccountName |  %{ $_.Key1 }\n    $destContext = New-AzureStorageContext -StorageAccountName $defaultStorageAccountName -StorageAccountKey $defaultStorageAccountKey\n    Remove-AzureStorageBlob -Context $destContext -Blob \"tutorials/useoozie/output/000000_0\" -Container $defaultBlobContainerName\n\n    Write-host \"Delete all the records from the log4jLogsCount table ...\" -ForegroundColor Green\n    $conn = New-Object System.Data.SqlClient.SqlConnection\n    $conn.ConnectionString = \"Data Source=$sqlDatabaseServerName.database.windows.net;Initial Catalog=$sqlDatabaseName;User ID=$sqlDatabaseLogin;Password=$sqlDatabasePassword;Encrypt=true;Trusted_Connection=false;\"\n    $conn.open()\n    $cmd = New-Object System.Data.SqlClient.SqlCommand\n    $cmd.connection = $conn\n    $cmd.commandtext = \"delete from $sqlDatabaseTableName\"\n    $cmd.executenonquery()\n\n    $conn.close()\n\n##Next steps\nIn this tutorial, you learned how to define an Oozie workflow and how to run an Oozie job by using PowerShell. To learn more, see the following articles:\n\n- [Use time-based Oozie Coordinator with HDInsight][hdinsight-oozie-coordinator-time]\n- [Get started using Hadoop with Hive in HDInsight to analyze mobile handset use][hdinsight-get-started]\n- [Get started with the HDInsight Emulator][hdinsight-get-started-emulator]\n- [Use Azure Blob storage with HDInsight][hdinsight-storage]\n- [Administer HDInsight using PowerShell][hdinsight-admin-powershell]\n- [Upload data for Hadoop jobs in HDInsight][hdinsight-upload-data]\n- [Use Sqoop with Hadoop in HDInsight][hdinsight-use-sqoop]\n- [Use Hive with Hadoop on HDInsight][hdinsight-use-hive]\n- [Use Pig with Hadoop on HDInsight][hdinsight-use-pig]\n- [Develop C# Hadoop streaming jobs for HDInsight][hdinsight-develop-streaming-jobs]\n- [Develop Java MapReduce programs for HDInsight][hdinsight-develop-mapreduce]\n\n\n[hdinsight-cmdlets-download]: http://go.microsoft.com/fwlink/?LinkID=325563\n\n\n\n[azure-data-factory-pig-hive]: ../data-factory/data-factory-data-transformation-activities.md\n[hdinsight-oozie-coordinator-time]: hdinsight-use-oozie-coordinator-time.md\n[hdinsight-versions]:  hdinsight-component-versioning.md\n[hdinsight-storage]: ../hdinsight-hadoop-use-blob-storage.md\n[hdinsight-get-started]: hdinsight-hadoop-linux-tutorial-get-started.md\n[hdinsight-admin-portal]: hdinsight-administer-use-management-portal.md\n\n\n[hdinsight-use-sqoop]: hdinsight-use-sqoop.md\n[hdinsight-provision]: hdinsight-provision-clusters.md\n[hdinsight-admin-powershell]: hdinsight-administer-use-powershell.md\n[hdinsight-upload-data]: hdinsight-upload-data.md\n[hdinsight-use-mapreduce]: hdinsight-use-mapreduce.md\n[hdinsight-use-hive]: hdinsight-use-hive.md\n[hdinsight-use-pig]: hdinsight-use-pig.md\n[hdinsight-storage]: ../hdinsight-hadoop-use-blob-storage.md\n[hdinsight-get-started-emulator]: ../hdinsight-get-started-emulator.md\n\n[hdinsight-develop-streaming-jobs]: hdinsight-hadoop-develop-deploy-streaming-jobs.md\n[hdinsight-develop-mapreduce]: hdinsight-develop-deploy-java-mapreduce.md\n\n[sqldatabase-create-configue]: ../sql-database-create-configure.md\n[sqldatabase-get-started]: ../sql-database-get-started.md\n\n[azure-management-portal]: https://portal.azure.com/\n[azure-create-storageaccount]: ../storage-create-storage-account.md\n\n[apache-hadoop]: http://hadoop.apache.org/\n[apache-oozie-400]: http://oozie.apache.org/docs/4.0.0/\n[apache-oozie-332]: http://oozie.apache.org/docs/3.3.2/\n\n[powershell-download]: http://azure.microsoft.com/downloads/\n[powershell-about-profiles]: http://go.microsoft.com/fwlink/?LinkID=113729\n[powershell-install-configure]: ../powershell-install-configure.md\n[powershell-start]: http://technet.microsoft.com/library/hh847889.aspx\n[powershell-script]: https://technet.microsoft.com/en-us/library/ee176961.aspx\n\n[cindygross-hive-tables]: http://blogs.msdn.com/b/cindygross/archive/2013/02/06/hdinsight-hive-internal-and-external-tables-intro.aspx\n\n[img-workflow-diagram]: ./media/hdinsight-use-oozie/HDI.UseOozie.Workflow.Diagram.png\n[img-preparation-output]: ./media/hdinsight-use-oozie/HDI.UseOozie.Preparation.Output1.png  \n[img-runworkflow-output]: ./media/hdinsight-use-oozie/HDI.UseOozie.RunWF.Output.png\n\n[technetwiki-hive-error]: http://social.technet.microsoft.com/wiki/contents/articles/23047.hdinsight-hive-error-unable-to-rename.aspx\n"
}