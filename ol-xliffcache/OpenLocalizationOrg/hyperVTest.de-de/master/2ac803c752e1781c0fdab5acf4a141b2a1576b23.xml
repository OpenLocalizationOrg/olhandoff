{
  "nodes": [
    {
      "pos": [
        28,
        122
      ],
      "content": "Use custom libraries with an HDInsight Spark cluster to analyze website logs | Microsoft Azure"
    },
    {
      "pos": [
        142,
        218
      ],
      "content": "Use custom libraries with an HDInsight Spark cluster to analyze website logs"
    },
    {
      "pos": [
        558,
        620
      ],
      "content": "Analyze logs in HDInsight Spark using a custom library (Linux)"
    },
    {
      "pos": [
        622,
        794
      ],
      "content": "This notebook demonstrates how to analyze log data using a custom library with Spark on HDInsight. The custom library we use is a Python library called <bpt id=\"p1\">**</bpt>iislogparser.py<ept id=\"p1\">**</ept>.",
      "nodes": [
        {
          "content": "This notebook demonstrates how to analyze log data using a custom library with Spark on HDInsight.",
          "pos": [
            0,
            98
          ]
        },
        {
          "content": "The custom library we use is a Python library called <bpt id=\"p1\">**</bpt>iislogparser.py<ept id=\"p1\">**</ept>.",
          "pos": [
            99,
            210
          ]
        }
      ]
    },
    {
      "pos": [
        798,
        1265
      ],
      "content": "<ph id=\"ph2\">[AZURE.TIP]</ph><ph id=\"ph3\"/> This tutorial is also available as a Jupyter notebook on a Spark (Linux) cluster that you create in HDInsight. The notebook experience lets you run the Python snippets from the notebook itself. To perform the tutorial from within a notebook, create a Spark cluster, launch a Jupyter notebook (<ph id=\"ph4\">`https://CLUSTERNAME.azurehdinsight.net/jupyter`</ph>), and then run the notebook <bpt id=\"p2\">**</bpt>Analyze logs with Spark using a custom library.ipynb<ept id=\"p2\">**</ept><ph id=\"ph5\"/> under the <bpt id=\"p3\">**</bpt>Python<ept id=\"p3\">**</ept><ph id=\"ph6\"/> folder.",
      "nodes": [
        {
          "content": "<ph id=\"ph2\">[AZURE.TIP]</ph><ph id=\"ph3\"/> This tutorial is also available as a Jupyter notebook on a Spark (Linux) cluster that you create in HDInsight.",
          "pos": [
            0,
            154
          ]
        },
        {
          "content": "The notebook experience lets you run the Python snippets from the notebook itself.",
          "pos": [
            155,
            237
          ]
        },
        {
          "content": "To perform the tutorial from within a notebook, create a Spark cluster, launch a Jupyter notebook (<ph id=\"ph4\">`https://CLUSTERNAME.azurehdinsight.net/jupyter`</ph>), and then run the notebook <bpt id=\"p2\">**</bpt>Analyze logs with Spark using a custom library.ipynb<ept id=\"p2\">**</ept><ph id=\"ph5\"/> under the <bpt id=\"p3\">**</bpt>Python<ept id=\"p3\">**</ept><ph id=\"ph6\"/> folder.",
          "pos": [
            238,
            621
          ]
        }
      ]
    },
    {
      "pos": [
        1267,
        1285
      ],
      "content": "<bpt id=\"p4\">**</bpt>Prerequisites:<ept id=\"p4\">**</ept>"
    },
    {
      "pos": [
        1287,
        1315
      ],
      "content": "You must have the following:"
    },
    {
      "pos": [
        1319,
        1473
      ],
      "content": "An Azure subscription. See <bpt id=\"p5\">[</bpt>Get Azure free trial<ept id=\"p5\">](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/)</ept>.",
      "nodes": [
        {
          "content": "An Azure subscription.",
          "pos": [
            0,
            22
          ]
        },
        {
          "content": "See <bpt id=\"p5\">[</bpt>Get Azure free trial<ept id=\"p5\">](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/)</ept>.",
          "pos": [
            23,
            192
          ]
        }
      ]
    },
    {
      "pos": [
        1476,
        1637
      ],
      "content": "An Apache Spark cluster on HDInsight Linux. For instructions, see <bpt id=\"p6\">[</bpt>Create Apache Spark clusters in Azure HDInsight<ept id=\"p6\">](hdinsight-apache-spark-jupyter-spark-sql.md)</ept>.",
      "nodes": [
        {
          "content": "An Apache Spark cluster on HDInsight Linux.",
          "pos": [
            0,
            43
          ]
        },
        {
          "content": "For instructions, see <bpt id=\"p6\">[</bpt>Create Apache Spark clusters in Azure HDInsight<ept id=\"p6\">](hdinsight-apache-spark-jupyter-spark-sql.md)</ept>.",
          "pos": [
            44,
            199
          ]
        }
      ]
    },
    {
      "pos": [
        1642,
        1665
      ],
      "content": "Save raw data as an RDD"
    },
    {
      "pos": [
        1667,
        1942
      ],
      "content": "In this section, we use the <bpt id=\"p7\">[</bpt>Jupyter<ept id=\"p7\">](https://jupyter.org)</ept><ph id=\"ph7\"/> notebook associated with an Apache Spark cluster in HDInsight to run jobs that process your raw sample data and save it as a Hive table. The sample data is a .csv file (hvac.csv) available on all clusters by default.",
      "nodes": [
        {
          "content": "In this section, we use the <bpt id=\"p7\">[</bpt>Jupyter<ept id=\"p7\">](https://jupyter.org)</ept><ph id=\"ph7\"/> notebook associated with an Apache Spark cluster in HDInsight to run jobs that process your raw sample data and save it as a Hive table.",
          "pos": [
            0,
            247
          ]
        },
        {
          "content": "The sample data is a .csv file (hvac.csv) available on all clusters by default.",
          "pos": [
            248,
            327
          ]
        }
      ]
    },
    {
      "pos": [
        1944,
        2083
      ],
      "content": "Once your data is saved as a Hive table, in the next section we will connect to the Hive table using BI tools such as Power BI and Tableau."
    },
    {
      "pos": [
        2088,
        2329
      ],
      "content": "From the <bpt id=\"p8\">[</bpt>Azure Preview Portal<ept id=\"p8\">](https://portal.azure.com/)</ept>, from the startboard, click the tile for your Spark cluster (if you pinned it to the startboard). You can also navigate to your cluster under <bpt id=\"p9\">**</bpt>Browse All<ept id=\"p9\">**</ept><ph id=\"ph8\"/> &gt; <bpt id=\"p10\">**</bpt>HDInsight Clusters<ept id=\"p10\">**</ept>.",
      "nodes": [
        {
          "content": "From the <bpt id=\"p8\">[</bpt>Azure Preview Portal<ept id=\"p8\">](https://portal.azure.com/)</ept>, from the startboard, click the tile for your Spark cluster (if you pinned it to the startboard).",
          "pos": [
            0,
            194
          ]
        },
        {
          "content": "You can also navigate to your cluster under <bpt id=\"p9\">**</bpt>Browse All<ept id=\"p9\">**</ept><ph id=\"ph8\"/> &gt; <bpt id=\"p10\">**</bpt>HDInsight Clusters<ept id=\"p10\">**</ept>.",
          "pos": [
            195,
            374
          ]
        }
      ]
    },
    {
      "pos": [
        2337,
        2522
      ],
      "content": "From the Spark cluster blade, click <bpt id=\"p11\">**</bpt>Quick Links<ept id=\"p11\">**</ept>, and then from the <bpt id=\"p12\">**</bpt>Cluster Dashboard<ept id=\"p12\">**</ept><ph id=\"ph9\"/> blade, click <bpt id=\"p13\">**</bpt>Jupyter Notebook<ept id=\"p13\">**</ept>. If prompted, enter the admin credentials for the cluster.",
      "nodes": [
        {
          "content": "From the Spark cluster blade, click <bpt id=\"p11\">**</bpt>Quick Links<ept id=\"p11\">**</ept>, and then from the <bpt id=\"p12\">**</bpt>Cluster Dashboard<ept id=\"p12\">**</ept><ph id=\"ph9\"/> blade, click <bpt id=\"p13\">**</bpt>Jupyter Notebook<ept id=\"p13\">**</ept>.",
          "pos": [
            0,
            261
          ]
        },
        {
          "content": "If prompted, enter the admin credentials for the cluster.",
          "pos": [
            262,
            319
          ]
        }
      ]
    },
    {
      "pos": [
        2530,
        2700
      ],
      "content": "<ph id=\"ph10\">[AZURE.NOTE]</ph><ph id=\"ph11\"/> You may also reach the Jupyter Notebook for your cluster by opening the following URL in your browser. Replace <bpt id=\"p14\">__</bpt>CLUSTERNAME<ept id=\"p14\">__</ept><ph id=\"ph12\"/> with the name of your cluster:",
      "nodes": [
        {
          "content": "<ph id=\"ph10\">[AZURE.NOTE]</ph><ph id=\"ph11\"/> You may also reach the Jupyter Notebook for your cluster by opening the following URL in your browser.",
          "pos": [
            0,
            149
          ]
        },
        {
          "content": "Replace <bpt id=\"p14\">__</bpt>CLUSTERNAME<ept id=\"p14\">__</ept><ph id=\"ph12\"/> with the name of your cluster:",
          "pos": [
            150,
            259
          ]
        }
      ]
    },
    {
      "pos": [
        2766,
        2832
      ],
      "content": "Create a new notebook. Click <bpt id=\"p15\">**</bpt>New<ept id=\"p15\">**</ept>, and then click <bpt id=\"p16\">**</bpt>Python 2<ept id=\"p16\">**</ept>.",
      "nodes": [
        {
          "content": "Create a new notebook.",
          "pos": [
            0,
            22
          ]
        },
        {
          "content": "Click <bpt id=\"p15\">**</bpt>New<ept id=\"p15\">**</ept>, and then click <bpt id=\"p16\">**</bpt>Python 2<ept id=\"p16\">**</ept>.",
          "pos": [
            23,
            146
          ]
        }
      ]
    },
    {
      "pos": [
        2838,
        3011
      ],
      "content": "<ph id=\"ph14\">![</ph>Create a new Jupyter notebook<ph id=\"ph15\">](./media/hdinsight-apache-spark-custom-library-website-log-analysis/hdispark.note.jupyter.createnotebook.png \"Create a new Jupyter notebook\")</ph>"
    },
    {
      "pos": [
        3016,
        3144
      ],
      "content": "A new notebook is created and opened with the name Untitled.pynb. Click the notebook name at the top, and enter a friendly name.",
      "nodes": [
        {
          "content": "A new notebook is created and opened with the name Untitled.pynb.",
          "pos": [
            0,
            65
          ]
        },
        {
          "content": "Click the notebook name at the top, and enter a friendly name.",
          "pos": [
            66,
            128
          ]
        }
      ]
    },
    {
      "pos": [
        3150,
        3326
      ],
      "content": "<ph id=\"ph16\">![</ph>Provide a name for the notebook<ph id=\"ph17\">](./media/hdinsight-apache-spark-custom-library-website-log-analysis/hdispark.note.jupyter.notebook.name.png \"Provide a name for the notebook\")</ph>"
    },
    {
      "pos": [
        3331,
        3477
      ],
      "content": "Import the required modules and create the Spark and SQL contexts. Paste the following snippet in an empty cell, and then press <bpt id=\"p17\">**</bpt>SHIFT + ENTER<ept id=\"p17\">**</ept>.",
      "nodes": [
        {
          "content": "Import the required modules and create the Spark and SQL contexts.",
          "pos": [
            0,
            66
          ]
        },
        {
          "content": "Paste the following snippet in an empty cell, and then press <bpt id=\"p17\">**</bpt>SHIFT + ENTER<ept id=\"p17\">**</ept>.",
          "pos": [
            67,
            186
          ]
        }
      ]
    },
    {
      "pos": [
        3943,
        4172
      ],
      "content": "Create an RDD using the sample log data already available on the cluster. You\ncan access the data in the default storage account associated with the cluster\nat <bpt id=\"p18\">**</bpt>\\HdiSamples\\HdiSamples\\WebsiteLogSampleData\\SampleLog\\909f2b.log<ept id=\"p18\">**</ept>.",
      "nodes": [
        {
          "content": "Create an RDD using the sample log data already available on the cluster.",
          "pos": [
            0,
            73
          ]
        },
        {
          "content": "You\ncan access the data in the default storage account associated with the cluster\nat <bpt id=\"p18\">**</bpt>\\HdiSamples\\HdiSamples\\WebsiteLogSampleData\\SampleLog\\909f2b.log<ept id=\"p18\">**</ept>.",
          "pos": [
            74,
            269
          ]
        }
      ]
    },
    {
      "pos": [
        4282,
        4364
      ],
      "content": "Retrieve a sample log set to verify that the previous step completed\nsuccessfully."
    },
    {
      "pos": [
        4392,
        4442
      ],
      "content": "You should see an output similar to the following:"
    },
    {
      "pos": [
        6053,
        6099
      ],
      "content": "Analyze log data using a custom Python library"
    },
    {
      "pos": [
        6104,
        6461
      ],
      "content": "In the output above, the first couple lines include the header information and\neach remaining line matches the schema described in that header. Parsing such\nlogs could be complicated. So, we use a custom Python library\n(<bpt id=\"p19\">**</bpt>iislogparser.py<ept id=\"p19\">**</ept>) that makes parsing such logs much easier. By default, this library is included with your Spark cluster on HDInsight.",
      "nodes": [
        {
          "content": "In the output above, the first couple lines include the header information and\neach remaining line matches the schema described in that header.",
          "pos": [
            0,
            143
          ]
        },
        {
          "content": "Parsing such\nlogs could be complicated.",
          "pos": [
            144,
            183
          ]
        },
        {
          "content": "So, we use a custom Python library\n(<bpt id=\"p19\">**</bpt>iislogparser.py<ept id=\"p19\">**</ept>) that makes parsing such logs much easier.",
          "pos": [
            184,
            322
          ]
        },
        {
          "content": "By default, this library is included with your Spark cluster on HDInsight.",
          "pos": [
            323,
            397
          ]
        }
      ]
    },
    {
      "pos": [
        6467,
        6660
      ],
      "content": "However, this library is not in the <ph id=\"ph18\">`PYTHONPATH`</ph><ph id=\"ph19\"/> so we cannot use it by using an import statement like <ph id=\"ph20\">`import iislogparser`</ph>. To use this library, we must distribute it to all the worker nodes.",
      "nodes": [
        {
          "content": "However, this library is not in the <ph id=\"ph18\">`PYTHONPATH`</ph><ph id=\"ph19\"/> so we cannot use it by using an import statement like <ph id=\"ph20\">`import iislogparser`</ph>.",
          "pos": [
            0,
            178
          ]
        },
        {
          "content": "To use this library, we must distribute it to all the worker nodes.",
          "pos": [
            179,
            246
          ]
        }
      ]
    },
    {
      "pos": [
        6666,
        6773
      ],
      "content": "You must then run the following snippet to distribute the library to all worker nodes in the Spark cluster."
    },
    {
      "pos": [
        6872,
        7118
      ],
      "content": "<ph id=\"ph21\">`iislogparser`</ph><ph id=\"ph22\"/> provides a function <ph id=\"ph23\">`parse_log_line`</ph><ph id=\"ph24\"/> that returns <ph id=\"ph25\">`None`</ph><ph id=\"ph26\"/> if a log\nline is a header row, and returns an instance of the <ph id=\"ph27\">`LogLine`</ph><ph id=\"ph28\"/> class if it\nencounters a log line. Use the <ph id=\"ph29\">`LogLine`</ph><ph id=\"ph30\"/> class to extract only the log lines\nfrom the RDD:",
      "nodes": [
        {
          "content": "<ph id=\"ph21\">`iislogparser`</ph><ph id=\"ph22\"/> provides a function <ph id=\"ph23\">`parse_log_line`</ph><ph id=\"ph24\"/> that returns <ph id=\"ph25\">`None`</ph><ph id=\"ph26\"/> if a log\nline is a header row, and returns an instance of the <ph id=\"ph27\">`LogLine`</ph><ph id=\"ph28\"/> class if it\nencounters a log line.",
          "pos": [
            0,
            314
          ]
        },
        {
          "content": "Use the <ph id=\"ph29\">`LogLine`</ph><ph id=\"ph30\"/> class to extract only the log lines\nfrom the RDD:",
          "pos": [
            315,
            416
          ]
        }
      ]
    },
    {
      "pos": [
        7315,
        7403
      ],
      "content": "Retrieve a couple of extracted log lines to verify that the step completed\nsuccessfully."
    },
    {
      "pos": [
        7435,
        7481
      ],
      "content": "The output should be similar to the following:"
    },
    {
      "pos": [
        8391,
        8635
      ],
      "content": "The <ph id=\"ph31\">`LogLine`</ph><ph id=\"ph32\"/> class, in turn, has some useful methods, like <ph id=\"ph33\">`is_error()`</ph>, which\nreturns whether a log entry has an error code. Use this to compute the number of\nerrors in the extracted log lines, and then log all the errors to a different\nfile.",
      "nodes": [
        {
          "content": "The <ph id=\"ph31\">`LogLine`</ph><ph id=\"ph32\"/> class, in turn, has some useful methods, like <ph id=\"ph33\">`is_error()`</ph>, which\nreturns whether a log entry has an error code.",
          "pos": [
            0,
            179
          ]
        },
        {
          "content": "Use this to compute the number of\nerrors in the extracted log lines, and then log all the errors to a different\nfile.",
          "pos": [
            180,
            297
          ]
        }
      ]
    },
    {
      "pos": [
        8975,
        9019
      ],
      "content": "You should see an output like the following:"
    },
    {
      "pos": [
        9159,
        9483
      ],
      "content": "You can also use <bpt id=\"p20\">**</bpt>Matplotlib<ept id=\"p20\">**</ept><ph id=\"ph34\"/> to construct a visualization of the data. For\nexample, if you want to isolate the cause of requests that run for a long time,\nyou might want to find the files that take the most time to serve on average.\nThe snippet below retrieves the top 25 resources that took most time to serve a\nrequest.",
      "nodes": [
        {
          "content": "You can also use <bpt id=\"p20\">**</bpt>Matplotlib<ept id=\"p20\">**</ept><ph id=\"ph34\"/> to construct a visualization of the data.",
          "pos": [
            0,
            128
          ]
        },
        {
          "content": "For\nexample, if you want to isolate the cause of requests that run for a long time,\nyou might want to find the files that take the most time to serve on average.",
          "pos": [
            129,
            290
          ]
        },
        {
          "content": "The snippet below retrieves the top 25 resources that took most time to serve a\nrequest.",
          "pos": [
            291,
            379
          ]
        }
      ]
    },
    {
      "pos": [
        9948,
        9992
      ],
      "content": "You should see an output like the following:"
    },
    {
      "pos": [
        11437,
        11600
      ],
      "content": "You can also present this information in the form of plot. The plot groups the\nlogs by time to see if there were any unusual latency spikes at any particular\ntime.",
      "nodes": [
        {
          "content": "You can also present this information in the form of plot.",
          "pos": [
            0,
            58
          ]
        },
        {
          "content": "The plot groups the\nlogs by time to see if there were any unusual latency spikes at any particular\ntime.",
          "pos": [
            59,
            163
          ]
        }
      ]
    },
    {
      "pos": [
        12014,
        12058
      ],
      "content": "You should see an output like the following:"
    },
    {
      "pos": [
        12064,
        12215
      ],
      "content": "<ph id=\"ph35\">![</ph>Matplotlib output<ph id=\"ph36\">](./media/hdinsight-apache-spark-custom-library-website-log-analysis/hdi-apache-spark-web-log-analysis-plot.png \"Matplotlib output\")</ph>"
    },
    {
      "pos": [
        12221,
        12447
      ],
      "content": "After you have finished running the application, you should shutdown the notebook to release the resources. To do so, from the <bpt id=\"p21\">**</bpt>File<ept id=\"p21\">**</ept><ph id=\"ph37\"/> menu on the notebook, click <bpt id=\"p22\">**</bpt>Close and Halt<ept id=\"p22\">**</ept>. This will shutdown and close the notebook.",
      "nodes": [
        {
          "content": "After you have finished running the application, you should shutdown the notebook to release the resources.",
          "pos": [
            0,
            107
          ]
        },
        {
          "content": "To do so, from the <bpt id=\"p21\">**</bpt>File<ept id=\"p21\">**</ept><ph id=\"ph37\"/> menu on the notebook, click <bpt id=\"p22\">**</bpt>Close and Halt<ept id=\"p22\">**</ept>.",
          "pos": [
            108,
            278
          ]
        },
        {
          "content": "This will shutdown and close the notebook.",
          "pos": [
            279,
            321
          ]
        }
      ]
    },
    {
      "pos": [
        12479,
        12487
      ],
      "content": "See also"
    },
    {
      "pos": [
        12492,
        12571
      ],
      "content": "<bpt id=\"p23\">[</bpt>Overview: Apache Spark on Azure HDInsight<ept id=\"p23\">](hdinsight-apache-spark-overview.md)</ept>"
    },
    {
      "pos": [
        12577,
        12586
      ],
      "content": "Scenarios"
    },
    {
      "pos": [
        12590,
        12719
      ],
      "content": "<bpt id=\"p24\">[</bpt>Spark with BI: Perform interactive data analysis using Spark in HDInsight with BI tools<ept id=\"p24\">](hdinsight-apache-spark-use-bi-tools.md)</ept>"
    },
    {
      "pos": [
        12723,
        12888
      ],
      "content": "<bpt id=\"p25\">[</bpt>Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data<ept id=\"p25\">](hdinsight-apache-spark-ipython-notebook-machine-learning.md)</ept>"
    },
    {
      "pos": [
        12892,
        13038
      ],
      "content": "<bpt id=\"p26\">[</bpt>Spark with Machine Learning: Use Spark in HDInsight to predict food inspection results<ept id=\"p26\">](hdinsight-apache-spark-machine-learning-mllib-ipython.md)</ept>"
    },
    {
      "pos": [
        13042,
        13175
      ],
      "content": "<bpt id=\"p27\">[</bpt>Spark Streaming: Use Spark in HDInsight for building real-time streaming applications<ept id=\"p27\">](hdinsight-apache-spark-eventhub-streaming.md)</ept>"
    },
    {
      "pos": [
        13181,
        13208
      ],
      "content": "Create and run applications"
    },
    {
      "pos": [
        13212,
        13314
      ],
      "content": "<bpt id=\"p28\">[</bpt>Create a standalone application using Scala<ept id=\"p28\">](hdinsight-apache-spark-create-standalone-application.md)</ept>"
    },
    {
      "pos": [
        13318,
        13414
      ],
      "content": "<bpt id=\"p29\">[</bpt>Run jobs remotely on a Spark cluster using Livy<ept id=\"p29\">](hdinsight-apache-spark-livy-rest-interface.md)</ept>"
    },
    {
      "pos": [
        13420,
        13440
      ],
      "content": "Tools and extensions"
    },
    {
      "pos": [
        13444,
        13583
      ],
      "content": "<bpt id=\"p30\">[</bpt>Use HDInsight Tools Plugin for IntelliJ IDEA to create and submit Spark Scala applicatons<ept id=\"p30\">](hdinsight-apache-spark-intellij-tool-plugin.md)</ept>"
    },
    {
      "pos": [
        13587,
        13694
      ],
      "content": "<bpt id=\"p31\">[</bpt>Use Zeppelin notebooks with a Spark cluster on HDInsight<ept id=\"p31\">](hdinsight-apache-spark-use-zeppelin-notebook.md)</ept>"
    },
    {
      "pos": [
        13698,
        13821
      ],
      "content": "<bpt id=\"p32\">[</bpt>Kernels available for Jupyter notebook in Spark cluster for HDInsight<ept id=\"p32\">](hdinsight-apache-spark-jupyter-notebook-kernels.md)</ept>"
    },
    {
      "pos": [
        13827,
        13843
      ],
      "content": "Manage resources"
    },
    {
      "pos": [
        13847,
        13957
      ],
      "content": "<bpt id=\"p33\">[</bpt>Manage resources for the Apache Spark cluster in Azure HDInsight<ept id=\"p33\">](hdinsight-apache-spark-resource-manager.md)</ept>"
    }
  ],
  "content": "<properties \n    pageTitle=\"Use custom libraries with an HDInsight Spark cluster to analyze website logs | Microsoft Azure\" \n    description=\"Use custom libraries with an HDInsight Spark cluster to analyze website logs\" \n    services=\"hdinsight\" \n    documentationCenter=\"\" \n    authors=\"nitinme\" \n    manager=\"paulettm\" \n    editor=\"cgronlun\"\n    tags=\"azure-portal\"/>\n\n<tags \n    ms.service=\"hdinsight\" \n    ms.workload=\"big-data\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"02/05/2016\" \n    ms.author=\"nitinme\"/>\n\n# Analyze logs in HDInsight Spark using a custom library (Linux)\n\nThis notebook demonstrates how to analyze log data using a custom library with Spark on HDInsight. The custom library we use is a Python library called **iislogparser.py**.\n\n> [AZURE.TIP] This tutorial is also available as a Jupyter notebook on a Spark (Linux) cluster that you create in HDInsight. The notebook experience lets you run the Python snippets from the notebook itself. To perform the tutorial from within a notebook, create a Spark cluster, launch a Jupyter notebook (`https://CLUSTERNAME.azurehdinsight.net/jupyter`), and then run the notebook **Analyze logs with Spark using a custom library.ipynb** under the **Python** folder.\n\n**Prerequisites:**\n\nYou must have the following:\n\n- An Azure subscription. See [Get Azure free trial](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/).\n- An Apache Spark cluster on HDInsight Linux. For instructions, see [Create Apache Spark clusters in Azure HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).\n\n## Save raw data as an RDD\n\nIn this section, we use the [Jupyter](https://jupyter.org) notebook associated with an Apache Spark cluster in HDInsight to run jobs that process your raw sample data and save it as a Hive table. The sample data is a .csv file (hvac.csv) available on all clusters by default.\n\nOnce your data is saved as a Hive table, in the next section we will connect to the Hive table using BI tools such as Power BI and Tableau.\n\n1. From the [Azure Preview Portal](https://portal.azure.com/), from the startboard, click the tile for your Spark cluster (if you pinned it to the startboard). You can also navigate to your cluster under **Browse All** > **HDInsight Clusters**.   \n\n2. From the Spark cluster blade, click **Quick Links**, and then from the **Cluster Dashboard** blade, click **Jupyter Notebook**. If prompted, enter the admin credentials for the cluster.\n\n    > [AZURE.NOTE] You may also reach the Jupyter Notebook for your cluster by opening the following URL in your browser. Replace __CLUSTERNAME__ with the name of your cluster:\n    >\n    > `https://CLUSTERNAME.azurehdinsight.net/jupyter`\n\n2. Create a new notebook. Click **New**, and then click **Python 2**.\n\n    ![Create a new Jupyter notebook](./media/hdinsight-apache-spark-custom-library-website-log-analysis/hdispark.note.jupyter.createnotebook.png \"Create a new Jupyter notebook\")\n\n3. A new notebook is created and opened with the name Untitled.pynb. Click the notebook name at the top, and enter a friendly name.\n\n    ![Provide a name for the notebook](./media/hdinsight-apache-spark-custom-library-website-log-analysis/hdispark.note.jupyter.notebook.name.png \"Provide a name for the notebook\")\n\n4. Import the required modules and create the Spark and SQL contexts. Paste the following snippet in an empty cell, and then press **SHIFT + ENTER**.\n\n\n        import pyspark\n        from pyspark import SparkConf\n        from pyspark import SparkContext\n        from pyspark.sql import SQLContext\n        %matplotlib inline\n        import matplotlib.pyplot as plt\n        from pyspark.sql import Row\n        from pyspark.sql.types import *\n        import atexit\n        sc = SparkContext(conf=SparkConf().setMaster('yarn-client'))\n        sqlContext = SQLContext(sc)\n        atexit.register(lambda: sc.stop())\n\n\n5. Create an RDD using the sample log data already available on the cluster. You\ncan access the data in the default storage account associated with the cluster\nat **\\HdiSamples\\HdiSamples\\WebsiteLogSampleData\\SampleLog\\909f2b.log**.\n\n\n        logs = sc.textFile('wasb:///HdiSamples/HdiSamples/WebsiteLogSampleData/SampleLog/909f2b.log')\n\n\n6. Retrieve a sample log set to verify that the previous step completed\nsuccessfully.\n\n        logs.take(5)\n\n    You should see an output similar to the following:\n\n        # -----------------\n        # THIS IS AN OUTPUT\n        # -----------------\n\n        [u'#Software: Microsoft Internet Information Services 8.0',\n         u'#Fields: date time s-sitename cs-method cs-uri-stem cs-uri-query s-port cs-username c-ip cs(User-Agent) cs(Cookie) cs(Referer) cs-host sc-status sc-substatus sc-win32-status sc-bytes cs-bytes time-taken',\n         u'2014-01-01 02:01:09 SAMPLEWEBSITE GET /blogposts/mvc4/step2.png X-ARR-LOG-ID=2ec4b8ad-3cf0-4442-93ab-837317ece6a1 80 - 1.54.23.196 Mozilla/5.0+(Windows+NT+6.3;+WOW64)+AppleWebKit/537.36+(KHTML,+like+Gecko)+Chrome/31.0.1650.63+Safari/537.36 - http://weblogs.asp.net/sample/archive/2007/12/09/asp-net-mvc-framework-part-4-handling-form-edit-and-post-scenarios.aspx www.sample.com 200 0 0 53175 871 46',\n         u'2014-01-01 02:01:09 SAMPLEWEBSITE GET /blogposts/mvc4/step3.png X-ARR-LOG-ID=9eace870-2f49-4efd-b204-0d170da46b4a 80 - 1.54.23.196 Mozilla/5.0+(Windows+NT+6.3;+WOW64)+AppleWebKit/537.36+(KHTML,+like+Gecko)+Chrome/31.0.1650.63+Safari/537.36 - http://weblogs.asp.net/sample/archive/2007/12/09/asp-net-mvc-framework-part-4-handling-form-edit-and-post-scenarios.aspx www.sample.com 200 0 0 51237 871 32',\n         u'2014-01-01 02:01:09 SAMPLEWEBSITE GET /blogposts/mvc4/step4.png X-ARR-LOG-ID=4bea5b3d-8ac9-46c9-9b8c-ec3e9500cbea 80 - 1.54.23.196 Mozilla/5.0+(Windows+NT+6.3;+WOW64)+AppleWebKit/537.36+(KHTML,+like+Gecko)+Chrome/31.0.1650.63+Safari/537.36 - http://weblogs.asp.net/sample/archive/2007/12/09/asp-net-mvc-framework-part-4-handling-form-edit-and-post-scenarios.aspx www.sample.com 200 0 0 72177 871 47']\n\n## Analyze log data using a custom Python library\n\n7. In the output above, the first couple lines include the header information and\neach remaining line matches the schema described in that header. Parsing such\nlogs could be complicated. So, we use a custom Python library\n(**iislogparser.py**) that makes parsing such logs much easier. By default, this library is included with your Spark cluster on HDInsight.\n\n    However, this library is not in the `PYTHONPATH` so we cannot use it by using an import statement like `import iislogparser`. To use this library, we must distribute it to all the worker nodes.\n\n    You must then run the following snippet to distribute the library to all worker nodes in the Spark cluster.\n\n\n        sc.addPyFile('wasb:///HdiSamples/HdiSamples/WebsiteLogSampleData/iislogparser.py')\n\n\n9. `iislogparser` provides a function `parse_log_line` that returns `None` if a log\nline is a header row, and returns an instance of the `LogLine` class if it\nencounters a log line. Use the `LogLine` class to extract only the log lines\nfrom the RDD:\n\n        def parse_line(l):\n            import iislogparser\n            return iislogparser.parse_log_line(l)\n        logLines = logs.map(parse_line).filter(lambda p: p is not None).cache()\n\n\n10. Retrieve a couple of extracted log lines to verify that the step completed\nsuccessfully.\n\n        logLines.take(2)\n\n    The output should be similar to the following:\n\n        # -----------------\n        # THIS IS AN OUTPUT\n        # -----------------\n\n        [2014-01-01 02:01:09 SAMPLEWEBSITE GET /blogposts/mvc4/step2.png X-ARR-LOG-ID=2ec4b8ad-3cf0-4442-93ab-837317ece6a1 80 - 1.54.23.196 Mozilla/5.0+(Windows+NT+6.3;+WOW64)+AppleWebKit/537.36+(KHTML,+like+Gecko)+Chrome/31.0.1650.63+Safari/537.36 - http://weblogs.asp.net/sample/archive/2007/12/09/asp-net-mvc-framework-part-4-handling-form-edit-and-post-scenarios.aspx www.sample.com 200 0 0 53175 871 46,\n        2014-01-01 02:01:09 SAMPLEWEBSITE GET /blogposts/mvc4/step3.png X-ARR-LOG-ID=9eace870-2f49-4efd-b204-0d170da46b4a 80 - 1.54.23.196 Mozilla/5.0+(Windows+NT+6.3;+WOW64)+AppleWebKit/537.36+(KHTML,+like+Gecko)+Chrome/31.0.1650.63+Safari/537.36 - http://weblogs.asp.net/sample/archive/2007/12/09/asp-net-mvc-framework-part-4-handling-form-edit-and-post-scenarios.aspx www.sample.com 200 0 0 51237 871 32]\n\n\n11. The `LogLine` class, in turn, has some useful methods, like `is_error()`, which\nreturns whether a log entry has an error code. Use this to compute the number of\nerrors in the extracted log lines, and then log all the errors to a different\nfile.\n\n        errors = logLines.filter(lambda p: p.is_error())\n        numLines = logLines.count()\n        numErrors = errors.count()\n        print 'There are', numErrors, 'errors and', numLines, 'log entries'\n        errors.map(lambda p: str(p)).saveAsTextFile('wasb:///HdiSamples/HdiSamples/WebsiteLogSampleData/SampleLog/909f2b-2.log')\n\n    You should see an output like the following:\n\n        # -----------------\n        # THIS IS AN OUTPUT\n        # -----------------\n\n        There are 30 errors and 646 log entries\n\n12. You can also use **Matplotlib** to construct a visualization of the data. For\nexample, if you want to isolate the cause of requests that run for a long time,\nyou might want to find the files that take the most time to serve on average.\nThe snippet below retrieves the top 25 resources that took most time to serve a\nrequest.\n\n        def avgTimeTakenByKey(rdd):\n            return rdd.combineByKey(lambda line: (line.time_taken, 1),\n                                    lambda x, line: (x[0] + line.time_taken, x[1] + 1),\n                                    lambda x, y: (x[0] + y[0], x[1] + y[1]))\\\n                      .map(lambda x: (x[0], float(x[1][0]) / float(x[1][1])))\n            \n        avgTimeTakenByKey(logLines.map(lambda p: (p.cs_uri_stem, p))).top(25, lambda x: x[1])\n\n    You should see an output like the following:\n\n        # -----------------\n        # THIS IS AN OUTPUT\n        # -----------------\n\n        [(u'/blogposts/mvc4/step13.png', 197.5),\n         (u'/blogposts/mvc2/step10.jpg', 179.5),\n         (u'/blogposts/extractusercontrol/step5.png', 170.0),\n         (u'/blogposts/mvc4/step8.png', 159.0),\n         (u'/blogposts/mvcrouting/step22.jpg', 155.0),\n         (u'/blogposts/mvcrouting/step3.jpg', 152.0),\n         (u'/blogposts/linqsproc1/step16.jpg', 138.75),\n         (u'/blogposts/linqsproc1/step26.jpg', 137.33333333333334),\n         (u'/blogposts/vs2008javascript/step10.jpg', 127.0),\n         (u'/blogposts/nested/step2.jpg', 126.0),\n         (u'/blogposts/adminpack/step1.png', 124.0),\n         (u'/BlogPosts/datalistpaging/step2.png', 118.0),\n         (u'/blogposts/mvc4/step35.png', 117.0),\n         (u'/blogposts/mvcrouting/step2.jpg', 116.5),\n         (u'/blogposts/aboutme/basketball.jpg', 109.0),\n         (u'/blogposts/anonymoustypes/step11.jpg', 109.0),\n         (u'/blogposts/mvc4/step12.png', 106.0),\n         (u'/blogposts/linq8/step0.jpg', 105.5),\n         (u'/blogposts/mvc2/step18.jpg', 104.0),\n         (u'/blogposts/mvc2/step11.jpg', 104.0),\n         (u'/blogposts/mvcrouting/step1.jpg', 104.0),\n         (u'/blogposts/extractusercontrol/step1.png', 103.0),\n         (u'/blogposts/sqlvideos/sqlvideos.jpg', 102.0),\n         (u'/blogposts/mvcrouting/step21.jpg', 101.0),\n         (u'/blogposts/mvc4/step1.png', 98.0)]\n\n\n13. You can also present this information in the form of plot. The plot groups the\nlogs by time to see if there were any unusual latency spikes at any particular\ntime.\n\n        avgTimeTakenByMinute = avgTimeTakenByKey(logLines.map(lambda p: (p.datetime.minute, p))).sortByKey()\n        minutes = avgTimeTakenByMinute.map(lambda pair: pair[0]).collect()\n        time = avgTimeTakenByMinute.map(lambda pair: pair[1]).collect()\n        plt.plot(minutes, time, marker='o', linestyle='--')\n        plt.xlabel('Time (min)')\n        plt.ylabel('Average time taken for request (ms)')\n\n    You should see an output like the following:\n\n    ![Matplotlib output](./media/hdinsight-apache-spark-custom-library-website-log-analysis/hdi-apache-spark-web-log-analysis-plot.png \"Matplotlib output\")\n\n14. After you have finished running the application, you should shutdown the notebook to release the resources. To do so, from the **File** menu on the notebook, click **Close and Halt**. This will shutdown and close the notebook.\n    \n\n## <a name=\"seealso\"></a>See also\n\n\n* [Overview: Apache Spark on Azure HDInsight](hdinsight-apache-spark-overview.md)\n\n### Scenarios\n\n* [Spark with BI: Perform interactive data analysis using Spark in HDInsight with BI tools](hdinsight-apache-spark-use-bi-tools.md)\n\n* [Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data](hdinsight-apache-spark-ipython-notebook-machine-learning.md)\n\n* [Spark with Machine Learning: Use Spark in HDInsight to predict food inspection results](hdinsight-apache-spark-machine-learning-mllib-ipython.md)\n\n* [Spark Streaming: Use Spark in HDInsight for building real-time streaming applications](hdinsight-apache-spark-eventhub-streaming.md)\n\n### Create and run applications\n\n* [Create a standalone application using Scala](hdinsight-apache-spark-create-standalone-application.md)\n\n* [Run jobs remotely on a Spark cluster using Livy](hdinsight-apache-spark-livy-rest-interface.md)\n\n### Tools and extensions\n\n* [Use HDInsight Tools Plugin for IntelliJ IDEA to create and submit Spark Scala applicatons](hdinsight-apache-spark-intellij-tool-plugin.md)\n\n* [Use Zeppelin notebooks with a Spark cluster on HDInsight](hdinsight-apache-spark-use-zeppelin-notebook.md)\n\n* [Kernels available for Jupyter notebook in Spark cluster for HDInsight](hdinsight-apache-spark-jupyter-notebook-kernels.md)\n\n### Manage resources\n\n* [Manage resources for the Apache Spark cluster in Azure HDInsight](hdinsight-apache-spark-resource-manager.md)"
}