{
  "nodes": [
    {
      "pos": [
        27,
        80
      ],
      "content": "Run the Hadoop samples in HDInsight | Microsoft Azure"
    },
    {
      "pos": [
        99,
        240
      ],
      "content": "Get started using the Azure HDInsight service with the samples provided. Use PowerShell scripts that run MapReduce programs on data clusters.",
      "nodes": [
        {
          "content": "Get started using the Azure HDInsight service with the samples provided.",
          "pos": [
            0,
            72
          ]
        },
        {
          "content": "Use PowerShell scripts that run MapReduce programs on data clusters.",
          "pos": [
            73,
            141
          ]
        }
      ]
    },
    {
      "pos": [
        563,
        618
      ],
      "content": "Run Hadoop MapReduce samples in Windows-based HDInsight"
    },
    {
      "pos": [
        706,
        1027
      ],
      "content": "A set of samples are provided to help you get started running MapReduce jobs on Hadoop clusters using Azure HDInsight. These samples are made available on each of the HDInsight managed clusters that you create. Running these samples will familiarize you with using Azure PowerShell cmdlets to run jobs on Hadoop clusters.",
      "nodes": [
        {
          "content": "A set of samples are provided to help you get started running MapReduce jobs on Hadoop clusters using Azure HDInsight.",
          "pos": [
            0,
            118
          ]
        },
        {
          "content": "These samples are made available on each of the HDInsight managed clusters that you create.",
          "pos": [
            119,
            210
          ]
        },
        {
          "content": "Running these samples will familiarize you with using Azure PowerShell cmdlets to run jobs on Hadoop clusters.",
          "pos": [
            211,
            321
          ]
        }
      ]
    },
    {
      "pos": [
        1031,
        1116
      ],
      "content": "<bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>Word count<ept id=\"p2\">**</ept><ept id=\"p1\">][hdinsight-sample-wordcount]</ept>: Counts word occurrences in a text file."
    },
    {
      "pos": [
        1119,
        1261
      ],
      "content": "<bpt id=\"p3\">[</bpt><bpt id=\"p4\">**</bpt>C# streaming word count<ept id=\"p4\">**</ept><ept id=\"p3\">][hdinsight-sample-csharp-streaming]</ept>: Counts word occurrences in a text file using the Hadoop streaming interface."
    },
    {
      "pos": [
        1264,
        1389
      ],
      "content": "<bpt id=\"p5\">[</bpt><bpt id=\"p6\">**</bpt>Pi estimator<ept id=\"p6\">**</ept><ept id=\"p5\">][hdinsight-sample-pi-estimator]</ept>: Uses a statistical (quasi-Monte Carlo) method to estimate the value of pi."
    },
    {
      "pos": [
        1392,
        1666
      ],
      "content": "<bpt id=\"p7\">[</bpt><bpt id=\"p8\">**</bpt>10-GB Graysort<ept id=\"p8\">**</ept><ept id=\"p7\">][hdinsight-sample-10gb-graysort]</ept>: Run a general purpose GraySort on a 10 GB file by using HDInsight. There are three jobs to run: Teragen to generate the data, Terasort to sort the data, and Teravalidate to confirm that the data has been properly sorted.",
      "nodes": [
        {
          "content": "<bpt id=\"p7\">[</bpt><bpt id=\"p8\">**</bpt>10-GB Graysort<ept id=\"p8\">**</ept><ept id=\"p7\">][hdinsight-sample-10gb-graysort]</ept>: Run a general purpose GraySort on a 10 GB file by using HDInsight.",
          "pos": [
            0,
            196
          ]
        },
        {
          "content": "There are three jobs to run: Teragen to generate the data, Terasort to sort the data, and Teravalidate to confirm that the data has been properly sorted.",
          "pos": [
            197,
            350
          ]
        }
      ]
    },
    {
      "pos": [
        1669,
        1727
      ],
      "content": "<ph id=\"ph3\">[AZURE.NOTE]</ph><ph id=\"ph4\"/> The source code can be found in the Appendix."
    },
    {
      "pos": [
        1730,
        1998
      ],
      "content": "Much additional documentation exists on the web for Hadoop-related technologies, such as Java-based MapReduce programming and streaming, and documentation about the cmdlets that are used in Windows PowerShell scripting. For more information about these resources, see:",
      "nodes": [
        {
          "content": "Much additional documentation exists on the web for Hadoop-related technologies, such as Java-based MapReduce programming and streaming, and documentation about the cmdlets that are used in Windows PowerShell scripting.",
          "pos": [
            0,
            219
          ]
        },
        {
          "content": "For more information about these resources, see:",
          "pos": [
            220,
            268
          ]
        }
      ]
    },
    {
      "pos": [
        2002,
        2103
      ],
      "content": "<bpt id=\"p9\">[</bpt>Develop Java MapReduce programs for Hadoop in HDInsight<ept id=\"p9\">](hdinsight-develop-deploy-java-mapreduce.md)</ept>"
    },
    {
      "pos": [
        2106,
        2209
      ],
      "content": "<bpt id=\"p10\">[</bpt>Develop C# Hadoop streaming programs for HDInsight<ept id=\"p10\">](hdinsight-hadoop-develop-deploy-streaming-jobs.md)</ept>"
    },
    {
      "pos": [
        2212,
        2295
      ],
      "content": "<bpt id=\"p11\">[</bpt>Submit Hadoop jobs in HDInsight<ept id=\"p11\">](hdinsight-submit-hadoop-jobs-programmatically.md)</ept>"
    },
    {
      "pos": [
        2298,
        2355
      ],
      "content": "<bpt id=\"p12\">[</bpt>Introduction to Azure HDInsight<ept id=\"p12\">][hdinsight-introduction]</ept>"
    },
    {
      "pos": [
        2357,
        2447
      ],
      "content": "Nowadays, a lot of people choose Hive and Pig over MapReduce.  For more information, see :",
      "nodes": [
        {
          "content": "Nowadays, a lot of people choose Hive and Pig over MapReduce.",
          "pos": [
            0,
            61
          ]
        },
        {
          "content": "For more information, see :",
          "pos": [
            63,
            90
          ]
        }
      ]
    },
    {
      "pos": [
        2451,
        2497
      ],
      "content": "<bpt id=\"p13\">[</bpt>Use Hive in HDInsight<ept id=\"p13\">](hdinsight-use-hive.md)</ept>"
    },
    {
      "pos": [
        2500,
        2544
      ],
      "content": "<bpt id=\"p14\">[</bpt>Use Pig in HDInsight<ept id=\"p14\">](hdinsight-use-pig.md)</ept>"
    },
    {
      "pos": [
        2547,
        2565
      ],
      "content": "<bpt id=\"p15\">**</bpt>Prerequisites<ept id=\"p15\">**</ept>:"
    },
    {
      "pos": [
        2569,
        2727
      ],
      "content": "<bpt id=\"p16\">**</bpt>An Azure subscription<ept id=\"p16\">**</ept>. See <bpt id=\"p17\">[</bpt>Get Azure free trial<ept id=\"p17\">](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/)</ept>.",
      "nodes": [
        {
          "content": "<bpt id=\"p16\">**</bpt>An Azure subscription<ept id=\"p16\">**</ept>.",
          "pos": [
            0,
            66
          ]
        },
        {
          "content": "See <bpt id=\"p17\">[</bpt>Get Azure free trial<ept id=\"p17\">](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/)</ept>.",
          "pos": [
            67,
            238
          ]
        }
      ]
    },
    {
      "pos": [
        2730,
        2907
      ],
      "content": "<bpt id=\"p18\">**</bpt>an HDInsight cluster<ept id=\"p18\">**</ept>. For instructions on the various ways in which such clusters can be created, see <bpt id=\"p19\">[</bpt>Create Hadoop clusters in HDInsight<ept id=\"p19\">](hdinsight-provision-clusters.md)</ept>.",
      "nodes": [
        {
          "content": "<bpt id=\"p18\">**</bpt>an HDInsight cluster<ept id=\"p18\">**</ept>.",
          "pos": [
            0,
            65
          ]
        },
        {
          "content": "For instructions on the various ways in which such clusters can be created, see <bpt id=\"p19\">[</bpt>Create Hadoop clusters in HDInsight<ept id=\"p19\">](hdinsight-provision-clusters.md)</ept>.",
          "pos": [
            66,
            257
          ]
        }
      ]
    },
    {
      "pos": [
        2910,
        3078
      ],
      "content": "<bpt id=\"p20\">**</bpt>A workstation with Azure PowerShell<ept id=\"p20\">**</ept>. See <bpt id=\"p21\">[</bpt>Install Azure PowerShell 1.0 and greater<ept id=\"p21\">](hdinsight-administer-use-powershell.md#install-azure-powershell-10-and-greater)</ept>.",
      "nodes": [
        {
          "content": "<bpt id=\"p20\">**</bpt>A workstation with Azure PowerShell<ept id=\"p20\">**</ept>.",
          "pos": [
            0,
            80
          ]
        },
        {
          "content": "See <bpt id=\"p21\">[</bpt>Install Azure PowerShell 1.0 and greater<ept id=\"p21\">](hdinsight-administer-use-powershell.md#install-azure-powershell-10-and-greater)</ept>.",
          "pos": [
            81,
            248
          ]
        }
      ]
    },
    {
      "pos": [
        3124,
        3141
      ],
      "content": "Word count - Java"
    },
    {
      "pos": [
        3144,
        3552
      ],
      "content": "To submit a MapReduce project, you first create a MapReduce job definition. In the job definition, you specify the MapReduce program jar file and the location of the jar file, which is <bpt id=\"p22\">**</bpt>wasb:///example/jars/hadoop-mapreduce-examples.jar<ept id=\"p22\">**</ept>, the class name, and the arguments.  The wordcount MapReduce program takes two arguments: the source file that will be used to count words, and the location for output.",
      "nodes": [
        {
          "content": "To submit a MapReduce project, you first create a MapReduce job definition.",
          "pos": [
            0,
            75
          ]
        },
        {
          "content": "In the job definition, you specify the MapReduce program jar file and the location of the jar file, which is <bpt id=\"p22\">**</bpt>wasb:///example/jars/hadoop-mapreduce-examples.jar<ept id=\"p22\">**</ept>, the class name, and the arguments.",
          "pos": [
            76,
            315
          ]
        },
        {
          "content": "The wordcount MapReduce program takes two arguments: the source file that will be used to count words, and the location for output.",
          "pos": [
            317,
            448
          ]
        }
      ]
    },
    {
      "pos": [
        3554,
        3658
      ],
      "content": "The source code can be found in the <bpt id=\"p23\">[</bpt>Appendix A<ept id=\"p23\">](#apendix-a---the-word-count-MapReduce-program-in-java)</ept>."
    },
    {
      "pos": [
        3660,
        3825
      ],
      "content": "For the procedure of developing a Java MapReduce program, see - <bpt id=\"p24\">[</bpt>Develop Java MapReduce programs for Hadoop in HDInsight<ept id=\"p24\">](hdinsight-develop-deploy-java-mapreduce.md)</ept>"
    },
    {
      "pos": [
        3828,
        3868
      ],
      "content": "<bpt id=\"p25\">**</bpt>To submit a word count MapReduce job<ept id=\"p25\">**</ept>"
    },
    {
      "pos": [
        3873,
        3999
      ],
      "content": "Open <bpt id=\"p26\">**</bpt>Windows PowerShell ISE<ept id=\"p26\">**</ept>. For instructions, see <bpt id=\"p27\">[</bpt>Install and configure Azure PowerShell<ept id=\"p27\">][powershell-install-configure]</ept>.",
      "nodes": [
        {
          "content": "Open <bpt id=\"p26\">**</bpt>Windows PowerShell ISE<ept id=\"p26\">**</ept>.",
          "pos": [
            0,
            72
          ]
        },
        {
          "content": "For instructions, see <bpt id=\"p27\">[</bpt>Install and configure Azure PowerShell<ept id=\"p27\">][powershell-install-configure]</ept>.",
          "pos": [
            73,
            206
          ]
        }
      ]
    },
    {
      "pos": [
        4003,
        4041
      ],
      "content": "Paste the following PowerShell script:"
    },
    {
      "pos": [
        6727,
        6909
      ],
      "content": "The MapReduce job produces a file named <bpt id=\"p28\">*</bpt>part-r-00000<ept id=\"p28\">*</ept>, which contains words and the counts. The script uses the <bpt id=\"p29\">**</bpt>findstr<ept id=\"p29\">**</ept><ph id=\"ph5\"/> command to list all of the words that contains <bpt id=\"p30\">*</bpt>\"there\"<ept id=\"p30\">*</ept>.",
      "nodes": [
        {
          "content": "The MapReduce job produces a file named <bpt id=\"p28\">*</bpt>part-r-00000<ept id=\"p28\">*</ept>, which contains words and the counts.",
          "pos": [
            0,
            132
          ]
        },
        {
          "content": "The script uses the <bpt id=\"p29\">**</bpt>findstr<ept id=\"p29\">**</ept><ph id=\"ph5\"/> command to list all of the words that contains <bpt id=\"p30\">*</bpt>\"there\"<ept id=\"p30\">*</ept>.",
          "pos": [
            133,
            316
          ]
        }
      ]
    },
    {
      "pos": [
        6914,
        6960
      ],
      "content": "Set the first 3 variables, and run the script."
    },
    {
      "pos": [
        7013,
        7038
      ],
      "content": "Word count - C# streaming"
    },
    {
      "pos": [
        7040,
        7167
      ],
      "content": "Hadoop provides a streaming API to MapReduce, which enables you to write map and reduce functions in languages other than Java."
    },
    {
      "pos": [
        7171,
        7415
      ],
      "content": "<ph id=\"ph6\">[AZURE.NOTE]</ph><ph id=\"ph7\"/> The steps in this tutorial apply only to Windows-based HDInsight clusters. For an example of streaming for Linux-based HDInsight clusters, see <bpt id=\"p31\">[</bpt>Develop Python streaming programs for HDInsight<ept id=\"p31\">](hdinsight-hadoop-streaming-python.md)</ept>.",
      "nodes": [
        {
          "content": "<ph id=\"ph6\">[AZURE.NOTE]</ph><ph id=\"ph7\"/> The steps in this tutorial apply only to Windows-based HDInsight clusters.",
          "pos": [
            0,
            119
          ]
        },
        {
          "content": "For an example of streaming for Linux-based HDInsight clusters, see <bpt id=\"p31\">[</bpt>Develop Python streaming programs for HDInsight<ept id=\"p31\">](hdinsight-hadoop-streaming-python.md)</ept>.",
          "pos": [
            120,
            316
          ]
        }
      ]
    },
    {
      "pos": [
        7417,
        7647
      ],
      "content": "In the example, the mapper and the reducer are executables that read the input from [stdin][stdin-stdout-stderr] (line-by-line) and emit the output to [stdout][stdin-stdout-stderr]. The program counts all of the words in the text.",
      "nodes": [
        {
          "content": "In the example, the mapper and the reducer are executables that read the input from [stdin][stdin-stdout-stderr] (line-by-line) and emit the output to [stdout][stdin-stdout-stderr].",
          "pos": [
            0,
            181
          ]
        },
        {
          "content": "The program counts all of the words in the text.",
          "pos": [
            182,
            230
          ]
        }
      ]
    },
    {
      "pos": [
        7649,
        7923
      ],
      "content": "When an executable is specified for <bpt id=\"p32\">**</bpt>mappers<ept id=\"p32\">**</ept>, each mapper task launches the executable as a separate process when the mapper is initialized. As the mapper task runs, it converts its input into lines, and feeds the lines to the [stdin][stdin-stdout-stderr] of the process.",
      "nodes": [
        {
          "content": "When an executable is specified for <bpt id=\"p32\">**</bpt>mappers<ept id=\"p32\">**</ept>, each mapper task launches the executable as a separate process when the mapper is initialized.",
          "pos": [
            0,
            183
          ]
        },
        {
          "content": "As the mapper task runs, it converts its input into lines, and feeds the lines to the [stdin][stdin-stdout-stderr] of the process.",
          "pos": [
            184,
            314
          ]
        }
      ]
    },
    {
      "pos": [
        7925,
        8363
      ],
      "content": "In the meantime, the mapper collects the line-oriented output from the stdout of the process. It converts each line into a key/value pair, which is collected as the output of the mapper. By default, the prefix of a line up to the first Tab character is the key, and the remainder of the line (excluding the Tab character) is the value. If there is no Tab character in the line, entire line is considered as the key, and the value is null.",
      "nodes": [
        {
          "content": "In the meantime, the mapper collects the line-oriented output from the stdout of the process.",
          "pos": [
            0,
            93
          ]
        },
        {
          "content": "It converts each line into a key/value pair, which is collected as the output of the mapper.",
          "pos": [
            94,
            186
          ]
        },
        {
          "content": "By default, the prefix of a line up to the first Tab character is the key, and the remainder of the line (excluding the Tab character) is the value.",
          "pos": [
            187,
            335
          ]
        },
        {
          "content": "If there is no Tab character in the line, entire line is considered as the key, and the value is null.",
          "pos": [
            336,
            438
          ]
        }
      ]
    },
    {
      "pos": [
        8365,
        8663
      ],
      "content": "When an executable is specified for <bpt id=\"p33\">**</bpt>reducers<ept id=\"p33\">**</ept>, each reducer task launches the executable as a separate process when the reducer is initialized. As the reducer task runs, it converts its input key/values pairs into lines, and it feeds the lines to the [stdin][stdin-stdout-stderr] of the process.",
      "nodes": [
        {
          "content": "When an executable is specified for <bpt id=\"p33\">**</bpt>reducers<ept id=\"p33\">**</ept>, each reducer task launches the executable as a separate process when the reducer is initialized.",
          "pos": [
            0,
            186
          ]
        },
        {
          "content": "As the reducer task runs, it converts its input key/values pairs into lines, and it feeds the lines to the [stdin][stdin-stdout-stderr] of the process.",
          "pos": [
            187,
            338
          ]
        }
      ]
    },
    {
      "pos": [
        8665,
        9023
      ],
      "content": "In the meantime, the reducer collects the line-oriented output from the [stdout][stdin-stdout-stderr] of the process. It converts each line to a key/value pair, which is collected as the output of the reducer. By default, the prefix of a line up to the first Tab character is the key, and the remainder of the line (excluding the Tab character) is the value.",
      "nodes": [
        {
          "content": "In the meantime, the reducer collects the line-oriented output from the [stdout][stdin-stdout-stderr] of the process.",
          "pos": [
            0,
            117
          ]
        },
        {
          "content": "It converts each line to a key/value pair, which is collected as the output of the reducer.",
          "pos": [
            118,
            209
          ]
        },
        {
          "content": "By default, the prefix of a line up to the first Tab character is the key, and the remainder of the line (excluding the Tab character) is the value.",
          "pos": [
            210,
            358
          ]
        }
      ]
    },
    {
      "pos": [
        9025,
        9125
      ],
      "content": "For more information about the Hadoop Streaming interface, see [Hadoop Streaming][hadoop-streaming]."
    },
    {
      "pos": [
        9127,
        9170
      ],
      "content": "<bpt id=\"p34\">**</bpt>To submit a C# streaming word count job<ept id=\"p34\">**</ept>"
    },
    {
      "pos": [
        9174,
        9286
      ],
      "content": "Follow the procdure in <bpt id=\"p35\">[</bpt>Word count - Java<ept id=\"p35\">](#word-count-java)</ept>, and replace the job definition with the following:"
    },
    {
      "pos": [
        9722,
        9747
      ],
      "content": "The output file shall be:"
    },
    {
      "pos": [
        9894,
        9906
      ],
      "content": "PI estimator"
    },
    {
      "pos": [
        9908,
        10411
      ],
      "content": "The pi estimator uses a statistical (quasi-Monte Carlo) method to estimate the value of pi. Points placed at random inside of a unit square also fall within a circle inscribed within that square with a probability equal to the area of the circle, pi/4. The value of pi can be estimated from the value of 4R, where R is the ratio of the number of points that are inside the circle to the total number of points that are within the square. The larger the sample of points used, the better the estimate is.",
      "nodes": [
        {
          "content": "The pi estimator uses a statistical (quasi-Monte Carlo) method to estimate the value of pi.",
          "pos": [
            0,
            91
          ]
        },
        {
          "content": "Points placed at random inside of a unit square also fall within a circle inscribed within that square with a probability equal to the area of the circle, pi/4.",
          "pos": [
            92,
            252
          ]
        },
        {
          "content": "The value of pi can be estimated from the value of 4R, where R is the ratio of the number of points that are inside the circle to the total number of points that are within the square.",
          "pos": [
            253,
            437
          ]
        },
        {
          "content": "The larger the sample of points used, the better the estimate is.",
          "pos": [
            438,
            503
          ]
        }
      ]
    },
    {
      "pos": [
        10413,
        10746
      ],
      "content": "The script provided for this sample submits a Hadoop jar job and is set up to run with a value 16 maps, each of which is required to compute 10 million sample points by the parameter values. These parameter values can be changed to improve the estimated value of pi. For reference, the first 10 decimal places of pi are 3.1415926535.",
      "nodes": [
        {
          "content": "The script provided for this sample submits a Hadoop jar job and is set up to run with a value 16 maps, each of which is required to compute 10 million sample points by the parameter values.",
          "pos": [
            0,
            190
          ]
        },
        {
          "content": "These parameter values can be changed to improve the estimated value of pi.",
          "pos": [
            191,
            266
          ]
        },
        {
          "content": "For reference, the first 10 decimal places of pi are 3.1415926535.",
          "pos": [
            267,
            333
          ]
        }
      ]
    },
    {
      "pos": [
        10748,
        10780
      ],
      "content": "<bpt id=\"p36\">**</bpt>To submit a pi estimator job<ept id=\"p36\">**</ept>"
    },
    {
      "pos": [
        10784,
        10896
      ],
      "content": "Follow the procdure in <bpt id=\"p37\">[</bpt>Word count - Java<ept id=\"p37\">](#word-count-java)</ept>, and replace the job definition with the following:"
    },
    {
      "pos": [
        11240,
        11254
      ],
      "content": "10-GB Graysort"
    },
    {
      "pos": [
        11256,
        11661
      ],
      "content": "This sample uses a modest 10GB of data so that it can be run relatively quickly. It uses the MapReduce applications developed by Owen O'Malley and Arun Murthy that won the annual general-purpose (\"daytona\") terabyte sort benchmark in 2009 with a rate of 0.578TB/min (100TB in 173 minutes). For more information on this and other sorting benchmarks, see the <bpt id=\"p38\">[</bpt>Sortbenchmark<ept id=\"p38\">](http://sortbenchmark.org/)</ept><ph id=\"ph8\"/> site.",
      "nodes": [
        {
          "content": "This sample uses a modest 10GB of data so that it can be run relatively quickly.",
          "pos": [
            0,
            80
          ]
        },
        {
          "content": "It uses the MapReduce applications developed by Owen O'Malley and Arun Murthy that won the annual general-purpose (\"daytona\") terabyte sort benchmark in 2009 with a rate of 0.578TB/min (100TB in 173 minutes).",
          "pos": [
            81,
            289
          ]
        },
        {
          "content": "For more information on this and other sorting benchmarks, see the <bpt id=\"p38\">[</bpt>Sortbenchmark<ept id=\"p38\">](http://sortbenchmark.org/)</ept><ph id=\"ph8\"/> site.",
          "pos": [
            290,
            459
          ]
        }
      ]
    },
    {
      "pos": [
        11663,
        11713
      ],
      "content": "This sample uses three sets of MapReduce programs:"
    },
    {
      "pos": [
        11718,
        11807
      ],
      "content": "<bpt id=\"p39\">**</bpt>TeraGen<ept id=\"p39\">**</ept><ph id=\"ph9\"/> is a MapReduce program that you can use to generate the rows of data to sort."
    },
    {
      "pos": [
        11811,
        12249
      ],
      "content": "<bpt id=\"p40\">**</bpt>TeraSort<ept id=\"p40\">**</ept><ph id=\"ph10\"/> samples the input data and uses MapReduce to sort the data into a total order. TeraSort is a standard sort of MapReduce functions, except for a custom partitioner that uses a sorted list of N-1 sampled keys that define the key range for each reduce. In particular, all keys such that sample[i-1] &lt;= key &lt; sample[i] are sent to reduce i. This guarantees that the outputs of reduce i are all less than the output of reduce i+1.",
      "nodes": [
        {
          "content": "<bpt id=\"p40\">**</bpt>TeraSort<ept id=\"p40\">**</ept><ph id=\"ph10\"/> samples the input data and uses MapReduce to sort the data into a total order. TeraS",
          "pos": [
            0,
            152
          ]
        },
        {
          "content": "ort is a standard sort of MapReduce functions, except for a custom partitioner that uses a sorted list of N-1 sampled keys that define the key range for each reduce. In pa",
          "pos": [
            152,
            323
          ]
        },
        {
          "content": "rticular, all keys such that sample[i-1] &lt;= key &lt; sample[i] are sent to reduce i.",
          "pos": [
            323,
            410
          ]
        },
        {
          "content": "This guarantees that the outputs of reduce i are all less than the output of reduce i+1.",
          "pos": [
            411,
            499
          ]
        }
      ]
    },
    {
      "pos": [
        12253,
        12748
      ],
      "content": "<bpt id=\"p41\">**</bpt>TeraValidate<ept id=\"p41\">**</ept><ph id=\"ph11\"/> is a MapReduce program that validates that the output is globally sorted. It creates one map per file in the output directory, and each map ensures that each key is less than or equal to the previous one. The map function also generates records of the first and last keys of each file, and the reduce function ensures that the first key of file i is greater than the last key of file i-1. Any problems are reported as an output of the reduce with the keys that are out of order.",
      "nodes": [
        {
          "content": "<bpt id=\"p41\">**</bpt>TeraValidate<ept id=\"p41\">**</ept><ph id=\"ph11\"/> is a MapReduce program that validates that the output is globally sorted.",
          "pos": [
            0,
            145
          ]
        },
        {
          "content": "It creates one map per file in the output directory, and each map ensures that each key is less than or equal to the previous one.",
          "pos": [
            146,
            276
          ]
        },
        {
          "content": "The map function also generates records of the first and last keys of each file, and the reduce function ensures that the first key of file i is greater than the last key of file i-1.",
          "pos": [
            277,
            460
          ]
        },
        {
          "content": "Any problems are reported as an output of the reduce with the keys that are out of order.",
          "pos": [
            461,
            550
          ]
        }
      ]
    },
    {
      "pos": [
        12750,
        13044
      ],
      "content": "The input and output format, used by all three applications, reads and writes the text files in the right format. The output of the reduce has replication set to 1, instead of the default 3, because the benchmark contest does not require that the output data be replicated on to multiple nodes.",
      "nodes": [
        {
          "content": "The input and output format, used by all three applications, reads and writes the text files in the right format.",
          "pos": [
            0,
            113
          ]
        },
        {
          "content": "The output of the reduce has replication set to 1, instead of the default 3, because the benchmark contest does not require that the output data be replicated on to multiple nodes.",
          "pos": [
            114,
            294
          ]
        }
      ]
    },
    {
      "pos": [
        13046,
        13168
      ],
      "content": "Three tasks are required by the sample, each corresponding to one of the MapReduce programs described in the introduction:"
    },
    {
      "pos": [
        13173,
        13244
      ],
      "content": "Generate the data for sorting by running the <bpt id=\"p42\">**</bpt>TeraGen<ept id=\"p42\">**</ept><ph id=\"ph12\"/> MapReduce job."
    },
    {
      "pos": [
        13248,
        13304
      ],
      "content": "Sort the data by running the <bpt id=\"p43\">**</bpt>TeraSort<ept id=\"p43\">**</ept><ph id=\"ph13\"/> MapReduce job."
    },
    {
      "pos": [
        13308,
        13402
      ],
      "content": "Confirm that the data has been correctly sorted by running the <bpt id=\"p44\">**</bpt>TeraValidate<ept id=\"p44\">**</ept><ph id=\"ph14\"/> MapReduce job."
    },
    {
      "pos": [
        13404,
        13426
      ],
      "content": "<bpt id=\"p45\">**</bpt>To submit the jobs<ept id=\"p45\">**</ept>"
    },
    {
      "pos": [
        13430,
        13530
      ],
      "content": "Follow the procdure in <bpt id=\"p46\">[</bpt>Word count - Java<ept id=\"p46\">](#word-count-java)</ept>, and use the following job definitions:"
    },
    {
      "pos": [
        13536,
        13848
      ],
      "leadings": [
        "",
        "  ",
        "  ",
        "  "
      ],
      "content": "$teragen = New-AzureRmHDInsightMapReduceJobDefinition <ph id=\"ph15\">`\n                              -JarFile \"/example/jars/hadoop-mapreduce-examples.jar\" `</ph><ph id=\"ph16\"/>\n                              -ClassName \"teragen\" `\n                              <ph id=\"ph17\"/>-Arguments \"-Dmapred.map.tasks=50\", \"100000000\", \"/example/data/10GB-sort-input\""
    },
    {
      "pos": [
        13858,
        14221
      ],
      "leadings": [
        "",
        "  ",
        "  ",
        "  "
      ],
      "content": "$terasort = New-AzureRmHDInsightMapReduceJobDefinition <ph id=\"ph18\">`\n                              -JarFile \"/example/jars/hadoop-mapreduce-examples.jar\" `</ph><ph id=\"ph19\"/>\n                              -ClassName \"terasort\" `\n                              <ph id=\"ph20\"/>-Arguments \"-Dmapred.map.tasks=50\", \"-Dmapred.reduce.tasks=25\", \"/example/data/10GB-sort-input\", \"/example/data/10GB-sort-output\""
    },
    {
      "pos": [
        14231,
        14605
      ],
      "leadings": [
        "",
        "  ",
        "  ",
        "  "
      ],
      "content": "$teravalidate = New-AzureRmHDInsightMapReduceJobDefinition <ph id=\"ph21\">`\n                              -JarFile \"/example/jars/hadoop-mapreduce-examples.jar\" `</ph><ph id=\"ph22\"/>\n                              -ClassName \"teravalidate\" `\n                              <ph id=\"ph23\"/>-Arguments \"-Dmapred.map.tasks=50\", \"-Dmapred.reduce.tasks=25\", \"/example/data/10GB-sort-output\", \"/example/data/10GB-sort-validate\""
    },
    {
      "pos": [
        14610,
        14620
      ],
      "content": "Next steps"
    },
    {
      "pos": [
        14623,
        14873
      ],
      "content": "From this article and the articles in each of the samples, you learned how to run the samples included with the HDInsight clusters by using Azure PowerShell. For tutorials about using Pig, Hive, and MapReduce with HDInsight, see the following topics:",
      "nodes": [
        {
          "content": "From this article and the articles in each of the samples, you learned how to run the samples included with the HDInsight clusters by using Azure PowerShell.",
          "pos": [
            0,
            157
          ]
        },
        {
          "content": "For tutorials about using Pig, Hive, and MapReduce with HDInsight, see the following topics:",
          "pos": [
            158,
            250
          ]
        }
      ]
    },
    {
      "pos": [
        14877,
        14979
      ],
      "content": "<bpt id=\"p47\">[</bpt>Get started using Hadoop with Hive in HDInsight to analyze mobile handset use<ept id=\"p47\">][hdinsight-get-started]</ept>"
    },
    {
      "pos": [
        14982,
        15035
      ],
      "content": "<bpt id=\"p48\">[</bpt>Use Pig with Hadoop on HDInsight<ept id=\"p48\">][hdinsight-use-pig]</ept>"
    },
    {
      "pos": [
        15038,
        15093
      ],
      "content": "<bpt id=\"p49\">[</bpt>Use Hive with Hadoop on HDInsight<ept id=\"p49\">][hdinsight-use-hive]</ept>"
    },
    {
      "pos": [
        15096,
        15153
      ],
      "content": "<bpt id=\"p50\">[</bpt>Submit Hadoop Jobs in HDInsight<ept id=\"p50\">] [hdinsight-submit-jobs]</ept>"
    },
    {
      "pos": [
        15156,
        15220
      ],
      "content": "<bpt id=\"p51\">[</bpt>Azure HDInsight SDK documentation<ept id=\"p51\">][hdinsight-sdk-documentation]</ept>"
    },
    {
      "pos": [
        15223,
        15285
      ],
      "content": "<bpt id=\"p52\">[</bpt>Debug Hadoop in HDInsight: Error messages<ept id=\"p52\">] [hdinsight-errors]</ept>"
    },
    {
      "pos": [
        15291,
        15330
      ],
      "content": "Appendix A - The Word count source code"
    },
    {
      "pos": [
        17797,
        17846
      ],
      "content": "Appendix B - The word count streaming source code"
    },
    {
      "pos": [
        17848,
        18220
      ],
      "content": "The MapReduce program uses the cat.exe application as a mapping interface to stream the text into the console and the wc.exe application as the reduce interface to count the number of words that are streamed from a document. Both the mapper and reducer read characters, line-by-line, from the standard input stream (stdin) and write to the standard output stream (stdout).",
      "nodes": [
        {
          "content": "The MapReduce program uses the cat.exe application as a mapping interface to stream the text into the console and the wc.exe application as the reduce interface to count the number of words that are streamed from a document.",
          "pos": [
            0,
            224
          ]
        },
        {
          "content": "Both the mapper and reducer read characters, line-by-line, from the standard input stream (stdin) and write to the standard output stream (stdout).",
          "pos": [
            225,
            372
          ]
        }
      ]
    },
    {
      "pos": [
        18761,
        19022
      ],
      "content": "The mapper code in the cat.cs file uses a <bpt id=\"p53\">[</bpt>StreamReader<ept id=\"p53\">][streamreader]</ept><ph id=\"ph24\"/> object to read the characters of the incoming stream to the console, which then writes the stream to the standard output stream with the static <bpt id=\"p54\">[</bpt>Console.Writeline<ept id=\"p54\">][console-writeline]</ept><ph id=\"ph25\"/> method."
    },
    {
      "pos": [
        19653,
        20112
      ],
      "content": "The reducer code in the wc.cs file uses a <bpt id=\"p55\">[</bpt>StreamReader<ept id=\"p55\">][streamreader]</ept><ph id=\"ph26\"/>   object to read characters from the standard input stream that have been output by the cat.exe mapper. As it reads the characters with the <bpt id=\"p56\">[</bpt>Console.Writeline<ept id=\"p56\">][console-writeline]</ept><ph id=\"ph27\"/> method, it counts the words by counting spaces and end-of-line characters at the end of each word. It then writes the total to the standard output stream with the <bpt id=\"p57\">[</bpt>Console.Writeline<ept id=\"p57\">][console-writeline]</ept><ph id=\"ph28\"/> method.",
      "nodes": [
        {
          "content": "The reducer code in the wc.cs file uses a <bpt id=\"p55\">[</bpt>StreamReader<ept id=\"p55\">][streamreader]</ept><ph id=\"ph26\"/>   object to read characters from the standard input stream that have been output by the cat.exe mapper.",
          "pos": [
            0,
            229
          ]
        },
        {
          "content": "As it reads the characters with the <bpt id=\"p56\">[</bpt>Console.Writeline<ept id=\"p56\">][console-writeline]</ept><ph id=\"ph27\"/> method, it counts the words by counting spaces and end-of-line characters at the end of each word.",
          "pos": [
            230,
            458
          ]
        },
        {
          "content": "It then writes the total to the standard output stream with the <bpt id=\"p57\">[</bpt>Console.Writeline<ept id=\"p57\">][console-writeline]</ept><ph id=\"ph28\"/> method.",
          "pos": [
            459,
            624
          ]
        }
      ]
    },
    {
      "pos": [
        20121,
        20162
      ],
      "content": "Appendix C - The Pi estimator source code"
    },
    {
      "pos": [
        20164,
        20689
      ],
      "content": "The pi estimator Java code that contains the mapper and reducer functions is available for inspection below. The mapper program generates a specified number of points placed at random inside of a unit square and then counts the number of those points that are inside the circle. The reducer program accumulates points counted by the mappers and then estimates the value of pi from the formula 4R, where R is the ratio of the number of points counted inside the circle to the total number of points that are within the square.",
      "nodes": [
        {
          "content": "The pi estimator Java code that contains the mapper and reducer functions is available for inspection below.",
          "pos": [
            0,
            108
          ]
        },
        {
          "content": "The mapper program generates a specified number of points placed at random inside of a unit square and then counts the number of those points that are inside the circle.",
          "pos": [
            109,
            278
          ]
        },
        {
          "content": "The reducer program accumulates points counted by the mappers and then estimates the value of pi from the formula 4R, where R is the ratio of the number of points counted inside the circle to the total number of points that are within the square.",
          "pos": [
            279,
            525
          ]
        }
      ]
    },
    {
      "pos": [
        32195,
        32237
      ],
      "content": "Appendix D - The 10gb graysort source code"
    },
    {
      "pos": [
        32239,
        32327
      ],
      "content": "The code for the TeraSort MapReduce program is presented for inspection in this section."
    }
  ],
  "content": "<properties\n    pageTitle=\"Run the Hadoop samples in HDInsight | Microsoft Azure\"\n    description=\"Get started using the Azure HDInsight service with the samples provided. Use PowerShell scripts that run MapReduce programs on data clusters.\"\n    services=\"hdinsight\"\n    documentationCenter=\"\"\n    tags=\"azure-portal\"\n    authors=\"mumian\"\n    manager=\"paulettm\"\n    editor=\"cgronlun\"/>\n\n<tags\n    ms.service=\"hdinsight\"\n    ms.workload=\"big-data\"\n    ms.tgt_pltfrm=\"na\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\"\n    ms.date=\"02/04/2016\"\n    ms.author=\"jgao\"/>\n\n#Run Hadoop MapReduce samples in Windows-based HDInsight\n\n[AZURE.INCLUDE [samples-selector](../../includes/hdinsight-run-samples-selector.md)]\n\nA set of samples are provided to help you get started running MapReduce jobs on Hadoop clusters using Azure HDInsight. These samples are made available on each of the HDInsight managed clusters that you create. Running these samples will familiarize you with using Azure PowerShell cmdlets to run jobs on Hadoop clusters.\n\n- [**Word count**][hdinsight-sample-wordcount]: Counts word occurrences in a text file.\n- [**C# streaming word count**][hdinsight-sample-csharp-streaming]: Counts word occurrences in a text file using the Hadoop streaming interface.\n- [**Pi estimator**][hdinsight-sample-pi-estimator]: Uses a statistical (quasi-Monte Carlo) method to estimate the value of pi.\n- [**10-GB Graysort**][hdinsight-sample-10gb-graysort]: Run a general purpose GraySort on a 10 GB file by using HDInsight. There are three jobs to run: Teragen to generate the data, Terasort to sort the data, and Teravalidate to confirm that the data has been properly sorted.\n\n>[AZURE.NOTE] The source code can be found in the Appendix. \n\nMuch additional documentation exists on the web for Hadoop-related technologies, such as Java-based MapReduce programming and streaming, and documentation about the cmdlets that are used in Windows PowerShell scripting. For more information about these resources, see:\n\n- [Develop Java MapReduce programs for Hadoop in HDInsight](hdinsight-develop-deploy-java-mapreduce.md)\n- [Develop C# Hadoop streaming programs for HDInsight](hdinsight-hadoop-develop-deploy-streaming-jobs.md)\n- [Submit Hadoop jobs in HDInsight](hdinsight-submit-hadoop-jobs-programmatically.md)\n- [Introduction to Azure HDInsight][hdinsight-introduction]\n\nNowadays, a lot of people choose Hive and Pig over MapReduce.  For more information, see :\n\n- [Use Hive in HDInsight](hdinsight-use-hive.md)\n- [Use Pig in HDInsight](hdinsight-use-pig.md)\n \n**Prerequisites**:\n\n- **An Azure subscription**. See [Get Azure free trial](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/).\n- **an HDInsight cluster**. For instructions on the various ways in which such clusters can be created, see [Create Hadoop clusters in HDInsight](hdinsight-provision-clusters.md).\n- **A workstation with Azure PowerShell**. See [Install Azure PowerShell 1.0 and greater](hdinsight-administer-use-powershell.md#install-azure-powershell-10-and-greater).\n\n## <a name=\"hdinsight-sample-wordcount\"></a>Word count - Java \n\nTo submit a MapReduce project, you first create a MapReduce job definition. In the job definition, you specify the MapReduce program jar file and the location of the jar file, which is **wasb:///example/jars/hadoop-mapreduce-examples.jar**, the class name, and the arguments.  The wordcount MapReduce program takes two arguments: the source file that will be used to count words, and the location for output.\n\nThe source code can be found in the [Appendix A](#apendix-a---the-word-count-MapReduce-program-in-java).\n\nFor the procedure of developing a Java MapReduce program, see - [Develop Java MapReduce programs for Hadoop in HDInsight](hdinsight-develop-deploy-java-mapreduce.md)\n \n**To submit a word count MapReduce job**\n\n1. Open **Windows PowerShell ISE**. For instructions, see [Install and configure Azure PowerShell][powershell-install-configure].\n2. Paste the following PowerShell script:\n\n        $subscriptionName = \"<Azure Subscription Name>\"\n        $resourceGroupName = \"<Resource Group Name>\"\n        $clusterName = \"<HDInsight cluster name>\"             # HDInsight cluster name\n        \n        Select-AzureRmSubscription $subscriptionName\n        \n        # Define the MapReduce job\n        $mrJobDefinition = New-AzureRmHDInsightMapReduceJobDefinition `\n                                    -JarFile \"wasb:///example/jars/hadoop-mapreduce-examples.jar\" `\n                                    -ClassName \"wordcount\" `\n                                    -Arguments \"wasb:///example/data/gutenberg/davinci.txt\", \"wasb:///example/data/WordCountOutput1\"\n        \n        # Submit the job and wait for job completion\n        $cred = Get-Credential -Message \"Enter the HDInsight cluster HTTP user credential:\" \n        $mrJob = Start-AzureRmHDInsightJob `\n                            -ResourceGroupName $resourceGroupName `\n                            -ClusterName $clusterName `\n                            -HttpCredential $cred `\n                            -JobDefinition $mrJobDefinition \n        \n        Wait-AzureRmHDInsightJob `\n            -ResourceGroupName $resourceGroupName `\n            -ClusterName $clusterName `\n            -HttpCredential $cred `\n            -JobId $mrJob.JobId \n        \n        # Get the job output\n        $cluster = Get-AzureRmHDInsightCluster -ResourceGroupName $resourceGroupName -ClusterName $clusterName\n        $defaultStorageAccount = $cluster.DefaultStorageAccount -replace '.blob.core.windows.net'\n        $defaultStorageAccountKey = Get-AzureRmStorageAccountKey -ResourceGroupName $resourceGroupName -Name $defaultStorageAccount |  %{ $_.Key1 }\n        $defaultStorageContainer = $cluster.DefaultStorageContainer\n        \n        Get-AzureRmHDInsightJobOutput `\n            -ResourceGroupName $resourceGroupName `\n            -ClusterName $clusterName `\n            -HttpCredential $cred `\n            -DefaultStorageAccountName $defaultStorageAccount `\n            -DefaultStorageAccountKey $defaultStorageAccountKey `\n            -DefaultContainer $defaultStorageContainer  `\n            -JobId $mrJob.JobId `\n            -DisplayOutputType StandardError\n\n        # Download the job output to the workstation\n        $storageContext = New-AzureStorageContext -StorageAccountName $defaultStorageAccount -StorageAccountKey $defaultStorageAccountKey \n        Get-AzureStorageBlobContent -Container $defaultStorageContainer -Blob example/data/WordCountOutput/part-r-00000 -Context $storageContext -Force\n        \n        # Display the output file\n        cat ./example/data/WordCountOutput/part-r-00000 | findstr \"there\"\n\n    The MapReduce job produces a file named *part-r-00000*, which contains words and the counts. The script uses the **findstr** command to list all of the words that contains *\"there\"*.\n\n3. Set the first 3 variables, and run the script.\n\n## <a name=\"hdinsight-sample-csharp-streaming\"></a>Word count - C# streaming\n\nHadoop provides a streaming API to MapReduce, which enables you to write map and reduce functions in languages other than Java.\n\n> [AZURE.NOTE] The steps in this tutorial apply only to Windows-based HDInsight clusters. For an example of streaming for Linux-based HDInsight clusters, see [Develop Python streaming programs for HDInsight](hdinsight-hadoop-streaming-python.md).\n\nIn the example, the mapper and the reducer are executables that read the input from [stdin][stdin-stdout-stderr] (line-by-line) and emit the output to [stdout][stdin-stdout-stderr]. The program counts all of the words in the text.\n\nWhen an executable is specified for **mappers**, each mapper task launches the executable as a separate process when the mapper is initialized. As the mapper task runs, it converts its input into lines, and feeds the lines to the [stdin][stdin-stdout-stderr] of the process.\n\nIn the meantime, the mapper collects the line-oriented output from the stdout of the process. It converts each line into a key/value pair, which is collected as the output of the mapper. By default, the prefix of a line up to the first Tab character is the key, and the remainder of the line (excluding the Tab character) is the value. If there is no Tab character in the line, entire line is considered as the key, and the value is null.\n\nWhen an executable is specified for **reducers**, each reducer task launches the executable as a separate process when the reducer is initialized. As the reducer task runs, it converts its input key/values pairs into lines, and it feeds the lines to the [stdin][stdin-stdout-stderr] of the process.\n\nIn the meantime, the reducer collects the line-oriented output from the [stdout][stdin-stdout-stderr] of the process. It converts each line to a key/value pair, which is collected as the output of the reducer. By default, the prefix of a line up to the first Tab character is the key, and the remainder of the line (excluding the Tab character) is the value.\n\nFor more information about the Hadoop Streaming interface, see [Hadoop Streaming][hadoop-streaming].\n\n**To submit a C# streaming word count job**\n\n- Follow the procdure in [Word count - Java](#word-count-java), and replace the job definition with the following:\n\n        $mrJobDefinition = New-AzureRmHDInsightStreamingMapReduceJobDefinition `\n                                    -File \"/example/apps/\" `\n                                    -Mapper \"cat.exe\" `\n                                    -Reducer \"wc.exe\" `\n                                    -InputPath \"/example/data/gutenberg/davinci.txt\" `\n                                    -OutputPath \"/example/data/StreamingOutput/wc.txt\"\n\n\n    The output file shall be:\n    \n        example/data/StreamingOutput/wc.txt/part-00000      \n                                \n## <a name=\"hdinsight-sample-pi-estimator\"></a>PI estimator\n\nThe pi estimator uses a statistical (quasi-Monte Carlo) method to estimate the value of pi. Points placed at random inside of a unit square also fall within a circle inscribed within that square with a probability equal to the area of the circle, pi/4. The value of pi can be estimated from the value of 4R, where R is the ratio of the number of points that are inside the circle to the total number of points that are within the square. The larger the sample of points used, the better the estimate is.\n\nThe script provided for this sample submits a Hadoop jar job and is set up to run with a value 16 maps, each of which is required to compute 10 million sample points by the parameter values. These parameter values can be changed to improve the estimated value of pi. For reference, the first 10 decimal places of pi are 3.1415926535.\n\n**To submit a pi estimator job**\n\n- Follow the procdure in [Word count - Java](#word-count-java), and replace the job definition with the following:\n\n        $mrJobJobDefinition = New-AzureRmHDInsightMapReduceJobDefinition `\n                                    -JarFile \"wasb:///example/jars/hadoop-mapreduce-examples.jar\" `\n                                    -ClassName \"pi\" `\n                                    -Arguments \"16\", \"10000000\"\n\n## <a name=\"hdinsight-sample-10gb-graysort\"></a>10-GB Graysort\n\nThis sample uses a modest 10GB of data so that it can be run relatively quickly. It uses the MapReduce applications developed by Owen O'Malley and Arun Murthy that won the annual general-purpose (\"daytona\") terabyte sort benchmark in 2009 with a rate of 0.578TB/min (100TB in 173 minutes). For more information on this and other sorting benchmarks, see the [Sortbenchmark](http://sortbenchmark.org/) site.\n\nThis sample uses three sets of MapReduce programs:\n\n1. **TeraGen** is a MapReduce program that you can use to generate the rows of data to sort.\n2. **TeraSort** samples the input data and uses MapReduce to sort the data into a total order. TeraSort is a standard sort of MapReduce functions, except for a custom partitioner that uses a sorted list of N-1 sampled keys that define the key range for each reduce. In particular, all keys such that sample[i-1] <= key < sample[i] are sent to reduce i. This guarantees that the outputs of reduce i are all less than the output of reduce i+1.\n3. **TeraValidate** is a MapReduce program that validates that the output is globally sorted. It creates one map per file in the output directory, and each map ensures that each key is less than or equal to the previous one. The map function also generates records of the first and last keys of each file, and the reduce function ensures that the first key of file i is greater than the last key of file i-1. Any problems are reported as an output of the reduce with the keys that are out of order.\n\nThe input and output format, used by all three applications, reads and writes the text files in the right format. The output of the reduce has replication set to 1, instead of the default 3, because the benchmark contest does not require that the output data be replicated on to multiple nodes.\n\nThree tasks are required by the sample, each corresponding to one of the MapReduce programs described in the introduction:\n\n1. Generate the data for sorting by running the **TeraGen** MapReduce job.\n2. Sort the data by running the **TeraSort** MapReduce job.\n3. Confirm that the data has been correctly sorted by running the **TeraValidate** MapReduce job.\n\n**To submit the jobs**\n\n- Follow the procdure in [Word count - Java](#word-count-java), and use the following job definitions:\n\n    $teragen = New-AzureRmHDInsightMapReduceJobDefinition `\n                                -JarFile \"/example/jars/hadoop-mapreduce-examples.jar\" `\n                                -ClassName \"teragen\" `\n                                -Arguments \"-Dmapred.map.tasks=50\", \"100000000\", \"/example/data/10GB-sort-input\"\n    \n    $terasort = New-AzureRmHDInsightMapReduceJobDefinition `\n                                -JarFile \"/example/jars/hadoop-mapreduce-examples.jar\" `\n                                -ClassName \"terasort\" `\n                                -Arguments \"-Dmapred.map.tasks=50\", \"-Dmapred.reduce.tasks=25\", \"/example/data/10GB-sort-input\", \"/example/data/10GB-sort-output\"\n    \n    $teravalidate = New-AzureRmHDInsightMapReduceJobDefinition `\n                                -JarFile \"/example/jars/hadoop-mapreduce-examples.jar\" `\n                                -ClassName \"teravalidate\" `\n                                -Arguments \"-Dmapred.map.tasks=50\", \"-Dmapred.reduce.tasks=25\", \"/example/data/10GB-sort-output\", \"/example/data/10GB-sort-validate\"\n\n\n##Next steps \n\nFrom this article and the articles in each of the samples, you learned how to run the samples included with the HDInsight clusters by using Azure PowerShell. For tutorials about using Pig, Hive, and MapReduce with HDInsight, see the following topics:\n\n* [Get started using Hadoop with Hive in HDInsight to analyze mobile handset use][hdinsight-get-started]\n* [Use Pig with Hadoop on HDInsight][hdinsight-use-pig]\n* [Use Hive with Hadoop on HDInsight][hdinsight-use-hive]\n* [Submit Hadoop Jobs in HDInsight] [hdinsight-submit-jobs]\n* [Azure HDInsight SDK documentation][hdinsight-sdk-documentation]\n* [Debug Hadoop in HDInsight: Error messages] [hdinsight-errors]\n\n\n## Appendix A - The Word count source code\n\n    package org.apache.hadoop.examples;\n    import java.io.IOException;\n    import java.util.StringTokenizer;\n    import org.apache.hadoop.conf.Configuration;\n    import org.apache.hadoop.fs.Path;\n    import org.apache.hadoop.io.IntWritable;\n    import org.apache.hadoop.io.Text;\n    import org.apache.hadoop.mapreduce.Job;\n    import org.apache.hadoop.mapreduce.Mapper;\n    import org.apache.hadoop.mapreduce.Reducer;\n    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n    import org.apache.hadoop.util.GenericOptionsParser;\n\n    public class WordCount {\n\n    public static class TokenizerMapper\n       extends Mapper<Object, Text, Text, IntWritable>{\n\n    private final static IntWritable one = new IntWritable(1);\n    private Text word = new Text();\n\n    public void map(Object key, Text value, Context context\n                    ) throws IOException, InterruptedException {\n      StringTokenizer itr = new StringTokenizer(value.toString());\n      while (itr.hasMoreTokens()) {\n        word.set(itr.nextToken());\n        context.write(word, one);\n        }\n      }\n    }\n\n    public static class IntSumReducer\n       extends Reducer<Text,IntWritable,Text,IntWritable> {\n    private IntWritable result = new IntWritable();\n\n    public void reduce(Text key, Iterable<IntWritable> values,\n                       Context context\n                       ) throws IOException, InterruptedException {\n      int sum = 0;\n      for (IntWritable val : values) {\n        sum += val.get();\n      }\n      result.set(sum);\n      context.write(key, result);\n      }\n    }\n\n    public static void main(String[] args) throws Exception {\n    Configuration conf = new Configuration();\n    String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs();\n    if (otherArgs.length != 2) {\n      System.err.println(\"Usage: wordcount <in> <out>\");\n      System.exit(2);\n        }\n    Job job = new Job(conf, \"word count\");\n    job.setJarByClass(WordCount.class);\n    job.setMapperClass(TokenizerMapper.class);\n    job.setCombinerClass(IntSumReducer.class);\n    job.setReducerClass(IntSumReducer.class);\n    job.setOutputKeyClass(Text.class);\n    job.setOutputValueClass(IntWritable.class);\n    FileInputFormat.addInputPath(job, new Path(otherArgs[0]));\n    FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));\n    System.exit(job.waitForCompletion(true) ? 0 : 1);\n    }\n    }\n\n\n## Appendix B - The word count streaming source code\n\nThe MapReduce program uses the cat.exe application as a mapping interface to stream the text into the console and the wc.exe application as the reduce interface to count the number of words that are streamed from a document. Both the mapper and reducer read characters, line-by-line, from the standard input stream (stdin) and write to the standard output stream (stdout).\n\n    // The source code for the cat.exe (Mapper).\n\n    using System;\n    using System.IO;\n\n    namespace cat\n    {\n        class cat\n        {\n            static void Main(string[] args)\n            {\n                if (args.Length > 0)\n                {\n                    Console.SetIn(new StreamReader(args[0]));\n                }\n\n                string line;\n                while ((line = Console.ReadLine()) != null)\n                {\n                    Console.WriteLine(line);\n                }\n            }\n        }\n    }\n\n\n\nThe mapper code in the cat.cs file uses a [StreamReader][streamreader] object to read the characters of the incoming stream to the console, which then writes the stream to the standard output stream with the static [Console.Writeline][console-writeline] method.\n\n\n    // The source code for wc.exe (Reducer) is:\n\n    using System;\n    using System.IO;\n    using System.Linq;\n\n    namespace wc\n    {\n        class wc\n        {\n            static void Main(string[] args)\n            {\n                string line;\n                var count = 0;\n\n                if (args.Length > 0){\n                    Console.SetIn(new StreamReader(args[0]));\n                }\n\n                while ((line = Console.ReadLine()) != null) {\n                    count += line.Count(cr => (cr == ' ' || cr == '\\n'));\n                }\n                Console.WriteLine(count);\n            }\n        }\n    }\n\n\nThe reducer code in the wc.cs file uses a [StreamReader][streamreader]   object to read characters from the standard input stream that have been output by the cat.exe mapper. As it reads the characters with the [Console.Writeline][console-writeline] method, it counts the words by counting spaces and end-of-line characters at the end of each word. It then writes the total to the standard output stream with the [Console.Writeline][console-writeline] method.\n\n\n\n\n\n## Appendix C - The Pi estimator source code\n\nThe pi estimator Java code that contains the mapper and reducer functions is available for inspection below. The mapper program generates a specified number of points placed at random inside of a unit square and then counts the number of those points that are inside the circle. The reducer program accumulates points counted by the mappers and then estimates the value of pi from the formula 4R, where R is the ratio of the number of points counted inside the circle to the total number of points that are within the square.\n\n    /**\n    * Licensed to the Apache Software Foundation (ASF) under one\n    * or more contributor license agreements. See the NOTICE file\n    * distributed with this work for additional information\n    * regarding copyright ownership. The ASF licenses this file\n    * to you under the Apache License, Version 2.0 (the\n    * \"License\"); you may not use this file except in compliance\n    * with the License. You may obtain a copy of the License at\n    *\n    * http://www.apache.org/licenses/LICENSE-2.0\n    *\n    * Unless required by applicable law or agreed to in writing, software\n    * distributed under the License is distributed on an \"AS IS\" BASIS,\n    * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or   implied.\n    * See the License for the specific language governing permissions and\n    * limitations under the License.\n    */\n\n    package org.apache.hadoop.examples;\n\n    import java.io.IOException;\n    import java.math.BigDecimal;\n    import java.util.Iterator;\n\n    import org.apache.hadoop.conf.Configured;\n    import org.apache.hadoop.fs.FileSystem;\n    import org.apache.hadoop.fs.Path;\n    import org.apache.hadoop.io.BooleanWritable;\n    import org.apache.hadoop.io.LongWritable;\n    import org.apache.hadoop.io.SequenceFile;\n    import org.apache.hadoop.io.Writable;\n    import org.apache.hadoop.io.WritableComparable;\n    import org.apache.hadoop.io.SequenceFile.CompressionType;\n    import org.apache.hadoop.mapred.FileInputFormat;\n    import org.apache.hadoop.mapred.FileOutputFormat;\n    import org.apache.hadoop.mapred.JobClient;\n    import org.apache.hadoop.mapred.JobConf;\n    import org.apache.hadoop.mapred.MapReduceBase;\n    import org.apache.hadoop.mapred.Mapper;\n    import org.apache.hadoop.mapred.OutputCollector;\n    import org.apache.hadoop.mapred.Reducer;\n    import org.apache.hadoop.mapred.Reporter;\n    import org.apache.hadoop.mapred.SequenceFileInputFormat;\n    import org.apache.hadoop.mapred.SequenceFileOutputFormat;\n    import org.apache.hadoop.util.Tool;\n    import org.apache.hadoop.util.ToolRunner;\n\n\n    //A Map-reduce program to estimate the value of Pi\n    //using quasi-Monte Carlo method.\n    //\n    //Mapper:\n    //Generate points in a unit square\n    //and then count points inside/outside of the inscribed circle of the square.\n    //\n    //Reducer:\n    //Accumulate points inside/outside results from the mappers.\n    //Let numTotal = numInside + numOutside.\n    //The fraction numInside/numTotal is a rational approximation of\n    //the value (Area of the circle)/(Area of the square),\n    //where the area of the inscribed circle is Pi/4\n    //and the area of unit square is 1.\n    //Then, Pi is estimated value to be 4(numInside/numTotal).\n    //\n\n    public class PiEstimator extends Configured implements Tool {\n    //tmp directory for input/output\n    static private final Path TMP_DIR = new Path(\n    PiEstimator.class.getSimpleName() + \"_TMP_3_141592654\");\n\n    //2-dimensional Halton sequence {H(i)},\n    //where H(i) is a 2-dimensional point and i >= 1 is the index.\n    //Halton sequence is used to generate sample points for Pi estimation.\n    private static class HaltonSequence {\n    // Bases\n    static final int[] P = {2, 3};\n    //Maximum number of digits allowed\n    static final int[] K = {63, 40};\n\n    private long index;\n    private double[] x;\n    private double[][] q;\n    private int[][] d;\n\n    //Initialize to H(startindex),\n    //so the sequence begins with H(startindex+1).\n    HaltonSequence(long startindex) {\n    index = startindex;\n    x = new double[K.length];\n    q = new double[K.length][];\n    d = new int[K.length][];\n    for(int i = 0; i < K.length; i++) {\n    q[i] = new double[K[i]];\n    d[i] = new int[K[i]];\n    }\n\n    for(int i = 0; i < K.length; i++) {\n    long k = index;\n    x[i] = 0;\n\n    for(int j = 0; j < K[i]; j++) {\n    q[i][j] = (j == 0? 1.0: q[i][j-1])/P[i];\n    d[i][j] = (int)(k % P[i]);\n    k = (k - d[i][j])/P[i];\n    x[i] += d[i][j] * q[i][j];\n    }\n    }\n    }\n\n    //Compute next point.\n    //Assume the current point is H(index).\n    //Compute H(index+1).\n    //@return a 2-dimensional point with coordinates in [0,1)^2\n    double[] nextPoint() {\n    index++;\n    for(int i = 0; i < K.length; i++) {\n    for(int j = 0; j < K[i]; j++) {\n    d[i][j]++;\n    x[i] += q[i][j];\n    if (d[i][j] < P[i]) {\n    break;\n    }\n    d[i][j] = 0;\n    x[i] -= (j == 0? 1.0: q[i][j-1]);\n    }\n    }\n    return x;\n    }\n    }\n\n    //Mapper class for Pi estimation.\n    //Generate points in a unit square and then\n    //count points inside/outside of the inscribed circle of the square.\n    public static class PiMapper extends MapReduceBase\n    implements Mapper<LongWritable, LongWritable, BooleanWritable, LongWritable> {\n\n    //Map method.\n    //@param offset samples starting from the (offset+1)th sample.\n    //@param size the number of samples for this map\n    //@param out output {ture->numInside, false->numOutside}\n    //@param reporter\n    public void map(LongWritable offset,\n    LongWritable size,\n    OutputCollector<BooleanWritable, LongWritable> out,\n    Reporter reporter) throws IOException {\n\n    final HaltonSequence haltonsequence = new HaltonSequence(offset.get());\n    long numInside = 0L;\n    long numOutside = 0L;\n\n    for(long i = 0; i < size.get(); ) {\n    //generate points in a unit square\n    final double[] point = haltonsequence.nextPoint();\n\n    //count points inside/outside of the inscribed circle of the square\n    final double x = point[0] - 0.5;\n    final double y = point[1] - 0.5;\n    if (x*x + y*y > 0.25) {\n    numOutside++;\n    } else {\n    numInside++;\n    }\n\n    //report status\n    i++;\n    if (i % 1000 == 0) {\n    reporter.setStatus(\"Generated \" + i + \" samples.\");\n    }\n    }\n\n    //output map results\n    out.collect(new BooleanWritable(true), new LongWritable(numInside));\n    out.collect(new BooleanWritable(false), new LongWritable(numOutside));\n    }\n    }\n\n\n    //Reducer class for Pi estimation.\n    //Accumulate points inside/outside results from the mappers.\n    public static class PiReducer extends MapReduceBase\n    implements Reducer<BooleanWritable, LongWritable, WritableComparable<?>, Writable> {\n\n    private long numInside = 0;\n    private long numOutside = 0;\n    private JobConf conf; //configuration for accessing the file system\n\n    //Store job configuration.\n    @Override\n    public void configure(JobConf job) {\n    conf = job;\n    }\n\n\n    // Accumulate number of points inside/outside results from the mappers.\n    // @param isInside Is the points inside?\n    // @param values An iterator to a list of point counts\n    // @param output dummy, not used here.\n    // @param reporter\n\n    public void reduce(BooleanWritable isInside,\n    Iterator<LongWritable> values,\n    OutputCollector<WritableComparable<?>, Writable> output,\n    Reporter reporter) throws IOException {\n    if (isInside.get()) {\n    for(; values.hasNext(); numInside += values.next().get());\n    } else {\n    for(; values.hasNext(); numOutside += values.next().get());\n    }\n    }\n\n    //Reduce task done, write output to a file.\n    @Override\n    public void close() throws IOException {\n    //write output to a file\n    Path outDir = new Path(TMP_DIR, \"out\");\n    Path outFile = new Path(outDir, \"reduce-out\");\n    FileSystem fileSys = FileSystem.get(conf);\n    SequenceFile.Writer writer = SequenceFile.createWriter(fileSys, conf,\n    outFile, LongWritable.class, LongWritable.class,\n    CompressionType.NONE);\n    writer.append(new LongWritable(numInside), new LongWritable(numOutside));\n    writer.close();\n    }\n    }\n\n    //Run a map/reduce job for estimating Pi.\n    //@return the estimated value of Pi.\n    public static BigDecimal estimate(int numMaps, long numPoints, JobConf jobConf\n    )\n    throws IOException {\n    //setup job conf\n    jobConf.setJobName(PiEstimator.class.getSimpleName());\n\n    jobConf.setInputFormat(SequenceFileInputFormat.class);\n\n    jobConf.setOutputKeyClass(BooleanWritable.class);\n    jobConf.setOutputValueClass(LongWritable.class);\n    jobConf.setOutputFormat(SequenceFileOutputFormat.class);\n\n    jobConf.setMapperClass(PiMapper.class);\n    jobConf.setNumMapTasks(numMaps);\n\n    jobConf.setReducerClass(PiReducer.class);\n    jobConf.setNumReduceTasks(1);\n\n    // turn off speculative execution, because DFS doesn't handle\n    // multiple writers to the same file.\n    jobConf.setSpeculativeExecution(false);\n\n    //setup input/output directories\n    final Path inDir = new Path(TMP_DIR, \"in\");\n    final Path outDir = new Path(TMP_DIR, \"out\");\n    FileInputFormat.setInputPaths(jobConf, inDir);\n    FileOutputFormat.setOutputPath(jobConf, outDir);\n\n    final FileSystem fs = FileSystem.get(jobConf);\n    if (fs.exists(TMP_DIR)) {\n     throw new IOException(\"Tmp directory \" + fs.makeQualified(TMP_DIR)\n     + \" already exists. Please remove it first.\");\n     }\n     if (!fs.mkdirs(inDir)) {\n     throw new IOException(\"Cannot create input directory \" + inDir);\n     }\n\n     //generate an input file for each map task\n     try {\n     for(int i=0; i < numMaps; ++i) {\n     final Path file = new Path(inDir, \"part\"+i);\n     final LongWritable offset = new LongWritable(i * numPoints);\n     final LongWritable size = new LongWritable(numPoints);\n     final SequenceFile.Writer writer = SequenceFile.createWriter(\n     fs, jobConf, file,\n     LongWritable.class, LongWritable.class, CompressionType.NONE);\n     try {\n     writer.append(offset, size);\n     } finally {\n     writer.close();\n     }\n     System.out.println(\"Wrote input for Map #\"+i);\n     }\n\n     //start a map/reduce job\n     System.out.println(\"Starting Job\");\n     final long startTime = System.currentTimeMillis();\n     JobClient.runJob(jobConf);\n     final double duration = (System.currentTimeMillis() - startTime)/1000.0;\n     System.out.println(\"Job Finished in \" + duration + \" seconds\");\n\n     //read outputs\n     Path inFile = new Path(outDir, \"reduce-out\");\n     LongWritable numInside = new LongWritable();\n     LongWritable numOutside = new LongWritable();\n     SequenceFile.Reader reader = new SequenceFile.Reader(fs, inFile, jobConf);\n     try {\n     reader.next(numInside, numOutside);\n     } finally {\n     reader.close();\n     }\n\n     //compute estimated value\n     return BigDecimal.valueOf(4).setScale(20)\n     .multiply(BigDecimal.valueOf(numInside.get()))\n     .divide(BigDecimal.valueOf(numMaps))\n     .divide(BigDecimal.valueOf(numPoints));\n     } finally {\n     fs.delete(TMP_DIR, true);\n     }\n     }\n\n    //Parse arguments and then runs a map/reduce job.\n    //Print output in standard out.\n    //@return a non-zero if there is an error. Otherwise, return 0.\n     public int run(String[] args) throws Exception {\n     if (args.length != 2) {\n     System.err.println(\"Usage: \"+getClass().getName()+\" <nMaps> <nSamples>\");\n     ToolRunner.printGenericCommandUsage(System.err);\n     return -1;\n     }\n\n     final int nMaps = Integer.parseInt(args[0]);\n     final long nSamples = Long.parseLong(args[1]);\n\n     System.out.println(\"Number of Maps = \" + nMaps);\n     System.out.println(\"Samples per Map = \" + nSamples);\n\n     final JobConf jobConf = new JobConf(getConf(), getClass());\n     System.out.println(\"Estimated value of Pi is \"\n     + estimate(nMaps, nSamples, jobConf));\n     return 0;\n     }\n\n     //main method for running it as a stand alone command.\n     public static void main(String[] argv) throws Exception {\n     System.exit(ToolRunner.run(null, new PiEstimator(), argv));\n     }\n     }\n     \n## Appendix D - The 10gb graysort source code\n\nThe code for the TeraSort MapReduce program is presented for inspection in this section.\n\n\n    /**\n     * Licensed to the Apache Software Foundation (ASF) under one\n     * or more contributor license agreements.  See the NOTICE file\n     * distributed with this work for additional information\n     * regarding copyright ownership.  The ASF licenses this file\n     * to you under the Apache License, Version 2.0 (the\n     * \"License\"); you may not use this file except in compliance\n     * with the License.  You may obtain a copy of the License at\n     *\n     *     http://www.apache.org/licenses/LICENSE-2.0\n     *\n     * Unless required by applicable law or agreed to in writing, software\n     * distributed under the License is distributed on an \"AS IS\" BASIS,\n     * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n     * See the License for the specific language governing permissions and\n     * limitations under the License.\n     */\n\n    package org.apache.hadoop.examples.terasort;\n\n    import java.io.IOException;\n    import java.io.PrintStream;\n    import java.net.URI;\n    import java.util.ArrayList;\n    import java.util.List;\n\n    import org.apache.commons.logging.Log;\n    import org.apache.commons.logging.LogFactory;\n    import org.apache.hadoop.conf.Configured;\n    import org.apache.hadoop.filecache.DistributedCache;\n    import org.apache.hadoop.fs.FileSystem;\n    import org.apache.hadoop.fs.Path;\n    import org.apache.hadoop.io.NullWritable;\n    import org.apache.hadoop.io.SequenceFile;\n    import org.apache.hadoop.io.Text;\n    import org.apache.hadoop.mapred.FileOutputFormat;\n    import org.apache.hadoop.mapred.JobClient;\n    import org.apache.hadoop.mapred.JobConf;\n    import org.apache.hadoop.mapred.Partitioner;\n    import org.apache.hadoop.util.Tool;\n    import org.apache.hadoop.util.ToolRunner;\n\n    /**\n     * Generates the sampled split points, launches the job,\n     * and waits for it to finish.\n     * <p>\n     * To run the program:\n     * <b>bin/hadoop jar hadoop-examples-*.jar terasort in-dir out-dir</b>\n     */\n\n    public class TeraSort extends Configured implements Tool {\n      private static final Log LOG = LogFactory.getLog(TeraSort.class);\n\n      /**\n       * A partitioner that splits text keys into roughly equal\n       * partitions in a global sorted order.\n       */\n\n      static class TotalOrderPartitioner implements Partitioner<Text,Text>{\n        private TrieNode trie;\n        private Text[] splitPoints;\n\n        /**\n         * A generic trie node\n         */\n        static abstract class TrieNode {\n          private int level;\n          TrieNode(int level) {\n            this.level = level;\n          }\n          abstract int findPartition(Text key);\n          abstract void print(PrintStream strm) throws IOException;\n          int getLevel() {\n            return level;\n          }\n        }\n\n        /**\n         * An inner trie node that contains 256 children based on the next\n         * character.\n         */\n        static class InnerTrieNode extends TrieNode {\n          private TrieNode[] child = new TrieNode[256];\n\n          InnerTrieNode(int level) {\n            super(level);\n          }\n          int findPartition(Text key) {\n            int level = getLevel();\n            if (key.getLength() <= level) {\n              return child[0].findPartition(key);\n            }\n            return child[key.getBytes()[level]].findPartition(key);\n          }\n          void setChild(int idx, TrieNode child) {\n            this.child[idx] = child;\n          }\n          void print(PrintStream strm) throws IOException {\n            for(int ch=0; ch < 255; ++ch) {\n              for(int i = 0; i < 2*getLevel(); ++i) {\n                strm.print(' ');\n              }\n              strm.print(ch);\n              strm.println(\" ->\");\n              if (child[ch] != null) {\n                child[ch].print(strm);\n              }\n            }\n          }\n        }\n\n        /**\n         * A leaf trie node that does string compares to figure out where the given\n         * key belongs between lower..upper.\n         */\n        static class LeafTrieNode extends TrieNode {\n          int lower;\n          int upper;\n          Text[] splitPoints;\n          LeafTrieNode(int level, Text[] splitPoints, int lower, int upper) {\n            super(level);\n            this.splitPoints = splitPoints;\n            this.lower = lower;\n            this.upper = upper;\n          }\n          int findPartition(Text key) {\n            for(int i=lower; i<upper; ++i) {\n              if (splitPoints[i].compareTo(key) >= 0) {\n                return i;\n              }\n            }\n            return upper;\n          }\n          void print(PrintStream strm) throws IOException {\n            for(int i = 0; i < 2*getLevel(); ++i) {\n              strm.print(' ');\n            }\n            strm.print(lower);\n            strm.print(\", \");\n            strm.println(upper);\n          }\n        }\n\n\n        /**\n         * Read the cut points from the given sequence file.\n         * @param fs the file system\n         * @param p the path to read\n         * @param job the job config\n         * @return the strings to split the partitions on\n         * @throws IOException\n         */\n        private static Text[] readPartitions(FileSystem fs, Path p,\n                                             JobConf job) throws IOException {\n          SequenceFile.Reader reader = new SequenceFile.Reader(fs, p, job);\n          List<Text> parts = new ArrayList<Text>();\n          Text key = new Text();\n          NullWritable value = NullWritable.get();\n          while (reader.next(key, value)) {\n            parts.add(key);\n            key = new Text();\n          }\n          reader.close();\n          return parts.toArray(new Text[parts.size()]);  \n        }\n\n        /**\n         * Given a sorted set of cut points, build a trie that will find the correct\n         * partition quickly.\n         * @param splits the list of cut points\n         * @param lower the lower bound of partitions 0..numPartitions-1\n         * @param upper the upper bound of partitions 0..numPartitions-1\n         * @param prefix the prefix that we have already checked against\n         * @param maxDepth the maximum depth we will build a trie for\n         * @return the trie node that will divide the splits correctly\n         */\n        private static TrieNode buildTrie(Text[] splits, int lower, int upper,\n                                          Text prefix, int maxDepth) {\n          int depth = prefix.getLength();\n          if (depth >= maxDepth || lower == upper) {\n            return new LeafTrieNode(depth, splits, lower, upper);\n          }\n          InnerTrieNode result = new InnerTrieNode(depth);\n          Text trial = new Text(prefix);\n          // append an extra byte on to the prefix\n          trial.append(new byte[1], 0, 1);\n          int currentBound = lower;\n          for(int ch = 0; ch < 255; ++ch) {\n            trial.getBytes()[depth] = (byte) (ch + 1);\n            lower = currentBound;\n            while (currentBound < upper) {\n              if (splits[currentBound].compareTo(trial) >= 0) {\n                break;\n              }\n              currentBound += 1;\n            }\n            trial.getBytes()[depth] = (byte) ch;\n            result.child[ch] = buildTrie(splits, lower, currentBound, trial,\n                                         maxDepth);\n          }\n          // pick up the rest\n          trial.getBytes()[depth] = 127;\n          result.child[255] = buildTrie(splits, currentBound, upper, trial,\n                                        maxDepth);\n          return result;\n        }\n\n        public void configure(JobConf job) {\n          try {\n            FileSystem fs = FileSystem.getLocal(job);\n            Path partFile = new Path(TeraInputFormat.PARTITION_FILENAME);\n            splitPoints = readPartitions(fs, partFile, job);\n            trie = buildTrie(splitPoints, 0, splitPoints.length, new Text(), 2);\n          } catch (IOException ie) {\n            throw new IllegalArgumentException(\"can't read paritions file\", ie);\n          }\n        }\n\n        public TotalOrderPartitioner() {\n        }\n\n        public int getPartition(Text key, Text value, int numPartitions) {\n          return trie.findPartition(key);\n        }\n\n      }\n\n      public int run(String[] args) throws Exception {\n        LOG.info(\"starting\");\n        JobConf job = (JobConf) getConf();\n        Path inputDir = new Path(args[0]);\n        inputDir = inputDir.makeQualified(inputDir.getFileSystem(job));\n        Path partitionFile = new Path(inputDir, TeraInputFormat.PARTITION_FILENAME);\n        URI partitionUri = new URI(partitionFile.toString() +\n                                   \"#\" + TeraInputFormat.PARTITION_FILENAME);\n        TeraInputFormat.setInputPaths(job, new Path(args[0]));\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\n        job.setJobName(\"TeraSort\");\n        job.setJarByClass(TeraSort.class);\n        job.setOutputKeyClass(Text.class);\n        job.setOutputValueClass(Text.class);\n        job.setInputFormat(TeraInputFormat.class);\n        job.setOutputFormat(TeraOutputFormat.class);\n        job.setPartitionerClass(TotalOrderPartitioner.class);\n        TeraInputFormat.writePartitionFile(job, partitionFile);\n        DistributedCache.addCacheFile(partitionUri, job);\n        DistributedCache.createSymlink(job);\n        job.setInt(\"dfs.replication\", 1);\n        TeraOutputFormat.setFinalSync(job, true);\n        JobClient.runJob(job);\n        LOG.info(\"done\");\n        return 0;\n      }\n\n      /**\n       * @param args\n       */\n\n      public static void main(String[] args) throws Exception {\n        int res = ToolRunner.run(new JobConf(), new TeraSort(), args);\n        System.exit(res);\n      }\n    }\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n[hdinsight-errors]: hdinsight-debug-jobs.md\n\n[hdinsight-sdk-documentation]: https://msdn.microsoft.com/library/azure/dn479185.aspx\n\n[hdinsight-submit-jobs]: hdinsight-submit-hadoop-jobs-programmatically.md\n[hdinsight-introduction]: hdinsight-hadoop-introduction.md\n\n\n[powershell-install-configure]: powershell-install-configure.md\n\n[hdinsight-get-started]: hdinsight-hadoop-linux-tutorial-get-started.md\n\n[hdinsight-samples]: hdinsight-run-samples.md\n[hdinsight-sample-10gb-graysort]: #hdinsight-sample-10gb-graysort\n[hdinsight-sample-csharp-streaming]: #hdinsight-sample-csharp-streaming\n[hdinsight-sample-pi-estimator]: #hdinsight-sample-pi-estimator\n[hdinsight-sample-wordcount]: #hdinsight-sample-wordcount\n\n[hdinsight-use-hive]: hdinsight-use-hive.md\n[hdinsight-use-pig]: hdinsight-use-pig.md\n\n[streamreader]: http://msdn.microsoft.com/library/system.io.streamreader.aspx\n[console-writeline]: http://msdn.microsoft.com/library/system.console.writeline\n"
}