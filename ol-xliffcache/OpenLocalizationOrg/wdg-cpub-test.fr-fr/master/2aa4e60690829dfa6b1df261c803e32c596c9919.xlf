<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="markdown" source-language="en-US" target-language="fr-fr">
    <header>
      <tool tool-id="mdxliff" tool-name="mdxliff" tool-version="1.0-48076a9" tool-company="Microsoft" />
      <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">2aa4e60690829dfa6b1df261c803e32c596c9919</xliffext:olfilehash>
      <xliffext:olfilepath xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">wdg-cpub-test\ndolci1\audio-video-camera\scene-analysis-for-media-capture.md</xliffext:olfilepath>
      <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">
      </xliffext:oltranslationpriority>
      <xliffext:oltranslationtype xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">Human Translation</xliffext:oltranslationtype>
    </header>
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>This article describes how to use the SceneAnalysisEffect and the FaceDetectionEffect to analyze the content of the media capture preview stream.</source>
          <target state="new">This article describes how to use the SceneAnalysisEffect and the FaceDetectionEffect to analyze the content of the media capture preview stream.</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Scene analysis for media capture</source>
          <target state="new">Scene analysis for media capture</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Scene analysis for media capture</source>
          <target state="new">Scene analysis for media capture</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>\[ Updated for UWP apps on Windows 10.</source>
          <target state="new">\[ Updated for UWP apps on Windows 10.</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>For Windows 8.x articles, see the <bpt id="p1">[</bpt>archive<ept id="p1">](http://go.microsoft.com/fwlink/p/?linkid=619132)</ept> \]</source>
          <target state="new">For Windows 8.x articles, see the <bpt id="p1">[</bpt>archive<ept id="p1">](http://go.microsoft.com/fwlink/p/?linkid=619132)</ept> \]</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>This article describes how to use the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>SceneAnalysisEffect<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948902)</ept> and the <bpt id="p3">[</bpt><bpt id="p4">**</bpt>FaceDetectionEffect<ept id="p4">**</ept><ept id="p3">](https://msdn.microsoft.com/library/windows/apps/dn948776)</ept> to analyze the content of the media capture preview stream.</source>
          <target state="new">This article describes how to use the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>SceneAnalysisEffect<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948902)</ept> and the <bpt id="p3">[</bpt><bpt id="p4">**</bpt>FaceDetectionEffect<ept id="p4">**</ept><ept id="p3">](https://msdn.microsoft.com/library/windows/apps/dn948776)</ept> to analyze the content of the media capture preview stream.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>Scene analysis effect</source>
          <target state="new">Scene analysis effect</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">[</bpt><bpt id="p2">**</bpt>SceneAnalysisEffect<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948902)</ept> analyzes the video frames in the media capture preview stream and recommends processing options to improve the capture result.</source>
          <target state="new">The <bpt id="p1">[</bpt><bpt id="p2">**</bpt>SceneAnalysisEffect<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948902)</ept> analyzes the video frames in the media capture preview stream and recommends processing options to improve the capture result.</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>Currently, the effect supports detecting whether the capture would be improved by using High Dynamic Range (HDR) processing.</source>
          <target state="new">Currently, the effect supports detecting whether the capture would be improved by using High Dynamic Range (HDR) processing.</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>If the effect recommends using HDR, you can do this in the following ways:</source>
          <target state="new">If the effect recommends using HDR, you can do this in the following ways:</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>Use the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>AdvancedPhotoCapture<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/mt181386)</ept> class to capture photos using the Windows built-in HDR processing algorithm.</source>
          <target state="new">Use the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>AdvancedPhotoCapture<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/mt181386)</ept> class to capture photos using the Windows built-in HDR processing algorithm.</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>For more information, see <bpt id="p1">[</bpt>High Dynamic Range (HDR) photo capture<ept id="p1">](high-dynamic-range-hdr-photo-capture.md)</ept>.</source>
          <target state="new">For more information, see <bpt id="p1">[</bpt>High Dynamic Range (HDR) photo capture<ept id="p1">](high-dynamic-range-hdr-photo-capture.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>Use the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>HdrVideoControl<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn926680)</ept> to capture video using the Windows built-in HDR processing algorithm.</source>
          <target state="new">Use the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>HdrVideoControl<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn926680)</ept> to capture video using the Windows built-in HDR processing algorithm.</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>For more information, see <bpt id="p1">[</bpt>Capture device controls for video capture<ept id="p1">](capture-device-controls-for-video-capture.md)</ept>.</source>
          <target state="new">For more information, see <bpt id="p1">[</bpt>Capture device controls for video capture<ept id="p1">](capture-device-controls-for-video-capture.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>Use the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>VariablePhotoSequenceControl<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn640573)</ept> to capture a sequence of frames that you can then composite using a custom HDR implementation.</source>
          <target state="new">Use the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>VariablePhotoSequenceControl<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn640573)</ept> to capture a sequence of frames that you can then composite using a custom HDR implementation.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>For more information, see <bpt id="p1">[</bpt>Variable photo sequence<ept id="p1">](variable-photo-sequence.md)</ept>.</source>
          <target state="new">For more information, see <bpt id="p1">[</bpt>Variable photo sequence<ept id="p1">](variable-photo-sequence.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>Scene analysis namespaces</source>
          <target state="new">Scene analysis namespaces</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>To use scene analysis, your app must include the following namespaces in addition to the namespaces required for basic media capture.</source>
          <target state="new">To use scene analysis, your app must include the following namespaces in addition to the namespaces required for basic media capture.</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>SceneAnalysisUsing</source>
          <target state="new">SceneAnalysisUsing</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>Initialize the scene analysis effect and add it to the preview stream</source>
          <target state="new">Initialize the scene analysis effect and add it to the preview stream</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>Video effects are implemented using two APIs, an effect definition, which provides settings that the capture device needs to initialize the effect, and an effect instance, which can be used to control the effect.</source>
          <target state="new">Video effects are implemented using two APIs, an effect definition, which provides settings that the capture device needs to initialize the effect, and an effect instance, which can be used to control the effect.</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>Since you may want to access the effect instance from multiple places within your code, you should typically declare a member variable to hold the object.</source>
          <target state="new">Since you may want to access the effect instance from multiple places within your code, you should typically declare a member variable to hold the object.</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>DeclareSceneAnalysisEffect</source>
          <target state="new">DeclareSceneAnalysisEffect</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>In your app, after you have initialized the <bpt id="p1">**</bpt>MediaCapture<ept id="p1">**</ept> object, create a new instance of <bpt id="p2">[</bpt><bpt id="p3">**</bpt>SceneAnalysisEffectDefinition<ept id="p3">**</ept><ept id="p2">](https://msdn.microsoft.com/library/windows/apps/dn948903)</ept>.</source>
          <target state="new">In your app, after you have initialized the <bpt id="p1">**</bpt>MediaCapture<ept id="p1">**</ept> object, create a new instance of <bpt id="p2">[</bpt><bpt id="p3">**</bpt>SceneAnalysisEffectDefinition<ept id="p3">**</ept><ept id="p2">](https://msdn.microsoft.com/library/windows/apps/dn948903)</ept>.</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>Register the effect with the capture device by calling <bpt id="p1">[</bpt><bpt id="p2">**</bpt>AddVideoEffectAsync<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn878035)</ept> on your <bpt id="p3">**</bpt>MediaCapture<ept id="p3">**</ept> object, providing the <bpt id="p4">**</bpt>SceneAnalysisEffectDefinition<ept id="p4">**</ept> and specifying <bpt id="p5">[</bpt><bpt id="p6">**</bpt>MediaStreamType.VideoPreview<ept id="p6">**</ept><ept id="p5">](https://msdn.microsoft.com/library/windows/apps/br226640)</ept> to indicate that the effect should be applied to the video preview stream, as opposed to the capture stream.</source>
          <target state="new">Register the effect with the capture device by calling <bpt id="p1">[</bpt><bpt id="p2">**</bpt>AddVideoEffectAsync<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn878035)</ept> on your <bpt id="p3">**</bpt>MediaCapture<ept id="p3">**</ept> object, providing the <bpt id="p4">**</bpt>SceneAnalysisEffectDefinition<ept id="p4">**</ept> and specifying <bpt id="p5">[</bpt><bpt id="p6">**</bpt>MediaStreamType.VideoPreview<ept id="p6">**</ept><ept id="p5">](https://msdn.microsoft.com/library/windows/apps/br226640)</ept> to indicate that the effect should be applied to the video preview stream, as opposed to the capture stream.</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>AddVideoEffectAsync<ept id="p1">**</ept> returns an instance of the added effect.</source>
          <target state="new"><bpt id="p1">**</bpt>AddVideoEffectAsync<ept id="p1">**</ept> returns an instance of the added effect.</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>Because this method can be used with multiple effect types, you must cast the returned instance to a <bpt id="p1">[</bpt><bpt id="p2">**</bpt>SceneAnalysisEffect<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948902)</ept> object.</source>
          <target state="new">Because this method can be used with multiple effect types, you must cast the returned instance to a <bpt id="p1">[</bpt><bpt id="p2">**</bpt>SceneAnalysisEffect<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948902)</ept> object.</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>To receive the results of the scene analysis, you must register a handler for the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>SceneAnalyzed<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948920)</ept> event.</source>
          <target state="new">To receive the results of the scene analysis, you must register a handler for the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>SceneAnalyzed<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948920)</ept> event.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>Currently, the scene analysis effect only includes the high dynamic range analyzer.</source>
          <target state="new">Currently, the scene analysis effect only includes the high dynamic range analyzer.</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>Enable HDR analysis by setting the effect's <bpt id="p1">[</bpt><bpt id="p2">**</bpt>HighDynamicRangeControl.Enabled<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948827)</ept> to true.</source>
          <target state="new">Enable HDR analysis by setting the effect's <bpt id="p1">[</bpt><bpt id="p2">**</bpt>HighDynamicRangeControl.Enabled<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948827)</ept> to true.</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>CreateSceneAnalysisEffectAsync</source>
          <target state="new">CreateSceneAnalysisEffectAsync</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>Implement the SceneAnalyzed event handler</source>
          <target state="new">Implement the SceneAnalyzed event handler</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>The results of the scene analysis are returned in the <bpt id="p1">**</bpt>SceneAnalyzed<ept id="p1">**</ept> event handler.</source>
          <target state="new">The results of the scene analysis are returned in the <bpt id="p1">**</bpt>SceneAnalyzed<ept id="p1">**</ept> event handler.</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">[</bpt><bpt id="p2">**</bpt>SceneAnalyzedEventArgs<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948922)</ept> object passed into the handler has a <bpt id="p3">[</bpt><bpt id="p4">**</bpt>SceneAnalysisEffectFrame<ept id="p4">**</ept><ept id="p3">](https://msdn.microsoft.com/library/windows/apps/dn948907)</ept> object which has a <bpt id="p5">[</bpt><bpt id="p6">**</bpt>HighDynamicRangeOutput<ept id="p6">**</ept><ept id="p5">](https://msdn.microsoft.com/library/windows/apps/dn948830)</ept> object.</source>
          <target state="new">The <bpt id="p1">[</bpt><bpt id="p2">**</bpt>SceneAnalyzedEventArgs<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948922)</ept> object passed into the handler has a <bpt id="p3">[</bpt><bpt id="p4">**</bpt>SceneAnalysisEffectFrame<ept id="p4">**</ept><ept id="p3">](https://msdn.microsoft.com/library/windows/apps/dn948907)</ept> object which has a <bpt id="p5">[</bpt><bpt id="p6">**</bpt>HighDynamicRangeOutput<ept id="p6">**</ept><ept id="p5">](https://msdn.microsoft.com/library/windows/apps/dn948830)</ept> object.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">[</bpt><bpt id="p2">**</bpt>Certainty<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948833)</ept> property of the high dynamic range output provides a value between 0 and 1.0 where 0 indicates that HDR processing would not help improve the capture result and 1.0 indicates that HDR processing would help.</source>
          <target state="new">The <bpt id="p1">[</bpt><bpt id="p2">**</bpt>Certainty<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948833)</ept> property of the high dynamic range output provides a value between 0 and 1.0 where 0 indicates that HDR processing would not help improve the capture result and 1.0 indicates that HDR processing would help.</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>Your can decide the threshold point at which you want to use HDR or show the results to the user and let the user decide.</source>
          <target state="new">Your can decide the threshold point at which you want to use HDR or show the results to the user and let the user decide.</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>SceneAnalyzed</source>
          <target state="new">SceneAnalyzed</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">[</bpt><bpt id="p2">**</bpt>HighDynamicRangeOutput<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948830)</ept> object passed into the handler also has a <bpt id="p3">[</bpt><bpt id="p4">**</bpt>FrameControllers<ept id="p4">**</ept><ept id="p3">](https://msdn.microsoft.com/library/windows/apps/dn948834)</ept> property which contains suggested frame controllers for capturing a variable photo sequence for HDR processing.</source>
          <target state="new">The <bpt id="p1">[</bpt><bpt id="p2">**</bpt>HighDynamicRangeOutput<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948830)</ept> object passed into the handler also has a <bpt id="p3">[</bpt><bpt id="p4">**</bpt>FrameControllers<ept id="p4">**</ept><ept id="p3">](https://msdn.microsoft.com/library/windows/apps/dn948834)</ept> property which contains suggested frame controllers for capturing a variable photo sequence for HDR processing.</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>For more information, see <bpt id="p1">[</bpt>Variable photo sequence<ept id="p1">](variable-photo-sequence.md)</ept>.</source>
          <target state="new">For more information, see <bpt id="p1">[</bpt>Variable photo sequence<ept id="p1">](variable-photo-sequence.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>Clean up the scene analysis effect</source>
          <target state="new">Clean up the scene analysis effect</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>When your app is done capturing, before disposing of the <bpt id="p1">**</bpt>MediaCapture<ept id="p1">**</ept> object, you should disable the scene analysis effect by setting the effect's <bpt id="p2">[</bpt><bpt id="p3">**</bpt>HighDynamicRangeAnalyzer.Enabled<ept id="p3">**</ept><ept id="p2">](https://msdn.microsoft.com/library/windows/apps/dn948827)</ept> property to false and unregister your <bpt id="p4">[</bpt><bpt id="p5">**</bpt>SceneAnalyzed<ept id="p5">**</ept><ept id="p4">](https://msdn.microsoft.com/library/windows/apps/dn948920)</ept> event handler.</source>
          <target state="new">When your app is done capturing, before disposing of the <bpt id="p1">**</bpt>MediaCapture<ept id="p1">**</ept> object, you should disable the scene analysis effect by setting the effect's <bpt id="p2">[</bpt><bpt id="p3">**</bpt>HighDynamicRangeAnalyzer.Enabled<ept id="p3">**</ept><ept id="p2">](https://msdn.microsoft.com/library/windows/apps/dn948827)</ept> property to false and unregister your <bpt id="p4">[</bpt><bpt id="p5">**</bpt>SceneAnalyzed<ept id="p5">**</ept><ept id="p4">](https://msdn.microsoft.com/library/windows/apps/dn948920)</ept> event handler.</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>Call <bpt id="p1">[</bpt><bpt id="p2">**</bpt>MediaCapture.ClearEffectsAsync<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/br226592)</ept>, specifying the video preview stream since that was the stream to which the effect was added.</source>
          <target state="new">Call <bpt id="p1">[</bpt><bpt id="p2">**</bpt>MediaCapture.ClearEffectsAsync<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/br226592)</ept>, specifying the video preview stream since that was the stream to which the effect was added.</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>Finally, set your member variable to null.</source>
          <target state="new">Finally, set your member variable to null.</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>CleanUpSceneAnalysisEffectAsync</source>
          <target state="new">CleanUpSceneAnalysisEffectAsync</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>Face detection effect</source>
          <target state="new">Face detection effect</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">[</bpt><bpt id="p2">**</bpt>FaceDetectionEffect<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948776)</ept> identifies the location of faces within the media capture preview stream.</source>
          <target state="new">The <bpt id="p1">[</bpt><bpt id="p2">**</bpt>FaceDetectionEffect<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948776)</ept> identifies the location of faces within the media capture preview stream.</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>The effect allows you to receive a notification whenever a face is detected in the preview stream and provides the bounding box for each detected face within the preview frame.</source>
          <target state="new">The effect allows you to receive a notification whenever a face is detected in the preview stream and provides the bounding box for each detected face within the preview frame.</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>On supported devices, the face detection effect also provides enhanced exposure and focus on the most important face in the scene.</source>
          <target state="new">On supported devices, the face detection effect also provides enhanced exposure and focus on the most important face in the scene.</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>Face detection namespaces</source>
          <target state="new">Face detection namespaces</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>To use face detection, your app must include the following namespaces in addition to the namespaces required for basic media capture.</source>
          <target state="new">To use face detection, your app must include the following namespaces in addition to the namespaces required for basic media capture.</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>FaceDetectionUsing</source>
          <target state="new">FaceDetectionUsing</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>Initialize the face detection effect and add it to the preview stream</source>
          <target state="new">Initialize the face detection effect and add it to the preview stream</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>Video effects are implemented using two APIs, an effect definition, which provides settings that the capture device needs to initialize the effect, and an effect instance, which can be used to control the effect.</source>
          <target state="new">Video effects are implemented using two APIs, an effect definition, which provides settings that the capture device needs to initialize the effect, and an effect instance, which can be used to control the effect.</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>Since you may want to access the effect instance from multiple places within your code, you should typically declare a member variable to hold the object.</source>
          <target state="new">Since you may want to access the effect instance from multiple places within your code, you should typically declare a member variable to hold the object.</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source>DeclareFaceDetectionEffect</source>
          <target state="new">DeclareFaceDetectionEffect</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>In your app, after you have initialized the <bpt id="p1">**</bpt>MediaCapture<ept id="p1">**</ept> object, create a new instance of <bpt id="p2">[</bpt><bpt id="p3">**</bpt>FaceDetectionEffectDefinition<ept id="p3">**</ept><ept id="p2">](https://msdn.microsoft.com/library/windows/apps/dn948778)</ept>.</source>
          <target state="new">In your app, after you have initialized the <bpt id="p1">**</bpt>MediaCapture<ept id="p1">**</ept> object, create a new instance of <bpt id="p2">[</bpt><bpt id="p3">**</bpt>FaceDetectionEffectDefinition<ept id="p3">**</ept><ept id="p2">](https://msdn.microsoft.com/library/windows/apps/dn948778)</ept>.</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>Set the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>DetectionMode<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948781)</ept> property to prioritize faster face detection or more accurate face detection.</source>
          <target state="new">Set the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>DetectionMode<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948781)</ept> property to prioritize faster face detection or more accurate face detection.</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>Set <bpt id="p1">[</bpt><bpt id="p2">**</bpt>SynchronousDetectionEnabled<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948786)</ept> to specify that incoming frames are not delayed waiting for face detection to complete as this can result in a choppy preview experience.</source>
          <target state="new">Set <bpt id="p1">[</bpt><bpt id="p2">**</bpt>SynchronousDetectionEnabled<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948786)</ept> to specify that incoming frames are not delayed waiting for face detection to complete as this can result in a choppy preview experience.</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>Register the effect with the capture device by calling <bpt id="p1">[</bpt><bpt id="p2">**</bpt>AddVideoEffectAsync<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn878035)</ept> on your <bpt id="p3">**</bpt>MediaCapture<ept id="p3">**</ept> object, providing the <bpt id="p4">**</bpt>FaceDetectionEffectDefinition<ept id="p4">**</ept> and specifying <bpt id="p5">[</bpt><bpt id="p6">**</bpt>MediaStreamType.VideoPreview<ept id="p6">**</ept><ept id="p5">](https://msdn.microsoft.com/library/windows/apps/br226640)</ept> to indicate that the effect should be applied to the video preview stream, as opposed to the capture stream.</source>
          <target state="new">Register the effect with the capture device by calling <bpt id="p1">[</bpt><bpt id="p2">**</bpt>AddVideoEffectAsync<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn878035)</ept> on your <bpt id="p3">**</bpt>MediaCapture<ept id="p3">**</ept> object, providing the <bpt id="p4">**</bpt>FaceDetectionEffectDefinition<ept id="p4">**</ept> and specifying <bpt id="p5">[</bpt><bpt id="p6">**</bpt>MediaStreamType.VideoPreview<ept id="p6">**</ept><ept id="p5">](https://msdn.microsoft.com/library/windows/apps/br226640)</ept> to indicate that the effect should be applied to the video preview stream, as opposed to the capture stream.</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>AddVideoEffectAsync<ept id="p1">**</ept> returns an instance of the added effect.</source>
          <target state="new"><bpt id="p1">**</bpt>AddVideoEffectAsync<ept id="p1">**</ept> returns an instance of the added effect.</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>Because this method can be used with multiple effect types, you must cast the returned instance to a <bpt id="p1">[</bpt><bpt id="p2">**</bpt>FaceDetectionEffect<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948776)</ept> object.</source>
          <target state="new">Because this method can be used with multiple effect types, you must cast the returned instance to a <bpt id="p1">[</bpt><bpt id="p2">**</bpt>FaceDetectionEffect<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948776)</ept> object.</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>Enable or disable the effect by setting the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>FaceDetectionEffect.Enabled<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948818)</ept> property.</source>
          <target state="new">Enable or disable the effect by setting the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>FaceDetectionEffect.Enabled<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948818)</ept> property.</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>Adjust how often the effect analyzes frames by setting the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>FaceDetectionEffect.DesiredDetectionInterval<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948814)</ept> property.</source>
          <target state="new">Adjust how often the effect analyzes frames by setting the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>FaceDetectionEffect.DesiredDetectionInterval<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948814)</ept> property.</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>Both of these properties can be adjusted while media capture is ongoing.</source>
          <target state="new">Both of these properties can be adjusted while media capture is ongoing.</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>CreateFaceDetectionEffectAsync</source>
          <target state="new">CreateFaceDetectionEffectAsync</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>Receive notifications when faces are detected</source>
          <target state="new">Receive notifications when faces are detected</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source>If you want to perform some action when faces are detected, such as drawing a box around detected faces in the video preview, you can register for the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>FaceDetected<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948820)</ept> event.</source>
          <target state="new">If you want to perform some action when faces are detected, such as drawing a box around detected faces in the video preview, you can register for the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>FaceDetected<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948820)</ept> event.</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source>RegisterFaceDetectionHandler</source>
          <target state="new">RegisterFaceDetectionHandler</target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source>In the handler for the event, you can get a list of all faces detected in a frame by accessing the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>FaceDetectionEffectFrame.DetectedFaces<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948792)</ept> property of the <bpt id="p3">[</bpt><bpt id="p4">**</bpt>FaceDetectedEventArgs<ept id="p4">**</ept><ept id="p3">](https://msdn.microsoft.com/library/windows/apps/dn948774)</ept>.</source>
          <target state="new">In the handler for the event, you can get a list of all faces detected in a frame by accessing the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>FaceDetectionEffectFrame.DetectedFaces<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn948792)</ept> property of the <bpt id="p3">[</bpt><bpt id="p4">**</bpt>FaceDetectedEventArgs<ept id="p4">**</ept><ept id="p3">](https://msdn.microsoft.com/library/windows/apps/dn948774)</ept>.</target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">[</bpt><bpt id="p2">**</bpt>FaceBox<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn974126)</ept> property is a <bpt id="p3">[</bpt><bpt id="p4">**</bpt>BitmapBounds<ept id="p4">**</ept><ept id="p3">](https://msdn.microsoft.com/library/windows/apps/br226169)</ept> structure that describes the rectangle containing the detected face in units relative to the preview stream dimensions.</source>
          <target state="new">The <bpt id="p1">[</bpt><bpt id="p2">**</bpt>FaceBox<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn974126)</ept> property is a <bpt id="p3">[</bpt><bpt id="p4">**</bpt>BitmapBounds<ept id="p4">**</ept><ept id="p3">](https://msdn.microsoft.com/library/windows/apps/br226169)</ept> structure that describes the rectangle containing the detected face in units relative to the preview stream dimensions.</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>To view sample code that transforms the preview stream coordinates into screen coordinates, see the <bpt id="p1">[</bpt>face detection UWP sample<ept id="p1">](http://go.microsoft.com/fwlink/?LinkId=619486)</ept>.</source>
          <target state="new">To view sample code that transforms the preview stream coordinates into screen coordinates, see the <bpt id="p1">[</bpt>face detection UWP sample<ept id="p1">](http://go.microsoft.com/fwlink/?LinkId=619486)</ept>.</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source>FaceDetected</source>
          <target state="new">FaceDetected</target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source>Clean up the face detection effect</source>
          <target state="new">Clean up the face detection effect</target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source>When your app is done capturing, before disposing of the <bpt id="p1">**</bpt>MediaCapture<ept id="p1">**</ept> object, you should disable the face detection effect with <bpt id="p2">[</bpt><bpt id="p3">**</bpt>FaceDetectionEffect.Enabled<ept id="p3">**</ept><ept id="p2">](https://msdn.microsoft.com/library/windows/apps/dn948818)</ept> and unregister your <bpt id="p4">[</bpt><bpt id="p5">**</bpt>FaceDetected<ept id="p5">**</ept><ept id="p4">](https://msdn.microsoft.com/library/windows/apps/dn948820)</ept> event handler if you previously registered one.</source>
          <target state="new">When your app is done capturing, before disposing of the <bpt id="p1">**</bpt>MediaCapture<ept id="p1">**</ept> object, you should disable the face detection effect with <bpt id="p2">[</bpt><bpt id="p3">**</bpt>FaceDetectionEffect.Enabled<ept id="p3">**</ept><ept id="p2">](https://msdn.microsoft.com/library/windows/apps/dn948818)</ept> and unregister your <bpt id="p4">[</bpt><bpt id="p5">**</bpt>FaceDetected<ept id="p5">**</ept><ept id="p4">](https://msdn.microsoft.com/library/windows/apps/dn948820)</ept> event handler if you previously registered one.</target>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source>Call <bpt id="p1">[</bpt><bpt id="p2">**</bpt>MediaCapture.ClearEffectsAsync<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/br226592)</ept>, specifying the video preview stream since that was the stream to which the effect was added.</source>
          <target state="new">Call <bpt id="p1">[</bpt><bpt id="p2">**</bpt>MediaCapture.ClearEffectsAsync<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/br226592)</ept>, specifying the video preview stream since that was the stream to which the effect was added.</target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source>Finally, set your member variable to null.</source>
          <target state="new">Finally, set your member variable to null.</target>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source>CleanUpFaceDetectionEffectAsync</source>
          <target state="new">CleanUpFaceDetectionEffectAsync</target>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source>Check for focus and exposure support for detected faces</source>
          <target state="new">Check for focus and exposure support for detected faces</target>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source>Not all devices have a capture device that can adjust its focus and exposure based on detected faces.</source>
          <target state="new">Not all devices have a capture device that can adjust its focus and exposure based on detected faces.</target>
        </trans-unit>
        <trans-unit id="180" translate="yes" xml:space="preserve">
          <source>Because face detection consumes device resources, you may only want to enable face detection on devices that can use the feature to enhance capture.</source>
          <target state="new">Because face detection consumes device resources, you may only want to enable face detection on devices that can use the feature to enhance capture.</target>
        </trans-unit>
        <trans-unit id="181" translate="yes" xml:space="preserve">
          <source>To see if face-based capture optimization is available, get the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>VideoDeviceController<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/br226825)</ept> for your initialized <bpt id="p3">[</bpt>MediaCapture<ept id="p3">](capture-photos-and-video-with-mediacapture.md)</ept> and then get the video device controller's <bpt id="p4">[</bpt><bpt id="p5">**</bpt>RegionsOfInterestControl<ept id="p5">**</ept><ept id="p4">](https://msdn.microsoft.com/library/windows/apps/dn279064)</ept>.</source>
          <target state="new">To see if face-based capture optimization is available, get the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>VideoDeviceController<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/br226825)</ept> for your initialized <bpt id="p3">[</bpt>MediaCapture<ept id="p3">](capture-photos-and-video-with-mediacapture.md)</ept> and then get the video device controller's <bpt id="p4">[</bpt><bpt id="p5">**</bpt>RegionsOfInterestControl<ept id="p5">**</ept><ept id="p4">](https://msdn.microsoft.com/library/windows/apps/dn279064)</ept>.</target>
        </trans-unit>
        <trans-unit id="182" translate="yes" xml:space="preserve">
          <source>Check to see if the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>MaxRegions<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn279069)</ept> supports at least one region.</source>
          <target state="new">Check to see if the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>MaxRegions<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn279069)</ept> supports at least one region.</target>
        </trans-unit>
        <trans-unit id="183" translate="yes" xml:space="preserve">
          <source>Then check to see if either <bpt id="p1">[</bpt><bpt id="p2">**</bpt>AutoExposureSupported<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn279065)</ept> or <bpt id="p3">[</bpt><bpt id="p4">**</bpt>AutoFocusSupported<ept id="p4">**</ept><ept id="p3">](https://msdn.microsoft.com/library/windows/apps/dn279066)</ept> are true.</source>
          <target state="new">Then check to see if either <bpt id="p1">[</bpt><bpt id="p2">**</bpt>AutoExposureSupported<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn279065)</ept> or <bpt id="p3">[</bpt><bpt id="p4">**</bpt>AutoFocusSupported<ept id="p4">**</ept><ept id="p3">](https://msdn.microsoft.com/library/windows/apps/dn279066)</ept> are true.</target>
        </trans-unit>
        <trans-unit id="184" translate="yes" xml:space="preserve">
          <source>If these conditions are met, then the device can take advantage of face detection to enhance capture.</source>
          <target state="new">If these conditions are met, then the device can take advantage of face detection to enhance capture.</target>
        </trans-unit>
        <trans-unit id="185" translate="yes" xml:space="preserve">
          <source>AreFaceFocusAndExposureSupported</source>
          <target state="new">AreFaceFocusAndExposureSupported</target>
        </trans-unit>
        <trans-unit id="186" translate="yes" xml:space="preserve">
          <source>Related topics</source>
          <target state="new">Related topics</target>
        </trans-unit>
        <trans-unit id="187" translate="yes" xml:space="preserve">
          <source>Capture photos and video with MediaCapture</source>
          <target state="new">Capture photos and video with MediaCapture</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>