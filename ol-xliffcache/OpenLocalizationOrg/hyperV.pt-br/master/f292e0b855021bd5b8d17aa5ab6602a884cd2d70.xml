{
  "nodes": [
    {
      "content": "This article provides background information and considerations for using the Azure A8, A9, A10, and A11 instances, also known as <bpt id=\"p1\">*</bpt>compute-intensive<ept id=\"p1\">*</ept> instances.",
      "pos": [
        2,
        162
      ]
    },
    {
      "content": "Key features of these instances include:",
      "pos": [
        163,
        203
      ]
    },
    {
      "pos": [
        207,
        470
      ],
      "content": "<bpt id=\"p1\">**</bpt>High-performance hardware<ept id=\"p1\">**</ept> – The Azure datacenter hardware that runs these instances is designed and optimized for compute-intensive and network-intensive applications, including high-performance computing (HPC) cluster applications, modeling, and simulations."
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>RDMA network connection for MPI applications<ept id=\"p1\">**</ept> – Set up the A8 and A9 instances to communicate with other A8 and A9 instances over a low-latency, high-throughput network in Azure that is based on remote direct memory access (RDMA) technology.",
      "pos": [
        474,
        718
      ]
    },
    {
      "content": "This feature can boost the performance of certain Linux and Windows Message Passing Interface (MPI) applications .",
      "pos": [
        719,
        833
      ]
    },
    {
      "pos": [
        837,
        1050
      ],
      "content": "<bpt id=\"p1\">**</bpt>Support for HPC clusters<ept id=\"p1\">**</ept> – Deploy cluster management and job scheduling software on the A8, A9, A10, and A11 instances in Azure to create a stand-alone HPC cluster or to add capacity to an on-premises cluster."
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph>A10 and A11 instances have the same performance optimizations and specifications as the A8 and A9 instances.",
      "pos": [
        1053,
        1173
      ]
    },
    {
      "content": "However, they do not include access to the RDMA network in Azure.",
      "pos": [
        1174,
        1239
      ]
    },
    {
      "content": "They are designed for HPC applications that do not require constant and low-latency communication between nodes, also known as parametric or embarrassingly parallel applications.",
      "pos": [
        1240,
        1418
      ]
    },
    {
      "content": "Specs",
      "pos": [
        1424,
        1429
      ]
    },
    {
      "content": "CPU and memory",
      "pos": [
        1435,
        1449
      ]
    },
    {
      "content": "The Azure A8, A9, A10, and A11 compute-intensive instances feature high-speed, multicore CPUs and large amounts of memory, as shown in the following table.",
      "pos": [
        1451,
        1606
      ]
    },
    {
      "content": "Size",
      "pos": [
        1608,
        1612
      ]
    },
    {
      "content": "CPU",
      "pos": [
        1615,
        1618
      ]
    },
    {
      "content": "Memory",
      "pos": [
        1621,
        1627
      ]
    },
    {
      "content": "A8 and A10",
      "pos": [
        1675,
        1685
      ]
    },
    {
      "content": "Intel Xeon E5-2670",
      "pos": [
        1688,
        1706
      ]
    },
    {
      "content": "8 cores @ 2.6 GHz",
      "pos": [
        1711,
        1728
      ]
    },
    {
      "content": "DDR3-1600 MHz",
      "pos": [
        1731,
        1744
      ]
    },
    {
      "content": "56 GB",
      "pos": [
        1749,
        1754
      ]
    },
    {
      "content": "A9 and A11",
      "pos": [
        1755,
        1765
      ]
    },
    {
      "content": "Intel Xeon E5-2670",
      "pos": [
        1768,
        1786
      ]
    },
    {
      "content": "16 cores @ 2.6 GHz",
      "pos": [
        1791,
        1809
      ]
    },
    {
      "content": "DDR3-1600 MHz",
      "pos": [
        1812,
        1825
      ]
    },
    {
      "content": "112 GB",
      "pos": [
        1830,
        1836
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph>Additional processor details, including supported instruction set extensions, are at the Intel.com website.",
      "pos": [
        1840,
        1959
      ]
    },
    {
      "content": "For VM storage capacities and disk details, see <bpt id=\"p1\">[</bpt>Sizes for virtual machines<ept id=\"p1\">](../articles/virtual-machines/virtual-machines-linux-sizes.md)</ept>.",
      "pos": [
        1960,
        2099
      ]
    },
    {
      "content": "Network adapters",
      "pos": [
        2105,
        2121
      ]
    },
    {
      "content": "A8 and A9 instances have two network adapters, which connect to the following two back-end Azure networks.",
      "pos": [
        2123,
        2229
      ]
    },
    {
      "content": "Network",
      "pos": [
        2232,
        2239
      ]
    },
    {
      "content": "Description",
      "pos": [
        2242,
        2253
      ]
    },
    {
      "content": "10-Gbps Ethernet",
      "pos": [
        2277,
        2293
      ]
    },
    {
      "content": "Connects to Azure services (such as Azure Storage and Azure Virtual Network) and to the Internet.",
      "pos": [
        2296,
        2393
      ]
    },
    {
      "content": "32-Gbps back end, RDMA capable",
      "pos": [
        2394,
        2424
      ]
    },
    {
      "content": "Enables low-latency, high-throughput application communication between instances within a single cloud service or availability set.",
      "pos": [
        2427,
        2558
      ]
    },
    {
      "content": "Reserved for MPI traffic only.",
      "pos": [
        2559,
        2589
      ]
    },
    {
      "pos": [
        2593,
        2761
      ],
      "content": "<ph id=\"ph1\">[AZURE.IMPORTANT]</ph>See <bpt id=\"p1\">[</bpt>Access to the RDMA network<ept id=\"p1\">](#access-the-rdma-network)</ept> in this article for additional requirements for MPI applications to access the RDMA netowrk."
    },
    {
      "content": "A10 and A11 instances have a single, 10-Gbps Ethernet network adapter that connects to Azure services and the Internet.",
      "pos": [
        2763,
        2882
      ]
    },
    {
      "content": "Deployment considerations",
      "pos": [
        2887,
        2912
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Azure account<ept id=\"p1\">**</ept> – If you want to deploy more than a small number of compute-intensive instances, consider a pay-as-you-go subscription or other purchase options.",
      "pos": [
        2916,
        3079
      ]
    },
    {
      "content": "You can also use your MSDN subscription.",
      "pos": [
        3080,
        3120
      ]
    },
    {
      "content": "See <bpt id=\"p1\">[</bpt>Azure benefit for MSDN subscribers<ept id=\"p1\">](https://azure.microsoft.com/pricing/member-offers/msdn-benefits-details/)</ept>.",
      "pos": [
        3121,
        3236
      ]
    },
    {
      "content": "If you're using an <bpt id=\"p1\">[</bpt>Azure free account<ept id=\"p1\">](https://azure.microsoft.com/pricing/free-trial/)</ept>, you can use only a limited number of Azure compute cores.",
      "pos": [
        3237,
        3384
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Cores quota<ept id=\"p1\">**</ept> – You might need to increase the cores quota in your Azure subscription from the default of 20 cores per subscription (if you use the classic deployment model) or 20 cores per region (if you use the Azure Resource Manager deployment model).",
      "pos": [
        3388,
        3644
      ]
    },
    {
      "content": "To request a quota increase, open a support ticket at no charge as shown in <bpt id=\"p1\">[</bpt>Understanding Azure limits and increases<ept id=\"p1\">](https://azure.microsoft.com/blog/2014/06/04/azure-limits-quotas-increase-requests/)</ept>.",
      "pos": [
        3645,
        3848
      ]
    },
    {
      "content": "<ph id=\"ph1\">[AZURE.NOTE]</ph>Azure quotas are credit limits, not capacity guarantees.",
      "pos": [
        3855,
        3923
      ]
    },
    {
      "content": "You are charged only for cores that you use.",
      "pos": [
        3924,
        3968
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Virtual network<ept id=\"p1\">**</ept> – An Azure <bpt id=\"p2\">[</bpt>virtual network<ept id=\"p2\">](https://azure.microsoft.com/documentation/services/virtual-network/)</ept> is not required to use the compute-intensive instances.",
      "pos": [
        3972,
        4145
      ]
    },
    {
      "content": "However, you may need at least a cloud-based Azure virtual network for many IaaS scenarios, or a site-to-site connection if you need to access on-premises resources such as an application license server.",
      "pos": [
        4146,
        4349
      ]
    },
    {
      "content": "You will need to create a new virtual network to deploy the instances.",
      "pos": [
        4350,
        4420
      ]
    },
    {
      "content": "Adding an A8, A9, A10, or A11 VM to a virtual network in an affinity group is not supported.",
      "pos": [
        4421,
        4513
      ]
    },
    {
      "pos": [
        4517,
        4789
      ],
      "content": "<bpt id=\"p1\">**</bpt>Cloud service or availability set<ept id=\"p1\">**</ept> – To connect through the RDMA network, the A8 and A9 instances must be deployed in the same cloud service (if you use the classic deployment model) or the same availability set (if you use the Azure Resource Manager deployment model)."
    },
    {
      "pos": [
        4793,
        4875
      ],
      "content": "<bpt id=\"p1\">**</bpt>Pricing<ept id=\"p1\">**</ept> - The A8–A11 VM sizes are available only in the Standard pricing tier."
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>Resizing<ept id=\"p1\">**</ept> – You can't resize an instance of size other than A8–A11 to one of the compute-intensive instance sizes (A8-11), and you can’t resize a compute-intensive instance to a non-compute-intensive size.",
      "pos": [
        4879,
        5087
      ]
    },
    {
      "content": "This is because of the specialized hardware and the performance optimizations that are specific to compute-intensive instances.",
      "pos": [
        5088,
        5215
      ]
    },
    {
      "content": "<bpt id=\"p1\">**</bpt>RDMA network address space<ept id=\"p1\">**</ept> - The RDMA network in Azure reserves the address space 172.16.0.0/12.",
      "pos": [
        5219,
        5319
      ]
    },
    {
      "content": "If you plan to run MPI applications on A8 and A9 instances in an Azure virtual network, make sure that the virtual network address space does not overlap the RDMA network.",
      "pos": [
        5320,
        5491
      ]
    }
  ],
  "content": "\n\nThis article provides background information and considerations for using the Azure A8, A9, A10, and A11 instances, also known as *compute-intensive* instances. Key features of these instances include:\n\n* **High-performance hardware** – The Azure datacenter hardware that runs these instances is designed and optimized for compute-intensive and network-intensive applications, including high-performance computing (HPC) cluster applications, modeling, and simulations.\n\n* **RDMA network connection for MPI applications** – Set up the A8 and A9 instances to communicate with other A8 and A9 instances over a low-latency, high-throughput network in Azure that is based on remote direct memory access (RDMA) technology. This feature can boost the performance of certain Linux and Windows Message Passing Interface (MPI) applications .\n\n* **Support for HPC clusters** – Deploy cluster management and job scheduling software on the A8, A9, A10, and A11 instances in Azure to create a stand-alone HPC cluster or to add capacity to an on-premises cluster.\n\n>[AZURE.NOTE]A10 and A11 instances have the same performance optimizations and specifications as the A8 and A9 instances. However, they do not include access to the RDMA network in Azure. They are designed for HPC applications that do not require constant and low-latency communication between nodes, also known as parametric or embarrassingly parallel applications.\n\n\n## Specs\n\n### CPU and memory\n\nThe Azure A8, A9, A10, and A11 compute-intensive instances feature high-speed, multicore CPUs and large amounts of memory, as shown in the following table.\n\nSize | CPU | Memory\n------------- | ----------- | ----------------\nA8 and A10 | Intel Xeon E5-2670<br/>8 cores @ 2.6 GHz | DDR3-1600 MHz<br/>56 GB\nA9 and A11 | Intel Xeon E5-2670<br/>16 cores @ 2.6 GHz | DDR3-1600 MHz<br/>112 GB\n\n\n>[AZURE.NOTE]Additional processor details, including supported instruction set extensions, are at the Intel.com website. For VM storage capacities and disk details, see [Sizes for virtual machines](../articles/virtual-machines/virtual-machines-linux-sizes.md).\n\n### Network adapters\n\nA8 and A9 instances have two network adapters, which connect to the following two back-end Azure networks.\n\n\nNetwork | Description\n-------- | -----------\n10-Gbps Ethernet | Connects to Azure services (such as Azure Storage and Azure Virtual Network) and to the Internet.\n32-Gbps back end, RDMA capable | Enables low-latency, high-throughput application communication between instances within a single cloud service or availability set. Reserved for MPI traffic only.\n\n\n>[AZURE.IMPORTANT]See [Access to the RDMA network](#access-the-rdma-network) in this article for additional requirements for MPI applications to access the RDMA netowrk.\n\nA10 and A11 instances have a single, 10-Gbps Ethernet network adapter that connects to Azure services and the Internet.\n\n## Deployment considerations\n\n* **Azure account** – If you want to deploy more than a small number of compute-intensive instances, consider a pay-as-you-go subscription or other purchase options. You can also use your MSDN subscription. See [Azure benefit for MSDN subscribers](https://azure.microsoft.com/pricing/member-offers/msdn-benefits-details/). If you're using an [Azure free account](https://azure.microsoft.com/pricing/free-trial/), you can use only a limited number of Azure compute cores.\n\n* **Cores quota** – You might need to increase the cores quota in your Azure subscription from the default of 20 cores per subscription (if you use the classic deployment model) or 20 cores per region (if you use the Azure Resource Manager deployment model). To request a quota increase, open a support ticket at no charge as shown in [Understanding Azure limits and increases](https://azure.microsoft.com/blog/2014/06/04/azure-limits-quotas-increase-requests/).\n\n    >[AZURE.NOTE]Azure quotas are credit limits, not capacity guarantees. You are charged only for cores that you use.\n\n* **Virtual network** – An Azure [virtual network](https://azure.microsoft.com/documentation/services/virtual-network/) is not required to use the compute-intensive instances. However, you may need at least a cloud-based Azure virtual network for many IaaS scenarios, or a site-to-site connection if you need to access on-premises resources such as an application license server. You will need to create a new virtual network to deploy the instances. Adding an A8, A9, A10, or A11 VM to a virtual network in an affinity group is not supported.\n\n* **Cloud service or availability set** – To connect through the RDMA network, the A8 and A9 instances must be deployed in the same cloud service (if you use the classic deployment model) or the same availability set (if you use the Azure Resource Manager deployment model).\n\n* **Pricing** - The A8–A11 VM sizes are available only in the Standard pricing tier.\n\n* **Resizing** – You can't resize an instance of size other than A8–A11 to one of the compute-intensive instance sizes (A8-11), and you can’t resize a compute-intensive instance to a non-compute-intensive size. This is because of the specialized hardware and the performance optimizations that are specific to compute-intensive instances.\n\n* **RDMA network address space** - The RDMA network in Azure reserves the address space 172.16.0.0/12. If you plan to run MPI applications on A8 and A9 instances in an Azure virtual network, make sure that the virtual network address space does not overlap the RDMA network.\n\n\n\n\n\n"
}