{
  "nodes": [
    {
      "pos": [
        27,
        103
      ],
      "content": "Ten things you can do on the Data science Virtual Machine  | Microsoft Azure"
    },
    {
      "pos": [
        122,
        209
      ],
      "content": "Perform various data exploration and modeling task on the Data science Virtual Machine."
    },
    {
      "pos": [
        547,
        604
      ],
      "content": "Ten things you can do on the Data science Virtual Machine"
    },
    {
      "pos": [
        607,
        1213
      ],
      "content": "The Microsoft Data Science Virtual Machine (DSVM) is a powerful data science development environment that enables you to perform various data exploration and modeling tasks. The environment comes already built and bundled with several popular data analytics tools that make it easy to get started quickly with your analysis. The DSVM works closely with many Azure services and is able to read and process data that is already stored on Azure in Azure SQL Data Warehouse, Azure Data Lake, Azure Storage, or DocumentDB. It can also leverage analytics tools like Azure Machine Learning and Azure Data Factory.",
      "nodes": [
        {
          "content": "The Microsoft Data Science Virtual Machine (DSVM) is a powerful data science development environment that enables you to perform various data exploration and modeling tasks.",
          "pos": [
            0,
            173
          ]
        },
        {
          "content": "The environment comes already built and bundled with several popular data analytics tools that make it easy to get started quickly with your analysis.",
          "pos": [
            174,
            324
          ]
        },
        {
          "content": "The DSVM works closely with many Azure services and is able to read and process data that is already stored on Azure in Azure SQL Data Warehouse, Azure Data Lake, Azure Storage, or DocumentDB.",
          "pos": [
            325,
            517
          ]
        },
        {
          "content": "It can also leverage analytics tools like Azure Machine Learning and Azure Data Factory.",
          "pos": [
            518,
            606
          ]
        }
      ]
    },
    {
      "pos": [
        1216,
        1402
      ],
      "content": "In this article we walk you through how to use your DSVM to perform various data science tasks and interact with other Azure services. Here is some  of the things you can do on the DSVM:",
      "nodes": [
        {
          "content": "In this article we walk you through how to use your DSVM to perform various data science tasks and interact with other Azure services.",
          "pos": [
            0,
            134
          ]
        },
        {
          "content": "Here is some  of the things you can do on the DSVM:",
          "pos": [
            135,
            186
          ]
        }
      ]
    },
    {
      "pos": [
        1407,
        1491
      ],
      "content": "Explore data and develop models locally on the DSVM using Microsoft R Server, Python"
    },
    {
      "pos": [
        1495,
        1671
      ],
      "content": "Use a Jupyter notebook to experiment with your data on a browser using Python 2, Python 3, Microsoft R an enterprise ready version of R designed for scalability and performance"
    },
    {
      "pos": [
        1675,
        1831
      ],
      "content": "Operationalize models built using R and Python on Azure Machine Learning so client applications can access your models using a simple web services interface"
    },
    {
      "pos": [
        1835,
        1900
      ],
      "content": "Administer your Azure resources using  Azure Portal or Powershell"
    },
    {
      "pos": [
        1905,
        2059
      ],
      "content": "Extend your storage space and share large scale datasets / code across your whole team by creating an Azure File Storage as a mountable drive on your DSVM"
    },
    {
      "pos": [
        2064,
        2186
      ],
      "content": "Share code with your team using Github and access your repository using the pre-installed Git clients - Git Bash, Git GUI."
    },
    {
      "pos": [
        2190,
        2361
      ],
      "content": "Access various Azure data and analytics services like Azure blob storage, Azure Data Lake, Azure HDInsight (Hadoop), Azure DocumentDB, Azure SQL Data Warehouse &amp; databases"
    },
    {
      "pos": [
        2365,
        2474
      ],
      "content": "Build reports and dashboard using the Power BI Desktop pre-installed on the DSVM and deploy them on the cloud"
    },
    {
      "pos": [
        2478,
        2532
      ],
      "content": "Dynamically scale your DSVM to meet your project needs"
    },
    {
      "pos": [
        2538,
        2586
      ],
      "content": "Install additional tools on your virtual machine"
    },
    {
      "pos": [
        2588,
        2945
      ],
      "content": "<bpt id=\"p1\">**</bpt>Prerequisites<ept id=\"p1\">**</ept>\nYou will need an Azure subscription. You can sign-up for a free trial <bpt id=\"p2\">[</bpt>here<ept id=\"p2\">](https://azure.microsoft.com/free/)</ept>\nInstructions for provisioning a Data Science Virtual Machine on the Azure Portal are available at <bpt id=\"p3\">[</bpt>Creating a virtual machine<ept id=\"p3\">](https://ms.portal.azure.com/#create/microsoft-ads.standard-data-science-vmstandard-data-science-vm)</ept>.",
      "nodes": [
        {
          "content": "<bpt id=\"p1\">**</bpt>Prerequisites<ept id=\"p1\">**</ept>\nYou will need an Azure subscription.",
          "pos": [
            0,
            92
          ]
        },
        {
          "content": "You can sign-up for a free trial <bpt id=\"p2\">[</bpt>here<ept id=\"p2\">](https://azure.microsoft.com/free/)</ept>\nInstructions for provisioning a Data Science Virtual Machine on the Azure Portal are available at <bpt id=\"p3\">[</bpt>Creating a virtual machine<ept id=\"p3\">](https://ms.portal.azure.com/#create/microsoft-ads.standard-data-science-vmstandard-data-science-vm)</ept>.",
          "pos": [
            93,
            471
          ]
        }
      ]
    },
    {
      "pos": [
        2950,
        3019
      ],
      "content": "1. Explore data and develop models using Microsoft R Server or Python"
    },
    {
      "pos": [
        3021,
        3105
      ],
      "content": "You can use languages like R and Python to do your data analytics right on the DSVM."
    },
    {
      "pos": [
        3108,
        3475
      ],
      "content": "For R, you can use an IDE called \"Revolution R Enterprise 8.0\" that can be found on the start menu or the desktop. Microsoft has provided additional libraries on top of the Open source/CRAN-R to enable scalable analytics and the ability to analyze data larger than the memory size allowed by doing parallel chunked analysis. Here is a screen shot of  using the R IDE.",
      "nodes": [
        {
          "content": "For R, you can use an IDE called \"Revolution R Enterprise 8.0\" that can be found on the start menu or the desktop.",
          "pos": [
            0,
            114
          ]
        },
        {
          "content": "Microsoft has provided additional libraries on top of the Open source/CRAN-R to enable scalable analytics and the ability to analyze data larger than the memory size allowed by doing parallel chunked analysis.",
          "pos": [
            115,
            324
          ]
        },
        {
          "content": "Here is a screen shot of  using the R IDE.",
          "pos": [
            325,
            367
          ]
        }
      ]
    },
    {
      "pos": [
        3478,
        3554
      ],
      "content": "<ph id=\"ph2\">![</ph>R IDE<ph id=\"ph3\">](./media/machine-learning-data-science-vm-do-ten-things/RevoIDE.png)</ph>"
    },
    {
      "pos": [
        3556,
        3889
      ],
      "content": "For Python, you can use an IDE like Visual Studio Community Edition which has the Python Tools for Visual Studio (PTVS) extension pre-installed. By default, only a basic Python 2.7 (without any analytics library like SciKit, Pandas) is configured on PTVS. In order to enable Anaconda Python 2.7 and 3.5, you need to do the following:",
      "nodes": [
        {
          "content": "For Python, you can use an IDE like Visual Studio Community Edition which has the Python Tools for Visual Studio (PTVS) extension pre-installed.",
          "pos": [
            0,
            144
          ]
        },
        {
          "content": "By default, only a basic Python 2.7 (without any analytics library like SciKit, Pandas) is configured on PTVS.",
          "pos": [
            145,
            255
          ]
        },
        {
          "content": "In order to enable Anaconda Python 2.7 and 3.5, you need to do the following:",
          "pos": [
            256,
            333
          ]
        }
      ]
    },
    {
      "pos": [
        3893,
        4071
      ],
      "content": "Create custom environments for each version by navigating to Tools -&gt; Python Tools -&gt; Python Environments and then clicking \"+ Custom\" in the Visual Studio 2015 Community Edition"
    },
    {
      "pos": [
        4074,
        4217
      ],
      "content": "Give a description and set the environment prefix paths as c:\\anaconda for Anaconda Python 2.7 OR c:\\anaconda\\envs\\py35 for Anaconda Python 3.5"
    },
    {
      "pos": [
        4221,
        4286
      ],
      "content": "Click <bpt id=\"p4\">**</bpt>Auto Detect<ept id=\"p4\">**</ept><ph id=\"ph4\"/> and then <bpt id=\"p5\">**</bpt>Apply<ept id=\"p5\">**</ept><ph id=\"ph5\"/> to save the environment."
    },
    {
      "pos": [
        4289,
        4359
      ],
      "content": "Here is what the custom environment setup looks like in Visual Studio."
    },
    {
      "pos": [
        4361,
        4444
      ],
      "content": "<ph id=\"ph6\">![</ph>PTVS Setup<ph id=\"ph7\">](./media/machine-learning-data-science-vm-do-ten-things/PTVSSetup.png)</ph>"
    },
    {
      "pos": [
        4446,
        4700
      ],
      "content": "See <bpt id=\"p6\">[</bpt>PTVS documentation<ept id=\"p6\">](https://github.com/Microsoft/PTVS/wiki/Selecting-and-Installing-Python-Interpreters#hey-i-already-have-an-interpreter-on-my-machine-but-ptvs-doesnt-seem-to-know-about-it)</ept><ph id=\"ph8\"/> for more details on how to create the Python Environments."
    },
    {
      "pos": [
        4705,
        4760
      ],
      "content": "Now you are set up to open a project and begin working!"
    },
    {
      "pos": [
        4765,
        4840
      ],
      "content": "2. Using a Jupyter Notebook to explore and model your data with Python or R"
    },
    {
      "pos": [
        4842,
        5507
      ],
      "content": "The Jupyter Notebook is a powerful environment that provides a browser-based \"IDE\" for data exploration and modeling. \n<ph id=\"ph9\"/>You can use Python 2, Python 3 or R (both Open Source and Microsoft R Server). \n<ph id=\"ph10\"/>To launch the Jupyter notebook click on the start menu icon / desktop icon titled <bpt id=\"p7\">**</bpt>Jupyter Notebook<ept id=\"p7\">**</ept>. On the DSVM you can also browse to \"https://localhost:9999/\" to access the Jupiter notebook. If it prompts you for a password, please use instructions found on the \n<bpt id=\"p8\">[</bpt>DSVM documentation page<ept id=\"p8\">](machine-learning-data-science-provision-vm.md/#how-to-create-a-strong-password-on-the-jupyter-notebook-server)</ept><ph id=\"ph11\"/> \nto create a strong password to access the Jupyter notebook.",
      "nodes": [
        {
          "content": "The Jupyter Notebook is a powerful environment that provides a browser-based \"IDE\" for data exploration and modeling.",
          "pos": [
            0,
            117
          ]
        },
        {
          "content": "<ph id=\"ph9\"/>You can use Python 2, Python 3 or R (both Open Source and Microsoft R Server).",
          "pos": [
            119,
            211
          ]
        },
        {
          "content": "<ph id=\"ph10\"/>To launch the Jupyter notebook click on the start menu icon / desktop icon titled <bpt id=\"p7\">**</bpt>Jupyter Notebook<ept id=\"p7\">**</ept>.",
          "pos": [
            213,
            369
          ]
        },
        {
          "content": "On the DSVM you can also browse to \"https://localhost:9999/\" to access the Jupiter notebook.",
          "pos": [
            370,
            462
          ]
        },
        {
          "content": "If it prompts you for a password, please use instructions found on the \n<bpt id=\"p8\">[</bpt>DSVM documentation page<ept id=\"p8\">](machine-learning-data-science-provision-vm.md/#how-to-create-a-strong-password-on-the-jupyter-notebook-server)</ept><ph id=\"ph11\"/> \nto create a strong password to access the Jupyter notebook.",
          "pos": [
            463,
            785
          ]
        }
      ]
    },
    {
      "pos": [
        5510,
        5526
      ],
      "content": "TBD: SCREEN SHOT"
    },
    {
      "pos": [
        5528,
        5829
      ],
      "content": "Once you are on the notebook, you will see a directory that contains a few example notebooks that are pre-packaged into the DSVM. You can click on the notebook and see the code. You can execute each cell by pressing <bpt id=\"p9\">**</bpt>SHIFT-ENTER<ept id=\"p9\">**</ept>. \n<ph id=\"ph12\"/>You can run the entire notebook by clicking on <bpt id=\"p10\">**</bpt>Cell<ept id=\"p10\">**</ept><ph id=\"ph13\"/> -&gt; <bpt id=\"p11\">**</bpt>Run<ept id=\"p11\">**</ept>.",
      "nodes": [
        {
          "content": "Once you are on the notebook, you will see a directory that contains a few example notebooks that are pre-packaged into the DSVM.",
          "pos": [
            0,
            129
          ]
        },
        {
          "content": "You can click on the notebook and see the code.",
          "pos": [
            130,
            177
          ]
        },
        {
          "content": "You can execute each cell by pressing <bpt id=\"p9\">**</bpt>SHIFT-ENTER<ept id=\"p9\">**</ept>.",
          "pos": [
            178,
            270
          ]
        },
        {
          "content": "<ph id=\"ph12\"/>You can run the entire notebook by clicking on <bpt id=\"p10\">**</bpt>Cell<ept id=\"p10\">**</ept><ph id=\"ph13\"/> -&gt; <bpt id=\"p11\">**</bpt>Run<ept id=\"p11\">**</ept>.",
          "pos": [
            272,
            452
          ]
        }
      ]
    },
    {
      "pos": [
        5831,
        6303
      ],
      "content": "You can create a new notebook by clicking on the Jupyter Icon (left top corner) and then clicking <bpt id=\"p12\">**</bpt>New<ept id=\"p12\">**</ept><ph id=\"ph14\"/> button on the right and then choosing the notebook language (also known as kernels). Currently we support Python 2.7, Python 3.5 and R. The R kernel supports programming in both Open source R as well as the enterprise scalable Microsoft R Server. Once you are in the notebook you can explore your data, build the model, test the model using your choice of libraries.",
      "nodes": [
        {
          "content": "You can create a new notebook by clicking on the Jupyter Icon (left top corner) and then clicking <bpt id=\"p12\">**</bpt>New<ept id=\"p12\">**</ept><ph id=\"ph14\"/> button on the right and then choosing the notebook language (also known as kernels).",
          "pos": [
            0,
            245
          ]
        },
        {
          "content": "Currently we support Python 2.7, Python 3.5 and R. The R kernel supports programming in both Open source R as well as the enterprise scalable Microsoft R Server.",
          "pos": [
            246,
            407
          ]
        },
        {
          "content": "Once you are in the notebook you can explore your data, build the model, test the model using your choice of libraries.",
          "pos": [
            408,
            527
          ]
        }
      ]
    },
    {
      "pos": [
        6310,
        6392
      ],
      "content": "3. Build models using R and Python and Operationalize it on Azure Machine Learning"
    },
    {
      "pos": [
        6394,
        7114
      ],
      "content": "Once you have built and validated your model the next step is usually to deploy it into production. \n<ph id=\"ph15\"/>This allows your client applications to invoke the model predictions on a real time or a batch basis. Azure Machine Learning provides a mechanism to operationalize the model built in either R or Python. \n<ph id=\"ph16\"/>When you operationalize your model in Azure Machine Learning, a web service is exposed that allows clients to make REST calls that pass in input parameters and receive predictions from the model as outputs. \n<ph id=\"ph17\"/>If you have not yet signed up for AzureML, you can obtain a free 8-hour guest access or a free workspace by visiting the <bpt id=\"p13\">[</bpt>AzureML Studio<ept id=\"p13\">](https://studio.azureml.net/)</ept><ph id=\"ph18\"/> home page and clicking on \"Get Started\".",
      "nodes": [
        {
          "content": "Once you have built and validated your model the next step is usually to deploy it into production.",
          "pos": [
            0,
            99
          ]
        },
        {
          "content": "<ph id=\"ph15\"/>This allows your client applications to invoke the model predictions on a real time or a batch basis.",
          "pos": [
            101,
            217
          ]
        },
        {
          "content": "Azure Machine Learning provides a mechanism to operationalize the model built in either R or Python.",
          "pos": [
            218,
            318
          ]
        },
        {
          "content": "<ph id=\"ph16\"/>When you operationalize your model in Azure Machine Learning, a web service is exposed that allows clients to make REST calls that pass in input parameters and receive predictions from the model as outputs.",
          "pos": [
            320,
            541
          ]
        },
        {
          "content": "<ph id=\"ph17\"/>If you have not yet signed up for AzureML, you can obtain a free 8-hour guest access or a free workspace by visiting the <bpt id=\"p13\">[</bpt>AzureML Studio<ept id=\"p13\">](https://studio.azureml.net/)</ept><ph id=\"ph18\"/> home page and clicking on \"Get Started\".",
          "pos": [
            543,
            820
          ]
        }
      ]
    },
    {
      "pos": [
        7122,
        7174
      ],
      "content": "Build and Operationalizing models built using Python"
    },
    {
      "pos": [
        7178,
        7338
      ],
      "content": "Upload the notebook entitled \"IrisClassifierPyMLWebService\" to your Jupyter. Here is a simple model built in Python using SciKit-learn as found in the notebook.",
      "nodes": [
        {
          "content": "Upload the notebook entitled \"IrisClassifierPyMLWebService\" to your Jupyter.",
          "pos": [
            0,
            76
          ]
        },
        {
          "content": "Here is a simple model built in Python using SciKit-learn as found in the notebook.",
          "pos": [
            77,
            160
          ]
        }
      ]
    },
    {
      "pos": [
        7547,
        7655
      ],
      "content": "The method used to deploy your python models to AzureML is to wrap the prediction of a model into a function"
    },
    {
      "pos": [
        7657,
        7809
      ],
      "content": "and decorate it with attributes provided by the AzureML library denoting your AzureML workspace ID, API Key, the input parameters and return parameters."
    },
    {
      "pos": [
        8153,
        8319
      ],
      "content": "A client can now make calls to the web service. There are convenience wrappers that construct the REST API requests. Here is a sample code to consume the web service.",
      "nodes": [
        {
          "content": "A client can now make calls to the web service.",
          "pos": [
            0,
            47
          ]
        },
        {
          "content": "There are convenience wrappers that construct the REST API requests.",
          "pos": [
            48,
            116
          ]
        },
        {
          "content": "Here is a sample code to consume the web service.",
          "pos": [
            117,
            166
          ]
        }
      ]
    },
    {
      "pos": [
        8645,
        8711
      ],
      "content": "NOTE: The AzureML library is support on Python 2.7 only currently."
    },
    {
      "pos": [
        8718,
        8751
      ],
      "content": "Build and Operationalize R models"
    },
    {
      "pos": [
        8753,
        9244
      ],
      "content": "You can deploy R models built on the Data Science Virtual Machine or elsewhere onto Azure ML in a manner that is similar to how it is done for Python. First create a settings.json file as below to provide your workspace ID and auth token.\nYou then write a wrapper for the model's predict function. Then you call the <ph id=\"ph19\">```publishWebService```</ph><ph id=\"ph20\"/> call in the AzureML library passing in the function wrapper.  \n<ph id=\"ph21\"/>Here is a code snippet that can be used to publish a model as a web service in Azure ML.",
      "nodes": [
        {
          "content": "You can deploy R models built on the Data Science Virtual Machine or elsewhere onto Azure ML in a manner that is similar to how it is done for Python.",
          "pos": [
            0,
            150
          ]
        },
        {
          "content": "First create a settings.json file as below to provide your workspace ID and auth token.",
          "pos": [
            151,
            238
          ]
        },
        {
          "content": "You then write a wrapper for the model's predict function.",
          "pos": [
            239,
            297
          ]
        },
        {
          "content": "Then you call the <ph id=\"ph19\">```publishWebService```</ph><ph id=\"ph20\"/> call in the AzureML library passing in the function wrapper.",
          "pos": [
            298,
            434
          ]
        },
        {
          "content": "<ph id=\"ph21\"/>Here is a code snippet that can be used to publish a model as a web service in Azure ML.",
          "pos": [
            437,
            540
          ]
        }
      ]
    },
    {
      "pos": [
        9251,
        9270
      ],
      "content": "Settings.json File:"
    },
    {
      "pos": [
        9437,
        9485
      ],
      "content": "Build a model in R and publishing it in Azure ML"
    },
    {
      "pos": [
        10035,
        10073
      ],
      "content": "Consume the model deployed in Azure ML"
    },
    {
      "pos": [
        10075,
        10422
      ],
      "content": "To consume the model from a client application, we use the AzureML library to lookup the published web service by name using the <ph id=\"ph22\">`services`</ph><ph id=\"ph23\"/> API call to determine the endpoint. Then you just call the <ph id=\"ph24\">`consume`</ph><ph id=\"ph25\"/> function and pass in the data frame to be predicted. \n<ph id=\"ph26\"/>The following code is used to consume the model published as an AzureML web service.",
      "nodes": [
        {
          "content": "To consume the model from a client application, we use the AzureML library to lookup the published web service by name using the <ph id=\"ph22\">`services`</ph><ph id=\"ph23\"/> API call to determine the endpoint.",
          "pos": [
            0,
            209
          ]
        },
        {
          "content": "Then you just call the <ph id=\"ph24\">`consume`</ph><ph id=\"ph25\"/> function and pass in the data frame to be predicted.",
          "pos": [
            210,
            329
          ]
        },
        {
          "content": "<ph id=\"ph26\"/>The following code is used to consume the model published as an AzureML web service.",
          "pos": [
            331,
            430
          ]
        }
      ]
    },
    {
      "pos": [
        10783,
        10850
      ],
      "content": "4. Administer your Azure resources using Azure Portal or Powershell"
    },
    {
      "pos": [
        10852,
        11147
      ],
      "content": "The DSVM not only allows you to build your analytics solution locally on the virtual machine, but also allows you to access services on Microsoft's Azure cloud. Azure provides several compute, storage, data analytics services and other services that you can administer and access from your DSVM.",
      "nodes": [
        {
          "content": "The DSVM not only allows you to build your analytics solution locally on the virtual machine, but also allows you to access services on Microsoft's Azure cloud.",
          "pos": [
            0,
            160
          ]
        },
        {
          "content": "Azure provides several compute, storage, data analytics services and other services that you can administer and access from your DSVM.",
          "pos": [
            161,
            295
          ]
        }
      ]
    },
    {
      "pos": [
        11149,
        11716
      ],
      "content": "To administer your Azure subscription and cloud resources you can use your browser and point to the \n<bpt id=\"p14\">[</bpt>Azure Portal<ept id=\"p14\">](portal.azure.com)</ept>. You can also use Azure Powershell to adminster your Azure subscription and resources via a script. \n<ph id=\"ph27\"/>You can run Azure Powershell from a shortcut on the desktop or from the start menu titled \"Microsoft Azure Powershell\". Refer to \n<bpt id=\"p15\">[</bpt>Microsoft Azure Powershell documentation<ept id=\"p15\">](../powershell-azure-resource-manager.md)</ept><ph id=\"ph28\"/> for more information on how you can administer your Azure subscription and resources using Windows Powershell scripts.",
      "nodes": [
        {
          "content": "To administer your Azure subscription and cloud resources you can use your browser and point to the \n<bpt id=\"p14\">[</bpt>Azure Portal<ept id=\"p14\">](portal.azure.com)</ept>.",
          "pos": [
            0,
            174
          ]
        },
        {
          "content": "You can also use Azure Powershell to adminster your Azure subscription and resources via a script.",
          "pos": [
            175,
            273
          ]
        },
        {
          "content": "<ph id=\"ph27\"/>You can run Azure Powershell from a shortcut on the desktop or from the start menu titled \"Microsoft Azure Powershell\".",
          "pos": [
            275,
            409
          ]
        },
        {
          "content": "Refer to \n<bpt id=\"p15\">[</bpt>Microsoft Azure Powershell documentation<ept id=\"p15\">](../powershell-azure-resource-manager.md)</ept><ph id=\"ph28\"/> for more information on how you can administer your Azure subscription and resources using Windows Powershell scripts.",
          "pos": [
            410,
            677
          ]
        }
      ]
    },
    {
      "pos": [
        11722,
        11776
      ],
      "content": "5. Extend your storage space with a shared file system"
    },
    {
      "pos": [
        11778,
        12005
      ],
      "content": "Data scientists can share large datasets, code etc within the team. The DSVM itself has about 70GB of space available. To extend your storage, you can use the Azure File Service and mount it on the DSVM or access via REST API. ",
      "nodes": [
        {
          "content": "Data scientists can share large datasets, code etc within the team.",
          "pos": [
            0,
            67
          ]
        },
        {
          "content": "The DSVM itself has about 70GB of space available.",
          "pos": [
            68,
            118
          ]
        },
        {
          "content": "To extend your storage, you can use the Azure File Service and mount it on the DSVM or access via REST API.",
          "pos": [
            119,
            226
          ]
        },
        {
          "content": " ",
          "pos": [
            226,
            227
          ]
        }
      ]
    },
    {
      "pos": [
        12006,
        12101
      ],
      "content": "The maximum space of the Azure File Service share is 5TB and individual file size limit is 1TB."
    },
    {
      "pos": [
        12104,
        12258
      ],
      "content": "You can use Azure Powershell to create a  Azure File Service share. Here is the script to run under Azure PowerShell to create a Azure File service share.",
      "nodes": [
        {
          "content": "You can use Azure Powershell to create a  Azure File Service share.",
          "pos": [
            0,
            67
          ]
        },
        {
          "content": "Here is the script to run under Azure PowerShell to create a Azure File service share.",
          "pos": [
            68,
            154
          ]
        }
      ]
    },
    {
      "pos": [
        13258,
        13577
      ],
      "content": "Now that you have creates an Azure file share, you can mount it in any virtual machine in Azure. It is highly recommended that the VM is in same Azure data center as the storage account to avoid latency and data transfer charges. Here is the commands to mount the drive on the DSVM that you can run on Azure Powershell.",
      "nodes": [
        {
          "content": "Now that you have creates an Azure file share, you can mount it in any virtual machine in Azure.",
          "pos": [
            0,
            96
          ]
        },
        {
          "content": "It is highly recommended that the VM is in same Azure data center as the storage account to avoid latency and data transfer charges.",
          "pos": [
            97,
            229
          ]
        },
        {
          "content": "Here is the commands to mount the drive on the DSVM that you can run on Azure Powershell.",
          "pos": [
            230,
            319
          ]
        }
      ]
    },
    {
      "pos": [
        14013,
        14073
      ],
      "content": "Now you can access this drive as any normal drive on the VM."
    },
    {
      "pos": [
        14079,
        14120
      ],
      "content": "6. Share code with your team using Github"
    },
    {
      "pos": [
        14123,
        14814
      ],
      "content": "Github is a code repository where you can find a lot of sample code and sources for different tools using various technologies shared by the developer community. It uses Git as the technology to track and store versions of the code files. Github is also a platform where you can create your own repository to store your team's shared code and documentation, implement version control and also control who have access to view and contribute code. Please visit the <bpt id=\"p16\">[</bpt>Github help pages<ept id=\"p16\">](https://help.github.com/)</ept><ph id=\"ph29\"/> for more information on using Git. You can use Github as one of the ways to collaborate with your team, use code developed by the community and contribute code back to the community.",
      "nodes": [
        {
          "content": "Github is a code repository where you can find a lot of sample code and sources for different tools using various technologies shared by the developer community.",
          "pos": [
            0,
            161
          ]
        },
        {
          "content": "It uses Git as the technology to track and store versions of the code files.",
          "pos": [
            162,
            238
          ]
        },
        {
          "content": "Github is also a platform where you can create your own repository to store your team's shared code and documentation, implement version control and also control who have access to view and contribute code.",
          "pos": [
            239,
            445
          ]
        },
        {
          "content": "Please visit the <bpt id=\"p16\">[</bpt>Github help pages<ept id=\"p16\">](https://help.github.com/)</ept><ph id=\"ph29\"/> for more information on using Git.",
          "pos": [
            446,
            598
          ]
        },
        {
          "content": "You can use Github as one of the ways to collaborate with your team, use code developed by the community and contribute code back to the community.",
          "pos": [
            599,
            746
          ]
        }
      ]
    },
    {
      "pos": [
        14817,
        15141
      ],
      "content": "The DSVM already comes loaded with client tools on both command line as well GUI to access Github repository. The command line tool to work with Git and Github is called <ph id=\"ph30\">```git-bash```</ph>. Visual Studio installed on the DSVM has the Git extensions. You can find start-up icons for these tools on the start menu and the desktop.",
      "nodes": [
        {
          "content": "The DSVM already comes loaded with client tools on both command line as well GUI to access Github repository.",
          "pos": [
            0,
            109
          ]
        },
        {
          "content": "The command line tool to work with Git and Github is called <ph id=\"ph30\">```git-bash```</ph>.",
          "pos": [
            110,
            204
          ]
        },
        {
          "content": "Visual Studio installed on the DSVM has the Git extensions.",
          "pos": [
            205,
            264
          ]
        },
        {
          "content": "You can find start-up icons for these tools on the start menu and the desktop.",
          "pos": [
            265,
            343
          ]
        }
      ]
    },
    {
      "pos": [
        15144,
        15391
      ],
      "content": "To download code from a Github repository you will use the <ph id=\"ph31\">```git clone```</ph><ph id=\"ph32\"/> command. For example to download data science repository published by Microsoft into the current directory you can run the following command once you are in <ph id=\"ph33\">```git-bash```</ph>.",
      "nodes": [
        {
          "content": "To download code from a Github repository you will use the <ph id=\"ph31\">```git clone```</ph><ph id=\"ph32\"/> command.",
          "pos": [
            0,
            117
          ]
        },
        {
          "content": "For example to download data science repository published by Microsoft into the current directory you can run the following command once you are in <ph id=\"ph33\">```git-bash```</ph>.",
          "pos": [
            118,
            300
          ]
        }
      ]
    },
    {
      "pos": [
        15472,
        15605
      ],
      "content": "In Visual Studio, you can do the same clone operation. See the screen-shot below for accessing Git and Github tools in Visual Studio.",
      "nodes": [
        {
          "content": "In Visual Studio, you can do the same clone operation.",
          "pos": [
            0,
            54
          ]
        },
        {
          "content": "See the screen-shot below for accessing Git and Github tools in Visual Studio.",
          "pos": [
            55,
            133
          ]
        }
      ]
    },
    {
      "pos": [
        15608,
        15697
      ],
      "content": "<ph id=\"ph34\">![</ph>Git in Visual Studio<ph id=\"ph35\">](./media/machine-learning-data-science-vm-do-ten-things/VSGit.PNG)</ph>"
    },
    {
      "pos": [
        15700,
        15954
      ],
      "content": "You can find more information on using Git to work with your Github repository by using several resources available on github.com. You may find the <bpt id=\"p17\">[</bpt>cheat sheet<ept id=\"p17\">](https://training.github.com/kit/downloads/github-git-cheat-sheet.pdf)</ept><ph id=\"ph36\"/> as a useful reference.",
      "nodes": [
        {
          "content": "You can find more information on using Git to work with your Github repository by using several resources available on github.com.",
          "pos": [
            0,
            130
          ]
        },
        {
          "content": "You may find the <bpt id=\"p17\">[</bpt>cheat sheet<ept id=\"p17\">](https://training.github.com/kit/downloads/github-git-cheat-sheet.pdf)</ept><ph id=\"ph36\"/> as a useful reference.",
          "pos": [
            131,
            309
          ]
        }
      ]
    },
    {
      "pos": [
        15961,
        16012
      ],
      "content": "7. Access various Azure data and analytics services"
    },
    {
      "pos": [
        16018,
        16028
      ],
      "content": "Azure Blob"
    },
    {
      "pos": [
        16030,
        16046
      ],
      "content": "<bpt id=\"p18\">**</bpt>Prerequisite<ept id=\"p18\">**</ept>"
    },
    {
      "pos": [
        16050,
        16138
      ],
      "content": "<bpt id=\"p19\">**</bpt>Create your Azure Blob storage account from <bpt id=\"p20\">[</bpt>Azure Portal<ept id=\"p20\">](http://portal.azure.com)</ept>.<ept id=\"p19\">**</ept>"
    },
    {
      "pos": [
        16140,
        16238
      ],
      "content": "<ph id=\"ph37\">![</ph>Create_Azure_Blob<ph id=\"ph38\">](./media/machine-learning-data-science-vm-do-ten-things/Create_Azure_Blob.PNG)</ph>"
    },
    {
      "pos": [
        16243,
        16497
      ],
      "content": "**Make sure the pre-installed AzCopy tool found at <ph id=\"ph39\">```C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\AzCopy\\azcopy.exe```</ph><ph id=\"ph40\"/> is added to your environment variable. For more info on AzCopy please refer to <bpt id=\"p21\">[</bpt>AzCopy documentation<ept id=\"p21\">](../storage/storage-use-azcopy.md)</ept>",
      "nodes": [
        {
          "content": "**Make sure the pre-installed AzCopy tool found at <ph id=\"ph39\">```C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\AzCopy\\azcopy.exe```</ph><ph id=\"ph40\"/> is added to your environment variable.",
          "pos": [
            0,
            191
          ]
        },
        {
          "content": "For more info on AzCopy please refer to <bpt id=\"p21\">[</bpt>AzCopy documentation<ept id=\"p21\">](../storage/storage-use-azcopy.md)</ept>",
          "pos": [
            192,
            328
          ]
        }
      ]
    },
    {
      "pos": [
        16502,
        16595
      ],
      "content": "<bpt id=\"p22\">**</bpt>Start the Azure Storage Explorer from <bpt id=\"p23\">[</bpt>here<ept id=\"p23\">](https://azurestorageexplorer.codeplex.com/)</ept>.<ept id=\"p22\">**</ept>"
    },
    {
      "pos": [
        16597,
        16707
      ],
      "content": "<ph id=\"ph41\">![</ph>AzureStorageExplorer_v4<ph id=\"ph42\">](./media/machine-learning-data-science-vm-do-ten-things/AzureStorageExplorer_v4.png)</ph>"
    },
    {
      "pos": [
        16709,
        16752
      ],
      "content": "<bpt id=\"p24\">**</bpt>Move data from VM to Azure Blob: AzCopy<ept id=\"p24\">**</ept>"
    },
    {
      "pos": [
        16754,
        17005
      ],
      "content": "To move data between your local files and blob storage, you can use AzCopy in command line or PowerShell:\n<ph id=\"ph43\">`AzCopy /Source:C:\\myfolder /Dest:https://&lt;mystorageaccount&gt;.blob.core.windows.net/&lt;mycontainer&gt; /DestKey:&lt;storage account key&gt; /Pattern:abc.txt`</ph>"
    },
    {
      "pos": [
        17008,
        17312
      ],
      "content": "Replace <bpt id=\"p25\">**</bpt>C:\\myfolder<ept id=\"p25\">**</ept><ph id=\"ph44\"/> to the path where your file is stored, <bpt id=\"p26\">**</bpt>mystorageaccount<ept id=\"p26\">**</ept><ph id=\"ph45\"/> to your blob storage account name, <bpt id=\"p27\">**</bpt>mycontainer<ept id=\"p27\">**</ept><ph id=\"ph46\"/> to the container name, <bpt id=\"p28\">**</bpt>storage account key<ept id=\"p28\">**</ept><ph id=\"ph47\"/> to your blob storage access key. You can find your storage account credentials in <bpt id=\"p29\">[</bpt>Azure Portal<ept id=\"p29\">](http://portal.azure.com)</ept>.",
      "nodes": [
        {
          "content": "Replace <bpt id=\"p25\">**</bpt>C:\\myfolder<ept id=\"p25\">**</ept><ph id=\"ph44\"/> to the path where your file is stored, <bpt id=\"p26\">**</bpt>mystorageaccount<ept id=\"p26\">**</ept><ph id=\"ph45\"/> to your blob storage account name, <bpt id=\"p27\">**</bpt>mycontainer<ept id=\"p27\">**</ept><ph id=\"ph46\"/> to the container name, <bpt id=\"p28\">**</bpt>storage account key<ept id=\"p28\">**</ept><ph id=\"ph47\"/> to your blob storage access key.",
          "pos": [
            0,
            434
          ]
        },
        {
          "content": "You can find your storage account credentials in <bpt id=\"p29\">[</bpt>Azure Portal<ept id=\"p29\">](http://portal.azure.com)</ept>.",
          "pos": [
            435,
            564
          ]
        }
      ]
    },
    {
      "pos": [
        17314,
        17432
      ],
      "content": "<ph id=\"ph48\">![</ph>StorageAccountCredential_v2<ph id=\"ph49\">](./media/machine-learning-data-science-vm-do-ten-things/StorageAccountCredential_v2.png)</ph>"
    },
    {
      "pos": [
        17434,
        17534
      ],
      "content": "Run AzCopy command in PowerShell or in command prompt. Here is some example usage of AzCopy command:",
      "nodes": [
        {
          "content": "Run AzCopy command in PowerShell or in command prompt.",
          "pos": [
            0,
            54
          ]
        },
        {
          "content": "Here is some example usage of AzCopy command:",
          "pos": [
            55,
            100
          ]
        }
      ]
    },
    {
      "pos": [
        18123,
        18242
      ],
      "content": "Once you run your AzCopy command to copy to an Azure blob you see your file shows up in Azure Storage Explorer shortly."
    },
    {
      "pos": [
        18245,
        18385
      ],
      "content": "<ph id=\"ph50\">![</ph>AzCopy_run_finshed_Storage_Explorer_v3<ph id=\"ph51\">](./media/machine-learning-data-science-vm-do-ten-things/AzCopy_run_finshed_Storage_Explorer_v3.png)</ph>"
    },
    {
      "pos": [
        18388,
        18447
      ],
      "content": "<bpt id=\"p30\">**</bpt>Move data from VM to Azure Blob: Azure Storage Explorer<ept id=\"p30\">**</ept>"
    },
    {
      "pos": [
        18449,
        18534
      ],
      "content": "You can also upload data from the local file in your VM using Azure Storage Explorer:"
    },
    {
      "pos": [
        18634,
        18682
      ],
      "content": "<bpt id=\"p31\">**</bpt>Read data from Azure Blob: AML reader module<ept id=\"p31\">**</ept>"
    },
    {
      "pos": [
        18684,
        18777
      ],
      "content": "In Azure Machine Learning Studio you can use a <bpt id=\"p32\">**</bpt>Reader module<ept id=\"p32\">**</ept><ph id=\"ph53\"/> to read data from your blob."
    },
    {
      "pos": [
        18780,
        18892
      ],
      "content": "<ph id=\"ph54\">![</ph>AML_ReaderBlob_Module_v3<ph id=\"ph55\">](./media/machine-learning-data-science-vm-do-ten-things/AML_ReaderBlob_Module_v3.png)</ph>"
    },
    {
      "pos": [
        18895,
        18937
      ],
      "content": "<bpt id=\"p33\">**</bpt>Read data from Azure Blob: Python ODBC<ept id=\"p33\">**</ept>"
    },
    {
      "pos": [
        18939,
        19096
      ],
      "content": "In the IPython Notebook <bpt id=\"p34\">**</bpt>NYC Data wrangling using IPython Notebook and Azure Blob Storage<ept id=\"p34\">**</ept>, you can use <bpt id=\"p35\">**</bpt>pyodbc<ept id=\"p35\">**</ept><ph id=\"ph56\"/> package to read data directly from blob."
    },
    {
      "pos": [
        19099,
        19131
      ],
      "content": "First, import required packages:"
    },
    {
      "pos": [
        19429,
        19502
      ],
      "content": "Then plug in your Azure Blob account credentials and read data from Blob:"
    },
    {
      "pos": [
        19798,
        19846
      ],
      "content": "You can see the data is read in as a data frame:"
    },
    {
      "pos": [
        19848,
        19944
      ],
      "content": "<ph id=\"ph57\">![</ph>IPNB_data_readin<ph id=\"ph58\">](./media/machine-learning-data-science-vm-do-ten-things/IPNB_data_readin.PNG)</ph>"
    },
    {
      "pos": [
        19951,
        19966
      ],
      "content": "Azure Data Lake"
    },
    {
      "pos": [
        19968,
        19984
      ],
      "content": "<bpt id=\"p36\">**</bpt>Prerequisite<ept id=\"p36\">**</ept>"
    },
    {
      "pos": [
        19988,
        20069
      ],
      "content": "Create your Azure Data Lake Analytics in <bpt id=\"p37\">[</bpt>Azure Portal<ept id=\"p37\">](http://portal.azure.com)</ept>."
    },
    {
      "pos": [
        20071,
        20185
      ],
      "content": "<ph id=\"ph59\">![</ph>Azure_Data_Lake_Create_v2<ph id=\"ph60\">](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Lake_Create_v2.png)</ph>"
    },
    {
      "pos": [
        20190,
        20579
      ],
      "content": "The  <bpt id=\"p38\">**</bpt>Azure Data Lake Tools<ept id=\"p38\">**</ept><ph id=\"ph61\"/> in <bpt id=\"p39\">**</bpt>Visual Studio<ept id=\"p39\">**</ept><ph id=\"ph62\"/> found at this  <bpt id=\"p40\">[</bpt>link<ept id=\"p40\">](https://www.microsoft.com/download/details.aspx?id=49504)</ept><ph id=\"ph63\"/> is already installed on the Visual Studio Community Edition which is on the virtual machione. After starting Visual Studio and logging in your Azure subscription, you will see your Azure Data Analytics account and storage in the left panel of Visual Studio.",
      "nodes": [
        {
          "content": "The  <bpt id=\"p38\">**</bpt>Azure Data Lake Tools<ept id=\"p38\">**</ept><ph id=\"ph61\"/> in <bpt id=\"p39\">**</bpt>Visual Studio<ept id=\"p39\">**</ept><ph id=\"ph62\"/> found at this  <bpt id=\"p40\">[</bpt>link<ept id=\"p40\">](https://www.microsoft.com/download/details.aspx?id=49504)</ept><ph id=\"ph63\"/> is already installed on the Visual Studio Community Edition which is on the virtual machione.",
          "pos": [
            0,
            390
          ]
        },
        {
          "content": "After starting Visual Studio and logging in your Azure subscription, you will see your Azure Data Analytics account and storage in the left panel of Visual Studio.",
          "pos": [
            391,
            554
          ]
        }
      ]
    },
    {
      "pos": [
        20582,
        20696
      ],
      "content": "<ph id=\"ph64\">![</ph>Azure_Data_Lake_PlugIn_v2<ph id=\"ph65\">](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Lake_PlugIn_v2.PNG)</ph>"
    },
    {
      "pos": [
        20701,
        20831
      ],
      "content": "Install <bpt id=\"p41\">**</bpt>Data Management Gateway<ept id=\"p41\">**</ept><ph id=\"ph66\"/> following this <bpt id=\"p42\">[</bpt>document<ept id=\"p42\">](../data-factory/data-factory-move-data-between-onprem-and-cloud.md)</ept>."
    },
    {
      "pos": [
        20833,
        20939
      ],
      "content": "<ph id=\"ph67\">![</ph>Azure_Data_Gateway_v2<ph id=\"ph68\">](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Gateway_v2.PNG)</ph>"
    },
    {
      "pos": [
        20944,
        21046
      ],
      "content": "Share the local folder where your data is stored to everyone so that Azure Data Gateway can access it."
    },
    {
      "pos": [
        21048,
        21136
      ],
      "content": "<ph id=\"ph69\">![</ph>Share_Folder<ph id=\"ph70\">](./media/machine-learning-data-science-vm-do-ten-things/Share_Folder.PNG)</ph>"
    },
    {
      "pos": [
        21139,
        21199
      ],
      "content": "<bpt id=\"p43\">**</bpt>Move data from VM to Data Lake: Azure Data Lake Explorer<ept id=\"p43\">**</ept>"
    },
    {
      "pos": [
        21201,
        21323
      ],
      "content": "You can use <bpt id=\"p44\">**</bpt>Azure Data Lake Explorer<ept id=\"p44\">**</ept><ph id=\"ph71\"/> to upload data from the local files in your Virtual Machine to Data Lake storage."
    },
    {
      "pos": [
        21325,
        21441
      ],
      "content": "<ph id=\"ph72\">![</ph>Azure_Data_Lake_UploadData<ph id=\"ph73\">](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Lake_UploadData.PNG)</ph>"
    },
    {
      "pos": [
        21444,
        21498
      ],
      "content": "<bpt id=\"p45\">**</bpt>Move data from VM to Data Lake: Azure Data Factory<ept id=\"p45\">**</ept>"
    },
    {
      "pos": [
        21500,
        21771
      ],
      "content": "Azure Data Factory is service to create, schedule, and manage data pipelines. We can use Azure Data Factory to move data between different storage. Create <bpt id=\"p46\">[</bpt>Azure Data Factory<ept id=\"p46\">](https://azure.microsoft.com/services/data-factory/)</ept><ph id=\"ph74\"/> in <bpt id=\"p47\">[</bpt>Azure Portal<ept id=\"p47\">](http://portal.azure.com)</ept>:",
      "nodes": [
        {
          "content": "Azure Data Factory is service to create, schedule, and manage data pipelines.",
          "pos": [
            0,
            77
          ]
        },
        {
          "content": "We can use Azure Data Factory to move data between different storage.",
          "pos": [
            78,
            147
          ]
        },
        {
          "content": "Create <bpt id=\"p46\">[</bpt>Azure Data Factory<ept id=\"p46\">](https://azure.microsoft.com/services/data-factory/)</ept><ph id=\"ph74\"/> in <bpt id=\"p47\">[</bpt>Azure Portal<ept id=\"p47\">](http://portal.azure.com)</ept>:",
          "pos": [
            148,
            366
          ]
        }
      ]
    },
    {
      "pos": [
        21773,
        21887
      ],
      "content": "<ph id=\"ph75\">![</ph>Azure_Data_Factory_Create<ph id=\"ph76\">](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Factory_Create.PNG)</ph>"
    },
    {
      "pos": [
        21889,
        22060
      ],
      "content": "Once it is created, you can build pipelines to move data between different storage. In <bpt id=\"p48\">**</bpt>Author and deploy<ept id=\"p48\">**</ept><ph id=\"ph77\"/> you can specify datasets to transfer and setup your pipelines.",
      "nodes": [
        {
          "content": "Once it is created, you can build pipelines to move data between different storage.",
          "pos": [
            0,
            83
          ]
        },
        {
          "content": "In <bpt id=\"p48\">**</bpt>Author and deploy<ept id=\"p48\">**</ept><ph id=\"ph77\"/> you can specify datasets to transfer and setup your pipelines.",
          "pos": [
            84,
            226
          ]
        }
      ]
    },
    {
      "pos": [
        22063,
        22187
      ],
      "content": "<ph id=\"ph78\">![</ph>Azure_Data_Factory_Overview_v4<ph id=\"ph79\">](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Factory_Overview_v4.PNG)</ph>"
    },
    {
      "pos": [
        22190,
        22458
      ],
      "content": "The major steps to move data from Virtual Machine to Azure Data Lake are as follows. Details about moving data using Azure Data Factory can be found <bpt id=\"p49\">[</bpt>here<ept id=\"p49\">](../data-factory/data-factory-data-movement-activities.md)</ept>. The JSON files below will be in your Data Science VM.",
      "nodes": [
        {
          "content": "The major steps to move data from Virtual Machine to Azure Data Lake are as follows.",
          "pos": [
            0,
            84
          ]
        },
        {
          "content": "Details about moving data using Azure Data Factory can be found <bpt id=\"p49\">[</bpt>here<ept id=\"p49\">](../data-factory/data-factory-data-movement-activities.md)</ept>.",
          "pos": [
            85,
            254
          ]
        },
        {
          "content": "The JSON files below will be in your Data Science VM.",
          "pos": [
            255,
            308
          ]
        }
      ]
    },
    {
      "pos": [
        22463,
        22489
      ],
      "content": "<bpt id=\"p50\">**</bpt>Create Linked Services<ept id=\"p50\">**</ept>"
    },
    {
      "pos": [
        22496,
        22619
      ],
      "content": "Click <bpt id=\"p51\">**</bpt>New Data Store<ept id=\"p51\">**</ept><ph id=\"ph80\"/> then choose <bpt id=\"p52\">**</bpt>Azure Data Lake Storage<ept id=\"p52\">**</ept>, plug in your credentials and parameters in the JSON file."
    },
    {
      "pos": [
        22626,
        22737
      ],
      "content": "Click <bpt id=\"p53\">**</bpt>New Data Store<ept id=\"p53\">**</ept><ph id=\"ph81\"/> then choose <bpt id=\"p54\">**</bpt>File System<ept id=\"p54\">**</ept>, plug in your credentials and parameters in the JSON file."
    },
    {
      "pos": [
        22741,
        22760
      ],
      "content": "<bpt id=\"p55\">**</bpt>Create Datasets<ept id=\"p55\">**</ept>"
    },
    {
      "pos": [
        22767,
        22891
      ],
      "content": "Click <bpt id=\"p56\">**</bpt>New dataset<ept id=\"p56\">**</ept><ph id=\"ph82\"/> then choose <bpt id=\"p57\">**</bpt>Azure Data Lake<ept id=\"p57\">**</ept>, plug in <bpt id=\"p58\">**</bpt>Linked Service<ept id=\"p58\">**</ept><ph id=\"ph83\"/> name and <bpt id=\"p59\">**</bpt>folder path<ept id=\"p59\">**</ept><ph id=\"ph84\"/> in the JSON file."
    },
    {
      "pos": [
        22898,
        23059
      ],
      "content": "Click <bpt id=\"p60\">**</bpt>New dataset<ept id=\"p60\">**</ept><ph id=\"ph85\"/> then choose <bpt id=\"p61\">**</bpt>On-premises file<ept id=\"p61\">**</ept>, plug in <bpt id=\"p62\">**</bpt>dataset schema<ept id=\"p62\">**</ept>, <bpt id=\"p63\">**</bpt>Linked Service<ept id=\"p63\">**</ept><ph id=\"ph86\"/> name, <bpt id=\"p64\">**</bpt>file name<ept id=\"p64\">**</ept>, and <bpt id=\"p65\">**</bpt>folder path<ept id=\"p65\">**</ept><ph id=\"ph87\"/> in the JSON file."
    },
    {
      "pos": [
        23063,
        23083
      ],
      "content": "<bpt id=\"p66\">**</bpt>Create Pipelines<ept id=\"p66\">**</ept>"
    },
    {
      "pos": [
        23090,
        23202
      ],
      "content": "Click <bpt id=\"p67\">**</bpt>New pipeline<ept id=\"p67\">**</ept>, specify the <bpt id=\"p68\">**</bpt>input and output datasets<ept id=\"p68\">**</ept>, <bpt id=\"p69\">**</bpt>activity type<ept id=\"p69\">**</ept>, and etc. in the JSON file."
    },
    {
      "pos": [
        23207,
        23231
      ],
      "content": "<bpt id=\"p70\">**</bpt>Create Data GateWays<ept id=\"p70\">**</ept>"
    },
    {
      "pos": [
        23238,
        23339
      ],
      "content": "Register your gateway key and make sure the registration is <bpt id=\"p71\">**</bpt>Registered<ept id=\"p71\">**</ept><ph id=\"ph88\"/> and status is <bpt id=\"p72\">**</bpt>Started<ept id=\"p72\">**</ept>."
    },
    {
      "pos": [
        23346,
        23519
      ],
      "content": "Click <bpt id=\"p73\">**</bpt>New data gateway<ept id=\"p73\">**</ept>, fill in **data gateway name **, the <bpt id=\"p74\">**</bpt>Configure<ept id=\"p74\">**</ept><ph id=\"ph89\"/> part will setup automatically if your data gateway is installed properly in the previous steps."
    },
    {
      "pos": [
        23521,
        23639
      ],
      "content": "<ph id=\"ph90\">![</ph>Azure_Data_Gateway_part2_v2<ph id=\"ph91\">](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Gateway_part2_v2.png)</ph>"
    },
    {
      "pos": [
        23641,
        23759
      ],
      "content": "<ph id=\"ph92\">![</ph>Azure_Data_Factory_Template<ph id=\"ph93\">](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Factory_Template.PNG)</ph>"
    },
    {
      "pos": [
        23761,
        23895
      ],
      "content": "<ph id=\"ph94\">![</ph>Azure_Data_Factory_Template_Json_v2<ph id=\"ph95\">](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Factory_Template_Json_v2.PNG)</ph>"
    },
    {
      "pos": [
        23897,
        23940
      ],
      "content": "The JSON files used in the above steps are:"
    },
    {
      "pos": [
        23942,
        23994
      ],
      "content": "<bpt id=\"p75\">**</bpt>Linked services: AzureDataLakeStoreLinkedService<ept id=\"p75\">**</ept>"
    },
    {
      "pos": [
        24556,
        24610
      ],
      "content": "<bpt id=\"p76\">**</bpt>Linked services: OnPremisesFileServerLinkedService<ept id=\"p76\">**</ept>"
    },
    {
      "pos": [
        25036,
        25064
      ],
      "content": "<bpt id=\"p77\">**</bpt>Datasets: OnPremisesFile<ept id=\"p77\">**</ept>"
    },
    {
      "pos": [
        25553,
        25579
      ],
      "content": "<bpt id=\"p78\">**</bpt>Datasets: weiglakenew1<ept id=\"p78\">**</ept>"
    },
    {
      "pos": [
        27802,
        27837
      ],
      "content": "<bpt id=\"p79\">**</bpt>Datasets: vmtodatalake_tripdata<ept id=\"p79\">**</ept>"
    },
    {
      "pos": [
        29400,
        29430
      ],
      "content": "<bpt id=\"p80\">**</bpt>Data Gateways: weiggateway<ept id=\"p80\">**</ept>"
    },
    {
      "pos": [
        29903,
        30016
      ],
      "content": "After the pipeline is built, you can look at the pipeline in the <bpt id=\"p81\">**</bpt>Diagram<ept id=\"p81\">**</ept><ph id=\"ph96\"/> in the Azure Data Factory dashboard."
    },
    {
      "pos": [
        30116,
        30162
      ],
      "content": "Click the <bpt id=\"p82\">**</bpt>Diagram<ept id=\"p82\">**</ept><ph id=\"ph98\"/> box to see the pipeline:"
    },
    {
      "pos": [
        30265,
        30344
      ],
      "content": "You can monitor the data pipeline in <bpt id=\"p83\">**</bpt>Contents<ept id=\"p83\">**</ept>-&gt;<bpt id=\"p84\">**</bpt>Datasets<ept id=\"p84\">**</ept>-&gt;<bpt id=\"p85\">**</bpt>Monitoring<ept id=\"p85\">**</ept>"
    },
    {
      "pos": [
        30443,
        30558
      ],
      "content": "You may also check if the data is moved to Azure Data Lake using <bpt id=\"p86\">**</bpt>Azure Data Lake Explorer<ept id=\"p86\">**</ept><ph id=\"ph101\"/> in <bpt id=\"p87\">**</bpt>Visual Studio<ept id=\"p87\">**</ept>."
    },
    {
      "pos": [
        30659,
        30708
      ],
      "content": "<bpt id=\"p88\">**</bpt>Read data from Azure Blob to Data Lake: U-SQL<ept id=\"p88\">**</ept>"
    },
    {
      "pos": [
        30710,
        31193
      ],
      "content": "If your data resides in Azure Blob storage, you can directly read data from Azure storage blob in U-SQL query. Before composing your U-SQL query, make sure your blob storage account is linked to your Azure Data Lake. Go to <bpt id=\"p89\">**</bpt>Azure Portal<ept id=\"p89\">**</ept>, find your Azure Data Lake Analytics dashboard, click <bpt id=\"p90\">**</bpt>Add Data Source<ept id=\"p90\">**</ept>, select storage type to <bpt id=\"p91\">**</bpt>Azure Storage<ept id=\"p91\">**</ept><ph id=\"ph103\"/> and plug in your Azure Storage Account Name and Key. Then you will be able to reference the data stored in the storage account.",
      "nodes": [
        {
          "content": "If your data resides in Azure Blob storage, you can directly read data from Azure storage blob in U-SQL query.",
          "pos": [
            0,
            110
          ]
        },
        {
          "content": "Before composing your U-SQL query, make sure your blob storage account is linked to your Azure Data Lake.",
          "pos": [
            111,
            216
          ]
        },
        {
          "content": "Go to <bpt id=\"p89\">**</bpt>Azure Portal<ept id=\"p89\">**</ept>, find your Azure Data Lake Analytics dashboard, click <bpt id=\"p90\">**</bpt>Add Data Source<ept id=\"p90\">**</ept>, select storage type to <bpt id=\"p91\">**</bpt>Azure Storage<ept id=\"p91\">**</ept><ph id=\"ph103\"/> and plug in your Azure Storage Account Name and Key.",
          "pos": [
            217,
            544
          ]
        },
        {
          "content": "Then you will be able to reference the data stored in the storage account.",
          "pos": [
            545,
            619
          ]
        }
      ]
    },
    {
      "pos": [
        31282,
        31584
      ],
      "content": "In Visual Studio, you can read data from blob, do some data manipulation, feature engineering, and output the resulting data to either Azure Data Lake or Azure Blob Storage. When you reference the data in blob storage, use <bpt id=\"p92\">**</bpt>wasb://<ept id=\"p92\">**</ept>; when you reference the data in Azure Data Lake, use <bpt id=\"p93\">**</bpt>swbhdfs://<ept id=\"p93\">**</ept>",
      "nodes": [
        {
          "content": "In Visual Studio, you can read data from blob, do some data manipulation, feature engineering, and output the resulting data to either Azure Data Lake or Azure Blob Storage.",
          "pos": [
            0,
            173
          ]
        },
        {
          "content": "When you reference the data in blob storage, use <bpt id=\"p92\">**</bpt>wasb://<ept id=\"p92\">**</ept>; when you reference the data in Azure Data Lake, use <bpt id=\"p93\">**</bpt>swbhdfs://<ept id=\"p93\">**</ept>",
          "pos": [
            174,
            382
          ]
        }
      ]
    },
    {
      "pos": [
        31670,
        31727
      ],
      "content": "You may use the following U-SQL queries in Visual Studio:"
    },
    {
      "pos": [
        33135,
        33239
      ],
      "content": "After your query is submitted to the server, a diagram showing the status of your job will be displayed."
    },
    {
      "pos": [
        33323,
        33357
      ],
      "content": "<bpt id=\"p94\">**</bpt>Query data in Data Lake: U-SQL<ept id=\"p94\">**</ept>"
    },
    {
      "pos": [
        33359,
        33729
      ],
      "content": "After the dataset is ingested into Azure Data Lake, you can use <bpt id=\"p95\">[</bpt>U-SQL language<ept id=\"p95\">](../data-lake-analytics/data-lake-analytics-u-sql-get-started.md)</ept><ph id=\"ph107\"/> to query and explore the data. U-SQL language is similar to T-SQL, but combines some features from C# so that users can write customized modules, User Defined Functions, and etc. You can use the scripts in the previous step.",
      "nodes": [
        {
          "content": "After the dataset is ingested into Azure Data Lake, you can use <bpt id=\"p95\">[</bpt>U-SQL language<ept id=\"p95\">](../data-lake-analytics/data-lake-analytics-u-sql-get-started.md)</ept><ph id=\"ph107\"/> to query and explore the data.",
          "pos": [
            0,
            232
          ]
        },
        {
          "content": "U-SQL language is similar to T-SQL, but combines some features from C# so that users can write customized modules, User Defined Functions, and etc. You can use the scripts in the previous step.",
          "pos": [
            233,
            426
          ]
        }
      ]
    },
    {
      "pos": [
        33732,
        33896
      ],
      "content": "After the query is submitted to server, tripdata_summary.CSV can be found shortly in <bpt id=\"p96\">**</bpt>Azure Data Lake Explorer<ept id=\"p96\">**</ept>, you may preview the data by right click the file."
    },
    {
      "pos": [
        33983,
        34011
      ],
      "content": "To see the file information:"
    },
    {
      "pos": [
        34105,
        34130
      ],
      "content": "HDInsight Hadoop Clusters"
    },
    {
      "pos": [
        34132,
        34148
      ],
      "content": "<bpt id=\"p97\">**</bpt>Prerequisite<ept id=\"p97\">**</ept>"
    },
    {
      "pos": [
        34152,
        34303
      ],
      "content": "Create your Azure Blob storage account from <bpt id=\"p98\">[</bpt>Azure Portal<ept id=\"p98\">](http://portal.azure.com)</ept>. This storage account is used to store data for HDInsight clusters.",
      "nodes": [
        {
          "content": "Create your Azure Blob storage account from <bpt id=\"p98\">[</bpt>Azure Portal<ept id=\"p98\">](http://portal.azure.com)</ept>.",
          "pos": [
            0,
            124
          ]
        },
        {
          "content": "This storage account is used to store data for HDInsight clusters.",
          "pos": [
            125,
            191
          ]
        }
      ]
    },
    {
      "pos": [
        34390,
        34510
      ],
      "content": "Customize Azure HDInsight Hadoop Clusters from <bpt id=\"p99\">[</bpt>Azure Portal<ept id=\"p99\">](machine-learning-data-science-customize-hadoop-cluster.md)</ept>"
    },
    {
      "pos": [
        34518,
        34697
      ],
      "content": "You must link the storage account created with your HDInsight cluster when it is created. This storage account is used for accessing data that can be processed within the cluster.",
      "nodes": [
        {
          "content": "You must link the storage account created with your HDInsight cluster when it is created.",
          "pos": [
            0,
            89
          ]
        },
        {
          "content": "This storage account is used for accessing data that can be processed within the cluster.",
          "pos": [
            90,
            179
          ]
        }
      ]
    },
    {
      "pos": [
        39507,
        39663
      ],
      "content": "Or you can follow this <bpt id=\"p100\">[</bpt>walkthrough<ept id=\"p100\">](machine-learning-data-science-process-hive-walkthrough.md)</ept><ph id=\"ph115\"/> to upload NYC Taxi data to HDI cluster. Major steps include:",
      "nodes": [
        {
          "content": "Or you can follow this <bpt id=\"p100\">[</bpt>walkthrough<ept id=\"p100\">](machine-learning-data-science-process-hive-walkthrough.md)</ept><ph id=\"ph115\"/> to upload NYC Taxi data to HDI cluster.",
          "pos": [
            0,
            193
          ]
        },
        {
          "content": "Major steps include:",
          "pos": [
            194,
            214
          ]
        }
      ]
    },
    {
      "pos": [
        39671,
        39738
      ],
      "content": "AzCopy: download zipped CSV's from public blob to your local folder"
    },
    {
      "pos": [
        39745,
        39807
      ],
      "content": "AzCopy: upload unzipped CSV's from local folder to HDI cluster"
    },
    {
      "pos": [
        39814,
        39896
      ],
      "content": "Log into the head node of Hadoop cluster and prepare for exploratory data analysis"
    },
    {
      "pos": [
        39898,
        40046
      ],
      "content": "After the data is loaded to HDI cluster, you can check your data in Azure Storage Explore. And you have a database nyctaxidb created in HDI cluster.",
      "nodes": [
        {
          "content": "After the data is loaded to HDI cluster, you can check your data in Azure Storage Explore.",
          "pos": [
            0,
            90
          ]
        },
        {
          "content": "And you have a database nyctaxidb created in HDI cluster.",
          "pos": [
            91,
            148
          ]
        }
      ]
    },
    {
      "pos": [
        40156,
        40200
      ],
      "content": "<bpt id=\"p101\">**</bpt>Data exploration: Hive Queries in Python<ept id=\"p101\">**</ept>"
    },
    {
      "pos": [
        40202,
        40438
      ],
      "content": "Since the data is in Hadoop cluster, you can use pyodbc package to connect to Hadoop Clusters and query database using Hive to do exploration and feature engineering. You can view the existing tables we created in the prerequisite step.",
      "nodes": [
        {
          "content": "Since the data is in Hadoop cluster, you can use pyodbc package to connect to Hadoop Clusters and query database using Hive to do exploration and feature engineering.",
          "pos": [
            0,
            166
          ]
        },
        {
          "content": "You can view the existing tables we created in the prerequisite step.",
          "pos": [
            167,
            236
          ]
        }
      ]
    },
    {
      "pos": [
        40652,
        40757
      ],
      "content": "Let's look at the number of records in each month and the frequencies of tipped or not in the trip table:"
    },
    {
      "pos": [
        41718,
        41837
      ],
      "content": "We can also compute the distance between pickup location and dropoff location and then compare it to the trip distance."
    },
    {
      "pos": [
        43666,
        43764
      ],
      "content": "Now let's prepare a downsampled (1%) data for modeling. We can use this data in AML reader module.",
      "nodes": [
        {
          "content": "Now let's prepare a downsampled (1%) data for modeling.",
          "pos": [
            0,
            55
          ]
        },
        {
          "content": "We can use this data in AML reader module.",
          "pos": [
            56,
            98
          ]
        }
      ]
    },
    {
      "pos": [
        47627,
        47698
      ],
      "content": "After a while, you can see the data has been loaded in Hadoop clusters:"
    },
    {
      "pos": [
        47974,
        48021
      ],
      "content": "<bpt id=\"p102\">**</bpt>Read data from HDI using AML: reader module<ept id=\"p102\">**</ept>"
    },
    {
      "pos": [
        48023,
        48274
      ],
      "content": "You may also use the <bpt id=\"p103\">**</bpt>reader<ept id=\"p103\">**</ept><ph id=\"ph123\"/> module in AML studio to access the database in Hadoop cluster. Plug in the credentials of your HDI clusters and Azure Storage Account and you will be able to build machine learning models using database in HDI clusters.",
      "nodes": [
        {
          "content": "You may also use the <bpt id=\"p103\">**</bpt>reader<ept id=\"p103\">**</ept><ph id=\"ph123\"/> module in AML studio to access the database in Hadoop cluster.",
          "pos": [
            0,
            152
          ]
        },
        {
          "content": "Plug in the credentials of your HDI clusters and Azure Storage Account and you will be able to build machine learning models using database in HDI clusters.",
          "pos": [
            153,
            309
          ]
        }
      ]
    },
    {
      "pos": [
        48358,
        48396
      ],
      "content": "The scored dataset can then be viewed:"
    },
    {
      "pos": [
        48486,
        48522
      ],
      "content": "Azure SQL Data Warehouse &amp; databases"
    },
    {
      "pos": [
        48529,
        48545
      ],
      "content": "Azure DocumentDB"
    },
    {
      "pos": [
        48547,
        48692
      ],
      "content": "Azure DocumentDB is a NoSQL database in the cloud. It allows you to work with documents like JSON and allow you to store and query the documents.",
      "nodes": [
        {
          "content": "Azure DocumentDB is a NoSQL database in the cloud.",
          "pos": [
            0,
            50
          ]
        },
        {
          "content": "It allows you to work with documents like JSON and allow you to store and query the documents.",
          "pos": [
            51,
            145
          ]
        }
      ]
    },
    {
      "pos": [
        48695,
        48780
      ],
      "content": "You need to do the following per-requisites steps to access DocumentDB from the DSVM."
    },
    {
      "pos": [
        48785,
        48871
      ],
      "content": "Install DocumentDB Python SDK (Run <ph id=\"ph126\">```pip install pydocumentdb```</ph><ph id=\"ph127\"/> from command prompt)"
    },
    {
      "pos": [
        48875,
        48971
      ],
      "content": "Create DocumentDB account and Document DB database from <bpt id=\"p104\">[</bpt>Azure portal<ept id=\"p104\">](https://portal.azure.com)</ept>"
    },
    {
      "pos": [
        48975,
        49160
      ],
      "content": "Download \"DocumentDB Migration tool\" from <bpt id=\"p105\">[</bpt>here<ept id=\"p105\">](http://www.microsoft.com/downloads/details.aspx?FamilyID=cda7703a-2774-4c07-adcc-ad02ddc1a44d)</ept><ph id=\"ph128\"/> and extract to a directory of your choice"
    },
    {
      "pos": [
        49164,
        49496
      ],
      "content": "Import JSON data (volcano data) stored on a <bpt id=\"p106\">[</bpt>public blob<ept id=\"p106\">](https://cahandson.blob.core.windows.net/samples/volcano.json)</ept><ph id=\"ph129\"/> into DocumentDB with following command parameters to the migration tool (dtui.exe from the directory where you installed the DocumentDB migration tool). Enter the source and target location parameters from below.",
      "nodes": [
        {
          "content": "Import JSON data (volcano data) stored on a <bpt id=\"p106\">[</bpt>public blob<ept id=\"p106\">](https://cahandson.blob.core.windows.net/samples/volcano.json)</ept><ph id=\"ph129\"/> into DocumentDB with following command parameters to the migration tool (dtui.exe from the directory where you installed the DocumentDB migration tool).",
          "pos": [
            0,
            330
          ]
        },
        {
          "content": "Enter the source and target location parameters from below.",
          "pos": [
            331,
            390
          ]
        }
      ]
    },
    {
      "pos": [
        49503,
        49748
      ],
      "content": "/s:JsonFile /s.Files:https://cahandson.blob.core.windows.net/samples/volcano.json /t:DocumentDBBulk /t.ConnectionString:AccountEndpoint=https://[DocDBAccountName].documents.azure.com:443/;AccountKey=[[KEY];Database=volcano /t.Collection:volcano1"
    },
    {
      "pos": [
        49750,
        50076
      ],
      "content": "Once you import the data, you can go to Jupyter and open the notebook titled <ph id=\"ph130\">```DocumentDBSample```</ph><ph id=\"ph131\"/> which contains python code to access DocumentDB and do some basic querying. You can learn more about DocumentDB by visiting the service <bpt id=\"p107\">[</bpt>documentation page<ept id=\"p107\">](https://azure.microsoft.com/documentation/learning-paths/documentdb/)</ept>",
      "nodes": [
        {
          "content": "Once you import the data, you can go to Jupyter and open the notebook titled <ph id=\"ph130\">```DocumentDBSample```</ph><ph id=\"ph131\"/> which contains python code to access DocumentDB and do some basic querying.",
          "pos": [
            0,
            211
          ]
        },
        {
          "content": "You can learn more about DocumentDB by visiting the service <bpt id=\"p107\">[</bpt>documentation page<ept id=\"p107\">](https://azure.microsoft.com/documentation/learning-paths/documentdb/)</ept>",
          "pos": [
            212,
            404
          ]
        }
      ]
    },
    {
      "pos": [
        50082,
        50139
      ],
      "content": "8. Build reports and dashboard using the Power BI Desktop"
    },
    {
      "pos": [
        50142,
        50402
      ],
      "content": "Let us visualize the Volcano JSON file we saw in the DocumentDB example above in Power BI to gain visual insights into the data. Detailed steps are found in the <bpt id=\"p108\">[</bpt>Power BI article<ept id=\"p108\">](../documentdb/documentdb-powerbi-visualize.md)</ept>. The high level steps are below :",
      "nodes": [
        {
          "content": "Let us visualize the Volcano JSON file we saw in the DocumentDB example above in Power BI to gain visual insights into the data.",
          "pos": [
            0,
            128
          ]
        },
        {
          "content": "Detailed steps are found in the <bpt id=\"p108\">[</bpt>Power BI article<ept id=\"p108\">](../documentdb/documentdb-powerbi-visualize.md)</ept>.",
          "pos": [
            129,
            269
          ]
        },
        {
          "content": "The high level steps are below :",
          "pos": [
            270,
            302
          ]
        }
      ]
    },
    {
      "pos": [
        50407,
        50528
      ],
      "content": "Open Power BI Desktop and do \"Get Data\". Specify the URL as: https://cahandson.blob.core.windows.net/samples/volcano.json",
      "nodes": [
        {
          "content": "Open Power BI Desktop and do \"Get Data\".",
          "pos": [
            0,
            40
          ]
        },
        {
          "content": "Specify the URL as: https://cahandson.blob.core.windows.net/samples/volcano.json",
          "pos": [
            41,
            121
          ]
        }
      ]
    },
    {
      "pos": [
        50532,
        50580
      ],
      "content": "You will see the JSON records imported as a list"
    },
    {
      "pos": [
        50584,
        50650
      ],
      "content": "Next convert the list to a table so PowerBI can work with the same"
    },
    {
      "pos": [
        50654,
        50794
      ],
      "content": "Then you expand the columns by clicking on the expand icon (the one with the \"left arrow and a right arrow\" icon on the right of the column)"
    },
    {
      "pos": [
        50798,
        50925
      ],
      "content": "You will see that location is a \"Record\" field. Expand the record and select only the coordinates. Corordinate is a list column",
      "nodes": [
        {
          "content": "You will see that location is a \"Record\" field.",
          "pos": [
            0,
            47
          ]
        },
        {
          "content": "Expand the record and select only the coordinates.",
          "pos": [
            48,
            98
          ]
        },
        {
          "content": "Corordinate is a list column",
          "pos": [
            99,
            127
          ]
        }
      ]
    },
    {
      "pos": [
        50929,
        51179
      ],
      "content": "Next you will add a new column to convert the list coordinate column into a comma separate LatLong column contatenating the two elements in the coordinate list field using the formula <ph id=\"ph132\">```Text.From([coordinates]{1})&amp;\",\"&amp;Text.From([coordinates]{0})```</ph>."
    },
    {
      "pos": [
        51184,
        51266
      ],
      "content": "Finally convert the <ph id=\"ph133\">```Elevation```</ph><ph id=\"ph134\"/> column to Decimal and hit the Close and Apply."
    },
    {
      "pos": [
        51268,
        51462
      ],
      "content": "Instead of steps above, you can paste the following code that scripts out the steps above in the Advanced Editor in PowerBI that allows you to write the data transformations in a query language."
    },
    {
      "pos": [
        52417,
        52517
      ],
      "content": "You now have the data in your Power BI data model. Your Power BI desktop should look as shown below.",
      "nodes": [
        {
          "content": "You now have the data in your Power BI data model.",
          "pos": [
            0,
            50
          ]
        },
        {
          "content": "Your Power BI desktop should look as shown below.",
          "pos": [
            51,
            100
          ]
        }
      ]
    },
    {
      "pos": [
        52604,
        52874
      ],
      "content": "You can start building reports and visualizations using the data model. You can follow the steps in this <bpt id=\"p109\">[</bpt>Power BI article<ept id=\"p109\">](../documentdb/documentdb-powerbi-visualize.md#build-the-reports)</ept><ph id=\"ph136\"/> to build a report. The end result will be a report that looks like the following.",
      "nodes": [
        {
          "content": "You can start building reports and visualizations using the data model.",
          "pos": [
            0,
            71
          ]
        },
        {
          "content": "You can follow the steps in this <bpt id=\"p109\">[</bpt>Power BI article<ept id=\"p109\">](../documentdb/documentdb-powerbi-visualize.md#build-the-reports)</ept><ph id=\"ph136\"/> to build a report.",
          "pos": [
            72,
            265
          ]
        },
        {
          "content": "The end result will be a report that looks like the following.",
          "pos": [
            266,
            328
          ]
        }
      ]
    },
    {
      "pos": [
        52877,
        52933
      ],
      "content": "TBD: Volcano Map Report image URL missing - not correct."
    },
    {
      "pos": [
        52938,
        52995
      ],
      "content": "9. Dynamically scale your DSVM to meet your project needs"
    },
    {
      "pos": [
        52997,
        53305
      ],
      "content": "You can scale up and down the DSVM to meet your project needs. If you dont need to use the VM in the evening or weekends, you can just shutdown the VM from the <bpt id=\"p110\">[</bpt>Azure Portal<ept id=\"p110\">](https://portal.azure.com)</ept>. NOTE:  that you will incur compute charges if you use just the Operating system shutdown button on the VM.",
      "nodes": [
        {
          "content": "You can scale up and down the DSVM to meet your project needs.",
          "pos": [
            0,
            62
          ]
        },
        {
          "content": "If you dont need to use the VM in the evening or weekends, you can just shutdown the VM from the <bpt id=\"p110\">[</bpt>Azure Portal<ept id=\"p110\">](https://portal.azure.com)</ept>.",
          "pos": [
            63,
            243
          ]
        },
        {
          "content": "NOTE:  that you will incur compute charges if you use just the Operating system shutdown button on the VM.",
          "pos": [
            244,
            350
          ]
        }
      ]
    },
    {
      "pos": [
        53309,
        53762
      ],
      "content": "If you need to handle some large scale analysis and need more CPU and/or memory and/or disk capacity you can find a large choice of VM sizes in terms of CPU cores, memory capacity and disk types (including Solid state drives) that meet your compute  and budgetary needs. The full list of VMs along with their hourly compute pricing is available on the <bpt id=\"p111\">[</bpt>Azure Virtual Machines Pricing<ept id=\"p111\">](https://azure.microsoft.com/pricing/details/virtual-machines/)</ept><ph id=\"ph137\"/> page.",
      "nodes": [
        {
          "content": "If you need to handle some large scale analysis and need more CPU and/or memory and/or disk capacity you can find a large choice of VM sizes in terms of CPU cores, memory capacity and disk types (including Solid state drives) that meet your compute  and budgetary needs.",
          "pos": [
            0,
            270
          ]
        },
        {
          "content": "The full list of VMs along with their hourly compute pricing is available on the <bpt id=\"p111\">[</bpt>Azure Virtual Machines Pricing<ept id=\"p111\">](https://azure.microsoft.com/pricing/details/virtual-machines/)</ept><ph id=\"ph137\"/> page.",
          "pos": [
            271,
            511
          ]
        }
      ]
    },
    {
      "pos": [
        53765,
        54048
      ],
      "content": "Similarly, if your needs for VM processing capacity reduces (for example: you moved a major workload to a Hadoop or a Spark cluster), you can scale down the cluster from the <bpt id=\"p112\">[</bpt>Azure Portal<ept id=\"p112\">](https://portal.azure.com)</ept><ph id=\"ph138\"/> and going to the settings of your VM instance. Here is a screenshot.",
      "nodes": [
        {
          "content": "Similarly, if your needs for VM processing capacity reduces (for example: you moved a major workload to a Hadoop or a Spark cluster), you can scale down the cluster from the <bpt id=\"p112\">[</bpt>Azure Portal<ept id=\"p112\">](https://portal.azure.com)</ept><ph id=\"ph138\"/> and going to the settings of your VM instance.",
          "pos": [
            0,
            319
          ]
        },
        {
          "content": "Here is a screenshot.",
          "pos": [
            320,
            341
          ]
        }
      ]
    },
    {
      "pos": [
        54130,
        54182
      ],
      "content": "10. Install additional tools on your virtual machine"
    },
    {
      "pos": [
        54184,
        55018
      ],
      "content": "We have packaged several tools that we believe will be able to address many of the common data analytics needs and save you time by avoiding  installing and configuring your environment one by one and paying for only what you use. You can leverage other Azure data and analytics services as seen earlier in this article to enhance your analytics environment. We understand that in some cases your needs may require additional tools including some proprietary third party tools. You have full administrative access on the virtual machine to install new tools you need. You can also install additional packages in Python and R that are not pre-installed. For Python  you can use either <ph id=\"ph140\">```conda```</ph><ph id=\"ph141\"/> or <ph id=\"ph142\">```pip```</ph>. For R you can use the <ph id=\"ph143\">```install.packages()```</ph><ph id=\"ph144\"/> in the R console or use the IDE and choose \"Packages -&gt; Install Packages...\".",
      "nodes": [
        {
          "content": "We have packaged several tools that we believe will be able to address many of the common data analytics needs and save you time by avoiding  installing and configuring your environment one by one and paying for only what you use.",
          "pos": [
            0,
            230
          ]
        },
        {
          "content": "You can leverage other Azure data and analytics services as seen earlier in this article to enhance your analytics environment.",
          "pos": [
            231,
            358
          ]
        },
        {
          "content": "We understand that in some cases your needs may require additional tools including some proprietary third party tools.",
          "pos": [
            359,
            477
          ]
        },
        {
          "content": "You have full administrative access on the virtual machine to install new tools you need.",
          "pos": [
            478,
            567
          ]
        },
        {
          "content": "You can also install additional packages in Python and R that are not pre-installed.",
          "pos": [
            568,
            652
          ]
        },
        {
          "content": "For Python  you can use either <ph id=\"ph140\">```conda```</ph><ph id=\"ph141\"/> or <ph id=\"ph142\">```pip```</ph>.",
          "pos": [
            653,
            765
          ]
        },
        {
          "content": "For R you can use the <ph id=\"ph143\">```install.packages()```</ph><ph id=\"ph144\"/> in the R console or use the IDE and choose \"Packages -&gt; Install Packages...\".",
          "pos": [
            766,
            929
          ]
        }
      ]
    },
    {
      "pos": [
        55021,
        55189
      ],
      "content": "This is just a few things you can do on the Microsoft Data Science Virtual Machine. There are many more things you can do to make it an effective analytics environment.",
      "nodes": [
        {
          "content": "This is just a few things you can do on the Microsoft Data Science Virtual Machine.",
          "pos": [
            0,
            83
          ]
        },
        {
          "content": "There are many more things you can do to make it an effective analytics environment.",
          "pos": [
            84,
            168
          ]
        }
      ]
    }
  ],
  "content": "<properties\n    pageTitle=\"Ten things you can do on the Data science Virtual Machine  | Microsoft Azure\"\n    description=\"Perform various data exploration and modeling task on the Data science Virtual Machine.\"\n    services=\"machine-learning\"\n    documentationCenter=\"\"\n    authors=\"bradsev\"\n    manager=\"paulettm\"\n    editor=\"cgronlun\"  />\n\n<tags\n    ms.service=\"machine-learning\"\n    ms.workload=\"data-services\"\n    ms.tgt_pltfrm=\"na\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\"\n    ms.date=\"02/13/2016\"\n    ms.author=\"gokuma;weig;bradsev\" />\n\n# Ten things you can do on the Data science Virtual Machine \n\nThe Microsoft Data Science Virtual Machine (DSVM) is a powerful data science development environment that enables you to perform various data exploration and modeling tasks. The environment comes already built and bundled with several popular data analytics tools that make it easy to get started quickly with your analysis. The DSVM works closely with many Azure services and is able to read and process data that is already stored on Azure in Azure SQL Data Warehouse, Azure Data Lake, Azure Storage, or DocumentDB. It can also leverage analytics tools like Azure Machine Learning and Azure Data Factory.\n\n\nIn this article we walk you through how to use your DSVM to perform various data science tasks and interact with other Azure services. Here is some  of the things you can do on the DSVM:\n\n1. Explore data and develop models locally on the DSVM using Microsoft R Server, Python\n2. Use a Jupyter notebook to experiment with your data on a browser using Python 2, Python 3, Microsoft R an enterprise ready version of R designed for scalability and performance\n3. Operationalize models built using R and Python on Azure Machine Learning so client applications can access your models using a simple web services interface\n4. Administer your Azure resources using  Azure Portal or Powershell \n5. Extend your storage space and share large scale datasets / code across your whole team by creating an Azure File Storage as a mountable drive on your DSVM \n6. Share code with your team using Github and access your repository using the pre-installed Git clients - Git Bash, Git GUI.\n7. Access various Azure data and analytics services like Azure blob storage, Azure Data Lake, Azure HDInsight (Hadoop), Azure DocumentDB, Azure SQL Data Warehouse & databases\n8. Build reports and dashboard using the Power BI Desktop pre-installed on the DSVM and deploy them on the cloud\n9. Dynamically scale your DSVM to meet your project needs \n10. Install additional tools on your virtual machine\n\n**Prerequisites**\nYou will need an Azure subscription. You can sign-up for a free trial [here](https://azure.microsoft.com/free/)\nInstructions for provisioning a Data Science Virtual Machine on the Azure Portal are available at [Creating a virtual machine](https://ms.portal.azure.com/#create/microsoft-ads.standard-data-science-vmstandard-data-science-vm).\n\n## 1. Explore data and develop models using Microsoft R Server or Python\n\nYou can use languages like R and Python to do your data analytics right on the DSVM. \n\nFor R, you can use an IDE called \"Revolution R Enterprise 8.0\" that can be found on the start menu or the desktop. Microsoft has provided additional libraries on top of the Open source/CRAN-R to enable scalable analytics and the ability to analyze data larger than the memory size allowed by doing parallel chunked analysis. Here is a screen shot of  using the R IDE. \n\n![R IDE](./media/machine-learning-data-science-vm-do-ten-things/RevoIDE.png)\n\nFor Python, you can use an IDE like Visual Studio Community Edition which has the Python Tools for Visual Studio (PTVS) extension pre-installed. By default, only a basic Python 2.7 (without any analytics library like SciKit, Pandas) is configured on PTVS. In order to enable Anaconda Python 2.7 and 3.5, you need to do the following:\n\n* Create custom environments for each version by navigating to Tools -> Python Tools -> Python Environments and then clicking \"+ Custom\" in the Visual Studio 2015 Community Edition\n* Give a description and set the environment prefix paths as c:\\anaconda for Anaconda Python 2.7 OR c:\\anaconda\\envs\\py35 for Anaconda Python 3.5 \n* Click **Auto Detect** and then **Apply** to save the environment. \n\nHere is what the custom environment setup looks like in Visual Studio.\n\n![PTVS Setup](./media/machine-learning-data-science-vm-do-ten-things/PTVSSetup.png)\n\nSee [PTVS documentation](https://github.com/Microsoft/PTVS/wiki/Selecting-and-Installing-Python-Interpreters#hey-i-already-have-an-interpreter-on-my-machine-but-ptvs-doesnt-seem-to-know-about-it) for more details on how to create the Python Environments. \n  \nNow you are set up to open a project and begin working!\n\n## 2. Using a Jupyter Notebook to explore and model your data with Python or R\n\nThe Jupyter Notebook is a powerful environment that provides a browser-based \"IDE\" for data exploration and modeling. \nYou can use Python 2, Python 3 or R (both Open Source and Microsoft R Server). \nTo launch the Jupyter notebook click on the start menu icon / desktop icon titled **Jupyter Notebook**. On the DSVM you can also browse to \"https://localhost:9999/\" to access the Jupiter notebook. If it prompts you for a password, please use instructions found on the \n[DSVM documentation page](machine-learning-data-science-provision-vm.md/#how-to-create-a-strong-password-on-the-jupyter-notebook-server) \nto create a strong password to access the Jupyter notebook. \n\nTBD: SCREEN SHOT\n\nOnce you are on the notebook, you will see a directory that contains a few example notebooks that are pre-packaged into the DSVM. You can click on the notebook and see the code. You can execute each cell by pressing **SHIFT-ENTER**. \nYou can run the entire notebook by clicking on **Cell** -> **Run**.\n\nYou can create a new notebook by clicking on the Jupyter Icon (left top corner) and then clicking **New** button on the right and then choosing the notebook language (also known as kernels). Currently we support Python 2.7, Python 3.5 and R. The R kernel supports programming in both Open source R as well as the enterprise scalable Microsoft R Server. Once you are in the notebook you can explore your data, build the model, test the model using your choice of libraries. \n\n\n## 3. Build models using R and Python and Operationalize it on Azure Machine Learning\n\nOnce you have built and validated your model the next step is usually to deploy it into production. \nThis allows your client applications to invoke the model predictions on a real time or a batch basis. Azure Machine Learning provides a mechanism to operationalize the model built in either R or Python. \nWhen you operationalize your model in Azure Machine Learning, a web service is exposed that allows clients to make REST calls that pass in input parameters and receive predictions from the model as outputs. \nIf you have not yet signed up for AzureML, you can obtain a free 8-hour guest access or a free workspace by visiting the [AzureML Studio](https://studio.azureml.net/) home page and clicking on \"Get Started\".  \n\n### Build and Operationalizing models built using Python  \n\nUpload the notebook entitled \"IrisClassifierPyMLWebService\" to your Jupyter. Here is a simple model built in Python using SciKit-learn as found in the notebook. \n  \n    python\n    #IRIS classification\n    from sklearn import datasets\n    from sklearn import svm\n    clf = svm.SVC()\n    iris = datasets.load_iris()\n    X, y = iris.data, iris.target\n    clf.fit(X, y) \n \nThe method used to deploy your python models to AzureML is to wrap the prediction of a model into a function \nand decorate it with attributes provided by the AzureML library denoting your AzureML workspace ID, API Key, the input parameters and return parameters.  \n\n    python\n    from azureml import services\n    @services.publish(workspaceid, auth_token)\n    @services.types(sep_l = float, sep_w = float, pet_l=float, pet_w=float)\n    @services.returns(int) #0, or 1, or 2\n    def predictIris(sep_l, sep_w, pet_l, pet_w):\n    inputArray = [sep_l, sep_w, pet_l, pet_w]\n    return clf.predict(inputArray)\n\nA client can now make calls to the web service. There are convenience wrappers that construct the REST API requests. Here is a sample code to consume the web service. \n\n\n    python\n    # Consume through web service URL and keys\n    from azureml import services\n    @services.service(url, api_key)\n    @services.types(sep_l = float, sep_w = float, pet_l=float, pet_w=float)\n    @services.returns(float)\n    def IrisPredictor(sep_l, sep_w, pet_l, pet_w):\n    pass\n\n    IrisPredictor(3,2,3,4)\n\n\nNOTE: The AzureML library is support on Python 2.7 only currently. \n\n### Build and Operationalize R models\n\nYou can deploy R models built on the Data Science Virtual Machine or elsewhere onto Azure ML in a manner that is similar to how it is done for Python. First create a settings.json file as below to provide your workspace ID and auth token.\nYou then write a wrapper for the model's predict function. Then you call the ```publishWebService``` call in the AzureML library passing in the function wrapper.  \nHere is a code snippet that can be used to publish a model as a web service in Azure ML.\n\n#### Settings.json File:\n\n    json\n    {\"workspace\":{\n    \"id\"                  : \"ENTER YOUR AZUREML WORKSPACE ID\",\n    \"authorization_token\" : \"ENTER YOUR AZUREML AUTH TOKEN\"\n    }}\n\n\n#### Build a model in R and publishing it in Azure ML\n\n    R\n    library(AzureML)\n    ws <- workspace(config=\"~/.azureml/settings.json\")\n\n    if(!require(\"lme4\")) install.packages(\"lme4\")\n    library(lme4)\n    set.seed(1)\n    train <- sleepstudy[sample(nrow(sleepstudy), 120),]\n    m <- lm(Reaction ~ Days + Subject, data = train)\n \n    # Define a prediction function to publish based on the model:\n    sleepyPredict <- function(newdata){\n        predict(m, newdata=newdata)\n    }\n \n    ep <- publishWebService(ws, fun = sleepyPredict, name=\"sleepy lm\", inputSchema = sleepstudy, data.frame=TRUE)\n\n#### Consume the model deployed in Azure ML\n\nTo consume the model from a client application, we use the AzureML library to lookup the published web service by name using the `services` API call to determine the endpoint. Then you just call the `consume` function and pass in the data frame to be predicted. \nThe following code is used to consume the model published as an AzureML web service. \n\n\n    R\n    library(AzureML)\n    library(lme4)\n    ws <- workspace(config=\"~/.azureml/settings.json\")\n\n    s <-  services(ws, name = \"sleepy lm\")\n    s <- tail(s, 1) # use the last published function, in case of duplicate function names\n\n    ep <- endpoints(ws, s)\n\n    # OK, try this out, and compare with raw data\n    ans = consume(ep, sleepstudy)$ans\n\n\n## 4. Administer your Azure resources using Azure Portal or Powershell\n\nThe DSVM not only allows you to build your analytics solution locally on the virtual machine, but also allows you to access services on Microsoft's Azure cloud. Azure provides several compute, storage, data analytics services and other services that you can administer and access from your DSVM.\n\nTo administer your Azure subscription and cloud resources you can use your browser and point to the \n[Azure Portal](portal.azure.com). You can also use Azure Powershell to adminster your Azure subscription and resources via a script. \nYou can run Azure Powershell from a shortcut on the desktop or from the start menu titled \"Microsoft Azure Powershell\". Refer to \n[Microsoft Azure Powershell documentation](../powershell-azure-resource-manager.md) for more information on how you can administer your Azure subscription and resources using Windows Powershell scripts.\n\n\n## 5. Extend your storage space with a shared file system\n\nData scientists can share large datasets, code etc within the team. The DSVM itself has about 70GB of space available. To extend your storage, you can use the Azure File Service and mount it on the DSVM or access via REST API. \nThe maximum space of the Azure File Service share is 5TB and individual file size limit is 1TB. \n\nYou can use Azure Powershell to create a  Azure File Service share. Here is the script to run under Azure PowerShell to create a Azure File service share. \n\n    # Authenticate to Azure. \n    Login-AzureRmAccount\n    # Select your subscription\n    Get-AzureRmSubscription –SubscriptionName \"<your subscription name>\" | Select-AzureRmSubscription\n    # Create a new resource group. \n    New-AzureRmResourceGroup -Name <dsvmdatarg>\n    # Create a new storage account. You can reuse existing storage account if you wish. \n    New-AzureRmStorageAccount -Name <mydatadisk> -ResourceGroupName <dsvmdatarg> -Location \"<Azure Data Center Name For eg. South Central US>\" -Type \"Standard_LRS\"\n    # Set your current working storage account\n    Set-AzureRmCurrentStorageAccount –ResourceGroupName \"<dsvmdatarg>\" –StorageAccountName <mydatadisk>\n    \n    # Create a Azure File Service Share\n    $s = New-AzureStorageShare <<teamsharename>>\n    # Create a directory under the FIle share. You can give it any name\n    New-AzureStorageDirectory -Share $s -Path <directory name>\n    # List the share to confirm that everything worked\n    Get-AzureStorageFile -Share $s\n\n\nNow that you have creates an Azure file share, you can mount it in any virtual machine in Azure. It is highly recommended that the VM is in same Azure data center as the storage account to avoid latency and data transfer charges. Here is the commands to mount the drive on the DSVM that you can run on Azure Powershell.\n\n\n    # Get storage key of the storage account that has the Azure file share from Azurer portal. Store it securely on the VM to avoid prompted in next command.\n    cmdkey /add:<<mydatadisk>>.file.core.windows.net /user:<<mydatadisk>> /pass:<storage key>\n    \n    # Mount the Azure file share as Z: drive on the VM. You can chose another drive letter if you wish\n    net use z:  \\\\<mydatadisk>.file.core.windows.net\\<<teamsharename>>\n\n\nNow you can access this drive as any normal drive on the VM. \n\n## 6. Share code with your team using Github \n\nGithub is a code repository where you can find a lot of sample code and sources for different tools using various technologies shared by the developer community. It uses Git as the technology to track and store versions of the code files. Github is also a platform where you can create your own repository to store your team's shared code and documentation, implement version control and also control who have access to view and contribute code. Please visit the [Github help pages](https://help.github.com/) for more information on using Git. You can use Github as one of the ways to collaborate with your team, use code developed by the community and contribute code back to the community. \n\nThe DSVM already comes loaded with client tools on both command line as well GUI to access Github repository. The command line tool to work with Git and Github is called ```git-bash```. Visual Studio installed on the DSVM has the Git extensions. You can find start-up icons for these tools on the start menu and the desktop. \n\nTo download code from a Github repository you will use the ```git clone``` command. For example to download data science repository published by Microsoft into the current directory you can run the following command once you are in ```git-bash```. \n\n    git clone https://github.com/Azure/Azure-MachineLearning-DataScience.git\n\nIn Visual Studio, you can do the same clone operation. See the screen-shot below for accessing Git and Github tools in Visual Studio. \n\n![Git in Visual Studio](./media/machine-learning-data-science-vm-do-ten-things/VSGit.PNG)\n\n\nYou can find more information on using Git to work with your Github repository by using several resources available on github.com. You may find the [cheat sheet](https://training.github.com/kit/downloads/github-git-cheat-sheet.pdf) as a useful reference. \n\n\n## 7. Access various Azure data and analytics services\n\n### Azure Blob\n\n**Prerequisite**\n\n- **Create your Azure Blob storage account from [Azure Portal](http://portal.azure.com).**\n\n![Create_Azure_Blob](./media/machine-learning-data-science-vm-do-ten-things/Create_Azure_Blob.PNG)\n\n\n- **Make sure the pre-installed AzCopy tool found at ```C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\AzCopy\\azcopy.exe``` is added to your environment variable. For more info on AzCopy please refer to [AzCopy documentation](../storage/storage-use-azcopy.md)\n\n\n- **Start the Azure Storage Explorer from [here](https://azurestorageexplorer.codeplex.com/).**\n\n![AzureStorageExplorer_v4](./media/machine-learning-data-science-vm-do-ten-things/AzureStorageExplorer_v4.png)\n\n**Move data from VM to Azure Blob: AzCopy**\n\nTo move data between your local files and blob storage, you can use AzCopy in command line or PowerShell:\n`AzCopy /Source:C:\\myfolder /Dest:https://<mystorageaccount>.blob.core.windows.net/<mycontainer> /DestKey:<storage account key> /Pattern:abc.txt` \n\nReplace **C:\\myfolder** to the path where your file is stored, **mystorageaccount** to your blob storage account name, **mycontainer** to the container name, **storage account key** to your blob storage access key. You can find your storage account credentials in [Azure Portal](http://portal.azure.com).\n\n![StorageAccountCredential_v2](./media/machine-learning-data-science-vm-do-ten-things/StorageAccountCredential_v2.png)\n\nRun AzCopy command in PowerShell or in command prompt. Here is some example usage of AzCopy command:\n\n\n    # Copy *.sql from local machine to a Azure Blob\n    \"C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\AzCopy\\azcopy\" /Source:\"c:\\Aaqs\\Data Science Scripts\" /Dest:https://[ENTER STORAGE ACCOUNT].blob.core.windows.net/[ENTER CONTAINER] /DestKey:[ENTER STORAGE KEY] /S /Pattern:*.sql\n    \n    # Copy back all files from Azure Blob container to Local machine\n    \n    \"C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\AzCopy\\azcopy\" /Dest:\"c:\\Aaqs\\Data Science Scripts\\temp\" /Source:https://[ENTER STORAGE ACCOUNT].blob.core.windows.net/[ENTER CONTAINER] /SourceKey:[ENTER STORAGE KEY] /S\n    \n\n\nOnce you run your AzCopy command to copy to an Azure blob you see your file shows up in Azure Storage Explorer shortly. \n\n![AzCopy_run_finshed_Storage_Explorer_v3](./media/machine-learning-data-science-vm-do-ten-things/AzCopy_run_finshed_Storage_Explorer_v3.png)\n\n\n**Move data from VM to Azure Blob: Azure Storage Explorer**\n\nYou can also upload data from the local file in your VM using Azure Storage Explorer: \n\n![](./media/machine-learning-data-science-vm-do-ten-things/AzureStorageExplorer_upload_v2.png)\n\n\n**Read data from Azure Blob: AML reader module**\n\nIn Azure Machine Learning Studio you can use a **Reader module** to read data from your blob. \n\n![AML_ReaderBlob_Module_v3](./media/machine-learning-data-science-vm-do-ten-things/AML_ReaderBlob_Module_v3.png)\n\n\n**Read data from Azure Blob: Python ODBC**\n\nIn the IPython Notebook **NYC Data wrangling using IPython Notebook and Azure Blob Storage**, you can use **pyodbc** package to read data directly from blob. \n\nFirst, import required packages:\n\n    import pandas as pd\n    from pandas import Series, DataFrame\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from time import time\n    import pyodbc\n    import os\n    from azure.storage import BlobService\n    import tables\n    import time\n    import zipfile\n    import random\n\nThen plug in your Azure Blob account credentials and read data from Blob:\n\n    CONTAINERNAME = 'xxx'\n    STORAGEACCOUNTNAME = 'xxxx'\n    STORAGEACCOUNTKEY = 'xxxxxxxxxxxxxxxx'\n    BLOBNAME = 'nyctaxidataset/nyctaxitrip/trip_data_1.csv'\n    localfilename = 'trip_data_1.csv'\n    LOCALDIRECTORY = os.getcwd()\n    LOCALFILE =  os.path.join(LOCALDIRECTORY, localfilename)\n\nYou can see the data is read in as a data frame:\n\n![IPNB_data_readin](./media/machine-learning-data-science-vm-do-ten-things/IPNB_data_readin.PNG)\n\n\n### Azure Data Lake\n\n**Prerequisite**\n\n- Create your Azure Data Lake Analytics in [Azure Portal](http://portal.azure.com).\n\n![Azure_Data_Lake_Create_v2](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Lake_Create_v2.png)\n\n\n- The  **Azure Data Lake Tools** in **Visual Studio** found at this  [link](https://www.microsoft.com/download/details.aspx?id=49504) is already installed on the Visual Studio Community Edition which is on the virtual machione. After starting Visual Studio and logging in your Azure subscription, you will see your Azure Data Analytics account and storage in the left panel of Visual Studio. \n\n![Azure_Data_Lake_PlugIn_v2](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Lake_PlugIn_v2.PNG)\n\n\n- Install **Data Management Gateway** following this [document](../data-factory/data-factory-move-data-between-onprem-and-cloud.md).\n\n![Azure_Data_Gateway_v2](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Gateway_v2.PNG)\n\n\n- Share the local folder where your data is stored to everyone so that Azure Data Gateway can access it.\n\n![Share_Folder](./media/machine-learning-data-science-vm-do-ten-things/Share_Folder.PNG)\n\n\n**Move data from VM to Data Lake: Azure Data Lake Explorer**\n\nYou can use **Azure Data Lake Explorer** to upload data from the local files in your Virtual Machine to Data Lake storage.\n\n![Azure_Data_Lake_UploadData](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Lake_UploadData.PNG)\n\n\n**Move data from VM to Data Lake: Azure Data Factory**\n\nAzure Data Factory is service to create, schedule, and manage data pipelines. We can use Azure Data Factory to move data between different storage. Create [Azure Data Factory](https://azure.microsoft.com/services/data-factory/) in [Azure Portal](http://portal.azure.com):\n\n![Azure_Data_Factory_Create](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Factory_Create.PNG)\n\nOnce it is created, you can build pipelines to move data between different storage. In **Author and deploy** you can specify datasets to transfer and setup your pipelines. \n\n![Azure_Data_Factory_Overview_v4](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Factory_Overview_v4.PNG)\n\n\nThe major steps to move data from Virtual Machine to Azure Data Lake are as follows. Details about moving data using Azure Data Factory can be found [here](../data-factory/data-factory-data-movement-activities.md). The JSON files below will be in your Data Science VM.\n\n1. **Create Linked Services**\n    - Click **New Data Store** then choose **Azure Data Lake Storage**, plug in your credentials and parameters in the JSON file.\n    - Click **New Data Store** then choose **File System**, plug in your credentials and parameters in the JSON file.\n2. **Create Datasets**\n    - Click **New dataset** then choose **Azure Data Lake**, plug in **Linked Service** name and **folder path** in the JSON file.\n    - Click **New dataset** then choose **On-premises file**, plug in **dataset schema**, **Linked Service** name, **file name**, and **folder path** in the JSON file.\n3. **Create Pipelines**\n    - Click **New pipeline**, specify the **input and output datasets**, **activity type**, and etc. in the JSON file. \n4. **Create Data GateWays**\n    - Register your gateway key and make sure the registration is **Registered** and status is **Started**.\n    - Click **New data gateway**, fill in **data gateway name **, the **Configure** part will setup automatically if your data gateway is installed properly in the previous steps.\n\n![Azure_Data_Gateway_part2_v2](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Gateway_part2_v2.png)\n\n![Azure_Data_Factory_Template](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Factory_Template.PNG)\n\n![Azure_Data_Factory_Template_Json_v2](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Factory_Template_Json_v2.PNG)\n\nThe JSON files used in the above steps are:\n\n**Linked services: AzureDataLakeStoreLinkedService**\n\n    {\n        \"name\": \"AzureDataLakeStoreLinkedService\",\n        \"properties\": {\n            \"description\": \"\",\n            \"hubName\": \"weigadf_hub\",\n            \"type\": \"AzureDataLakeStore\",\n            \"typeProperties\": {\n                \"dataLakeStoreUri\": \"https://cdspkona.azuredatalakestore.net/webhdfs/v1\",\n                \"authorization\": \"**********\",\n                \"sessionId\": \"**********\",\n                \"subscriptionId\": \"49bb74df-a9b8-4275-9439-198b33ae0f5f\",\n                \"resourceGroupName\": \"weiguodsvn\"\n            }\n        }\n    }\n\n\n**Linked services: OnPremisesFileServerLinkedService**\n\n    {\n        \"name\": \"OnPremisesFileServerLinkedService\",\n        \"properties\": {\n            \"description\": \"dsvm\",\n            \"hubName\": \"weigadf_hub\",\n            \"type\": \"OnPremisesFileServer\",\n            \"typeProperties\": {\n                \"host\": \"localhost\",\n                \"gatewayName\": \"weiggateway\",\n                \"userId\": \"weiguo\",\n                \"password\": \"**********\"\n            }\n        }\n    }\n\n\n**Datasets: OnPremisesFile**\n\n    {\n        \"name\": \"OnPremisesFile\",\n        \"properties\": {\n            \"published\": false,\n            \"type\": \"FileShare\",\n            \"linkedServiceName\": \"OnPremisesFileServerLinkedService\",\n            \"typeProperties\": {\n                \"folderPath\": \"\\\\\\\\dsvmjanc1ssd\\\\share\"\n            },\n            \"availability\": {\n                \"frequency\": \"Hour\",\n                \"interval\": 1\n            },\n            \"external\": true,\n            \"policy\": {}\n        }\n    }\n\n\n**Datasets: weiglakenew1**\n\n    {\n        \"name\": \"Datalakenew\",\n        \"properties\": {\n            \"structure\": [\n                {\n                    \"name\": \"medallion\",\n                    \"type\": \"String\"\n                },\n                {\n                    \"name\": \"hack_license\",\n                    \"type\": \"String\"\n                },\n                {\n                    \"name\": \"vendor_id\",\n                    \"type\": \"String\"\n                },\n                {\n                    \"name\": \"rate_code\",\n                    \"type\": \"String\"\n                },\n                {\n                    \"name\": \"store_and_fwd_flag\",\n                    \"type\": \"String\"\n                },\n                {\n                    \"name\": \"pickup_datetime\",\n                    \"type\": \"Datetime\"\n                },\n                {\n                    \"name\": \"dropoff_datetime\",\n                    \"type\": \"Datetime\"\n                },\n                {\n                    \"name\": \"passenger_count\",\n                    \"type\": \"Double\"\n                },\n                {\n                    \"name\": \"trip_time_in_secs\",\n                    \"type\": \"Decimal\"\n                },\n                {\n                    \"name\": \"trip_distance\",\n                    \"type\": \"Decimal\"\n                },\n                {\n                    \"name\": \"pickup_longitude\",\n                    \"type\": \"String\"\n                },\n                {\n                    \"name\": \"pickup_latitude\",\n                    \"type\": \"String\"\n                },\n                {\n                    \"name\": \"dropoff_longitude\",\n                    \"type\": \"String\"\n                },\n                {\n                    \"name\": \"dropoff_latitude\",\n                    \"type\": \"String\"\n                }\n            ],\n            \"published\": false,\n            \"type\": \"AzureDataLakeStore\",\n            \"linkedServiceName\": \"AzureDataLakeStoreLinkedService\",\n            \"typeProperties\": {\n                \"fileName\": \"UploadedFromVM_success.CSV\",\n                \"folderPath\": \"cdsp-data/nyctaxi_weig/\"\n            },\n            \"availability\": {\n                \"frequency\": \"Hour\",\n                \"interval\": 1\n            }\n        }\n    }\n\n\n**Datasets: vmtodatalake_tripdata**\n\n    {\n        \"name\": \"vmtodatalake_tripdata\",\n        \"properties\": {\n            \"description\": \"vmtodatalake\",\n            \"activities\": [\n                {\n                    \"type\": \"Copy\",\n                    \"typeProperties\": {\n                        \"source\": {\n                            \"type\": \"FileSystemSource\"\n                        },\n                        \"sink\": {\n                            \"type\": \"AzureDataLakeStoreSink\",\n                            \"writeBatchSize\": 0,\n                            \"writeBatchTimeout\": \"00:00:00\"\n                        }\n                    },\n                    \"inputs\": [\n                        {\n                            \"name\": \"OnPremisesFile\"\n                        }\n                    ],\n                    \"outputs\": [\n                        {\n                            \"name\": \"weiglakenew1\"\n                        }\n                    ],\n                    \"policy\": {\n                        \"timeout\": \"01:00:00\",\n                        \"concurrency\": 1\n                    },\n                    \"scheduler\": {\n                        \"frequency\": \"Hour\",\n                        \"interval\": 1\n                    },\n                    \"name\": \"AzureBlobtoDataLake\",\n                    \"description\": \"Copy Activity\"\n                }\n            ],\n            \"start\": \"2016-02-01T00:00:00Z\",\n            \"end\": \"2016-02-01T01:00:00Z\",\n            \"isPaused\": false,\n            \"hubName\": \"weigadf_hub\",\n            \"pipelineMode\": \"Scheduled\"\n        }\n    }\n\n\n**Data Gateways: weiggateway**\n\n    {\n        \"name\": \"VMtoLakegateway\",\n        \"properties\": {\n            \"description\": \"\",\n            \"hostServiceUri\": \"https://dsvmjanc1ssd:8050/HostServiceRemote.svc/\",\n            \"dataFactoryName\": \"weigadf\",\n            \"status\": \"Online\",\n            \"versionStatus\": \"UpToDate\",\n            \"version\": \"1.9.5865.2\",\n            \"registerTime\": \"2016-01-28T20:21:51.2375545Z\",\n            \"lastConnectTime\": \"2016-02-12T00:06:55.3445063Z\"\n        }\n    }\n\n\n\nAfter the pipeline is built, you can look at the pipeline in the **Diagram** in the Azure Data Factory dashboard.\n\n\n![](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Factory_Find_Diagram.PNG)\n\nClick the **Diagram** box to see the pipeline:\n\n![](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Factory_PipeLine_Diagram.PNG)\n\nYou can monitor the data pipeline in **Contents**->**Datasets**->**Monitoring** \n\n![](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Factory_Monitor_v2.PNG)\n\n\nYou may also check if the data is moved to Azure Data Lake using **Azure Data Lake Explorer** in **Visual Studio**.\n\n\n![](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Factory_Monitor_inVS.PNG)\n\n\n**Read data from Azure Blob to Data Lake: U-SQL**\n\nIf your data resides in Azure Blob storage, you can directly read data from Azure storage blob in U-SQL query. Before composing your U-SQL query, make sure your blob storage account is linked to your Azure Data Lake. Go to **Azure Portal**, find your Azure Data Lake Analytics dashboard, click **Add Data Source**, select storage type to **Azure Storage** and plug in your Azure Storage Account Name and Key. Then you will be able to reference the data stored in the storage account.\n\n![](./media/machine-learning-data-science-vm-do-ten-things/Link_Blob_to_ADLA_v2.PNG)\n\n\nIn Visual Studio, you can read data from blob, do some data manipulation, feature engineering, and output the resulting data to either Azure Data Lake or Azure Blob Storage. When you reference the data in blob storage, use **wasb://**; when you reference the data in Azure Data Lake, use **swbhdfs://** \n\n![](./media/machine-learning-data-science-vm-do-ten-things/USQL_Read_Blob_v2.PNG)\n\nYou may use the following U-SQL queries in Visual Studio:\n\n    @a =\n        EXTRACT medallion string,\n                hack_license string,\n                vendor_id string,\n                rate_code string,\n                store_and_fwd_flag string,\n                pickup_datetime string,\n                dropoff_datetime string,\n                passenger_count int,\n                trip_time_in_secs double,\n                trip_distance double,\n                pickup_longitude string,\n                pickup_latitude string,\n                dropoff_longitude string,\n                dropoff_latitude string\n    \n        FROM \"wasb://<Container name>@<Azure Blob Storage Account Name>.blob.core.windows.net/<Input Data File Name>\"\n        USING Extractors.Csv();\n    \n    @b = \n        SELECT vendor_id, \n        COUNT(medallion) AS cnt_medallion,\n        SUM(passenger_count) AS cnt_passenger,\n        AVG(trip_distance) AS avg_trip_dist,\n        MIN(trip_distance) AS min_trip_dist,\n        MAX(trip_distance) AS max_trip_dist,\n        AVG(trip_time_in_secs) AS avg_trip_time\n        FROM @a\n        GROUP BY vendor_id;\n    \n    OUTPUT @b   \n    TO \"swebhdfs://<Azure Data Lake Storage Account Name>.azuredatalakestore.net/<Folder Name>/<Output Data File Name>\"\n    USING Outputters.Csv();\n    \n    OUTPUT @b   \n    TO \"wasb://<Container name>@<Azure Blob Storage Account Name>.blob.core.windows.net/<Output Data File Name>\"\n    USING Outputters.Csv();\n    \n\n\nAfter your query is submitted to the server, a diagram showing the status of your job will be displayed.\n\n![](./media/machine-learning-data-science-vm-do-ten-things/USQL_Job_Status.PNG)\n\n\n**Query data in Data Lake: U-SQL**\n\nAfter the dataset is ingested into Azure Data Lake, you can use [U-SQL language](../data-lake-analytics/data-lake-analytics-u-sql-get-started.md) to query and explore the data. U-SQL language is similar to T-SQL, but combines some features from C# so that users can write customized modules, User Defined Functions, and etc. You can use the scripts in the previous step. \n\nAfter the query is submitted to server, tripdata_summary.CSV can be found shortly in **Azure Data Lake Explorer**, you may preview the data by right click the file.\n\n![](./media/machine-learning-data-science-vm-do-ten-things/USQL_create_summary.png)\n\nTo see the file information:\n\n![](./media/machine-learning-data-science-vm-do-ten-things/USQL_tripdata_summary.png)\n\n\n### HDInsight Hadoop Clusters\n\n**Prerequisite**\n\n- Create your Azure Blob storage account from [Azure Portal](http://portal.azure.com). This storage account is used to store data for HDInsight clusters.\n\n![](./media/machine-learning-data-science-vm-do-ten-things/Create_Azure_Blob.PNG)\n\n- Customize Azure HDInsight Hadoop Clusters from [Azure Portal](machine-learning-data-science-customize-hadoop-cluster.md)\n\n    - You must link the storage account created with your HDInsight cluster when it is created. This storage account is used for accessing data that can be processed within the cluster.\n    \n![](./media/machine-learning-data-science-vm-do-ten-things/Create_HDI_v4.PNG)\n\n    - You must enable **Remote Access** to the head node of the cluster after it is created. Remember the remote access credentials you specify here (different from those specified for the cluster at its creation): you will need them below.\n\n![](./media/machine-learning-data-science-vm-do-ten-things/Create_HDI_dashboard_v3.PNG)\n\n    - Create an Azure ML workspace. Your Machine Learning Experiments will be stored in this ML workspace.\n\n![](./media/machine-learning-data-science-vm-do-ten-things/Create_ML_Space.PNG)\n\n\n    - Then select the Remote Desktop:\n\n![](./media/machine-learning-data-science-vm-do-ten-things/Create_ML_Space_step2_v2.PNG)\n\n    - Upload data using IPython Notebook. First import required packages, plug in credentials, create a db in your storage account, then load data to HDI clusters. \n\n\n        #Import required Packages\n        import pyodbc\n        import time as time\n        import json\n        import os\n        import urllib\n        import urllib2\n        import warnings\n        import re\n        import pandas as pd\n        import matplotlib.pyplot as plt\n        from azure.storage.blob import BlobService\n        warnings.filterwarnings(\"ignore\", category=UserWarning, module='urllib2')\n\n\n        #Create the connection to Hive using ODBC\n        SERVER_NAME='xxx.azurehdinsight.net'\n        DATABASE_NAME='nyctaxidb'\n        USERID='xxx'\n        PASSWORD='xxxx'\n        DB_DRIVER='Microsoft Hive ODBC Driver'\n        driver = 'DRIVER={' + DB_DRIVER + '}'\n        server = 'Host=' + SERVER_NAME + ';Port=443'\n        database = 'Schema=' + DATABASE_NAME\n        hiveserv = 'HiveServerType=2'\n        auth = 'AuthMech=6'\n        uid = 'UID=' + USERID \n        pwd = 'PWD=' + PASSWORD\n        CONNECTION_STRING = ';'.join([driver,server,database,hiveserv,auth,uid,pwd])\n        connection = pyodbc.connect(CONNECTION_STRING, autocommit=True)\n        cursor=connection.cursor()\n\n\n        #Create Hive database and tables\n        queryString = \"create database if not exists nyctaxidb;\"\n        cursor.execute(queryString)\n        \n        queryString = \"\"\"\n                        create external table if not exists nyctaxidb.trip\n                        ( \n                            medallion string, \n                            hack_license string,\n                            vendor_id string, \n                            rate_code string, \n                            store_and_fwd_flag string, \n                            pickup_datetime string, \n                            dropoff_datetime string, \n                            passenger_count int, \n                            trip_time_in_secs double, \n                            trip_distance double, \n                            pickup_longitude double, \n                            pickup_latitude double, \n                            dropoff_longitude double, \n                            dropoff_latitude double)  \n                        PARTITIONED BY (month int) \n                        ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' lines terminated by '\\\\n'\n                        STORED AS TEXTFILE LOCATION 'wasb:///nyctaxidbdata/trip' TBLPROPERTIES('skip.header.line.count'='1');\n                    \"\"\"\n        cursor.execute(queryString)\n        \n        queryString = \"\"\"\n                        create external table if not exists nyctaxidb.fare \n                        ( \n                            medallion string, \n                            hack_license string, \n                            vendor_id string, \n                            pickup_datetime string, \n                            payment_type string, \n                            fare_amount double, \n                            surcharge double,\n                            mta_tax double,\n                            tip_amount double,\n                            tolls_amount double,\n                            total_amount double)\n                        PARTITIONED BY (month int) \n                        ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' lines terminated by '\\\\n'\n                        STORED AS TEXTFILE LOCATION 'wasb:///nyctaxidbdata/fare' TBLPROPERTIES('skip.header.line.count'='1');\n                    \"\"\"\n        cursor.execute(queryString)\n    \n    \n        #Upload data from blob storage to HDI cluster\n        for i in range(1,13):\n            queryString = \"LOAD DATA INPATH 'wasb:///nyctaxitripraw2/trip_data_%d.csv' INTO TABLE nyctaxidb2.trip PARTITION (month=%d);\"%(i,i)\n            cursor.execute(queryString)\n            queryString = \"LOAD DATA INPATH 'wasb:///nyctaxifareraw2/trip_fare_%d.csv' INTO TABLE nyctaxidb2.fare PARTITION (month=%d);\"%(i,i)  \n            cursor.execute(queryString)\n\n\n- Or you can follow this [walkthrough](machine-learning-data-science-process-hive-walkthrough.md) to upload NYC Taxi data to HDI cluster. Major steps include:\n\n    - AzCopy: download zipped CSV's from public blob to your local folder\n    - AzCopy: upload unzipped CSV's from local folder to HDI cluster\n    - Log into the head node of Hadoop cluster and prepare for exploratory data analysis\n\nAfter the data is loaded to HDI cluster, you can check your data in Azure Storage Explore. And you have a database nyctaxidb created in HDI cluster.\n\n![](./media/machine-learning-data-science-vm-do-ten-things/Upload_Data_to_HDI_cluster_Azure_Explorer.PNG)\n\n\n**Data exploration: Hive Queries in Python**\n\nSince the data is in Hadoop cluster, you can use pyodbc package to connect to Hadoop Clusters and query database using Hive to do exploration and feature engineering. You can view the existing tables we created in the prerequisite step.\n\n    queryString = \"\"\"\n        show tables in nyctaxidb2;\n        \"\"\"\n    pd.read_sql(queryString,connection)\n\n\n![](./media/machine-learning-data-science-vm-do-ten-things/Python_View_Existing_Tables_Hive_v3.PNG)\n\nLet's look at the number of records in each month and the frequencies of tipped or not in the trip table:\n\n    queryString = \"\"\"\n        select month, count(*) from nyctaxidb.trip group by month;\n        \"\"\"\n    results = pd.read_sql(queryString,connection)\n    \n    %matplotlib inline\n    \n    results.columns = ['month', 'trip_count']\n    df = results.copy()\n    df.index = df['month']\n    df['trip_count'].plot(kind='bar')\n\n\n![](./media/machine-learning-data-science-vm-do-ten-things/Exploration_Number_Records_by_Month_v3.PNG)\n\n\n    queryString = \"\"\"\n        SELECT tipped, COUNT(*) AS tip_freq \n        FROM \n        (\n            SELECT if(tip_amount > 0, 1, 0) as tipped, tip_amount\n            FROM nyctaxidb.fare\n        )tc\n        GROUP BY tipped;\n        \"\"\"\n    results = pd.read_sql(queryString,connection)\n    \n    results.columns = ['tipped', 'trip_count']\n    df = results.copy()\n    df.index = df['tipped']\n    df['trip_count'].plot(kind='bar')\n\n\n![](./media/machine-learning-data-science-vm-do-ten-things/Exploration_Frequency_tip_or_not_v3.PNG)\n\nWe can also compute the distance between pickup location and dropoff location and then compare it to the trip distance.\n\n    queryString = \"\"\"\n                    select pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude, trip_distance, trip_time_in_secs,\n                        3959*2*2*atan((1-sqrt(1-pow(sin((dropoff_latitude-pickup_latitude)\n                        *radians(180)/180/2),2)-cos(pickup_latitude*radians(180)/180)\n                        *cos(dropoff_latitude*radians(180)/180)*pow(sin((dropoff_longitude-pickup_longitude)*radians(180)/180/2),2)))\n                        /sqrt(pow(sin((dropoff_latitude-pickup_latitude)*radians(180)/180/2),2)\n                        +cos(pickup_latitude*radians(180)/180)*cos(dropoff_latitude*radians(180)/180)*\n                        pow(sin((dropoff_longitude-pickup_longitude)*radians(180)/180/2),2))) as direct_distance \n                        from nyctaxidb.trip \n                        where month=1 \n                            and pickup_longitude between -90 and -30\n                            and pickup_latitude between 30 and 90\n                            and dropoff_longitude between -90 and -30\n                            and dropoff_latitude between 30 and 90;\n                \"\"\"\n    results = pd.read_sql(queryString,connection)\n    results.head(5)\n\n\n![](./media/machine-learning-data-science-vm-do-ten-things/Exploration_compute_pickup_dropoff_distance_v2.PNG)\n\n    results.columns = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', \n                       'dropoff_latitude', 'trip_distance', 'trip_time_in_secs', 'direct_distance']\n    df = results.loc[results['trip_distance']<=100] #remove outliers\n    df = df.loc[df['direct_distance']<=100] #remove outliers\n    plt.scatter(df['direct_distance'], df['trip_distance'])\n\n\n![](./media/machine-learning-data-science-vm-do-ten-things/Exploration_direct_distance_trip_distance_v2.PNG)\n\n\nNow let's prepare a downsampled (1%) data for modeling. We can use this data in AML reader module. \n\n\n        queryString = \"\"\"\n        create  table if not exists nyctaxi_downsampled_dataset_testNEW (\n        medallion string,\n        hack_license string,\n        vendor_id string,\n        rate_code string,\n        store_and_fwd_flag string,\n        pickup_datetime string,\n        dropoff_datetime string,\n        pickup_hour string,\n        pickup_week string,\n        weekday string,\n        passenger_count int,\n        trip_time_in_secs double,\n        trip_distance double,\n        pickup_longitude double,\n        pickup_latitude double,\n        dropoff_longitude double,\n        dropoff_latitude double,\n        direct_distance double,\n        payment_type string,\n        fare_amount double,\n        surcharge double,\n        mta_tax double,\n        tip_amount double,\n        tolls_amount double,\n        total_amount double,\n        tipped string,\n        tip_class string\n        )\n        row format delimited fields terminated by ','\n        lines terminated by '\\\\n'\n        stored as textfile;\n        \"\"\"\n        cursor.execute(queryString)\n\n        --- now insert contents of the join into the above internal table\n\n        queryString = \"\"\"\n        insert overwrite table nyctaxi_downsampled_dataset_testNEW\n        select\n        t.medallion,\n        t.hack_license,\n        t.vendor_id,\n        t.rate_code,\n        t.store_and_fwd_flag,\n        t.pickup_datetime,\n        t.dropoff_datetime,\n        hour(t.pickup_datetime) as pickup_hour,\n        weekofyear(t.pickup_datetime) as pickup_week,\n        from_unixtime(unix_timestamp(t.pickup_datetime, 'yyyy-MM-dd HH:mm:ss'),'u') as weekday,\n        t.passenger_count,\n        t.trip_time_in_secs,\n        t.trip_distance,\n        t.pickup_longitude,\n        t.pickup_latitude,\n        t.dropoff_longitude,\n        t.dropoff_latitude,\n        t.direct_distance,\n        f.payment_type,\n        f.fare_amount,\n        f.surcharge,\n        f.mta_tax,\n        f.tip_amount,\n        f.tolls_amount,\n        f.total_amount,\n        if(tip_amount>0,1,0) as tipped,\n        if(tip_amount=0,0,\n        if(tip_amount>0 and tip_amount<=5,1,\n        if(tip_amount>5 and tip_amount<=10,2,\n        if(tip_amount>10 and tip_amount<=20,3,4)))) as tip_class\n        from\n        (\n        select\n        medallion,\n        hack_license,\n        vendor_id,\n        rate_code,\n        store_and_fwd_flag,\n        pickup_datetime,\n        dropoff_datetime,\n        passenger_count,\n        trip_time_in_secs,\n        trip_distance,\n        pickup_longitude,\n        pickup_latitude,\n        dropoff_longitude,\n        dropoff_latitude,\n        3959*2*2*atan((1-sqrt(1-pow(sin((dropoff_latitude-pickup_latitude)\n        radians(180)/180/2),2)-cos(pickup_latitude*radians(180)/180)\n        *cos(dropoff_latitude*radians(180)/180)*pow(sin((dropoff_longitude-pickup_longitude)*radians(180)/180/2),2)))\n        /sqrt(pow(sin((dropoff_latitude-pickup_latitude)*radians(180)/180/2),2)\n        +cos(pickup_latitude*radians(180)/180)*cos(dropoff_latitude*radians(180)/180)*pow(sin((dropoff_longitude-pickup_longitude)*radians(180)/180/2),2))) as direct_distance,\n        rand() as sample_key\n    \n        from trip\n        where pickup_latitude between 30 and 90\n            and pickup_longitude between -90 and -30\n            and dropoff_latitude between 30 and 90\n            and dropoff_longitude between -90 and -30\n        )t\n        join\n        (\n        select\n        medallion,\n        hack_license,\n        vendor_id,\n        pickup_datetime,\n        payment_type,\n        fare_amount,\n        surcharge,\n        mta_tax,\n        tip_amount,\n        tolls_amount,\n        total_amount\n        from fare\n        )f\n        on t.medallion=f.medallion and t.hack_license=f.hack_license and t.pickup_datetime=f.pickup_datetime\n        where t.sample_key<=0.01\n        \"\"\"\n        cursor.execute(queryString)\n\nAfter a while, you can see the data has been loaded in Hadoop clusters:\n        \n    queryString = \"\"\"\n        select * from nyctaxi_downsampled_dataset limit 10;\n        \"\"\"\n    cursor.execute(queryString)\n    pd.read_sql(queryString,connection)\n\n\n![](./media/machine-learning-data-science-vm-do-ten-things/DownSample_Data_For_Modeling_v2.PNG)\n\n\n**Read data from HDI using AML: reader module**\n\nYou may also use the **reader** module in AML studio to access the database in Hadoop cluster. Plug in the credentials of your HDI clusters and Azure Storage Account and you will be able to build machine learning models using database in HDI clusters. \n\n![](./media/machine-learning-data-science-vm-do-ten-things/AML_Reader_Hive.PNG)\n\nThe scored dataset can then be viewed:\n\n![](./media/machine-learning-data-science-vm-do-ten-things/AML_Model_Results.PNG)\n\n\n### Azure SQL Data Warehouse & databases\n\n#### Azure DocumentDB\n\nAzure DocumentDB is a NoSQL database in the cloud. It allows you to work with documents like JSON and allow you to store and query the documents. \n\nYou need to do the following per-requisites steps to access DocumentDB from the DSVM.\n\n1. Install DocumentDB Python SDK (Run ```pip install pydocumentdb``` from command prompt)\n1. Create DocumentDB account and Document DB database from [Azure portal](https://portal.azure.com)\n1. Download \"DocumentDB Migration tool\" from [here](http://www.microsoft.com/downloads/details.aspx?FamilyID=cda7703a-2774-4c07-adcc-ad02ddc1a44d) and extract to a directory of your choice\n1. Import JSON data (volcano data) stored on a [public blob](https://cahandson.blob.core.windows.net/samples/volcano.json) into DocumentDB with following command parameters to the migration tool (dtui.exe from the directory where you installed the DocumentDB migration tool). Enter the source and target location parameters from below. \n\n    /s:JsonFile /s.Files:https://cahandson.blob.core.windows.net/samples/volcano.json /t:DocumentDBBulk /t.ConnectionString:AccountEndpoint=https://[DocDBAccountName].documents.azure.com:443/;AccountKey=[[KEY];Database=volcano /t.Collection:volcano1\n\nOnce you import the data, you can go to Jupyter and open the notebook titled ```DocumentDBSample``` which contains python code to access DocumentDB and do some basic querying. You can learn more about DocumentDB by visiting the service [documentation page](https://azure.microsoft.com/documentation/learning-paths/documentdb/)\n\n\n## 8. Build reports and dashboard using the Power BI Desktop \n\nLet us visualize the Volcano JSON file we saw in the DocumentDB example above in Power BI to gain visual insights into the data. Detailed steps are found in the [Power BI article](../documentdb/documentdb-powerbi-visualize.md). The high level steps are below :\n\n1. Open Power BI Desktop and do \"Get Data\". Specify the URL as: https://cahandson.blob.core.windows.net/samples/volcano.json\n2. You will see the JSON records imported as a list\n3. Next convert the list to a table so PowerBI can work with the same\n4. Then you expand the columns by clicking on the expand icon (the one with the \"left arrow and a right arrow\" icon on the right of the column)\n5. You will see that location is a \"Record\" field. Expand the record and select only the coordinates. Corordinate is a list column\n6. Next you will add a new column to convert the list coordinate column into a comma separate LatLong column contatenating the two elements in the coordinate list field using the formula ```Text.From([coordinates]{1})&\",\"&Text.From([coordinates]{0})```. \n7. Finally convert the ```Elevation``` column to Decimal and hit the Close and Apply.\n\nInstead of steps above, you can paste the following code that scripts out the steps above in the Advanced Editor in PowerBI that allows you to write the data transformations in a query language. \n\n\n    let\n        Source = Json.Document(Web.Contents(\"https://cahandson.blob.core.windows.net/samples/volcano.json\")),\n        #\"Converted to Table\" = Table.FromList(Source, Splitter.SplitByNothing(), null, null, ExtraValues.Error),\n        #\"Expanded Column1\" = Table.ExpandRecordColumn(#\"Converted to Table\", \"Column1\", {\"Volcano Name\", \"Country\", \"Region\", \"Location\", \"Elevation\", \"Type\", \"Status\", \"Last Known Eruption\", \"id\"}, {\"Volcano Name\", \"Country\", \"Region\", \"Location\", \"Elevation\", \"Type\", \"Status\", \"Last Known Eruption\", \"id\"}),\n        #\"Expanded Location\" = Table.ExpandRecordColumn(#\"Expanded Column1\", \"Location\", {\"coordinates\"}, {\"coordinates\"}),\n        #\"Added Custom\" = Table.AddColumn(#\"Expanded Location\", \"LatLong\", each Text.From([coordinates]{1})&\",\"&Text.From([coordinates]{0})),\n        #\"Changed Type\" = Table.TransformColumnTypes(#\"Added Custom\",{{\"Elevation\", type number}})\n    in\n        #\"Changed Type\"\n        \n\n\nYou now have the data in your Power BI data model. Your Power BI desktop should look as shown below. \n\n![](./media/machine-learning-data-science-vm-do-ten-things/PowerBIVolcanoData.png)\n\nYou can start building reports and visualizations using the data model. You can follow the steps in this [Power BI article](../documentdb/documentdb-powerbi-visualize.md#build-the-reports) to build a report. The end result will be a report that looks like the following.\n\n\nTBD: Volcano Map Report image URL missing - not correct.\n\n## 9. Dynamically scale your DSVM to meet your project needs\n\nYou can scale up and down the DSVM to meet your project needs. If you dont need to use the VM in the evening or weekends, you can just shutdown the VM from the [Azure Portal](https://portal.azure.com). NOTE:  that you will incur compute charges if you use just the Operating system shutdown button on the VM.  \n\nIf you need to handle some large scale analysis and need more CPU and/or memory and/or disk capacity you can find a large choice of VM sizes in terms of CPU cores, memory capacity and disk types (including Solid state drives) that meet your compute  and budgetary needs. The full list of VMs along with their hourly compute pricing is available on the [Azure Virtual Machines Pricing](https://azure.microsoft.com/pricing/details/virtual-machines/) page. \n\nSimilarly, if your needs for VM processing capacity reduces (for example: you moved a major workload to a Hadoop or a Spark cluster), you can scale down the cluster from the [Azure Portal](https://portal.azure.com) and going to the settings of your VM instance. Here is a screenshot.\n\n\n![](./media/machine-learning-data-science-vm-do-ten-things/VMScaling.PNG)\n\n\n## 10. Install additional tools on your virtual machine\n\nWe have packaged several tools that we believe will be able to address many of the common data analytics needs and save you time by avoiding  installing and configuring your environment one by one and paying for only what you use. You can leverage other Azure data and analytics services as seen earlier in this article to enhance your analytics environment. We understand that in some cases your needs may require additional tools including some proprietary third party tools. You have full administrative access on the virtual machine to install new tools you need. You can also install additional packages in Python and R that are not pre-installed. For Python  you can use either ```conda``` or ```pip```. For R you can use the ```install.packages()``` in the R console or use the IDE and choose \"Packages -> Install Packages...\". \n\nThis is just a few things you can do on the Microsoft Data Science Virtual Machine. There are many more things you can do to make it an effective analytics environment."
}