{
  "nodes": [
    {
      "pos": [
        26,
        101
      ],
      "content": "Analyze and Process JSON documents with Hive in HDInsight | Microsoft Azure"
    },
    {
      "pos": [
        119,
        192
      ],
      "content": "Learn how to use JSON documents and analyze them using Hive in HDInsight."
    },
    {
      "pos": [
        484,
        542
      ],
      "content": "Process and analyze JSON documents using Hive in HDInsight"
    },
    {
      "pos": [
        544,
        669
      ],
      "content": "Learn how to process and analyze JSON files using Hive in HDInsight. The following JSON document will be used in the tutorial",
      "nodes": [
        {
          "content": "Learn how to process and analyze JSON files using Hive in HDInsight.",
          "pos": [
            0,
            68
          ]
        },
        {
          "content": "The following JSON document will be used in the tutorial",
          "pos": [
            69,
            125
          ]
        }
      ]
    },
    {
      "pos": [
        1699,
        2032
      ],
      "content": "The file can be found at wasb://processjson@hditutorialdata.blob.core.windows.net/. For more information on using Azure Blob storage with HDInsight, see <bpt id=\"p1\">[</bpt>Use HDFS-compatible Azure Blob storage with Hadoop in HDInsight<ept id=\"p1\">](hdinsight-hadoop-use-blob-storage.md)</ept>. You can copy the file to the default container of your cluster if you want.",
      "nodes": [
        {
          "content": "The file can be found at wasb://processjson@hditutorialdata.blob.core.windows.net/.",
          "pos": [
            0,
            83
          ]
        },
        {
          "content": "For more information on using Azure Blob storage with HDInsight, see <bpt id=\"p1\">[</bpt>Use HDFS-compatible Azure Blob storage with Hadoop in HDInsight<ept id=\"p1\">](hdinsight-hadoop-use-blob-storage.md)</ept>.",
          "pos": [
            84,
            295
          ]
        },
        {
          "content": "You can copy the file to the default container of your cluster if you want.",
          "pos": [
            296,
            371
          ]
        }
      ]
    },
    {
      "pos": [
        2034,
        2235
      ],
      "content": "In this tutorial, you will use the Hive console.  For instructions of opening the Hive console, see <bpt id=\"p2\">[</bpt>Use Hive with Hadoop on HDInsight with Remote Desktop<ept id=\"p2\">](hdinsight-hadoop-use-hive-remote-desktop.md)</ept>.",
      "nodes": [
        {
          "content": "In this tutorial, you will use the Hive console.",
          "pos": [
            0,
            48
          ]
        },
        {
          "content": "For instructions of opening the Hive console, see <bpt id=\"p2\">[</bpt>Use Hive with Hadoop on HDInsight with Remote Desktop<ept id=\"p2\">](hdinsight-hadoop-use-hive-remote-desktop.md)</ept>.",
          "pos": [
            50,
            239
          ]
        }
      ]
    },
    {
      "pos": [
        2239,
        2261
      ],
      "content": "Flatten JSON documents"
    },
    {
      "pos": [
        2263,
        2522
      ],
      "content": "The methods listed in the next section require the JSON document in a single row. So you must flatten the JSON document to a string. If your JSON document is already flattened, you can skip this step and go straight to the next section on Analyzing JSON data.",
      "nodes": [
        {
          "content": "The methods listed in the next section require the JSON document in a single row.",
          "pos": [
            0,
            81
          ]
        },
        {
          "content": "So you must flatten the JSON document to a string.",
          "pos": [
            82,
            132
          ]
        },
        {
          "content": "If your JSON document is already flattened, you can skip this step and go straight to the next section on Analyzing JSON data.",
          "pos": [
            133,
            259
          ]
        }
      ]
    },
    {
      "pos": [
        3239,
        3408
      ],
      "content": "The raw JSON file is located at <bpt id=\"p3\">**</bpt>wasb://processjson@hditutorialdata.blob.core.windows.net/<ept id=\"p3\">**</ept>. The <bpt id=\"p4\">*</bpt>StudentsRaw<ept id=\"p4\">*</ept><ph id=\"ph2\"/> Hive table points to the raw un-flattened JSON document.",
      "nodes": [
        {
          "content": "The raw JSON file is located at <bpt id=\"p3\">**</bpt>wasb://processjson@hditutorialdata.blob.core.windows.net/<ept id=\"p3\">**</ept>.",
          "pos": [
            0,
            132
          ]
        },
        {
          "content": "The <bpt id=\"p4\">*</bpt>StudentsRaw<ept id=\"p4\">*</ept><ph id=\"ph2\"/> Hive table points to the raw un-flattened JSON document.",
          "pos": [
            133,
            259
          ]
        }
      ]
    },
    {
      "pos": [
        3410,
        3533
      ],
      "content": "The <bpt id=\"p5\">*</bpt>StudentsOneLine<ept id=\"p5\">*</ept><ph id=\"ph3\"/> Hive table will store the data in the HDInsight default file system under the <bpt id=\"p6\">*</bpt>/json/students/<ept id=\"p6\">*</ept><ph id=\"ph4\"/> path."
    },
    {
      "pos": [
        3535,
        3619
      ],
      "content": "The INSERT statement populate the StudentOneLine table with the flattened JSON data."
    },
    {
      "pos": [
        3621,
        3666
      ],
      "content": "The SELECT statement shall only return 1 row."
    },
    {
      "pos": [
        3668,
        3711
      ],
      "content": "Here is the output of the SELECT statement:"
    },
    {
      "pos": [
        3713,
        3776
      ],
      "content": "<ph id=\"ph5\">![</ph>Flattening of the JSON document.<ph id=\"ph6\">][image-hdi-hivejson-flatten]</ph>"
    },
    {
      "pos": [
        3780,
        3810
      ],
      "content": "Analyze JSON documents in Hive"
    },
    {
      "pos": [
        3812,
        3886
      ],
      "content": "Hive provides three different mechanisms to run queries on JSON documents:"
    },
    {
      "pos": [
        3890,
        3943
      ],
      "content": "use the GET\\_JSON\\_OBJECT UDF (User Defined Function)"
    },
    {
      "pos": [
        3946,
        3968
      ],
      "content": "use the JSON_TUPLE UDF"
    },
    {
      "pos": [
        3971,
        3987
      ],
      "content": "use custom SerDe"
    },
    {
      "pos": [
        3990,
        4120
      ],
      "content": "write you own UDF using Python or other languages. See <bpt id=\"p7\">[</bpt>this article<ept id=\"p7\">][hdinsight-python]</ept><ph id=\"ph7\"/> on running your own Python code with Hive.",
      "nodes": [
        {
          "content": "write you own UDF using Python or other languages.",
          "pos": [
            0,
            50
          ]
        },
        {
          "content": "See <bpt id=\"p7\">[</bpt>this article<ept id=\"p7\">][hdinsight-python]</ept><ph id=\"ph7\"/> on running your own Python code with Hive.",
          "pos": [
            51,
            182
          ]
        }
      ]
    },
    {
      "pos": [
        4127,
        4155
      ],
      "content": "Use the GET\\_JSON_OBJECT UDF"
    },
    {
      "pos": [
        4156,
        4560
      ],
      "content": "Hive provides a built-in UDF called <bpt id=\"p8\">[</bpt>get json object<ept id=\"p8\">](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-get_json_object)</ept><ph id=\"ph8\"/> which can perform JSON querying during run time. This method takes two arguments – the table name and method name which has the flattened JSON document and the JSON field that needs to be parsed. Let’s look at an example to see how this UDF works.",
      "nodes": [
        {
          "content": "Hive provides a built-in UDF called <bpt id=\"p8\">[</bpt>get json object<ept id=\"p8\">](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-get_json_object)</ept><ph id=\"ph8\"/> which can perform JSON querying during run time.",
          "pos": [
            0,
            257
          ]
        },
        {
          "content": "This method takes two arguments – the table name and method name which has the flattened JSON document and the JSON field that needs to be parsed.",
          "pos": [
            258,
            404
          ]
        },
        {
          "content": "Let’s look at an example to see how this UDF works.",
          "pos": [
            405,
            456
          ]
        }
      ]
    },
    {
      "pos": [
        4562,
        4611
      ],
      "content": "Get the first name and last name for each student"
    },
    {
      "pos": [
        4810,
        4871
      ],
      "content": "Here is the output when running this query in console window."
    },
    {
      "pos": [
        4873,
        4929
      ],
      "content": "<ph id=\"ph9\">![</ph>get_json_object UDF<ph id=\"ph10\">][image-hdi-hivejson-getjsonobject]</ph>"
    },
    {
      "pos": [
        4931,
        4986
      ],
      "content": "There are a few limitations of the get-json_object UDF."
    },
    {
      "pos": [
        4991,
        5081
      ],
      "content": "Because each field in the query requires re-parsing the query, it affects the performance."
    },
    {
      "pos": [
        5084,
        5309
      ],
      "content": "GET\\_JSON_OBJECT() returns the string representation of an array. To convert this to a Hive array, you will have to use regular expressions to replace the square brackets ‘[‘ and ‘]’ and then also call split to get the array.",
      "nodes": [
        {
          "content": "GET\\_JSON_OBJECT() returns the string representation of an array.",
          "pos": [
            0,
            65
          ]
        },
        {
          "content": "To convert this to a Hive array, you will have to use regular expressions to replace the square brackets ‘[‘ and ‘]’ and then also call split to get the array.",
          "pos": [
            66,
            225
          ]
        }
      ]
    },
    {
      "pos": [
        5312,
        5366
      ],
      "content": "This is why the Hive wiki recommends using json_tuple."
    },
    {
      "pos": [
        5374,
        5396
      ],
      "content": "Use the JSON_TUPLE UDF"
    },
    {
      "pos": [
        5398,
        5880
      ],
      "content": "Another UDF provided by Hive is called <bpt id=\"p9\">[</bpt>json_tuple<ept id=\"p9\">](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-json_tuple)</ept><ph id=\"ph11\"/> which performs better than <bpt id=\"p10\">[</bpt>get_ json _object<ept id=\"p10\">](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-get_json_object)</ept>. This method takes a set of keys and a JSON string, and returns a tuple of values using one function. The following query returns the student id and the grade from the JSON document:",
      "nodes": [
        {
          "content": "Another UDF provided by Hive is called <bpt id=\"p9\">[</bpt>json_tuple<ept id=\"p9\">](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-json_tuple)</ept><ph id=\"ph11\"/> which performs better than <bpt id=\"p10\">[</bpt>get_ json _object<ept id=\"p10\">](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-get_json_object)</ept>.",
          "pos": [
            0,
            393
          ]
        },
        {
          "content": "This method takes a set of keys and a JSON string, and returns a tuple of values using one function.",
          "pos": [
            394,
            494
          ]
        },
        {
          "content": "The following query returns the student id and the grade from the JSON document:",
          "pos": [
            495,
            575
          ]
        }
      ]
    },
    {
      "pos": [
        6046,
        6092
      ],
      "content": "The output of this script in the Hive console:"
    },
    {
      "pos": [
        6094,
        6141
      ],
      "content": "<ph id=\"ph12\">![</ph>json_tuple UDF<ph id=\"ph13\">][image-hdi-hivejson-jsontuple]</ph>"
    },
    {
      "pos": [
        6143,
        6517
      ],
      "content": "JSON\\_TUPLE uses the <bpt id=\"p11\">[</bpt>lateral view<ept id=\"p11\">](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+LateralView)</ept><ph id=\"ph14\"/> syntax in Hive which allows json\\_tuple to create a virtual table by applying the UDT function to each row of the original table.  Complex JSONs become too unwieldy because of the repeated use of LATERAL VIEW. Furthermore, JSON_TUPLE cannot handle nested JSONs.",
      "nodes": [
        {
          "content": "JSON\\_TUPLE uses the <bpt id=\"p11\">[</bpt>lateral view<ept id=\"p11\">](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+LateralView)</ept><ph id=\"ph14\"/> syntax in Hive which allows json\\_tuple to create a virtual table by applying the UDT function to each row of the original table.",
          "pos": [
            0,
            297
          ]
        },
        {
          "content": "Complex JSONs become too unwieldy because of the repeated use of LATERAL VIEW.",
          "pos": [
            299,
            377
          ]
        },
        {
          "content": "Furthermore, JSON_TUPLE cannot handle nested JSONs.",
          "pos": [
            378,
            429
          ]
        }
      ]
    },
    {
      "pos": [
        6523,
        6539
      ],
      "content": "Use custom SerDe"
    },
    {
      "pos": [
        6541,
        6811
      ],
      "content": "SerDe is the best choice for parsing nested JSON documents, it allows you to define the JSON schema, and use the schema to parse the documents. In this tutorial, you will use one of the more popular SerDe that has been developed by <bpt id=\"p12\">[</bpt>rcongiu<ept id=\"p12\">](https://github.com/rcongiu)</ept>.",
      "nodes": [
        {
          "content": "SerDe is the best choice for parsing nested JSON documents, it allows you to define the JSON schema, and use the schema to parse the documents.",
          "pos": [
            0,
            143
          ]
        },
        {
          "content": "In this tutorial, you will use one of the more popular SerDe that has been developed by <bpt id=\"p12\">[</bpt>rcongiu<ept id=\"p12\">](https://github.com/rcongiu)</ept>.",
          "pos": [
            144,
            310
          ]
        }
      ]
    },
    {
      "pos": [
        6813,
        6841
      ],
      "content": "<bpt id=\"p13\">**</bpt>To use the custom SerDe:<ept id=\"p13\">**</ept>"
    },
    {
      "pos": [
        6846,
        7122
      ],
      "content": "Install <bpt id=\"p14\">[</bpt>Java SE Development Kit 7u55 JDK 1.7.0_55<ept id=\"p14\">](http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html#jdk-7u55-oth-JPR)</ept>. Choose the Windows X64 version of the JDK if you are going to be using the Windows deployment of HDInsight",
      "nodes": [
        {
          "content": "Install <bpt id=\"p14\">[</bpt>Java SE Development Kit 7u55 JDK 1.7.0_55<ept id=\"p14\">](http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html#jdk-7u55-oth-JPR)</ept>.",
          "pos": [
            0,
            209
          ]
        },
        {
          "content": "Choose the Windows X64 version of the JDK if you are going to be using the Windows deployment of HDInsight",
          "pos": [
            210,
            316
          ]
        }
      ]
    },
    {
      "pos": [
        7129,
        7182
      ],
      "content": "<ph id=\"ph15\">[AZURE.WARNING]</ph><ph id=\"ph16\"/> JDK 1.8 doesn't work with this SerDe."
    },
    {
      "pos": [
        7189,
        7262
      ],
      "content": "After the installation is completed, add a new user environment variable:"
    },
    {
      "pos": [
        7271,
        7334
      ],
      "content": "Open <bpt id=\"p15\">**</bpt>View advanced system settings<ept id=\"p15\">**</ept><ph id=\"ph17\"/> from the Windows screen."
    },
    {
      "pos": [
        7342,
        7374
      ],
      "content": "Click <bpt id=\"p16\">**</bpt>Environment Variables<ept id=\"p16\">**</ept>."
    },
    {
      "pos": [
        7384,
        7516
      ],
      "content": "Add a new <bpt id=\"p17\">**</bpt>JAVA_HOME<ept id=\"p17\">**</ept><ph id=\"ph18\"/> environment variable is pointing to <bpt id=\"p18\">**</bpt>C:\\Program Files\\Java\\jdk1.7.0_55<ept id=\"p18\">**</ept><ph id=\"ph19\"/> or wherever your JDK is installed."
    },
    {
      "pos": [
        7522,
        7589
      ],
      "content": "<ph id=\"ph20\">![</ph>Setting up correct config values for JDK<ph id=\"ph21\">][image-hdi-hivejson-jdk]</ph>"
    },
    {
      "pos": [
        7594,
        7711
      ],
      "content": "Install <bpt id=\"p19\">[</bpt>Maven 3.3.1<ept id=\"p19\">](http://mirror.olnevhost.net/pub/apache/maven/maven-3/3.3.1/binaries/apache-maven-3.3.1-bin.zip)</ept>"
    },
    {
      "pos": [
        7718,
        7891
      ],
      "content": "Add the bin folder to your path by going to Control Panel--&gt;Edit the System Variables for your account Environment variables. The screenshot below shows you how to do this.",
      "nodes": [
        {
          "content": "Add the bin folder to your path by going to Control Panel--&gt;Edit the System Variables for your account Environment variables.",
          "pos": [
            0,
            129
          ]
        },
        {
          "content": "The screenshot below shows you how to do this.",
          "pos": [
            130,
            176
          ]
        }
      ]
    },
    {
      "pos": [
        7898,
        7943
      ],
      "content": "<ph id=\"ph22\">![</ph>Setting up Maven<ph id=\"ph23\">][image-hdi-hivejson-maven]</ph>"
    },
    {
      "pos": [
        7948,
        8153
      ],
      "content": "Clone the project from <bpt id=\"p20\">[</bpt>Hive-JSON-SerDe<ept id=\"p20\">](https://github.com/sheetaldolas/Hive-JSON-Serde/tree/master)</ept><ph id=\"ph24\"/> github site. You can do this by clicking on the “Download Zip” button as shown in the screenshot below.",
      "nodes": [
        {
          "content": "Clone the project from <bpt id=\"p20\">[</bpt>Hive-JSON-SerDe<ept id=\"p20\">](https://github.com/sheetaldolas/Hive-JSON-Serde/tree/master)</ept><ph id=\"ph24\"/> github site.",
          "pos": [
            0,
            169
          ]
        },
        {
          "content": "You can do this by clicking on the “Download Zip” button as shown in the screenshot below.",
          "pos": [
            170,
            260
          ]
        }
      ]
    },
    {
      "pos": [
        8159,
        8207
      ],
      "content": "<ph id=\"ph25\">![</ph>Cloning the project<ph id=\"ph26\">][image-hdi-hivejson-serde]</ph>"
    },
    {
      "pos": [
        8209,
        8379
      ],
      "content": "4: Go to the folder where you have downloaded this package and  type “mvn package”. This should create the necessary jar files that you can then copy over to the cluster.",
      "nodes": [
        {
          "content": "4: Go to the folder where you have downloaded this package and  type “mvn package”.",
          "pos": [
            0,
            83
          ]
        },
        {
          "content": "This should create the necessary jar files that you can then copy over to the cluster.",
          "pos": [
            84,
            170
          ]
        }
      ]
    },
    {
      "pos": [
        8382,
        8674
      ],
      "content": "5: Go to the target folder under the root folder where you downloaded the package. Upload the json-serde-1.1.9.9-Hive13-jar-with-dependencies.jar file to head-node of your cluster. I usually put it under the hive binary folder: C:\\apps\\dist\\hive-0.13.0.2.1.11.0-2316\\bin or something similar.",
      "nodes": [
        {
          "content": "5: Go to the target folder under the root folder where you downloaded the package.",
          "pos": [
            0,
            82
          ]
        },
        {
          "content": "Upload the json-serde-1.1.9.9-Hive13-jar-with-dependencies.jar file to head-node of your cluster.",
          "pos": [
            83,
            180
          ]
        },
        {
          "content": "I usually put it under the hive binary folder: C:\\apps\\dist\\hive-0.13.0.2.1.11.0-2316\\bin or something similar.",
          "pos": [
            181,
            292
          ]
        }
      ]
    },
    {
      "pos": [
        8677,
        8906
      ],
      "content": "6: In the hive prompt, type “add jar /path/to/json-serde-1.1.9.9-Hive13-jar-with-dependencies.jar”. Since in my case, the jar is in the C:\\apps\\dist\\hive-0.13.x\\bin folder, I can directly add the jar with the name as shown below:",
      "nodes": [
        {
          "content": "6: In the hive prompt, type “add jar /path/to/json-serde-1.1.9.9-Hive13-jar-with-dependencies.jar”.",
          "pos": [
            0,
            99
          ]
        },
        {
          "content": "Since in my case, the jar is in the C:\\apps\\dist\\hive-0.13.x\\bin folder, I can directly add the jar with the name as shown below:",
          "pos": [
            100,
            229
          ]
        }
      ]
    },
    {
      "pos": [
        9036,
        9113
      ],
      "content": "Now, you are ready to use the SerDe to run queries against the JSON document."
    },
    {
      "pos": [
        9115,
        9175
      ],
      "content": "The following statement create a table with a defined schema"
    },
    {
      "pos": [
        9737,
        9788
      ],
      "content": "To list the first name and last name of the student"
    },
    {
      "pos": [
        9869,
        9910
      ],
      "content": "Here is the result from the Hive console."
    },
    {
      "pos": [
        9912,
        9961
      ],
      "content": "<ph id=\"ph27\">![</ph>SerDe Query 1<ph id=\"ph28\">][image-hdi-hivejson-serde_query1]</ph>"
    },
    {
      "pos": [
        9963,
        10014
      ],
      "content": "To calculate the sum of scores of the JSON document"
    },
    {
      "pos": [
        10152,
        10334
      ],
      "content": "The query above uses <bpt id=\"p21\">[</bpt>lateral view explode<ept id=\"p21\">](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+LateralView)</ept><ph id=\"ph29\"/> UDF to expand the array of scores so that they can be summed."
    },
    {
      "pos": [
        10337,
        10378
      ],
      "content": "Here is the output from the Hive console."
    },
    {
      "pos": [
        10380,
        10429
      ],
      "content": "<ph id=\"ph30\">![</ph>SerDe Query 2<ph id=\"ph31\">][image-hdi-hivejson-serde_query2]</ph>"
    },
    {
      "pos": [
        10431,
        10676
      ],
      "content": "To find which subjects a given student has scored more than 80 points\n    <ph id=\"ph32\"/>SELECT  \n      <ph id=\"ph33\"/>jt.StudentClassCollection.ClassId \n    <ph id=\"ph34\"/>FROM json_table jt\n      <ph id=\"ph35\"/>lateral view explode(jt.StudentClassCollection.Score) collection as score  where score &gt; 80;"
    },
    {
      "pos": [
        10684,
        10769
      ],
      "content": "The query above returns a Hive array unlike get\\_json\\_object which returns a string."
    },
    {
      "pos": [
        10772,
        10821
      ],
      "content": "<ph id=\"ph36\">![</ph>SerDe Query 3<ph id=\"ph37\">][image-hdi-hivejson-serde_query3]</ph>"
    },
    {
      "pos": [
        10823,
        11017
      ],
      "content": "If you want to skil malformed JSON, then as explained in the <bpt id=\"p22\">[</bpt>wiki page<ept id=\"p22\">](https://github.com/sheetaldolas/Hive-JSON-Serde/tree/master)</ept><ph id=\"ph38\"/> of this SerDe you can achieve that by typing the code below:"
    },
    {
      "pos": [
        11111,
        11118
      ],
      "content": "Summary"
    },
    {
      "pos": [
        11119,
        11483
      ],
      "content": "In conclusion, the type of JSON operator in Hive that you choose depends on your scenario. If you have a simple JSON document and you only have one field to look up on – you can choose to use the Hive UDF get\\_json\\_object. If you have more than one keys to look up on then you can use json_tuple. If you have a nested document, then you should use the JSON SerDe.",
      "nodes": [
        {
          "content": "In conclusion, the type of JSON operator in Hive that you choose depends on your scenario.",
          "pos": [
            0,
            90
          ]
        },
        {
          "content": "If you have a simple JSON document and you only have one field to look up on – you can choose to use the Hive UDF get\\_json\\_object.",
          "pos": [
            91,
            223
          ]
        },
        {
          "content": "If you have more than one keys to look up on then you can use json_tuple.",
          "pos": [
            224,
            297
          ]
        },
        {
          "content": "If you have a nested document, then you should use the JSON SerDe.",
          "pos": [
            298,
            364
          ]
        }
      ]
    },
    {
      "pos": [
        11485,
        11516
      ],
      "content": "For other related articles, see"
    },
    {
      "pos": [
        11520,
        11627
      ],
      "content": "<bpt id=\"p23\">[</bpt>Use Hive and HiveQL with Hadoop in HDInsight to analyze a sample Apache log4j file<ept id=\"p23\">](hdinsight-use-hive.md)</ept>"
    },
    {
      "pos": [
        11630,
        11724
      ],
      "content": "<bpt id=\"p24\">[</bpt>Analyze flight delay data by using Hive in HDInsight<ept id=\"p24\">](hdinsight-analyze-flight-delay-data.md)</ept>"
    },
    {
      "pos": [
        11727,
        11808
      ],
      "content": "<bpt id=\"p25\">[</bpt>Analyze Twitter data using Hive in HDInsight<ept id=\"p25\">](hdinsight-analyze-twitter-data.md)</ept>"
    },
    {
      "pos": [
        11811,
        11901
      ],
      "content": "<bpt id=\"p26\">[</bpt>Run a Hadoop job using DocumentDB and HDInsight<ept id=\"p26\">](documentdb-run-hadoop-with-hdinsight.md)</ept>"
    }
  ],
  "content": "<properties\n   pageTitle=\"Analyze and Process JSON documents with Hive in HDInsight | Microsoft Azure\"\n   description=\"Learn how to use JSON documents and analyze them using Hive in HDInsight.\"\n   services=\"hdinsight\"\n   documentationCenter=\"\"\n   authors=\"rashimg\"\n   manager=\"mwinkle\"\n   editor=\"cgronlun\"/>\n\n<tags\n   ms.service=\"hdinsight\"\n   ms.devlang=\"na\"\n   ms.topic=\"article\"\n   ms.tgt_pltfrm=\"na\"\n   ms.workload=\"big-data\"\n   ms.date=\"06/22/2015\"\n   ms.author=\"rashimg\"/>\n\n\n# Process and analyze JSON documents using Hive in HDInsight\n\nLearn how to process and analyze JSON files using Hive in HDInsight. The following JSON document will be used in the tutorial\n\n    {\n        \"StudentId\": \"trgfg-5454-fdfdg-4346\",\n        \"Grade\": 7,\n        \"StudentDetails\": [\n            {\n                \"FirstName\": \"Peggy\",\n                \"LastName\": \"Williams\",\n                \"YearJoined\": 2012\n            }\n        ],\n        \"StudentClassCollection\": [\n            {\n                \"ClassId\": \"89084343\",\n                \"ClassParticipation\": \"Satisfied\",\n                \"ClassParticipationRank\": \"High\",\n                \"Score\": 93,\n                \"PerformedActivity\": false\n            },\n            {\n                \"ClassId\": \"78547522\",\n                \"ClassParticipation\": \"NotSatisfied\",\n                \"ClassParticipationRank\": \"None\",\n                \"Score\": 74,\n                \"PerformedActivity\": false\n            },\n            {\n                \"ClassId\": \"78675563\",\n                \"ClassParticipation\": \"Satisfied\",\n                \"ClassParticipationRank\": \"Low\",\n                \"Score\": 83,\n                \"PerformedActivity\": true\n            }\n        ]\n    }\n\nThe file can be found at wasb://processjson@hditutorialdata.blob.core.windows.net/. For more information on using Azure Blob storage with HDInsight, see [Use HDFS-compatible Azure Blob storage with Hadoop in HDInsight](hdinsight-hadoop-use-blob-storage.md). You can copy the file to the default container of your cluster if you want.\n\nIn this tutorial, you will use the Hive console.  For instructions of opening the Hive console, see [Use Hive with Hadoop on HDInsight with Remote Desktop](hdinsight-hadoop-use-hive-remote-desktop.md).\n\n##Flatten JSON documents\n\nThe methods listed in the next section require the JSON document in a single row. So you must flatten the JSON document to a string. If your JSON document is already flattened, you can skip this step and go straight to the next section on Analyzing JSON data.\n\n    DROP TABLE IF EXISTS StudentsRaw;\n    CREATE EXTERNAL TABLE StudentsRaw (textcol string) STORED AS TEXTFILE LOCATION \"wasb://processjson@hditutorialdata.blob.core.windows.net/\";\n    \n    DROP TABLE IF EXISTS StudentsOneLine;\n    CREATE EXTERNAL TABLE StudentsOneLine\n    (\n      json_body string\n    )\n    STORED AS TEXTFILE LOCATION '/json/students';\n    \n    INSERT OVERWRITE TABLE StudentsOneLine\n    SELECT CONCAT_WS(' ',COLLECT_LIST(textcol)) AS singlelineJSON \n          FROM (SELECT INPUT__FILE__NAME,BLOCK__OFFSET__INSIDE__FILE, textcol FROM StudentsRaw DISTRIBUTE BY INPUT__FILE__NAME SORT BY BLOCK__OFFSET__INSIDE__FILE) x\n          GROUP BY INPUT__FILE__NAME;\n    \n    SELECT * FROM StudentsOneLine\n\nThe raw JSON file is located at **wasb://processjson@hditutorialdata.blob.core.windows.net/**. The *StudentsRaw* Hive table points to the raw un-flattened JSON document.\n\nThe *StudentsOneLine* Hive table will store the data in the HDInsight default file system under the */json/students/* path.\n\nThe INSERT statement populate the StudentOneLine table with the flattened JSON data.\n\nThe SELECT statement shall only return 1 row.\n\nHere is the output of the SELECT statement:\n\n![Flattening of the JSON document.][image-hdi-hivejson-flatten]\n\n##Analyze JSON documents in Hive\n\nHive provides three different mechanisms to run queries on JSON documents:\n\n- use the GET\\_JSON\\_OBJECT UDF (User Defined Function)\n- use the JSON_TUPLE UDF\n- use custom SerDe\n- write you own UDF using Python or other languages. See [this article][hdinsight-python] on running your own Python code with Hive. \n\n### Use the GET\\_JSON_OBJECT UDF\nHive provides a built-in UDF called [get json object](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-get_json_object) which can perform JSON querying during run time. This method takes two arguments – the table name and method name which has the flattened JSON document and the JSON field that needs to be parsed. Let’s look at an example to see how this UDF works.\n\nGet the first name and last name for each student\n\n    SELECT \n      GET_JSON_OBJECT(StudentsOneLine.json_body,'$.StudentDetails.FirstName'), \n      GET_JSON_OBJECT(StudentsOneLine.json_body,'$.StudentDetails.LastName') \n    FROM StudentsOneLine;\n\nHere is the output when running this query in console window.\n\n![get_json_object UDF][image-hdi-hivejson-getjsonobject]\n\nThere are a few limitations of the get-json_object UDF. \n\n- Because each field in the query requires re-parsing the query, it affects the performance.\n- GET\\_JSON_OBJECT() returns the string representation of an array. To convert this to a Hive array, you will have to use regular expressions to replace the square brackets ‘[‘ and ‘]’ and then also call split to get the array.\n\n\nThis is why the Hive wiki recommends using json_tuple.  \n\n### Use the JSON_TUPLE UDF\n\nAnother UDF provided by Hive is called [json_tuple](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-json_tuple) which performs better than [get_ json _object](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-get_json_object). This method takes a set of keys and a JSON string, and returns a tuple of values using one function. The following query returns the student id and the grade from the JSON document:\n\n    SELECT q1.StudentId, q1.Grade \n      FROM StudentsOneLine jt\n      LATERAL VIEW JSON_TUPLE(jt.json_body, 'StudentId', 'Grade') q1\n        AS StudentId, Grade;\n\nThe output of this script in the Hive console:\n\n![json_tuple UDF][image-hdi-hivejson-jsontuple]\n\nJSON\\_TUPLE uses the [lateral view](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+LateralView) syntax in Hive which allows json\\_tuple to create a virtual table by applying the UDT function to each row of the original table.  Complex JSONs become too unwieldy because of the repeated use of LATERAL VIEW. Furthermore, JSON_TUPLE cannot handle nested JSONs.\n\n\n###Use custom SerDe\n\nSerDe is the best choice for parsing nested JSON documents, it allows you to define the JSON schema, and use the schema to parse the documents. In this tutorial, you will use one of the more popular SerDe that has been developed by [rcongiu](https://github.com/rcongiu).\n\n**To use the custom SerDe:**\n\n1. Install [Java SE Development Kit 7u55 JDK 1.7.0_55](http://www.oracle.com/technetwork/java/javase/downloads/java-archive-downloads-javase7-521261.html#jdk-7u55-oth-JPR). Choose the Windows X64 version of the JDK if you are going to be using the Windows deployment of HDInsight\n\n    >[AZURE.WARNING] JDK 1.8 doesn't work with this SerDe. \n\n    After the installation is completed, add a new user environment variable:\n\n    1. Open **View advanced system settings** from the Windows screen.\n    2. Click **Environment Variables**.  \n    3. Add a new **JAVA_HOME** environment variable is pointing to **C:\\Program Files\\Java\\jdk1.7.0_55** or wherever your JDK is installed.\n\n    ![Setting up correct config values for JDK][image-hdi-hivejson-jdk]\n\n2. Install [Maven 3.3.1](http://mirror.olnevhost.net/pub/apache/maven/maven-3/3.3.1/binaries/apache-maven-3.3.1-bin.zip) \n\n    Add the bin folder to your path by going to Control Panel-->Edit the System Variables for your account Environment variables. The screenshot below shows you how to do this. \n\n    ![Setting up Maven][image-hdi-hivejson-maven]\n\n3. Clone the project from [Hive-JSON-SerDe](https://github.com/sheetaldolas/Hive-JSON-Serde/tree/master) github site. You can do this by clicking on the “Download Zip” button as shown in the screenshot below.\n\n    ![Cloning the project][image-hdi-hivejson-serde]\n\n4: Go to the folder where you have downloaded this package and  type “mvn package”. This should create the necessary jar files that you can then copy over to the cluster. \n\n5: Go to the target folder under the root folder where you downloaded the package. Upload the json-serde-1.1.9.9-Hive13-jar-with-dependencies.jar file to head-node of your cluster. I usually put it under the hive binary folder: C:\\apps\\dist\\hive-0.13.0.2.1.11.0-2316\\bin or something similar.\n \n6: In the hive prompt, type “add jar /path/to/json-serde-1.1.9.9-Hive13-jar-with-dependencies.jar”. Since in my case, the jar is in the C:\\apps\\dist\\hive-0.13.x\\bin folder, I can directly add the jar with the name as shown below:\n\n    add jar json-serde-1.1.9.9-Hive13-jar-with-dependencies.jar;\n\n    ![Adding JAR to your project][image-hdi-hivejson-addjar]\n\nNow, you are ready to use the SerDe to run queries against the JSON document.\n\nThe following statement create a table with a defined schema\n\n    DROP TABLE json_table;\n    CREATE EXTERNAL TABLE json_table (\n      StudentId string, \n      Grade int,\n      StudentDetails array<struct<\n          FirstName:string,\n          LastName:string,\n          YearJoined:int\n          >\n      >,\n      StudentClassCollection array<struct<\n          ClassId:string,\n          ClassParticipation:string,\n          ClassParticipationRank:string,\n          Score:int,\n          PerformedActivity:boolean\n          >\n      >\n    ) ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'\n    LOCATION '/json/students';\n\nTo list the first name and last name of the student\n\n    SELECT StudentDetails.FirstName, StudentDetails.LastName FROM json_table;\n\nHere is the result from the Hive console.\n\n![SerDe Query 1][image-hdi-hivejson-serde_query1]\n\nTo calculate the sum of scores of the JSON document\n\n    SELECT SUM(scores)\n    FROM json_table jt\n      lateral view explode(jt.StudentClassCollection.Score) collection as scores;\n       \nThe query above uses [lateral view explode](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+LateralView) UDF to expand the array of scores so that they can be summed. \n\nHere is the output from the Hive console.\n\n![SerDe Query 2][image-hdi-hivejson-serde_query2]\n\nTo find which subjects a given student has scored more than 80 points\n    SELECT  \n      jt.StudentClassCollection.ClassId \n    FROM json_table jt\n      lateral view explode(jt.StudentClassCollection.Score) collection as score  where score > 80;\n      \nThe query above returns a Hive array unlike get\\_json\\_object which returns a string. \n\n![SerDe Query 3][image-hdi-hivejson-serde_query3]\n\nIf you want to skil malformed JSON, then as explained in the [wiki page](https://github.com/sheetaldolas/Hive-JSON-Serde/tree/master) of this SerDe you can achieve that by typing the code below:  \n\n    ALTER TABLE json_table SET SERDEPROPERTIES ( \"ignore.malformed.json\" = \"true\");\n\n\n\n\n##Summary\nIn conclusion, the type of JSON operator in Hive that you choose depends on your scenario. If you have a simple JSON document and you only have one field to look up on – you can choose to use the Hive UDF get\\_json\\_object. If you have more than one keys to look up on then you can use json_tuple. If you have a nested document, then you should use the JSON SerDe.\n\nFor other related articles, see\n\n- [Use Hive and HiveQL with Hadoop in HDInsight to analyze a sample Apache log4j file](hdinsight-use-hive.md)\n- [Analyze flight delay data by using Hive in HDInsight](hdinsight-analyze-flight-delay-data.md)\n- [Analyze Twitter data using Hive in HDInsight](hdinsight-analyze-twitter-data.md)\n- [Run a Hadoop job using DocumentDB and HDInsight](documentdb-run-hadoop-with-hdinsight.md)\n\n[hdinsight-python]: hdinsight-python.md\n\n[image-hdi-hivejson-flatten]: ./media/hdinsight-using-json-in-hive/flatten.png\n[image-hdi-hivejson-getjsonobject]: ./media/hdinsight-using-json-in-hive/getjsonobject.png\n[image-hdi-hivejson-jsontuple]: ./media/hdinsight-using-json-in-hive/jsontuple.png\n[image-hdi-hivejson-jdk]: ./media/hdinsight-using-json-in-hive/jdk.png\n[image-hdi-hivejson-maven]: ./media/hdinsight-using-json-in-hive/maven.png\n[image-hdi-hivejson-serde]: ./media/hdinsight-using-json-in-hive/serde.png\n[image-hdi-hivejson-addjar]: ./media/hdinsight-using-json-in-hive/addjar.png\n[image-hdi-hivejson-serde_query1]: ./media/hdinsight-using-json-in-hive/serde_query1.png\n[image-hdi-hivejson-serde_query2]: ./media/hdinsight-using-json-in-hive/serde_query2.png\n[image-hdi-hivejson-serde_query3]: ./media/hdinsight-using-json-in-hive/serde_query3.png\n[image-hdi-hivejson-serde_result]: ./media/hdinsight-using-json-in-hive/serde_result.png\n "
}