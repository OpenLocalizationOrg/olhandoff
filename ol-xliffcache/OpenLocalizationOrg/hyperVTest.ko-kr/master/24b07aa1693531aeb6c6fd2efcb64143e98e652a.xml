{
  "nodes": [
    {
      "pos": [
        26,
        99
      ],
      "content": "Process events from Event Hubs with Storm on HDInsight using Java | Azure"
    },
    {
      "pos": [
        117,
        200
      ],
      "content": "Learn how to process Event Hubs data with a Java Storm topology created with Maven."
    },
    {
      "pos": [
        516,
        583
      ],
      "content": "Process events from Azure Event Hubs with Storm on HDInsight (Java)"
    },
    {
      "pos": [
        585,
        863
      ],
      "content": "Azure Event Hubs allows you to process massive amounts of data from websites, apps, and devices. The Event Hubs spout makes it easy to use Apache Storm on HDInsight to analyze this data in real time. You can also write data to Event Hubs from Storm by using the Event Hubs bolt.",
      "nodes": [
        {
          "content": "Azure Event Hubs allows you to process massive amounts of data from websites, apps, and devices.",
          "pos": [
            0,
            96
          ]
        },
        {
          "content": "The Event Hubs spout makes it easy to use Apache Storm on HDInsight to analyze this data in real time.",
          "pos": [
            97,
            199
          ]
        },
        {
          "content": "You can also write data to Event Hubs from Storm by using the Event Hubs bolt.",
          "pos": [
            200,
            278
          ]
        }
      ]
    },
    {
      "pos": [
        865,
        993
      ],
      "content": "In this tutorial, you will learn how to use the Event Hubs spout and bolt to read and write data in a Java-based Storm topology."
    },
    {
      "pos": [
        998,
        1011
      ],
      "content": "Prerequisites"
    },
    {
      "pos": [
        1015,
        1123
      ],
      "content": "An Apache Storm on HDInsight cluster. Use one of the following getting started articles to create a cluster:",
      "nodes": [
        {
          "content": "An Apache Storm on HDInsight cluster.",
          "pos": [
            0,
            37
          ]
        },
        {
          "content": "Use one of the following getting started articles to create a cluster:",
          "pos": [
            38,
            108
          ]
        }
      ]
    },
    {
      "pos": [
        1131,
        1312
      ],
      "content": "A <bpt id=\"p1\">[</bpt>Linux-based cluster<ept id=\"p1\">](hdinsight-apache-storm-tutorial-get-started-linux.md)</ept>: Select this if you want to use SSH to work with the cluster from Linux, Unix, OS X, or Windows clients"
    },
    {
      "pos": [
        1320,
        1483
      ],
      "content": "A <bpt id=\"p2\">[</bpt>Windows-based cluster<ept id=\"p2\">](hdinsight-apache-storm-tutorial-get-started.md)</ept>: Select this if you want to use PowerShell to work with the cluster from a Windows client"
    },
    {
      "pos": [
        1491,
        1629
      ],
      "content": "<ph id=\"ph2\">[AZURE.NOTE]</ph><ph id=\"ph3\"/> The only difference between the two cluster types is whether you use SSH to submit the topology to the cluster or a web form."
    },
    {
      "pos": [
        1633,
        1706
      ],
      "content": "An <bpt id=\"p3\">[</bpt>Azure Event Hub<ept id=\"p3\">](../event-hubs/event-hubs-csharp-ephcs-getstarted.md)</ept>"
    },
    {
      "pos": [
        1710,
        1898
      ],
      "content": "<bpt id=\"p4\">[</bpt>Oracle Java Developer Kit (JDK) version 7<ept id=\"p4\">](https://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html)</ept><ph id=\"ph4\"/> or equivalent, such as <bpt id=\"p5\">[</bpt>OpenJDK<ept id=\"p5\">](http://openjdk.java.net/)</ept>"
    },
    {
      "pos": [
        1902,
        1999
      ],
      "content": "<bpt id=\"p6\">[</bpt>Maven<ept id=\"p6\">](https://maven.apache.org/download.cgi)</ept>: Maven is a project build system for Java projects"
    },
    {
      "pos": [
        2003,
        2065
      ],
      "content": "A text editor or Java integrated development environment (IDE)"
    },
    {
      "pos": [
        2073,
        2320
      ],
      "content": "<ph id=\"ph5\">[AZURE.NOTE]</ph><ph id=\"ph6\"/> Your editor or IDE may have specific functionality for working with Maven that is not addressed in this document. For information about the capabilities of your editing environment, see the documentation for the product you are using.",
      "nodes": [
        {
          "content": "<ph id=\"ph5\">[AZURE.NOTE]</ph><ph id=\"ph6\"/> Your editor or IDE may have specific functionality for working with Maven that is not addressed in this document.",
          "pos": [
            0,
            158
          ]
        },
        {
          "content": "For information about the capabilities of your editing environment, see the documentation for the product you are using.",
          "pos": [
            159,
            279
          ]
        }
      ]
    },
    {
      "pos": [
        2325,
        2423
      ],
      "content": "An SSH client. See one of the following articles for more information on using SSH with HDInsight:",
      "nodes": [
        {
          "content": "An SSH client.",
          "pos": [
            0,
            14
          ]
        },
        {
          "content": "See one of the following articles for more information on using SSH with HDInsight:",
          "pos": [
            15,
            98
          ]
        }
      ]
    },
    {
      "pos": [
        2431,
        2543
      ],
      "content": "<bpt id=\"p7\">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X<ept id=\"p7\">](hdinsight-hadoop-linux-use-ssh-unix.md)</ept>"
    },
    {
      "pos": [
        2551,
        2653
      ],
      "content": "<bpt id=\"p8\">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Windows<ept id=\"p8\">](hdinsight-hadoop-linux-use-ssh-windows.md)</ept>"
    },
    {
      "pos": [
        2657,
        2884
      ],
      "content": "An SCP client. This is provided with all Linux, Unix, and OS X systems. For Windows clients, we recommend PSCP, which is available from the <bpt id=\"p9\">[</bpt>PuTTY download page<ept id=\"p9\">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.",
      "nodes": [
        {
          "content": "An SCP client.",
          "pos": [
            0,
            14
          ]
        },
        {
          "content": "This is provided with all Linux, Unix, and OS X systems.",
          "pos": [
            15,
            71
          ]
        },
        {
          "content": "For Windows clients, we recommend PSCP, which is available from the <bpt id=\"p9\">[</bpt>PuTTY download page<ept id=\"p9\">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.",
          "pos": [
            72,
            265
          ]
        }
      ]
    },
    {
      "pos": [
        2888,
        2913
      ],
      "content": "Understanding the example"
    },
    {
      "pos": [
        2915,
        3043
      ],
      "content": "The <bpt id=\"p10\">[</bpt>hdinsight-java-storm-eventhub<ept id=\"p10\">](https://github.com/Blackmist/hdinsight-java-storm-eventhub)</ept><ph id=\"ph7\"/> example contains two topologies:"
    },
    {
      "pos": [
        3045,
        3282
      ],
      "content": "<bpt id=\"p11\">__</bpt>com.microsoft.example.EventHubWriter<ept id=\"p11\">__</ept><ph id=\"ph8\"/> writes random data to an Azure Event Hub. The data is generated by a spout, and is a random device ID and device value. So it's simulating some hardware that emits a string ID and a numeric value.",
      "nodes": [
        {
          "content": "<bpt id=\"p11\">__</bpt>com.microsoft.example.EventHubWriter<ept id=\"p11\">__</ept><ph id=\"ph8\"/> writes random data to an Azure Event Hub.",
          "pos": [
            0,
            136
          ]
        },
        {
          "content": "The data is generated by a spout, and is a random device ID and device value.",
          "pos": [
            137,
            214
          ]
        },
        {
          "content": "So it's simulating some hardware that emits a string ID and a numeric value.",
          "pos": [
            215,
            291
          ]
        }
      ]
    },
    {
      "pos": [
        3284,
        3516
      ],
      "content": "<bpt id=\"p12\">__</bpt>com.microsoft.example.EventHubReader<ept id=\"p12\">__</ept><ph id=\"ph9\"/> reads data from Event Hub (the data written by EventHubWriter,) and stores it to HDFS (WASB in this case, since this was written and tested with Azure HDInsight) in the /devicedata directory."
    },
    {
      "pos": [
        3518,
        3694
      ],
      "content": "The data is formatted as a JSON document before it is written to Event Hub, and when read by the reader it is parsed out of JSON and into tuples. The JSON format is as follows:",
      "nodes": [
        {
          "content": "The data is formatted as a JSON document before it is written to Event Hub, and when read by the reader it is parsed out of JSON and into tuples.",
          "pos": [
            0,
            145
          ]
        },
        {
          "content": "The JSON format is as follows:",
          "pos": [
            146,
            176
          ]
        }
      ]
    },
    {
      "pos": [
        3764,
        3961
      ],
      "content": "The reason for using a JSON document to store the data into Event Hub is so that we know what the format is, instead of relying on the internal formatting mechanics of the Event Hub Spout and Bolt."
    },
    {
      "pos": [
        3966,
        3987
      ],
      "content": "Project configuration"
    },
    {
      "pos": [
        3989,
        4096
      ],
      "content": "The <bpt id=\"p13\">**</bpt>POM.xml<ept id=\"p13\">**</ept><ph id=\"ph10\"/> file contains configuration information for this Maven project. The interesting pieces are:",
      "nodes": [
        {
          "content": "The <bpt id=\"p13\">**</bpt>POM.xml<ept id=\"p13\">**</ept><ph id=\"ph10\"/> file contains configuration information for this Maven project.",
          "pos": [
            0,
            134
          ]
        },
        {
          "content": "The interesting pieces are:",
          "pos": [
            135,
            162
          ]
        }
      ]
    },
    {
      "pos": [
        4102,
        4138
      ],
      "content": "The EventHubs Storm Spout dependency"
    },
    {
      "pos": [
        4309,
        4457
      ],
      "content": "This adds a dependency for the eventhubs-storm-spout package, which contains both a spout for reading from Event Hubs, and a bolt for writing to it."
    },
    {
      "pos": [
        4461,
        4592
      ],
      "content": "<ph id=\"ph11\">[AZURE.NOTE]</ph><ph id=\"ph12\"/> This package is not available on Maven, and will be manually installed in your local Maven repository in a later step."
    },
    {
      "pos": [
        4598,
        4630
      ],
      "content": "The HdfsBolt and WASB components"
    },
    {
      "pos": [
        4632,
        4892
      ],
      "content": "The HdfsBolt is normally used to store data to the Hadoop Distributed File System HDFS. However HDInsight clusters use Azure Storage (WASB) as the default data store, so we have to load several components that allow HdfsBolt to understand the WASB file system.",
      "nodes": [
        {
          "content": "The HdfsBolt is normally used to store data to the Hadoop Distributed File System HDFS.",
          "pos": [
            0,
            87
          ]
        },
        {
          "content": "However HDInsight clusters use Azure Storage (WASB) as the default data store, so we have to load several components that allow HdfsBolt to understand the WASB file system.",
          "pos": [
            88,
            260
          ]
        }
      ]
    },
    {
      "pos": [
        6945,
        7076
      ],
      "content": "<ph id=\"ph13\">[AZURE.NOTE]</ph><ph id=\"ph14\"/> The packages to enable WASB are not available on the Maven repository, and will be manually installed in a later step."
    },
    {
      "pos": [
        7082,
        7107
      ],
      "content": "The maven-compiler-plugin"
    },
    {
      "pos": [
        7374,
        7502
      ],
      "content": "This tells Maven that the project should be compiled with compatibility for Java 7, which is what is used by HDInsight clusters."
    },
    {
      "pos": [
        7508,
        7530
      ],
      "content": "The maven-shade-plugin"
    },
    {
      "pos": [
        8609,
        8745
      ],
      "content": "This is used to package the solution into an uber jar that contains both the project code and required dependencies. It is also used to:",
      "nodes": [
        {
          "content": "This is used to package the solution into an uber jar that contains both the project code and required dependencies.",
          "pos": [
            0,
            116
          ]
        },
        {
          "content": "It is also used to:",
          "pos": [
            117,
            136
          ]
        }
      ]
    },
    {
      "pos": [
        8749,
        8884
      ],
      "content": "Rename license files for the dependencies: if this isn't done it can result in an error at runtime on Windows-based HDInsight clusters."
    },
    {
      "pos": [
        8888,
        8997
      ],
      "content": "Exclude security/signatures: if this isn't done it can result in an error at runtime on the HDInsight cluster"
    },
    {
      "pos": [
        9003,
        9024
      ],
      "content": "The exec-maven-plugin"
    },
    {
      "pos": [
        9638,
        9742
      ],
      "content": "This allows you to run the topology locally on your development environment using the following command:"
    },
    {
      "pos": [
        9800,
        9891
      ],
      "content": "For example, <ph id=\"ph15\">`mvn compile exec:java -Dstorm.topology=com.microsoft.example.EventHubWriter`</ph>."
    },
    {
      "pos": [
        9897,
        9918
      ],
      "content": "The resources section"
    },
    {
      "pos": [
        10205,
        10252
      ],
      "content": "This defines resources required by the project:"
    },
    {
      "pos": [
        10256,
        10340
      ],
      "content": "<bpt id=\"p14\">**</bpt>EventHubs.properties<ept id=\"p14\">**</ept>: contains information used to connect to an Azure Event Hub"
    },
    {
      "pos": [
        10343,
        10437
      ],
      "content": "<bpt id=\"p15\">**</bpt>core-site.xml<ept id=\"p15\">**</ept>: contains information about the Azure Storage used by the HDInsight cluster."
    },
    {
      "pos": [
        10439,
        10531
      ],
      "content": "You must populate both of these with information about your Event Hub and HDInsight cluster."
    },
    {
      "pos": [
        10535,
        10566
      ],
      "content": "Configure environment variables"
    },
    {
      "pos": [
        10568,
        10781
      ],
      "content": "The following environment variables may be set when you install Java and the JDK on your development workstation. However, you should check that they exist and that they contain the correct values for your system.",
      "nodes": [
        {
          "content": "The following environment variables may be set when you install Java and the JDK on your development workstation.",
          "pos": [
            0,
            113
          ]
        },
        {
          "content": "However, you should check that they exist and that they contain the correct values for your system.",
          "pos": [
            114,
            213
          ]
        }
      ]
    },
    {
      "pos": [
        10785,
        11077
      ],
      "content": "<bpt id=\"p16\">**</bpt>JAVA_HOME<ept id=\"p16\">**</ept><ph id=\"ph16\"/> - should point to the directory where the Java runtime environment (JRE) is installed. For example, in a Unix or Linux distribution, it should have a value similar to <ph id=\"ph17\">`/usr/lib/jvm/java-7-oracle`</ph>. In Windows, it would have a value similar to <ph id=\"ph18\">`c:\\Program Files (x86)\\Java\\jre1.7`</ph>",
      "nodes": [
        {
          "content": "<bpt id=\"p16\">**</bpt>JAVA_HOME<ept id=\"p16\">**</ept><ph id=\"ph16\"/> - should point to the directory where the Java runtime environment (JRE) is installed.",
          "pos": [
            0,
            155
          ]
        },
        {
          "content": "For example, in a Unix or Linux distribution, it should have a value similar to <ph id=\"ph17\">`/usr/lib/jvm/java-7-oracle`</ph>.",
          "pos": [
            156,
            284
          ]
        },
        {
          "content": "In Windows, it would have a value similar to <ph id=\"ph18\">`c:\\Program Files (x86)\\Java\\jre1.7`</ph>",
          "pos": [
            285,
            385
          ]
        }
      ]
    },
    {
      "pos": [
        11081,
        11127
      ],
      "content": "<bpt id=\"p17\">**</bpt>PATH<ept id=\"p17\">**</ept><ph id=\"ph19\"/> - should contain the following paths:"
    },
    {
      "pos": [
        11135,
        11173
      ],
      "content": "<bpt id=\"p18\">**</bpt>JAVA_HOME<ept id=\"p18\">**</ept><ph id=\"ph20\"/> (or the equivalent path)"
    },
    {
      "pos": [
        11181,
        11223
      ],
      "content": "<bpt id=\"p19\">**</bpt>JAVA_HOME\\bin<ept id=\"p19\">**</ept><ph id=\"ph21\"/> (or the equivalent path)"
    },
    {
      "pos": [
        11231,
        11269
      ],
      "content": "The directory where Maven is installed"
    },
    {
      "pos": [
        11274,
        11293
      ],
      "content": "Configure Event Hub"
    },
    {
      "pos": [
        11295,
        11393
      ],
      "content": "Event Hubs is the data source for this example. Use the following steps to create a new Event Hub.",
      "nodes": [
        {
          "content": "Event Hubs is the data source for this example.",
          "pos": [
            0,
            47
          ]
        },
        {
          "content": "Use the following steps to create a new Event Hub.",
          "pos": [
            48,
            98
          ]
        }
      ]
    },
    {
      "pos": [
        11398,
        11533
      ],
      "content": "From the <bpt id=\"p20\">[</bpt>Azure Classic Portal<ept id=\"p20\">](https://manage.windowsazure.com)</ept>, select <bpt id=\"p21\">**</bpt>NEW<ept id=\"p21\">**</ept><ph id=\"ph22\"/> &gt; <bpt id=\"p22\">**</bpt>Service Bus<ept id=\"p22\">**</ept><ph id=\"ph23\"/> &gt; <bpt id=\"p23\">**</bpt>Event Hub<ept id=\"p23\">**</ept><ph id=\"ph24\"/> &gt; <bpt id=\"p24\">**</bpt>Custom Create<ept id=\"p24\">**</ept>."
    },
    {
      "pos": [
        11538,
        11736
      ],
      "content": "On the <bpt id=\"p25\">**</bpt>Add a new Event Hub<ept id=\"p25\">**</ept><ph id=\"ph25\"/> screen, enter an <bpt id=\"p26\">**</bpt>Event Hub Name<ept id=\"p26\">**</ept>, select the <bpt id=\"p27\">**</bpt>Region<ept id=\"p27\">**</ept><ph id=\"ph26\"/> to create the hub in, and create a new namespace or select an existing one. Click the <bpt id=\"p28\">**</bpt>Arrow<ept id=\"p28\">**</ept><ph id=\"ph27\"/> to continue.",
      "nodes": [
        {
          "content": "On the <bpt id=\"p25\">**</bpt>Add a new Event Hub<ept id=\"p25\">**</ept><ph id=\"ph25\"/> screen, enter an <bpt id=\"p26\">**</bpt>Event Hub Name<ept id=\"p26\">**</ept>, select the <bpt id=\"p27\">**</bpt>Region<ept id=\"p27\">**</ept><ph id=\"ph26\"/> to create the hub in, and create a new namespace or select an existing one.",
          "pos": [
            0,
            315
          ]
        },
        {
          "content": "Click the <bpt id=\"p28\">**</bpt>Arrow<ept id=\"p28\">**</ept><ph id=\"ph27\"/> to continue.",
          "pos": [
            316,
            403
          ]
        }
      ]
    },
    {
      "pos": [
        11742,
        11826
      ],
      "content": "<ph id=\"ph28\">![</ph>wizard page 1<ph id=\"ph29\">](./media/hdinsight-storm-develop-csharp-event-hub-topology/wiz1.png)</ph>"
    },
    {
      "pos": [
        11834,
        11949
      ],
      "content": "<ph id=\"ph30\">[AZURE.NOTE]</ph><ph id=\"ph31\"/> You should select the same <bpt id=\"p29\">**</bpt>Location<ept id=\"p29\">**</ept><ph id=\"ph32\"/> as your Storm on HDInsight server to reduce latency and costs."
    },
    {
      "pos": [
        11954,
        12197
      ],
      "content": "On the <bpt id=\"p30\">**</bpt>Configure Event Hub<ept id=\"p30\">**</ept><ph id=\"ph33\"/> screen, enter the <bpt id=\"p31\">**</bpt>Partition count<ept id=\"p31\">**</ept><ph id=\"ph34\"/> and <bpt id=\"p32\">**</bpt>Message Retention<ept id=\"p32\">**</ept><ph id=\"ph35\"/> values. For this example, use a partition count of 10 and a message retention of 1. Note the partition count because you will need this value later.",
      "nodes": [
        {
          "content": "On the <bpt id=\"p30\">**</bpt>Configure Event Hub<ept id=\"p30\">**</ept><ph id=\"ph33\"/> screen, enter the <bpt id=\"p31\">**</bpt>Partition count<ept id=\"p31\">**</ept><ph id=\"ph34\"/> and <bpt id=\"p32\">**</bpt>Message Retention<ept id=\"p32\">**</ept><ph id=\"ph35\"/> values.",
          "pos": [
            0,
            267
          ]
        },
        {
          "content": "For this example, use a partition count of 10 and a message retention of 1.",
          "pos": [
            268,
            343
          ]
        },
        {
          "content": "Note the partition count because you will need this value later.",
          "pos": [
            344,
            408
          ]
        }
      ]
    },
    {
      "pos": [
        12203,
        12287
      ],
      "content": "<ph id=\"ph36\">![</ph>wizard page 2<ph id=\"ph37\">](./media/hdinsight-storm-develop-csharp-event-hub-topology/wiz2.png)</ph>"
    },
    {
      "pos": [
        12292,
        12430
      ],
      "content": "After the event hub has been created, select the namespace, select <bpt id=\"p33\">**</bpt>Event Hubs<ept id=\"p33\">**</ept>, and then select the event hub that you created earlier."
    },
    {
      "pos": [
        12435,
        12528
      ],
      "content": "Select <bpt id=\"p34\">**</bpt>Configure<ept id=\"p34\">**</ept>, then create two new access policies by using the following information."
    },
    {
      "pos": [
        12534,
        12687
      ],
      "leadings": [
        "",
        "   ",
        "   ",
        "   ",
        "   "
      ],
      "content": "<ph id=\"ph38\">&lt;table&gt;</ph><ph id=\"ph39\">\n &lt;tr&gt;</ph><ph id=\"ph40\">&lt;th&gt;</ph>Name<ph id=\"ph41\">&lt;/th&gt;</ph><ph id=\"ph42\">&lt;th&gt;</ph>Permissions<ph id=\"ph43\">&lt;/th&gt;</ph><ph id=\"ph44\">&lt;/tr&gt;</ph><ph id=\"ph45\">\n &lt;tr&gt;</ph><ph id=\"ph46\">&lt;td&gt;</ph>Writer<ph id=\"ph47\">&lt;/td&gt;</ph><ph id=\"ph48\">&lt;td&gt;</ph>Send<ph id=\"ph49\">&lt;/td&gt;</ph><ph id=\"ph50\">&lt;/tr&gt;</ph><ph id=\"ph51\">\n &lt;tr&gt;</ph><ph id=\"ph52\">&lt;td&gt;</ph>Reader<ph id=\"ph53\">&lt;/td&gt;</ph><ph id=\"ph54\">&lt;td&gt;</ph>Listen<ph id=\"ph55\">&lt;/td&gt;</ph><ph id=\"ph56\">&lt;/tr&gt;</ph><ph id=\"ph57\">\n &lt;/table&gt;</ph>"
    },
    {
      "pos": [
        12693,
        12892
      ],
      "content": "After You create the permissions, select the <bpt id=\"p35\">**</bpt>Save<ept id=\"p35\">**</ept><ph id=\"ph58\"/> icon at the bottom of the page. This creates the shared access policies that will be used to send (writer) and listen (reader) to this Event Hub.",
      "nodes": [
        {
          "content": "After You create the permissions, select the <bpt id=\"p35\">**</bpt>Save<ept id=\"p35\">**</ept><ph id=\"ph58\"/> icon at the bottom of the page.",
          "pos": [
            0,
            140
          ]
        },
        {
          "content": "This creates the shared access policies that will be used to send (writer) and listen (reader) to this Event Hub.",
          "pos": [
            141,
            254
          ]
        }
      ]
    },
    {
      "pos": [
        12898,
        12979
      ],
      "content": "<ph id=\"ph59\">![</ph>policies<ph id=\"ph60\">](./media/hdinsight-storm-develop-csharp-event-hub-topology/policy.png)</ph>"
    },
    {
      "pos": [
        12984,
        13186
      ],
      "content": "After you save the policies, use the <bpt id=\"p36\">**</bpt>Shared access key generator<ept id=\"p36\">**</ept><ph id=\"ph61\"/> at the bottom of the page to retrieve the key for the <bpt id=\"p37\">**</bpt>writer<ept id=\"p37\">**</ept><ph id=\"ph62\"/> and <bpt id=\"p38\">**</bpt>reader<ept id=\"p38\">**</ept><ph id=\"ph63\"/> policies. Save these because they will be used later.",
      "nodes": [
        {
          "content": "After you save the policies, use the <bpt id=\"p36\">**</bpt>Shared access key generator<ept id=\"p36\">**</ept><ph id=\"ph61\"/> at the bottom of the page to retrieve the key for the <bpt id=\"p37\">**</bpt>writer<ept id=\"p37\">**</ept><ph id=\"ph62\"/> and <bpt id=\"p38\">**</bpt>reader<ept id=\"p38\">**</ept><ph id=\"ph63\"/> policies.",
          "pos": [
            0,
            323
          ]
        },
        {
          "content": "Save these because they will be used later.",
          "pos": [
            324,
            367
          ]
        }
      ]
    },
    {
      "pos": [
        13191,
        13221
      ],
      "content": "Download and build the project"
    },
    {
      "pos": [
        13226,
        13471
      ],
      "content": "Download the project from GitHub: <bpt id=\"p39\">[</bpt>hdinsight-java-storm-eventhub<ept id=\"p39\">](https://github.com/Blackmist/hdinsight-java-storm-eventhub)</ept>. You can either download the package as a zip archive, or use <bpt id=\"p40\">[</bpt>git<ept id=\"p40\">](https://git-scm.com/)</ept><ph id=\"ph64\"/> to clone the project locally.",
      "nodes": [
        {
          "content": "Download the project from GitHub: <bpt id=\"p39\">[</bpt>hdinsight-java-storm-eventhub<ept id=\"p39\">](https://github.com/Blackmist/hdinsight-java-storm-eventhub)</ept>.",
          "pos": [
            0,
            166
          ]
        },
        {
          "content": "You can either download the package as a zip archive, or use <bpt id=\"p40\">[</bpt>git<ept id=\"p40\">](https://git-scm.com/)</ept><ph id=\"ph64\"/> to clone the project locally.",
          "pos": [
            167,
            340
          ]
        }
      ]
    },
    {
      "pos": [
        13476,
        13700
      ],
      "content": "Use the following commands to install packages included in the project into your local Maven repository. These enable the Event Hub spout and bolt, as well as the ability to use the HdfsBolt to write to Azure Storage (WASB).",
      "nodes": [
        {
          "content": "Use the following commands to install packages included in the project into your local Maven repository.",
          "pos": [
            0,
            104
          ]
        },
        {
          "content": "These enable the Event Hub spout and bolt, as well as the ability to use the HdfsBolt to write to Azure Storage (WASB).",
          "pos": [
            105,
            224
          ]
        }
      ]
    },
    {
      "pos": [
        15068,
        15226
      ],
      "content": "<ph id=\"ph65\">[AZURE.NOTE]</ph><ph id=\"ph66\"/> If you're using Powershell, you mau have to put the <ph id=\"ph67\">`-D`</ph><ph id=\"ph68\"/> parameters in quotes. For example, <ph id=\"ph69\">`\"-Dfile=lib/hadoop/hadoop-main-3.0.0-SNAPSHOT.pom\"`</ph>.",
      "nodes": [
        {
          "content": "<ph id=\"ph65\">[AZURE.NOTE]</ph><ph id=\"ph66\"/> If you're using Powershell, you mau have to put the <ph id=\"ph67\">`-D`</ph><ph id=\"ph68\"/> parameters in quotes.",
          "pos": [
            0,
            159
          ]
        },
        {
          "content": "For example, <ph id=\"ph69\">`\"-Dfile=lib/hadoop/hadoop-main-3.0.0-SNAPSHOT.pom\"`</ph>.",
          "pos": [
            160,
            245
          ]
        }
      ]
    },
    {
      "pos": [
        15232,
        15362
      ],
      "content": "Also these files are originally from https://github.com/hdinsight/hdinsight-storm-examples, so can find the latest versions there."
    },
    {
      "pos": [
        15367,
        15418
      ],
      "content": "Use the following to build and package the project:"
    },
    {
      "pos": [
        15445,
        15619
      ],
      "content": "This will download required dependencies, build, and then package the project. The output will be stored in the <bpt id=\"p41\">__</bpt>/target<ept id=\"p41\">__</ept><ph id=\"ph70\"/> directory as <bpt id=\"p42\">__</bpt>EventHubExample-1.0-SNAPSHOT.jar<ept id=\"p42\">__</ept>.",
      "nodes": [
        {
          "content": "This will download required dependencies, build, and then package the project.",
          "pos": [
            0,
            78
          ]
        },
        {
          "content": "The output will be stored in the <bpt id=\"p41\">__</bpt>/target<ept id=\"p41\">__</ept><ph id=\"ph70\"/> directory as <bpt id=\"p42\">__</bpt>EventHubExample-1.0-SNAPSHOT.jar<ept id=\"p42\">__</ept>.",
          "pos": [
            79,
            269
          ]
        }
      ]
    },
    {
      "pos": [
        15624,
        15645
      ],
      "content": "Deploy the topologies"
    },
    {
      "pos": [
        15647,
        15921
      ],
      "content": "The jar created by this project contains two topologies; <bpt id=\"p43\">__</bpt>com.microsoft.example.EventHubWriter<ept id=\"p43\">__</ept><ph id=\"ph71\"/> and <bpt id=\"p44\">__</bpt>com.microsoft.example.EventHubReader<ept id=\"p44\">__</ept>. The EventHubWriter topology should be started first, as it writes events in to Event Hub that are then read by the EventHubReader.",
      "nodes": [
        {
          "content": "The jar created by this project contains two topologies; <bpt id=\"p43\">__</bpt>com.microsoft.example.EventHubWriter<ept id=\"p43\">__</ept><ph id=\"ph71\"/> and <bpt id=\"p44\">__</bpt>com.microsoft.example.EventHubReader<ept id=\"p44\">__</ept>.",
          "pos": [
            0,
            238
          ]
        },
        {
          "content": "The EventHubWriter topology should be started first, as it writes events in to Event Hub that are then read by the EventHubReader.",
          "pos": [
            239,
            369
          ]
        }
      ]
    },
    {
      "pos": [
        15926,
        15956
      ],
      "content": "If using a Linux-based cluster"
    },
    {
      "pos": [
        15961,
        16133
      ],
      "content": "Use SCP to copy the jar package to your HDInsight cluster. Replace USERNAME with the SSH user for your cluster. Replace CLUSTERNAME with the name of your HDInsight cluster:",
      "nodes": [
        {
          "content": "Use SCP to copy the jar package to your HDInsight cluster.",
          "pos": [
            0,
            58
          ]
        },
        {
          "content": "Replace USERNAME with the SSH user for your cluster.",
          "pos": [
            59,
            111
          ]
        },
        {
          "content": "Replace CLUSTERNAME with the name of your HDInsight cluster:",
          "pos": [
            112,
            172
          ]
        }
      ]
    },
    {
      "pos": [
        16240,
        16570
      ],
      "content": "If you used a password for your SSH account, you will be prompted to enter the password. If you used an SSH key with the account, you may need to use the <ph id=\"ph72\">`-i`</ph><ph id=\"ph73\"/> parameter to specify the path to the key file. For example, <ph id=\"ph74\">`scp -i ~/.ssh/id_rsa ./target/EventHubExample-1.0-SNAPSHOT.jar USERNAME@CLUSTERNAME-ssh.azurehdinsight.net:.`</ph>.",
      "nodes": [
        {
          "content": "If you used a password for your SSH account, you will be prompted to enter the password.",
          "pos": [
            0,
            88
          ]
        },
        {
          "content": "If you used an SSH key with the account, you may need to use the <ph id=\"ph72\">`-i`</ph><ph id=\"ph73\"/> parameter to specify the path to the key file.",
          "pos": [
            89,
            239
          ]
        },
        {
          "content": "For example, <ph id=\"ph74\">`scp -i ~/.ssh/id_rsa ./target/EventHubExample-1.0-SNAPSHOT.jar USERNAME@CLUSTERNAME-ssh.azurehdinsight.net:.`</ph>.",
          "pos": [
            240,
            383
          ]
        }
      ]
    },
    {
      "pos": [
        16578,
        16814
      ],
      "content": "<ph id=\"ph75\">[AZURE.NOTE]</ph><ph id=\"ph76\"/> If your client is a Windows workstation, you may not have an SCP command installed. We recommend PSCP, which can be downloaded from the <bpt id=\"p45\">[</bpt>PuTTY download page<ept id=\"p45\">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.",
      "nodes": [
        {
          "content": "<ph id=\"ph75\">[AZURE.NOTE]</ph><ph id=\"ph76\"/> If your client is a Windows workstation, you may not have an SCP command installed.",
          "pos": [
            0,
            130
          ]
        },
        {
          "content": "We recommend PSCP, which can be downloaded from the <bpt id=\"p45\">[</bpt>PuTTY download page<ept id=\"p45\">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.",
          "pos": [
            131,
            310
          ]
        }
      ]
    },
    {
      "pos": [
        16820,
        16906
      ],
      "content": "This command will copy the file to the home directory of your SSH user on the cluster."
    },
    {
      "pos": [
        16911,
        17104
      ],
      "content": "Once the file has finished uploading, use SSH to connect to the HDInsight cluster. Replace <bpt id=\"p46\">**</bpt>USERNAME<ept id=\"p46\">**</ept><ph id=\"ph77\"/> the the name of your SSH login. Replace <bpt id=\"p47\">**</bpt>CLUSTERNAME<ept id=\"p47\">**</ept><ph id=\"ph78\"/> with your HDInsight cluster name:",
      "nodes": [
        {
          "content": "Once the file has finished uploading, use SSH to connect to the HDInsight cluster.",
          "pos": [
            0,
            82
          ]
        },
        {
          "content": "Replace <bpt id=\"p46\">**</bpt>USERNAME<ept id=\"p46\">**</ept><ph id=\"ph77\"/> the the name of your SSH login.",
          "pos": [
            83,
            190
          ]
        },
        {
          "content": "Replace <bpt id=\"p47\">**</bpt>CLUSTERNAME<ept id=\"p47\">**</ept><ph id=\"ph78\"/> with your HDInsight cluster name:",
          "pos": [
            191,
            303
          ]
        }
      ]
    },
    {
      "pos": [
        17169,
        17457
      ],
      "content": "<ph id=\"ph79\">[AZURE.NOTE]</ph><ph id=\"ph80\"/> If you used a password for your SSH account, you will be prompted to enter the password. If you used an SSH key with the account, you may need to use the <ph id=\"ph81\">`-i`</ph><ph id=\"ph82\"/> parameter to specify the path to the key file. The following example will load the private key from <ph id=\"ph83\">`~/.ssh/id_rsa`</ph>:",
      "nodes": [
        {
          "content": "<ph id=\"ph79\">[AZURE.NOTE]</ph><ph id=\"ph80\"/> If you used a password for your SSH account, you will be prompted to enter the password.",
          "pos": [
            0,
            135
          ]
        },
        {
          "content": "If you used an SSH key with the account, you may need to use the <ph id=\"ph81\">`-i`</ph><ph id=\"ph82\"/> parameter to specify the path to the key file.",
          "pos": [
            136,
            286
          ]
        },
        {
          "content": "The following example will load the private key from <ph id=\"ph83\">`~/.ssh/id_rsa`</ph>:",
          "pos": [
            287,
            375
          ]
        }
      ]
    },
    {
      "pos": [
        17542,
        17741
      ],
      "content": "If you are using PuTTY, enter <ph id=\"ph85\">`CLUSTERNAME-ssh.azurehdinsight.net`</ph><ph id=\"ph86\"/> in the <bpt id=\"p48\">__</bpt>Host Name (or IP address)<ept id=\"p48\">__</ept><ph id=\"ph87\"/> field, and then click <bpt id=\"p49\">__</bpt>Open<ept id=\"p49\">__</ept><ph id=\"ph88\"/> to connect. You will be prompted to enter your SSH account name.",
      "nodes": [
        {
          "content": "If you are using PuTTY, enter <ph id=\"ph85\">`CLUSTERNAME-ssh.azurehdinsight.net`</ph><ph id=\"ph86\"/> in the <bpt id=\"p48\">__</bpt>Host Name (or IP address)<ept id=\"p48\">__</ept><ph id=\"ph87\"/> field, and then click <bpt id=\"p49\">__</bpt>Open<ept id=\"p49\">__</ept><ph id=\"ph88\"/> to connect.",
          "pos": [
            0,
            290
          ]
        },
        {
          "content": "You will be prompted to enter your SSH account name.",
          "pos": [
            291,
            343
          ]
        }
      ]
    },
    {
      "pos": [
        17749,
        17950
      ],
      "content": "<ph id=\"ph89\">[AZURE.NOTE]</ph><ph id=\"ph90\"/> If you used a password for your SSH account, you will be prompted to enter the password. If you used an SSH key with the account, you may need to use the following steps to select the key:",
      "nodes": [
        {
          "content": "<ph id=\"ph89\">[AZURE.NOTE]</ph><ph id=\"ph90\"/> If you used a password for your SSH account, you will be prompted to enter the password.",
          "pos": [
            0,
            135
          ]
        },
        {
          "content": "If you used an SSH key with the account, you may need to use the following steps to select the key:",
          "pos": [
            136,
            235
          ]
        }
      ]
    },
    {
      "pos": [
        17966,
        18042
      ],
      "content": "In <bpt id=\"p50\">**</bpt>Category<ept id=\"p50\">**</ept>, expand <bpt id=\"p51\">**</bpt>Connection<ept id=\"p51\">**</ept>, expand <bpt id=\"p52\">**</bpt>SSH<ept id=\"p52\">**</ept>, and select <bpt id=\"p53\">**</bpt>Auth<ept id=\"p53\">**</ept>."
    },
    {
      "pos": [
        18052,
        18125
      ],
      "content": "Click <bpt id=\"p54\">**</bpt>Browse<ept id=\"p54\">**</ept><ph id=\"ph91\"/> and select the .ppk file that contains your private key."
    },
    {
      "pos": [
        18135,
        18161
      ],
      "content": "Click <bpt id=\"p55\">__</bpt>Open<ept id=\"p55\">__</ept><ph id=\"ph92\"/> to connect."
    },
    {
      "pos": [
        18166,
        18216
      ],
      "content": "Use the following command to start the topologies:"
    },
    {
      "pos": [
        18413,
        18499
      ],
      "content": "This will start the topologies and give them a friendly name of \"reader\" and \"writer\"."
    },
    {
      "pos": [
        18504,
        18697
      ],
      "content": "Wait a minute or two to allow the topologies to write and read events from event hub, then use the following command to verify that the EventHubReader is storing data to your HDInsight storage:"
    },
    {
      "pos": [
        18738,
        18798
      ],
      "content": "This should return a list of files similar to the following:"
    },
    {
      "pos": [
        19370,
        19506
      ],
      "content": "<ph id=\"ph93\">[AZURE.NOTE]</ph><ph id=\"ph94\"/> Some files may show a size of 0, as they have been created by the EventHubReader, but data has not been stored to them yet."
    },
    {
      "pos": [
        19512,
        19584
      ],
      "content": "You can view the contents of these files by using the following command:"
    },
    {
      "pos": [
        19633,
        19680
      ],
      "content": "This will return data similar to the following:"
    },
    {
      "pos": [
        20079,
        20167
      ],
      "content": "The first column contains the device ID value and the second column is the device value."
    },
    {
      "pos": [
        20172,
        20222
      ],
      "content": "Use the following commands to stop the topologies:"
    },
    {
      "pos": [
        20280,
        20312
      ],
      "content": "If using a Windows-based cluster"
    },
    {
      "pos": [
        20317,
        20496
      ],
      "content": "Open your browser to https://CLUSTERNAME.azurehdinsight.net. When prompted, enter the administrator credentials for your HDInsight cluster. You will arrive at the Storm Dashboard.",
      "nodes": [
        {
          "content": "Open your browser to https://CLUSTERNAME.azurehdinsight.net.",
          "pos": [
            0,
            60
          ]
        },
        {
          "content": "When prompted, enter the administrator credentials for your HDInsight cluster.",
          "pos": [
            61,
            139
          ]
        },
        {
          "content": "You will arrive at the Storm Dashboard.",
          "pos": [
            140,
            179
          ]
        }
      ]
    },
    {
      "pos": [
        20501,
        20622
      ],
      "content": "Use the <bpt id=\"p56\">__</bpt>Jar File<ept id=\"p56\">__</ept><ph id=\"ph95\"/> dropdown to browse and select the EventHubExample-1.0-SNAPSHOT.jar file from your build environment."
    },
    {
      "pos": [
        20627,
        20691
      ],
      "content": "For <bpt id=\"p57\">__</bpt>Class Name<ept id=\"p57\">__</ept>, enter <ph id=\"ph96\">`com.mirosoft.example.EventHubWriter`</ph>."
    },
    {
      "pos": [
        20696,
        20825
      ],
      "content": "For <bpt id=\"p58\">__</bpt>Additional Parameters<ept id=\"p58\">__</ept>, enter <ph id=\"ph97\">`writer`</ph>. Finally, click <bpt id=\"p59\">__</bpt>Submit<ept id=\"p59\">__</ept><ph id=\"ph98\"/> to upload the jar and start the EventHubWriter topology.",
      "nodes": [
        {
          "content": "For <bpt id=\"p58\">__</bpt>Additional Parameters<ept id=\"p58\">__</ept>, enter <ph id=\"ph97\">`writer`</ph>.",
          "pos": [
            0,
            105
          ]
        },
        {
          "content": "Finally, click <bpt id=\"p59\">__</bpt>Submit<ept id=\"p59\">__</ept><ph id=\"ph98\"/> to upload the jar and start the EventHubWriter topology.",
          "pos": [
            106,
            243
          ]
        }
      ]
    },
    {
      "pos": [
        20830,
        20902
      ],
      "content": "Once the topology has started, use the form to start the EventHubReader:"
    },
    {
      "pos": [
        20910,
        20996
      ],
      "content": "<bpt id=\"p60\">__</bpt>Jar File<ept id=\"p60\">__</ept>: select the EventHubExample-1.0-SNAPSHOT.jar that was previously uploaded"
    },
    {
      "pos": [
        21003,
        21063
      ],
      "content": "<bpt id=\"p61\">__</bpt>Class Name<ept id=\"p61\">__</ept>: enter <ph id=\"ph99\">`com.microsoft.example.EventHubReader`</ph>"
    },
    {
      "pos": [
        21070,
        21111
      ],
      "content": "<bpt id=\"p62\">__</bpt>Additional Parameters<ept id=\"p62\">__</ept>: enter <ph id=\"ph100\">`reader`</ph>"
    },
    {
      "pos": [
        21117,
        21167
      ],
      "content": "Click submit to start the EventHubReader topology."
    },
    {
      "pos": [
        21172,
        21348
      ],
      "content": "Wait a few minutes to allow the topologies to generate events and store then to Azure Storage, then select the <bpt id=\"p63\">__</bpt>Query Console<ept id=\"p63\">__</ept><ph id=\"ph101\"/> tab at the top of the <bpt id=\"p64\">__</bpt>Storm Dashboard<ept id=\"p64\">__</ept><ph id=\"ph102\"/> page."
    },
    {
      "pos": [
        21353,
        21477
      ],
      "content": "On the <bpt id=\"p65\">__</bpt>Query Console<ept id=\"p65\">__</ept>, select <bpt id=\"p66\">__</bpt>Hive Editor<ept id=\"p66\">__</ept><ph id=\"ph103\"/> and replace the default <ph id=\"ph104\">`select * from hivesampletable`</ph><ph id=\"ph105\"/> with the following:"
    },
    {
      "pos": [
        21700,
        21900
      ],
      "content": "Click <bpt id=\"p67\">__</bpt>Select<ept id=\"p67\">__</ept><ph id=\"ph106\"/> to run the query. This will return 10 rows from the data written to Azure Storage (WASB) by the EventHubReader. Once the query completes, you should see data similar to the following:",
      "nodes": [
        {
          "content": "Click <bpt id=\"p67\">__</bpt>Select<ept id=\"p67\">__</ept><ph id=\"ph106\"/> to run the query.",
          "pos": [
            0,
            90
          ]
        },
        {
          "content": "This will return 10 rows from the data written to Azure Storage (WASB) by the EventHubReader.",
          "pos": [
            91,
            184
          ]
        },
        {
          "content": "Once the query completes, you should see data similar to the following:",
          "pos": [
            185,
            256
          ]
        }
      ]
    },
    {
      "pos": [
        22298,
        22548
      ],
      "content": "Select the <bpt id=\"p68\">__</bpt>Storm Dashboard<ept id=\"p68\">__</ept><ph id=\"ph107\"/> at the top of the page, then select <bpt id=\"p69\">__</bpt>Storm UI<ept id=\"p69\">__</ept>. From the <bpt id=\"p70\">__</bpt>Storm UI<ept id=\"p70\">__</ept>, select the link for the <bpt id=\"p71\">__</bpt>reader<ept id=\"p71\">__</ept><ph id=\"ph108\"/> topology and then use the <bpt id=\"p72\">__</bpt>Kill<ept id=\"p72\">__</ept><ph id=\"ph109\"/> button to stop the topology. Repeat the process for the <bpt id=\"p73\">__</bpt>writer<ept id=\"p73\">__</ept><ph id=\"ph110\"/> topology.",
      "nodes": [
        {
          "content": "Select the <bpt id=\"p68\">__</bpt>Storm Dashboard<ept id=\"p68\">__</ept><ph id=\"ph107\"/> at the top of the page, then select <bpt id=\"p69\">__</bpt>Storm UI<ept id=\"p69\">__</ept>.",
          "pos": [
            0,
            176
          ]
        },
        {
          "content": "From the <bpt id=\"p70\">__</bpt>Storm UI<ept id=\"p70\">__</ept>, select the link for the <bpt id=\"p71\">__</bpt>reader<ept id=\"p71\">__</ept><ph id=\"ph108\"/> topology and then use the <bpt id=\"p72\">__</bpt>Kill<ept id=\"p72\">__</ept><ph id=\"ph109\"/> button to stop the topology.",
          "pos": [
            177,
            450
          ]
        },
        {
          "content": "Repeat the process for the <bpt id=\"p73\">__</bpt>writer<ept id=\"p73\">__</ept><ph id=\"ph110\"/> topology.",
          "pos": [
            451,
            554
          ]
        }
      ]
    },
    {
      "pos": [
        22556,
        22569
      ],
      "content": "Checkpointing"
    },
    {
      "pos": [
        22571,
        22813
      ],
      "content": "The EventHubSpout periodically checkpoints its state to the Zookeeper node, which saves the current offset for messages read from the queue. This allows the component to start receiving messages at the saved offset in the following scenarios:",
      "nodes": [
        {
          "content": "The EventHubSpout periodically checkpoints its state to the Zookeeper node, which saves the current offset for messages read from the queue.",
          "pos": [
            0,
            140
          ]
        },
        {
          "content": "This allows the component to start receiving messages at the saved offset in the following scenarios:",
          "pos": [
            141,
            242
          ]
        }
      ]
    },
    {
      "pos": [
        22817,
        22863
      ],
      "content": "The component instance fails and is restarted."
    },
    {
      "pos": [
        22867,
        22926
      ],
      "content": "You grow or shrink the cluster by adding or removing nodes."
    },
    {
      "pos": [
        22930,
        22990
      ],
      "content": "The topology is killed and restarted <bpt id=\"p74\">**</bpt>with the same name<ept id=\"p74\">**</ept>."
    },
    {
      "pos": [
        22996,
        23031
      ],
      "content": "On Windows-based HDInsight clusters"
    },
    {
      "pos": [
        23033,
        23279
      ],
      "content": "You can export and import the persisted checkpoints to WASB (the Azure Storage used by your HDInsight cluster.) The scripts to do this are located on the Storm on HDInsight cluster, at <bpt id=\"p75\">**</bpt>c:\\apps\\dist\\storm-0.9.3.2.2.1.0-2340\\zkdatatool-1.0\\bin<ept id=\"p75\">**</ept>."
    },
    {
      "pos": [
        23282,
        23418
      ],
      "content": "<ph id=\"ph111\">[AZURE.NOTE]</ph><ph id=\"ph112\"/> The version number in the path may be different, as the version of Storm installed on the cluster may change in the future."
    },
    {
      "pos": [
        23420,
        23454
      ],
      "content": "The scripts in this directory are:"
    },
    {
      "pos": [
        23458,
        23568
      ],
      "content": "<bpt id=\"p76\">**</bpt>stormmeta_import.cmd<ept id=\"p76\">**</ept>: Import all Storm metadata from the cluster default storage container into Zookeeper."
    },
    {
      "pos": [
        23572,
        23680
      ],
      "content": "<bpt id=\"p77\">**</bpt>stormmeta_export.cmd<ept id=\"p77\">**</ept>: Export all Storm metadata from Zookeeper to the cluster default storage container."
    },
    {
      "pos": [
        23684,
        23751
      ],
      "content": "<bpt id=\"p78\">**</bpt>stormmeta_delete.cmd<ept id=\"p78\">**</ept>: Delete all Storm metadata from Zookeeper."
    },
    {
      "pos": [
        23753,
        23951
      ],
      "content": "Export an import allows you to persist checkpoint data when you need to delete the cluster, but want to resume processing from the current offset in the hub when you bring a new cluster back online."
    },
    {
      "pos": [
        23955,
        24122
      ],
      "content": "<ph id=\"ph113\">[AZURE.NOTE]</ph><ph id=\"ph114\"/> Since the data is persisted to the default storage container, the new cluster <bpt id=\"p79\">**</bpt>must<ept id=\"p79\">**</ept><ph id=\"ph115\"/> use the same storage account and container as the previous cluster."
    },
    {
      "pos": [
        24126,
        24141
      ],
      "content": "Troubleshooting"
    },
    {
      "pos": [
        24143,
        24377
      ],
      "content": "If you do not see files being stored to the the /devicedata location (either using the <ph id=\"ph116\">`hadoop fs -ls /devicedata`</ph><ph id=\"ph117\"/> command or the Hive commandd in the Query Console,) use the Storm UI to look for any errors returned by the topologies."
    },
    {
      "pos": [
        24379,
        24448
      ],
      "content": "For more information on using the Storm UI, see the following topics:"
    },
    {
      "pos": [
        24452,
        24637
      ],
      "content": "If you are using a <bpt id=\"p80\">__</bpt>Linux-based<ept id=\"p80\">__</ept><ph id=\"ph118\"/> Storm on HDInsight cluster, see <bpt id=\"p81\">[</bpt>Deploy and manage Apache Storm topologies on Linux-based HDInsight<ept id=\"p81\">](hdinsight-storm-deploy-monitor-topology-linux.md)</ept>"
    },
    {
      "pos": [
        24641,
        24830
      ],
      "content": "If you are using a <bpt id=\"p82\">__</bpt>Windows-based<ept id=\"p82\">__</ept><ph id=\"ph119\"/> Storm on HDInsight cluster, see <bpt id=\"p83\">[</bpt>Deploy and manage Apache Storm topologies on Windows-based HDInsight<ept id=\"p83\">](hdinsight-storm-deploy-monitor-topology-linux.md)</ept>"
    },
    {
      "pos": [
        24834,
        24844
      ],
      "content": "Next steps"
    },
    {
      "pos": [
        24848,
        24928
      ],
      "content": "<bpt id=\"p84\">[</bpt>Example topologies for Storm on HDInsight<ept id=\"p84\">](hdinsight-storm-example-topology.md)</ept>"
    }
  ],
  "content": "<properties\n   pageTitle=\"Process events from Event Hubs with Storm on HDInsight using Java | Azure\"\n   description=\"Learn how to process Event Hubs data with a Java Storm topology created with Maven.\"\n   services=\"hdinsight,notification hubs\"\n   documentationCenter=\"\"\n   authors=\"Blackmist\"\n   manager=\"paulettm\"\n   editor=\"cgronlun\"/>\n\n<tags\n   ms.service=\"hdinsight\"\n   ms.devlang=\"dotnet\"\n   ms.topic=\"article\"\n   ms.tgt_pltfrm=\"na\"\n   ms.workload=\"big-data\"\n   ms.date=\"01/28/2016\"\n   ms.author=\"larryfr\"/>\n\n# Process events from Azure Event Hubs with Storm on HDInsight (Java)\n\nAzure Event Hubs allows you to process massive amounts of data from websites, apps, and devices. The Event Hubs spout makes it easy to use Apache Storm on HDInsight to analyze this data in real time. You can also write data to Event Hubs from Storm by using the Event Hubs bolt.\n\nIn this tutorial, you will learn how to use the Event Hubs spout and bolt to read and write data in a Java-based Storm topology.\n\n## Prerequisites\n\n* An Apache Storm on HDInsight cluster. Use one of the following getting started articles to create a cluster:\n\n    - A [Linux-based cluster](hdinsight-apache-storm-tutorial-get-started-linux.md): Select this if you want to use SSH to work with the cluster from Linux, Unix, OS X, or Windows clients\n\n    - A [Windows-based cluster](hdinsight-apache-storm-tutorial-get-started.md): Select this if you want to use PowerShell to work with the cluster from a Windows client\n\n    > [AZURE.NOTE] The only difference between the two cluster types is whether you use SSH to submit the topology to the cluster or a web form.\n\n* An [Azure Event Hub](../event-hubs/event-hubs-csharp-ephcs-getstarted.md)\n\n* [Oracle Java Developer Kit (JDK) version 7](https://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html) or equivalent, such as [OpenJDK](http://openjdk.java.net/)\n\n* [Maven](https://maven.apache.org/download.cgi): Maven is a project build system for Java projects\n\n* A text editor or Java integrated development environment (IDE)\n\n    > [AZURE.NOTE] Your editor or IDE may have specific functionality for working with Maven that is not addressed in this document. For information about the capabilities of your editing environment, see the documentation for the product you are using.\n\n * An SSH client. See one of the following articles for more information on using SSH with HDInsight:\n\n    - [Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X](hdinsight-hadoop-linux-use-ssh-unix.md)\n\n    - [Use SSH with Linux-based Hadoop on HDInsight from Windows](hdinsight-hadoop-linux-use-ssh-windows.md)\n\n* An SCP client. This is provided with all Linux, Unix, and OS X systems. For Windows clients, we recommend PSCP, which is available from the [PuTTY download page](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html).\n\n##Understanding the example\n\nThe [hdinsight-java-storm-eventhub](https://github.com/Blackmist/hdinsight-java-storm-eventhub) example contains two topologies:\n\n__com.microsoft.example.EventHubWriter__ writes random data to an Azure Event Hub. The data is generated by a spout, and is a random device ID and device value. So it's simulating some hardware that emits a string ID and a numeric value.\n\n__com.microsoft.example.EventHubReader__ reads data from Event Hub (the data written by EventHubWriter,) and stores it to HDFS (WASB in this case, since this was written and tested with Azure HDInsight) in the /devicedata directory.\n\nThe data is formatted as a JSON document before it is written to Event Hub, and when read by the reader it is parsed out of JSON and into tuples. The JSON format is as follows:\n\n    { \"deviceId\": \"unique identifier\", \"deviceValue\": some value }\n\nThe reason for using a JSON document to store the data into Event Hub is so that we know what the format is, instead of relying on the internal formatting mechanics of the Event Hub Spout and Bolt.\n\n###Project configuration\n\nThe **POM.xml** file contains configuration information for this Maven project. The interesting pieces are:\n\n####The EventHubs Storm Spout dependency\n\n    <dependency>\n      <groupId>com.microsoft.eventhubs</groupId>\n      <artifactId>eventhubs-storm-spout</artifactId>\n      <version>0.9.3</version>\n    </dependency>\n\nThis adds a dependency for the eventhubs-storm-spout package, which contains both a spout for reading from Event Hubs, and a bolt for writing to it.\n\n> [AZURE.NOTE] This package is not available on Maven, and will be manually installed in your local Maven repository in a later step.\n\n####The HdfsBolt and WASB components\n\nThe HdfsBolt is normally used to store data to the Hadoop Distributed File System HDFS. However HDInsight clusters use Azure Storage (WASB) as the default data store, so we have to load several components that allow HdfsBolt to understand the WASB file system.\n\n      <!--HdfsBolt stuff -->\n      <dependency>\n        <groupId>org.apache.storm</groupId>\n        <artifactId>storm-hdfs</artifactId>\n        <exclusions>\n          <exclusion>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-client</artifactId>\n          </exclusion>\n          <exclusion>\n            <groupId>org.apache.hadoop</groupId>\n            <artifactId>hadoop-hdfs</artifactId>\n          </exclusion>\n        </exclusions>\n        <version>0.9.3</version>\n      </dependency>\n      <!--\n     This is a temporary workaround to make HdfsBolt work with WASB through hadoop-azure project.\n     For now, we have to build hadoop-client, hadoop-hdfs and hadoop-azure from Hadoop trunk\n     (which defaults to 3.0.0-SNAPSHOT version). And push those jars and dependencies to local\n     mvn repo (take a look at push_lib_mvn.ps1).\n\n     Once Hadoop 2.7 is released, we can just switch to that version.\n     Note that hadoop-azure is added to Hadoop on Hadoop 2.7.\n     -->\n     <dependency>\n       <groupId>org.apache.hadoop</groupId>\n       <artifactId>hadoop-client</artifactId>\n       <version>3.0.0-SNAPSHOT</version>\n     </dependency>\n     <dependency>\n       <groupId>org.apache.hadoop</groupId>\n       <artifactId>hadoop-hdfs</artifactId>\n       <version>3.0.0-SNAPSHOT</version>\n     </dependency>\n     <dependency>\n       <groupId>org.apache.hadoop</groupId>\n       <artifactId>hadoop-azure</artifactId>\n       <version>3.0.0-SNAPSHOT</version>\n     </dependency>\n     <dependency>\n       <groupId>org.apache.hadoop</groupId>\n       <artifactId>hadoop-common</artifactId>\n       <version>3.0.0-SNAPSHOT</version>\n       <exclusions>\n         <exclusion>\n           <groupId>org.slf4j</groupId>\n           <artifactId>slf4j-log4j12</artifactId>\n         </exclusion>\n       </exclusions>\n     </dependency>\n     <dependency>\n       <groupId>com.microsoft.windowsazure.storage</groupId>\n       <artifactId>microsoft-windowsazure-storage-sdk</artifactId>\n       <version>0.6.0</version>\n     </dependency>\n\n> [AZURE.NOTE] The packages to enable WASB are not available on the Maven repository, and will be manually installed in a later step.\n\n####The maven-compiler-plugin\n\n    <plugin>\n      <groupId>org.apache.maven.plugins</groupId>\n      <artifactId>maven-compiler-plugin</artifactId>\n      <version>2.3.2</version>\n      <configuration>\n        <source>1.7</source>\n        <target>1.7</target>\n      </configuration>\n    </plugin>\n\nThis tells Maven that the project should be compiled with compatibility for Java 7, which is what is used by HDInsight clusters.\n\n####The maven-shade-plugin\n\n      <plugin>\n        <groupId>org.apache.maven.plugins</groupId>\n        <artifactId>maven-shade-plugin</artifactId>\n        <version>2.3</version>\n        <configuration>\n          <!-- Keep us from getting a can't overwrite file error -->\n          <transformers>\n            <transformer implementation=\"org.apache.maven.plugins.shade.resource.ApacheLicenseResourceTransformer\">\n            </transformer>\n          </transformers>\n          <!-- Keep us from getting a bad signature error -->\n          <filters>\n            <filter>\n                <artifact>*:*</artifact>\n                <excludes>\n                    <exclude>META-INF/*.SF</exclude>\n                    <exclude>META-INF/*.DSA</exclude>\n                    <exclude>META-INF/*.RSA</exclude>\n                </excludes>\n            </filter>\n        </filters>\n        </configuration>\n        <executions>\n          <execution>\n            <phase>package</phase>\n            <goals>\n              <goal>shade</goal>\n            </goals>\n          </execution>\n        </executions>\n      </plugin>\n\nThis is used to package the solution into an uber jar that contains both the project code and required dependencies. It is also used to:\n\n* Rename license files for the dependencies: if this isn't done it can result in an error at runtime on Windows-based HDInsight clusters.\n\n* Exclude security/signatures: if this isn't done it can result in an error at runtime on the HDInsight cluster\n\n####The exec-maven-plugin\n\n    <plugin>\n      <groupId>org.codehaus.mojo</groupId>\n      <artifactId>exec-maven-plugin</artifactId>\n      <version>1.2.1</version>\n      <executions>\n        <execution>\n        <goals>\n          <goal>exec</goal>\n        </goals>\n        </execution>\n      </executions>\n      <configuration>\n        <executable>java</executable>\n        <includeProjectDependencies>true</includeProjectDependencies>\n        <includePluginDependencies>false</includePluginDependencies>\n        <classpathScope>compile</classpathScope>\n        <mainClass>${storm.topology}</mainClass>\n      </configuration>\n    </plugin>\n\nThis allows you to run the topology locally on your development environment using the following command:\n\n    mvn compile exec:java -Dstorm.topology=<CLASSNAME>\n\nFor example, `mvn compile exec:java -Dstorm.topology=com.microsoft.example.EventHubWriter`.\n\n####The resources section\n\n    <resources>\n      <resource>\n        <directory>${basedir}/conf</directory>\n        <filtering>false</filtering>\n        <includes>\n          <include>EventHubs.properties</include>\n          <include>core-site.xml</include>\n        </includes>\n      </resource>\n    </resources>\n\nThis defines resources required by the project:\n\n- **EventHubs.properties**: contains information used to connect to an Azure Event Hub\n- **core-site.xml**: contains information about the Azure Storage used by the HDInsight cluster.\n\nYou must populate both of these with information about your Event Hub and HDInsight cluster.\n\n##Configure environment variables\n\nThe following environment variables may be set when you install Java and the JDK on your development workstation. However, you should check that they exist and that they contain the correct values for your system.\n\n* **JAVA_HOME** - should point to the directory where the Java runtime environment (JRE) is installed. For example, in a Unix or Linux distribution, it should have a value similar to `/usr/lib/jvm/java-7-oracle`. In Windows, it would have a value similar to `c:\\Program Files (x86)\\Java\\jre1.7`\n\n* **PATH** - should contain the following paths:\n\n    * **JAVA_HOME** (or the equivalent path)\n\n    * **JAVA_HOME\\bin** (or the equivalent path)\n\n    * The directory where Maven is installed\n\n## Configure Event Hub\n\nEvent Hubs is the data source for this example. Use the following steps to create a new Event Hub.\n\n1. From the [Azure Classic Portal](https://manage.windowsazure.com), select **NEW** > **Service Bus** > **Event Hub** > **Custom Create**.\n\n2. On the **Add a new Event Hub** screen, enter an **Event Hub Name**, select the **Region** to create the hub in, and create a new namespace or select an existing one. Click the **Arrow** to continue.\n\n    ![wizard page 1](./media/hdinsight-storm-develop-csharp-event-hub-topology/wiz1.png)\n\n    > [AZURE.NOTE] You should select the same **Location** as your Storm on HDInsight server to reduce latency and costs.\n\n2. On the **Configure Event Hub** screen, enter the **Partition count** and **Message Retention** values. For this example, use a partition count of 10 and a message retention of 1. Note the partition count because you will need this value later.\n\n    ![wizard page 2](./media/hdinsight-storm-develop-csharp-event-hub-topology/wiz2.png)\n\n3. After the event hub has been created, select the namespace, select **Event Hubs**, and then select the event hub that you created earlier.\n\n4. Select **Configure**, then create two new access policies by using the following information.\n\n    <table>\n    <tr><th>Name</th><th>Permissions</th></tr>\n    <tr><td>Writer</td><td>Send</td></tr>\n    <tr><td>Reader</td><td>Listen</td></tr>\n    </table>\n\n    After You create the permissions, select the **Save** icon at the bottom of the page. This creates the shared access policies that will be used to send (writer) and listen (reader) to this Event Hub.\n\n    ![policies](./media/hdinsight-storm-develop-csharp-event-hub-topology/policy.png)\n\n5. After you save the policies, use the **Shared access key generator** at the bottom of the page to retrieve the key for the **writer** and **reader** policies. Save these because they will be used later.\n\n## Download and build the project\n\n1. Download the project from GitHub: [hdinsight-java-storm-eventhub](https://github.com/Blackmist/hdinsight-java-storm-eventhub). You can either download the package as a zip archive, or use [git](https://git-scm.com/) to clone the project locally.\n\n2. Use the following commands to install packages included in the project into your local Maven repository. These enable the Event Hub spout and bolt, as well as the ability to use the HdfsBolt to write to Azure Storage (WASB).\n\n        mvn -q install:install-file -Dfile=lib/eventhubs/eventhubs-storm-spout-0.9.3-jar-with-dependencies.jar -DgroupId=com.microsoft.eventhubs -DartifactId=eventhubs-storm-spout -Dversion=0.9.3 -Dpackaging=jar\n\n        mvn -q org.apache.maven.plugins:maven-install-plugin:2.5.2:install-file -Dfile=lib/hadoop/hadoop-azure-3.0.0-SNAPSHOT.jar\n\n        mvn -q org.apache.maven.plugins:maven-install-plugin:2.5.2:install-file -Dfile=lib/hadoop/hadoop-client-3.0.0-SNAPSHOT.jar\n\n        mvn -q org.apache.maven.plugins:maven-install-plugin:2.5.2:install-file -Dfile=lib/hadoop/hadoop-hdfs-3.0.0-SNAPSHOT.jar\n\n        mvn -q org.apache.maven.plugins:maven-install-plugin:2.5.2:install-file -Dfile=lib/hadoop/hadoop-common-3.0.0-SNAPSHOT.jar -DpomFile=lib/hadoop/hadoop-common-3.0.0-SNAPSHOT.pom\n\n        mvn -q org.apache.maven.plugins:maven-install-plugin:2.5.2:install-file -Dfile=lib/hadoop/hadoop-project-dist-3.0.0-SNAPSHOT.pom -DpomFile=lib/hadoop/hadoop-project-dist-3.0.0-SNAPSHOT.pom\n\n        mvn -q org.apache.maven.plugins:maven-install-plugin:2.5.2:install-file -Dfile=lib/hadoop/hadoop-project-3.0.0-SNAPSHOT.pom -DpomFile=lib/hadoop/hadoop-project-3.0.0-SNAPSHOT.pom\n\n        mvn -q org.apache.maven.plugins:maven-install-plugin:2.5.2:install-file -Dfile=lib/hadoop/hadoop-main-3.0.0-SNAPSHOT.pom -DpomFile=lib/hadoop/hadoop-main-3.0.0-SNAPSHOT.pom\n\n    > [AZURE.NOTE] If you're using Powershell, you mau have to put the `-D` parameters in quotes. For example, `\"-Dfile=lib/hadoop/hadoop-main-3.0.0-SNAPSHOT.pom\"`.\n\n    Also these files are originally from https://github.com/hdinsight/hdinsight-storm-examples, so can find the latest versions there.\n\n3. Use the following to build and package the project:\n\n        mvn package\n\n    This will download required dependencies, build, and then package the project. The output will be stored in the __/target__ directory as __EventHubExample-1.0-SNAPSHOT.jar__.\n\n## Deploy the topologies\n\nThe jar created by this project contains two topologies; __com.microsoft.example.EventHubWriter__ and __com.microsoft.example.EventHubReader__. The EventHubWriter topology should be started first, as it writes events in to Event Hub that are then read by the EventHubReader.\n\n###If using a Linux-based cluster\n\n1. Use SCP to copy the jar package to your HDInsight cluster. Replace USERNAME with the SSH user for your cluster. Replace CLUSTERNAME with the name of your HDInsight cluster:\n\n        scp ./target/EventHubExample-1.0-SNAPSHOT.jar USERNAME@CLUSTERNAME-ssh.azurehdinsight.net:.\n\n    If you used a password for your SSH account, you will be prompted to enter the password. If you used an SSH key with the account, you may need to use the `-i` parameter to specify the path to the key file. For example, `scp -i ~/.ssh/id_rsa ./target/EventHubExample-1.0-SNAPSHOT.jar USERNAME@CLUSTERNAME-ssh.azurehdinsight.net:.`.\n\n    > [AZURE.NOTE] If your client is a Windows workstation, you may not have an SCP command installed. We recommend PSCP, which can be downloaded from the [PuTTY download page](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html).\n\n    This command will copy the file to the home directory of your SSH user on the cluster.\n\n1. Once the file has finished uploading, use SSH to connect to the HDInsight cluster. Replace **USERNAME** the the name of your SSH login. Replace **CLUSTERNAME** with your HDInsight cluster name:\n\n        ssh USERNAME@CLUSTERNAME-ssh.azurehdinsight.net\n\n    > [AZURE.NOTE] If you used a password for your SSH account, you will be prompted to enter the password. If you used an SSH key with the account, you may need to use the `-i` parameter to specify the path to the key file. The following example will load the private key from `~/.ssh/id_rsa`:\n    >\n    > `ssh -i ~/.ssh/id_rsa USERNAME@CLUSTERNAME-ssh.azurehdinsight.net`\n\n    If you are using PuTTY, enter `CLUSTERNAME-ssh.azurehdinsight.net` in the __Host Name (or IP address)__ field, and then click __Open__ to connect. You will be prompted to enter your SSH account name.\n\n    > [AZURE.NOTE] If you used a password for your SSH account, you will be prompted to enter the password. If you used an SSH key with the account, you may need to use the following steps to select the key:\n    >\n    > 1. In **Category**, expand **Connection**, expand **SSH**, and select **Auth**.\n    > 2. Click **Browse** and select the .ppk file that contains your private key.\n    > 3. Click __Open__ to connect.\n\n2. Use the following command to start the topologies:\n\n        storm jar EventHubExample-1.0-SNAPSHOT.jar com.microsoft.example.EventHubWriter writer\n        storm jar EventHubExample-1.0-SNAPSHOT.jar com.microsoft.example.EventHubReader reader\n\n    This will start the topologies and give them a friendly name of \"reader\" and \"writer\".\n\n3. Wait a minute or two to allow the topologies to write and read events from event hub, then use the following command to verify that the EventHubReader is storing data to your HDInsight storage:\n\n        hadoop fs -ls /devicedata\n\n    This should return a list of files similar to the following:\n\n        -rw-r--r--   1 storm supergroup      10283 2015-08-11 19:35 /devicedata/wasbbolt-14-0-1439321744110.txt\n        -rw-r--r--   1 storm supergroup      10277 2015-08-11 19:35 /devicedata/wasbbolt-14-1-1439321748237.txt\n        -rw-r--r--   1 storm supergroup      10280 2015-08-11 19:36 /devicedata/wasbbolt-14-10-1439321760398.txt\n        -rw-r--r--   1 storm supergroup      10267 2015-08-11 19:36 /devicedata/wasbbolt-14-11-1439321761090.txt\n        -rw-r--r--   1 storm supergroup      10259 2015-08-11 19:36 /devicedata/wasbbolt-14-12-1439321762679.txt\n\n    > [AZURE.NOTE] Some files may show a size of 0, as they have been created by the EventHubReader, but data has not been stored to them yet.\n\n    You can view the contents of these files by using the following command:\n\n        hadoop fs -text /devicedata/*.txt\n\n    This will return data similar to the following:\n\n        3409e622-c85d-4d64-8622-af45e30bf774,848981614\n        c3305f7e-6948-4cce-89b0-d9fbc2330c36,-1638780537\n        788b9796-e2ab-49c4-91e3-bc5b6af1f07e,-1662107246\n        6403df8a-6495-402f-bca0-3244be67f225,275738503\n        d7c7f96c-581a-45b1-b66c-e32de6d47fce,543829859\n        9a692795-e6aa-4946-98c1-2de381b37593,1857409996\n        3c8d199b-0003-4a79-8d03-24e13bde7086,-1271260574\n\n    The first column contains the device ID value and the second column is the device value.\n\n4. Use the following commands to stop the topologies:\n\n        storm kill reader\n        storm kill writer\n\n###If using a Windows-based cluster\n\n1. Open your browser to https://CLUSTERNAME.azurehdinsight.net. When prompted, enter the administrator credentials for your HDInsight cluster. You will arrive at the Storm Dashboard.\n\n2. Use the __Jar File__ dropdown to browse and select the EventHubExample-1.0-SNAPSHOT.jar file from your build environment.\n\n3. For __Class Name__, enter `com.mirosoft.example.EventHubWriter`.\n\n4. For __Additional Parameters__, enter `writer`. Finally, click __Submit__ to upload the jar and start the EventHubWriter topology.\n\n5. Once the topology has started, use the form to start the EventHubReader:\n\n    * __Jar File__: select the EventHubExample-1.0-SNAPSHOT.jar that was previously uploaded\n    * __Class Name__: enter `com.microsoft.example.EventHubReader`\n    * __Additional Parameters__: enter `reader`\n\n    Click submit to start the EventHubReader topology.\n\n6. Wait a few minutes to allow the topologies to generate events and store then to Azure Storage, then select the __Query Console__ tab at the top of the __Storm Dashboard__ page.\n\n7. On the __Query Console__, select __Hive Editor__ and replace the default `select * from hivesampletable` with the following:\n\n        create external table devicedata (deviceid string, devicevalue int) row format delimited fields terminated by ',' stored as textfile location 'wasb:///devicedata/';\n        select * from devicedata limit 10;\n\n    Click __Select__ to run the query. This will return 10 rows from the data written to Azure Storage (WASB) by the EventHubReader. Once the query completes, you should see data similar to the following:\n\n        3409e622-c85d-4d64-8622-af45e30bf774,848981614\n        c3305f7e-6948-4cce-89b0-d9fbc2330c36,-1638780537\n        788b9796-e2ab-49c4-91e3-bc5b6af1f07e,-1662107246\n        6403df8a-6495-402f-bca0-3244be67f225,275738503\n        d7c7f96c-581a-45b1-b66c-e32de6d47fce,543829859\n        9a692795-e6aa-4946-98c1-2de381b37593,1857409996\n        3c8d199b-0003-4a79-8d03-24e13bde7086,-1271260574\n\n8. Select the __Storm Dashboard__ at the top of the page, then select __Storm UI__. From the __Storm UI__, select the link for the __reader__ topology and then use the __Kill__ button to stop the topology. Repeat the process for the __writer__ topology.\n\n\n\n### Checkpointing\n\nThe EventHubSpout periodically checkpoints its state to the Zookeeper node, which saves the current offset for messages read from the queue. This allows the component to start receiving messages at the saved offset in the following scenarios:\n\n* The component instance fails and is restarted.\n\n* You grow or shrink the cluster by adding or removing nodes.\n\n* The topology is killed and restarted **with the same name**.\n\n####On Windows-based HDInsight clusters\n\nYou can export and import the persisted checkpoints to WASB (the Azure Storage used by your HDInsight cluster.) The scripts to do this are located on the Storm on HDInsight cluster, at **c:\\apps\\dist\\storm-0.9.3.2.2.1.0-2340\\zkdatatool-1.0\\bin**.\n\n>[AZURE.NOTE] The version number in the path may be different, as the version of Storm installed on the cluster may change in the future.\n\nThe scripts in this directory are:\n\n* **stormmeta_import.cmd**: Import all Storm metadata from the cluster default storage container into Zookeeper.\n\n* **stormmeta_export.cmd**: Export all Storm metadata from Zookeeper to the cluster default storage container.\n\n* **stormmeta_delete.cmd**: Delete all Storm metadata from Zookeeper.\n\nExport an import allows you to persist checkpoint data when you need to delete the cluster, but want to resume processing from the current offset in the hub when you bring a new cluster back online.\n\n> [AZURE.NOTE] Since the data is persisted to the default storage container, the new cluster **must** use the same storage account and container as the previous cluster.\n\n##Troubleshooting\n\nIf you do not see files being stored to the the /devicedata location (either using the `hadoop fs -ls /devicedata` command or the Hive commandd in the Query Console,) use the Storm UI to look for any errors returned by the topologies.\n\nFor more information on using the Storm UI, see the following topics:\n\n* If you are using a __Linux-based__ Storm on HDInsight cluster, see [Deploy and manage Apache Storm topologies on Linux-based HDInsight](hdinsight-storm-deploy-monitor-topology-linux.md)\n\n* If you are using a __Windows-based__ Storm on HDInsight cluster, see [Deploy and manage Apache Storm topologies on Windows-based HDInsight](hdinsight-storm-deploy-monitor-topology-linux.md)\n\n##Next steps\n\n* [Example topologies for Storm on HDInsight](hdinsight-storm-example-topology.md)\n"
}