{
  "nodes": [
    {
      "pos": [
        28,
        117
      ],
      "content": "Create standalone scala applications to run on HDInsight Spark clusters | Microsoft Azure"
    },
    {
      "pos": [
        137,
        223
      ],
      "content": "Learn how to create a standalone Spark application to run on HDInsight Spark clusters."
    },
    {
      "pos": [
        564,
        647
      ],
      "content": "Create a standalone Scala application and to run on HDInsight Spark cluster (Linux)"
    },
    {
      "pos": [
        649,
        1002
      ],
      "content": "This article provides step-by-step guidance on developing standalone Spark applications written in Scala using IntelliJ IDEA. The article uses Apache Maven as the build system and start with an existing Maven archetype for Scala provided by IntelliJ IDEA.  At a high-level, creating a Scala application in IntelliJ IDEA will involve the following steps:",
      "nodes": [
        {
          "content": "This article provides step-by-step guidance on developing standalone Spark applications written in Scala using IntelliJ IDEA.",
          "pos": [
            0,
            125
          ]
        },
        {
          "content": "The article uses Apache Maven as the build system and start with an existing Maven archetype for Scala provided by IntelliJ IDEA.",
          "pos": [
            126,
            255
          ]
        },
        {
          "content": "At a high-level, creating a Scala application in IntelliJ IDEA will involve the following steps:",
          "pos": [
            257,
            353
          ]
        }
      ]
    },
    {
      "pos": [
        1007,
        1037
      ],
      "content": "Use Maven as the build system."
    },
    {
      "pos": [
        1040,
        1115
      ],
      "content": "Update Project Object Model (POM) file to resolve Spark module dependencies"
    },
    {
      "pos": [
        1118,
        1150
      ],
      "content": "Write your application in Scala."
    },
    {
      "pos": [
        1153,
        1223
      ],
      "content": "Generate a jar file that can be submitted to HDInsight Spark clusters."
    },
    {
      "pos": [
        1226,
        1282
      ],
      "content": "Run the application on Spark cluster using spark-submit."
    },
    {
      "pos": [
        1284,
        1301
      ],
      "content": "<bpt id=\"p1\">**</bpt>Prerequisites<ept id=\"p1\">**</ept>"
    },
    {
      "pos": [
        1305,
        1459
      ],
      "content": "An Azure subscription. See <bpt id=\"p2\">[</bpt>Get Azure free trial<ept id=\"p2\">](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/)</ept>.",
      "nodes": [
        {
          "content": "An Azure subscription.",
          "pos": [
            0,
            22
          ]
        },
        {
          "content": "See <bpt id=\"p2\">[</bpt>Get Azure free trial<ept id=\"p2\">](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/)</ept>.",
          "pos": [
            23,
            192
          ]
        }
      ]
    },
    {
      "pos": [
        1462,
        1623
      ],
      "content": "An Apache Spark cluster on HDInsight Linux. For instructions, see <bpt id=\"p3\">[</bpt>Create Apache Spark clusters in Azure HDInsight<ept id=\"p3\">](hdinsight-apache-spark-jupyter-spark-sql.md)</ept>.",
      "nodes": [
        {
          "content": "An Apache Spark cluster on HDInsight Linux.",
          "pos": [
            0,
            43
          ]
        },
        {
          "content": "For instructions, see <bpt id=\"p3\">[</bpt>Create Apache Spark clusters in Azure HDInsight<ept id=\"p3\">](hdinsight-apache-spark-jupyter-spark-sql.md)</ept>.",
          "pos": [
            44,
            199
          ]
        }
      ]
    },
    {
      "pos": [
        1626,
        1771
      ],
      "content": "Oracle Java Development kit. You can install it from <bpt id=\"p4\">[</bpt>here<ept id=\"p4\">](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)</ept>.",
      "nodes": [
        {
          "content": "Oracle Java Development kit.",
          "pos": [
            0,
            28
          ]
        },
        {
          "content": "You can install it from <bpt id=\"p4\">[</bpt>here<ept id=\"p4\">](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)</ept>.",
          "pos": [
            29,
            183
          ]
        }
      ]
    },
    {
      "pos": [
        1774,
        1899
      ],
      "content": "A Java IDE. This article uses IntelliJ IDEA 15.0.1. You can install it from <bpt id=\"p5\">[</bpt>here<ept id=\"p5\">](https://www.jetbrains.com/idea/download/)</ept>.",
      "nodes": [
        {
          "content": "A Java IDE.",
          "pos": [
            0,
            11
          ]
        },
        {
          "content": "This article uses IntelliJ IDEA 15.0.1.",
          "pos": [
            12,
            51
          ]
        },
        {
          "content": "You can install it from <bpt id=\"p5\">[</bpt>here<ept id=\"p5\">](https://www.jetbrains.com/idea/download/)</ept>.",
          "pos": [
            52,
            163
          ]
        }
      ]
    },
    {
      "pos": [
        1906,
        1944
      ],
      "content": "Install Scala plugin for IntelliJ IDEA"
    },
    {
      "pos": [
        1946,
        2100
      ],
      "content": "If IntelliJ IDEA installation did not not prompt for enabling Scala plugin, launch IntelliJ IDEA and go through the following steps to install the plugin:"
    },
    {
      "pos": [
        2105,
        2196
      ],
      "content": "Start IntelliJ IDEA and from welcome screen click <bpt id=\"p6\">**</bpt>Configure<ept id=\"p6\">**</ept><ph id=\"ph2\"/> and then click <bpt id=\"p7\">**</bpt>Plugins<ept id=\"p7\">**</ept>."
    },
    {
      "pos": [
        2202,
        2310
      ],
      "content": "<ph id=\"ph3\">![</ph>Enable scala plugin<ph id=\"ph4\">](./media/hdinsight-apache-spark-create-standalone-application/enable-scala-plugin.png)</ph>"
    },
    {
      "pos": [
        2315,
        2501
      ],
      "content": "In the next screen, click <bpt id=\"p8\">**</bpt>Install JetBrains plugin<ept id=\"p8\">**</ept><ph id=\"ph5\"/> from the lower left corner. In the <bpt id=\"p9\">**</bpt>Browse JetBrains Plugins<ept id=\"p9\">**</ept><ph id=\"ph6\"/> dialog box that opens, search for Scala and then click <bpt id=\"p10\">**</bpt>Install<ept id=\"p10\">**</ept>.",
      "nodes": [
        {
          "content": "In the next screen, click <bpt id=\"p8\">**</bpt>Install JetBrains plugin<ept id=\"p8\">**</ept><ph id=\"ph5\"/> from the lower left corner.",
          "pos": [
            0,
            134
          ]
        },
        {
          "content": "In the <bpt id=\"p9\">**</bpt>Browse JetBrains Plugins<ept id=\"p9\">**</ept><ph id=\"ph6\"/> dialog box that opens, search for Scala and then click <bpt id=\"p10\">**</bpt>Install<ept id=\"p10\">**</ept>.",
          "pos": [
            135,
            330
          ]
        }
      ]
    },
    {
      "pos": [
        2507,
        2617
      ],
      "content": "<ph id=\"ph7\">![</ph>Install scala plugin<ph id=\"ph8\">](./media/hdinsight-apache-spark-create-standalone-application/install-scala-plugin.png)</ph>"
    },
    {
      "pos": [
        2622,
        2724
      ],
      "content": "After the plugin installs successfully, click the <bpt id=\"p11\">**</bpt>Restart IntelliJ IDEA button<ept id=\"p11\">**</ept><ph id=\"ph9\"/> to restart the IDE."
    },
    {
      "pos": [
        2729,
        2762
      ],
      "content": "Create a standalone Scala project"
    },
    {
      "pos": [
        2767,
        2897
      ],
      "content": "Launch IntelliJ IDEA and create a new project. In the new project dialog box, make the following choices, and then click <bpt id=\"p12\">**</bpt>Next<ept id=\"p12\">**</ept>.",
      "nodes": [
        {
          "content": "Launch IntelliJ IDEA and create a new project.",
          "pos": [
            0,
            46
          ]
        },
        {
          "content": "In the new project dialog box, make the following choices, and then click <bpt id=\"p12\">**</bpt>Next<ept id=\"p12\">**</ept>.",
          "pos": [
            47,
            170
          ]
        }
      ]
    },
    {
      "pos": [
        2903,
        3013
      ],
      "content": "<ph id=\"ph10\">![</ph>Create Maven project<ph id=\"ph11\">](./media/hdinsight-apache-spark-create-standalone-application/create-maven-project.png)</ph>"
    },
    {
      "pos": [
        3021,
        3058
      ],
      "content": "Select <bpt id=\"p13\">**</bpt>Maven<ept id=\"p13\">**</ept><ph id=\"ph12\"/> as the project type."
    },
    {
      "pos": [
        3065,
        3197
      ],
      "content": "Specify a <bpt id=\"p14\">**</bpt>Project SDK<ept id=\"p14\">**</ept>. Click New and navigate to the Java installation directory, typically <ph id=\"ph13\">`C:\\Program Files\\Java\\jdk1.8.0_66`</ph>.",
      "nodes": [
        {
          "content": "Specify a <bpt id=\"p14\">**</bpt>Project SDK<ept id=\"p14\">**</ept>.",
          "pos": [
            0,
            66
          ]
        },
        {
          "content": "Click New and navigate to the Java installation directory, typically <ph id=\"ph13\">`C:\\Program Files\\Java\\jdk1.8.0_66`</ph>.",
          "pos": [
            67,
            191
          ]
        }
      ]
    },
    {
      "pos": [
        3204,
        3248
      ],
      "content": "Select the <bpt id=\"p15\">**</bpt>Create from archetype<ept id=\"p15\">**</ept><ph id=\"ph14\"/> option."
    },
    {
      "pos": [
        3255,
        3463
      ],
      "content": "From the list of archetypes, select <bpt id=\"p16\">**</bpt>org.scala-tools.archetypes:scala-archetype-simple<ept id=\"p16\">**</ept>. This will create the right directory structure and download the required default dependencies to write Scala program.",
      "nodes": [
        {
          "content": "From the list of archetypes, select <bpt id=\"p16\">**</bpt>org.scala-tools.archetypes:scala-archetype-simple<ept id=\"p16\">**</ept>.",
          "pos": [
            0,
            130
          ]
        },
        {
          "content": "This will create the right directory structure and download the required default dependencies to write Scala program.",
          "pos": [
            131,
            248
          ]
        }
      ]
    },
    {
      "pos": [
        3468,
        3557
      ],
      "content": "Provide relevant values for <bpt id=\"p17\">**</bpt>GroupId<ept id=\"p17\">**</ept>, <bpt id=\"p18\">**</bpt>ArtifactId<ept id=\"p18\">**</ept>, and <bpt id=\"p19\">**</bpt>Version<ept id=\"p19\">**</ept>. Click <bpt id=\"p20\">**</bpt>Next<ept id=\"p20\">**</ept>.",
      "nodes": [
        {
          "content": "Provide relevant values for <bpt id=\"p17\">**</bpt>GroupId<ept id=\"p17\">**</ept>, <bpt id=\"p18\">**</bpt>ArtifactId<ept id=\"p18\">**</ept>, and <bpt id=\"p19\">**</bpt>Version<ept id=\"p19\">**</ept>.",
          "pos": [
            0,
            193
          ]
        },
        {
          "content": "Click <bpt id=\"p20\">**</bpt>Next<ept id=\"p20\">**</ept>.",
          "pos": [
            194,
            249
          ]
        }
      ]
    },
    {
      "pos": [
        3562,
        3689
      ],
      "content": "In the next dialog box, where you specify Maven home directory and other user settings, accept the defaults and click <bpt id=\"p21\">**</bpt>Next<ept id=\"p21\">**</ept>."
    },
    {
      "pos": [
        3694,
        3780
      ],
      "content": "In the last dialog box, specify a project name and location and then click <bpt id=\"p22\">**</bpt>Finish<ept id=\"p22\">**</ept>."
    },
    {
      "pos": [
        3785,
        3910
      ],
      "content": "Delete the <bpt id=\"p23\">**</bpt>MySpec.Scala<ept id=\"p23\">**</ept><ph id=\"ph15\"/> file at <bpt id=\"p24\">**</bpt>src\\test\\scala\\com\\microsoft\\spark\\example<ept id=\"p24\">**</ept>. You do not need this for the application.",
      "nodes": [
        {
          "content": "Delete the <bpt id=\"p23\">**</bpt>MySpec.Scala<ept id=\"p23\">**</ept><ph id=\"ph15\"/> file at <bpt id=\"p24\">**</bpt>src\\test\\scala\\com\\microsoft\\spark\\example<ept id=\"p24\">**</ept>.",
          "pos": [
            0,
            178
          ]
        },
        {
          "content": "You do not need this for the application.",
          "pos": [
            179,
            220
          ]
        }
      ]
    },
    {
      "pos": [
        3915,
        4229
      ],
      "content": "If required, rename the default source and test files. From the left pane in the IntelliJ IDEA, navigate to <bpt id=\"p25\">**</bpt>src\\main\\scala\\com.microsoft.spark.example<ept id=\"p25\">**</ept>. Right-click <bpt id=\"p26\">**</bpt>App.scala<ept id=\"p26\">**</ept>, click <bpt id=\"p27\">**</bpt>Refactor<ept id=\"p27\">**</ept>, click Rename file, and in the dialog box, provide the new name for the application and then click <bpt id=\"p28\">**</bpt>Refactor<ept id=\"p28\">**</ept>.",
      "nodes": [
        {
          "content": "If required, rename the default source and test files.",
          "pos": [
            0,
            54
          ]
        },
        {
          "content": "From the left pane in the IntelliJ IDEA, navigate to <bpt id=\"p25\">**</bpt>src\\main\\scala\\com.microsoft.spark.example<ept id=\"p25\">**</ept>.",
          "pos": [
            55,
            195
          ]
        },
        {
          "content": "Right-click <bpt id=\"p26\">**</bpt>App.scala<ept id=\"p26\">**</ept>, click <bpt id=\"p27\">**</bpt>Refactor<ept id=\"p27\">**</ept>, click Rename file, and in the dialog box, provide the new name for the application and then click <bpt id=\"p28\">**</bpt>Refactor<ept id=\"p28\">**</ept>.",
          "pos": [
            196,
            474
          ]
        }
      ]
    },
    {
      "pos": [
        4235,
        4335
      ],
      "content": "<ph id=\"ph16\">![</ph>Rename files<ph id=\"ph17\">](./media/hdinsight-apache-spark-create-standalone-application/rename-scala-files.png)</ph>"
    },
    {
      "pos": [
        4342,
        4560
      ],
      "content": "In the subsequent steps, you will update the pom.xml to define the dependencies for the Spark Scala application. For those dependencies to be downloaded and resolved automatically, you must configure Maven accordingly.",
      "nodes": [
        {
          "content": "In the subsequent steps, you will update the pom.xml to define the dependencies for the Spark Scala application.",
          "pos": [
            0,
            112
          ]
        },
        {
          "content": "For those dependencies to be downloaded and resolved automatically, you must configure Maven accordingly.",
          "pos": [
            113,
            218
          ]
        }
      ]
    },
    {
      "pos": [
        4566,
        4690
      ],
      "content": "<ph id=\"ph18\">![</ph>Configure Maven for automatic downloads<ph id=\"ph19\">](./media/hdinsight-apache-spark-create-standalone-application/configure-maven.png)</ph>"
    },
    {
      "pos": [
        4699,
        4742
      ],
      "content": "From the <bpt id=\"p29\">**</bpt>File<ept id=\"p29\">**</ept><ph id=\"ph20\"/> menu, click <bpt id=\"p30\">**</bpt>Settings<ept id=\"p30\">**</ept>."
    },
    {
      "pos": [
        4750,
        4873
      ],
      "content": "In the <bpt id=\"p31\">**</bpt>Settings<ept id=\"p31\">**</ept><ph id=\"ph21\"/> dialog box, navigate to <bpt id=\"p32\">**</bpt>Build, Execution, Deployment<ept id=\"p32\">**</ept><ph id=\"ph22\"/> &gt; <bpt id=\"p33\">**</bpt>Build Tools<ept id=\"p33\">**</ept><ph id=\"ph23\"/> &gt; <bpt id=\"p34\">**</bpt>Maven<ept id=\"p34\">**</ept><ph id=\"ph24\"/> &gt; <bpt id=\"p35\">**</bpt>Importing<ept id=\"p35\">**</ept>."
    },
    {
      "pos": [
        4881,
        4942
      ],
      "content": "Select the option to <bpt id=\"p36\">**</bpt>Import Maven projects automatically<ept id=\"p36\">**</ept>."
    },
    {
      "pos": [
        4950,
        4989
      ],
      "content": "Click <bpt id=\"p37\">**</bpt>Apply<ept id=\"p37\">**</ept>, and then click <bpt id=\"p38\">**</bpt>OK<ept id=\"p38\">**</ept>."
    },
    {
      "pos": [
        4996,
        5390
      ],
      "content": "Update the Scala source file to include your application code. Open and replace the existing sample code with the following code and save the changes. This code reads the data from the HVAC.csv (available on all HDInsight Spark clusters), retrieves the rows that only have one digit in the sixth column, and writes the output to <bpt id=\"p39\">**</bpt>/HVACOut<ept id=\"p39\">**</ept><ph id=\"ph25\"/> under the default storage container for the cluster.",
      "nodes": [
        {
          "content": "Update the Scala source file to include your application code.",
          "pos": [
            0,
            62
          ]
        },
        {
          "content": "Open and replace the existing sample code with the following code and save the changes.",
          "pos": [
            63,
            150
          ]
        },
        {
          "content": "This code reads the data from the HVAC.csv (available on all HDInsight Spark clusters), retrieves the rows that only have one digit in the sixth column, and writes the output to <bpt id=\"p39\">**</bpt>/HVACOut<ept id=\"p39\">**</ept><ph id=\"ph25\"/> under the default storage container for the cluster.",
          "pos": [
            151,
            449
          ]
        }
      ]
    },
    {
      "pos": [
        6128,
        6147
      ],
      "content": "Update the pom.xml."
    },
    {
      "pos": [
        6157,
        6207
      ],
      "content": "Within <ph id=\"ph26\">`&lt;project&gt;\\&lt;properties&gt;`</ph><ph id=\"ph27\"/> add the following:"
    },
    {
      "pos": [
        6393,
        6445
      ],
      "content": "Within <ph id=\"ph28\">`&lt;project&gt;\\&lt;dependencies&gt;`</ph><ph id=\"ph29\"/> add the following:"
    },
    {
      "pos": [
        6666,
        6690
      ],
      "content": "Save changes to pom.xml."
    },
    {
      "pos": [
        6696,
        6813
      ],
      "content": "Create the .jar file. IntelliJ IDEA enables creation of JAR as an artifact of a project. Perform the following steps.",
      "nodes": [
        {
          "content": "Create the .jar file.",
          "pos": [
            0,
            21
          ]
        },
        {
          "content": "IntelliJ IDEA enables creation of JAR as an artifact of a project.",
          "pos": [
            22,
            88
          ]
        },
        {
          "content": "Perform the following steps.",
          "pos": [
            89,
            117
          ]
        }
      ]
    },
    {
      "pos": [
        6822,
        6874
      ],
      "content": "From the <bpt id=\"p40\">**</bpt>File<ept id=\"p40\">**</ept><ph id=\"ph30\"/> menu, click <bpt id=\"p41\">**</bpt>Project Structure<ept id=\"p41\">**</ept>."
    },
    {
      "pos": [
        6882,
        7068
      ],
      "content": "In the <bpt id=\"p42\">**</bpt>Project Structure<ept id=\"p42\">**</ept><ph id=\"ph31\"/> dialog box, click <bpt id=\"p43\">**</bpt>Artifacts<ept id=\"p43\">**</ept><ph id=\"ph32\"/> and then click the plus symbol. From the pop-up dialog box, click <bpt id=\"p44\">**</bpt>JAR<ept id=\"p44\">**</ept>, and then click <bpt id=\"p45\">**</bpt>From modules with dependencies<ept id=\"p45\">**</ept>.",
      "nodes": [
        {
          "content": "In the <bpt id=\"p42\">**</bpt>Project Structure<ept id=\"p42\">**</ept><ph id=\"ph31\"/> dialog box, click <bpt id=\"p43\">**</bpt>Artifacts<ept id=\"p43\">**</ept><ph id=\"ph32\"/> and then click the plus symbol.",
          "pos": [
            0,
            202
          ]
        },
        {
          "content": "From the pop-up dialog box, click <bpt id=\"p44\">**</bpt>JAR<ept id=\"p44\">**</ept>, and then click <bpt id=\"p45\">**</bpt>From modules with dependencies<ept id=\"p45\">**</ept>.",
          "pos": [
            203,
            376
          ]
        }
      ]
    },
    {
      "pos": [
        7078,
        7170
      ],
      "content": "<ph id=\"ph33\">![</ph>Create JAR<ph id=\"ph34\">](./media/hdinsight-apache-spark-create-standalone-application/create-jar-1.png)</ph>"
    },
    {
      "pos": [
        7179,
        7362
      ],
      "content": "In the <bpt id=\"p46\">**</bpt>Create JAR from Modules<ept id=\"p46\">**</ept><ph id=\"ph35\"/> dialog box, click the ellipsis (<ph id=\"ph36\">![</ph>ellipsis<ph id=\"ph37\">](./media/hdinsight-apache-spark-create-standalone-application/ellipsis.png)</ph> ) against the <bpt id=\"p47\">**</bpt>Main Class<ept id=\"p47\">**</ept>."
    },
    {
      "pos": [
        7371,
        7475
      ],
      "content": "In the <bpt id=\"p48\">**</bpt>Select Main Class<ept id=\"p48\">**</ept><ph id=\"ph38\"/> dialog box, select the class that appears by default and then click <bpt id=\"p49\">**</bpt>OK<ept id=\"p49\">**</ept>."
    },
    {
      "pos": [
        7485,
        7577
      ],
      "content": "<ph id=\"ph39\">![</ph>Create JAR<ph id=\"ph40\">](./media/hdinsight-apache-spark-create-standalone-application/create-jar-2.png)</ph>"
    },
    {
      "pos": [
        7586,
        7776
      ],
      "content": "In the <bpt id=\"p50\">**</bpt>Create JAR from Modules<ept id=\"p50\">**</ept><ph id=\"ph41\"/> dialog box, make sure that the option to <bpt id=\"p51\">**</bpt>extract to the target JAR<ept id=\"p51\">**</ept><ph id=\"ph42\"/> is selected, and then click <bpt id=\"p52\">**</bpt>OK<ept id=\"p52\">**</ept>. This creates a single JAR with all dependencies.",
      "nodes": [
        {
          "content": "In the <bpt id=\"p50\">**</bpt>Create JAR from Modules<ept id=\"p50\">**</ept><ph id=\"ph41\"/> dialog box, make sure that the option to <bpt id=\"p51\">**</bpt>extract to the target JAR<ept id=\"p51\">**</ept><ph id=\"ph42\"/> is selected, and then click <bpt id=\"p52\">**</bpt>OK<ept id=\"p52\">**</ept>.",
          "pos": [
            0,
            291
          ]
        },
        {
          "content": "This creates a single JAR with all dependencies.",
          "pos": [
            292,
            340
          ]
        }
      ]
    },
    {
      "pos": [
        7786,
        7878
      ],
      "content": "<ph id=\"ph43\">![</ph>Create JAR<ph id=\"ph44\">](./media/hdinsight-apache-spark-create-standalone-application/create-jar-3.png)</ph>"
    },
    {
      "pos": [
        7887,
        8244
      ],
      "content": "The output layout tab lists all the jars that are included as part of the Maven project. You can select and delete the ones on which the Scala application has no direct dependency. For the application we are creating here, you can remove all but the last one (<bpt id=\"p53\">**</bpt>SparkSimpleApp compile output<ept id=\"p53\">**</ept>). Select the jars to delete and then click the <bpt id=\"p54\">**</bpt>Delete<ept id=\"p54\">**</ept><ph id=\"ph45\"/> icon.",
      "nodes": [
        {
          "content": "The output layout tab lists all the jars that are included as part of the Maven project.",
          "pos": [
            0,
            88
          ]
        },
        {
          "content": "You can select and delete the ones on which the Scala application has no direct dependency.",
          "pos": [
            89,
            180
          ]
        },
        {
          "content": "For the application we are creating here, you can remove all but the last one (<bpt id=\"p53\">**</bpt>SparkSimpleApp compile output<ept id=\"p53\">**</ept>).",
          "pos": [
            181,
            335
          ]
        },
        {
          "content": "Select the jars to delete and then click the <bpt id=\"p54\">**</bpt>Delete<ept id=\"p54\">**</ept><ph id=\"ph45\"/> icon.",
          "pos": [
            336,
            452
          ]
        }
      ]
    },
    {
      "pos": [
        8254,
        8352
      ],
      "content": "<ph id=\"ph46\">![</ph>Create JAR<ph id=\"ph47\">](./media/hdinsight-apache-spark-create-standalone-application/delete-output-jars.png)</ph>"
    },
    {
      "pos": [
        8362,
        8521
      ],
      "content": "Make sure <bpt id=\"p55\">**</bpt>Build on make<ept id=\"p55\">**</ept><ph id=\"ph48\"/> box is selected, which ensures that the jar is created every time the project is built or updated. Click <bpt id=\"p56\">**</bpt>Apply<ept id=\"p56\">**</ept><ph id=\"ph49\"/> and then <bpt id=\"p57\">**</bpt>OK<ept id=\"p57\">**</ept>.",
      "nodes": [
        {
          "content": "Make sure <bpt id=\"p55\">**</bpt>Build on make<ept id=\"p55\">**</ept><ph id=\"ph48\"/> box is selected, which ensures that the jar is created every time the project is built or updated.",
          "pos": [
            0,
            181
          ]
        },
        {
          "content": "Click <bpt id=\"p56\">**</bpt>Apply<ept id=\"p56\">**</ept><ph id=\"ph49\"/> and then <bpt id=\"p57\">**</bpt>OK<ept id=\"p57\">**</ept>.",
          "pos": [
            182,
            309
          ]
        }
      ]
    },
    {
      "pos": [
        8530,
        8708
      ],
      "content": "From the menu bar, click <bpt id=\"p58\">**</bpt>Build<ept id=\"p58\">**</ept>, and then click <bpt id=\"p59\">**</bpt>Make Project<ept id=\"p59\">**</ept>. You can also click <bpt id=\"p60\">**</bpt>Build Artifacts<ept id=\"p60\">**</ept><ph id=\"ph50\"/> to create the jar. The output jar is created under <bpt id=\"p61\">**</bpt>\\out\\artifacts<ept id=\"p61\">**</ept>.",
      "nodes": [
        {
          "content": "From the menu bar, click <bpt id=\"p58\">**</bpt>Build<ept id=\"p58\">**</ept>, and then click <bpt id=\"p59\">**</bpt>Make Project<ept id=\"p59\">**</ept>.",
          "pos": [
            0,
            148
          ]
        },
        {
          "content": "You can also click <bpt id=\"p60\">**</bpt>Build Artifacts<ept id=\"p60\">**</ept><ph id=\"ph50\"/> to create the jar.",
          "pos": [
            149,
            261
          ]
        },
        {
          "content": "The output jar is created under <bpt id=\"p61\">**</bpt>\\out\\artifacts<ept id=\"p61\">**</ept>.",
          "pos": [
            262,
            353
          ]
        }
      ]
    },
    {
      "pos": [
        8718,
        8804
      ],
      "content": "<ph id=\"ph51\">![</ph>Create JAR<ph id=\"ph52\">](./media/hdinsight-apache-spark-create-standalone-application/output.png)</ph>"
    },
    {
      "pos": [
        8809,
        8849
      ],
      "content": "Run the application on the Spark cluster"
    },
    {
      "pos": [
        8851,
        8916
      ],
      "content": "To run the application on the cluster, you must do the following:"
    },
    {
      "pos": [
        8920,
        9270
      ],
      "content": "<bpt id=\"p62\">**</bpt>Copy the application jar to the Azure storage blob<ept id=\"p62\">**</ept><ph id=\"ph53\"/> associated with the cluster. You can use <bpt id=\"p63\">[</bpt><bpt id=\"p64\">**</bpt>AzCopy<ept id=\"p64\">**</ept><ept id=\"p63\">](storage/storage-use-azcopy.md)</ept>, a command line utility, to do so. There are a lot of other clients as well that you can use to upload data. You can find more about them at <bpt id=\"p65\">[</bpt>Upload data for Hadoop jobs in HDInsight<ept id=\"p65\">](hdinsight-upload-data.md)</ept>.",
      "nodes": [
        {
          "content": "<bpt id=\"p62\">**</bpt>Copy the application jar to the Azure storage blob<ept id=\"p62\">**</ept><ph id=\"ph53\"/> associated with the cluster.",
          "pos": [
            0,
            138
          ]
        },
        {
          "content": "You can use <bpt id=\"p63\">[</bpt><bpt id=\"p64\">**</bpt>AzCopy<ept id=\"p64\">**</ept><ept id=\"p63\">](storage/storage-use-azcopy.md)</ept>, a command line utility, to do so.",
          "pos": [
            139,
            309
          ]
        },
        {
          "content": "There are a lot of other clients as well that you can use to upload data.",
          "pos": [
            310,
            383
          ]
        },
        {
          "content": "You can find more about them at <bpt id=\"p65\">[</bpt>Upload data for Hadoop jobs in HDInsight<ept id=\"p65\">](hdinsight-upload-data.md)</ept>.",
          "pos": [
            384,
            525
          ]
        }
      ]
    },
    {
      "pos": [
        9274,
        9594
      ],
      "content": "<bpt id=\"p66\">**</bpt>Use Livy to submit an application job remotely<ept id=\"p66\">**</ept><ph id=\"ph54\"/> to the Spark cluster. Spark clusters on HDInsight includes Livy that exposes REST endpoints to remotely submit Spark jobs. For more information, see <bpt id=\"p67\">[</bpt>Submit Spark jobs remotely using Livy with Spark clusters on HDInsight<ept id=\"p67\">](hdinsight-apache-spark-livy-rest-interface.md)</ept>.",
      "nodes": [
        {
          "content": "<bpt id=\"p66\">**</bpt>Use Livy to submit an application job remotely<ept id=\"p66\">**</ept><ph id=\"ph54\"/> to the Spark cluster.",
          "pos": [
            0,
            127
          ]
        },
        {
          "content": "Spark clusters on HDInsight includes Livy that exposes REST endpoints to remotely submit Spark jobs.",
          "pos": [
            128,
            228
          ]
        },
        {
          "content": "For more information, see <bpt id=\"p67\">[</bpt>Submit Spark jobs remotely using Livy with Spark clusters on HDInsight<ept id=\"p67\">](hdinsight-apache-spark-livy-rest-interface.md)</ept>.",
          "pos": [
            229,
            415
          ]
        }
      ]
    },
    {
      "pos": [
        9622,
        9630
      ],
      "content": "See also"
    },
    {
      "pos": [
        9635,
        9714
      ],
      "content": "<bpt id=\"p68\">[</bpt>Overview: Apache Spark on Azure HDInsight<ept id=\"p68\">](hdinsight-apache-spark-overview.md)</ept>"
    },
    {
      "pos": [
        9720,
        9729
      ],
      "content": "Scenarios"
    },
    {
      "pos": [
        9733,
        9862
      ],
      "content": "<bpt id=\"p69\">[</bpt>Spark with BI: Perform interactive data analysis using Spark in HDInsight with BI tools<ept id=\"p69\">](hdinsight-apache-spark-use-bi-tools.md)</ept>"
    },
    {
      "pos": [
        9866,
        10031
      ],
      "content": "<bpt id=\"p70\">[</bpt>Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data<ept id=\"p70\">](hdinsight-apache-spark-ipython-notebook-machine-learning.md)</ept>"
    },
    {
      "pos": [
        10035,
        10181
      ],
      "content": "<bpt id=\"p71\">[</bpt>Spark with Machine Learning: Use Spark in HDInsight to predict food inspection results<ept id=\"p71\">](hdinsight-apache-spark-machine-learning-mllib-ipython.md)</ept>"
    },
    {
      "pos": [
        10185,
        10318
      ],
      "content": "<bpt id=\"p72\">[</bpt>Spark Streaming: Use Spark in HDInsight for building real-time streaming applications<ept id=\"p72\">](hdinsight-apache-spark-eventhub-streaming.md)</ept>"
    },
    {
      "pos": [
        10322,
        10432
      ],
      "content": "<bpt id=\"p73\">[</bpt>Website log analysis using Spark in HDInsight<ept id=\"p73\">](hdinsight-apache-spark-custom-library-website-log-analysis.md)</ept>"
    },
    {
      "pos": [
        10438,
        10465
      ],
      "content": "Create and run applications"
    },
    {
      "pos": [
        10469,
        10565
      ],
      "content": "<bpt id=\"p74\">[</bpt>Run jobs remotely on a Spark cluster using Livy<ept id=\"p74\">](hdinsight-apache-spark-livy-rest-interface.md)</ept>"
    },
    {
      "pos": [
        10571,
        10591
      ],
      "content": "Tools and extensions"
    },
    {
      "pos": [
        10595,
        10734
      ],
      "content": "<bpt id=\"p75\">[</bpt>Use HDInsight Tools Plugin for IntelliJ IDEA to create and submit Spark Scala applicatons<ept id=\"p75\">](hdinsight-apache-spark-intellij-tool-plugin.md)</ept>"
    },
    {
      "pos": [
        10738,
        10845
      ],
      "content": "<bpt id=\"p76\">[</bpt>Use Zeppelin notebooks with a Spark cluster on HDInsight<ept id=\"p76\">](hdinsight-apache-spark-use-zeppelin-notebook.md)</ept>"
    },
    {
      "pos": [
        10849,
        10972
      ],
      "content": "<bpt id=\"p77\">[</bpt>Kernels available for Jupyter notebook in Spark cluster for HDInsight<ept id=\"p77\">](hdinsight-apache-spark-jupyter-notebook-kernels.md)</ept>"
    },
    {
      "pos": [
        10978,
        10994
      ],
      "content": "Manage resources"
    },
    {
      "pos": [
        10998,
        11108
      ],
      "content": "<bpt id=\"p78\">[</bpt>Manage resources for the Apache Spark cluster in Azure HDInsight<ept id=\"p78\">](hdinsight-apache-spark-resource-manager.md)</ept>"
    }
  ],
  "content": "<properties \n    pageTitle=\"Create standalone scala applications to run on HDInsight Spark clusters | Microsoft Azure\" \n    description=\"Learn how to create a standalone Spark application to run on HDInsight Spark clusters.\" \n    services=\"hdinsight\" \n    documentationCenter=\"\" \n    authors=\"nitinme\" \n    manager=\"paulettm\" \n    editor=\"cgronlun\"\n    tags=\"azure-portal\"/>\n\n<tags \n    ms.service=\"hdinsight\" \n    ms.workload=\"big-data\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"02/05/2016\" \n    ms.author=\"nitinme\"/>\n\n\n# Create a standalone Scala application and to run on HDInsight Spark cluster (Linux)\n\nThis article provides step-by-step guidance on developing standalone Spark applications written in Scala using IntelliJ IDEA. The article uses Apache Maven as the build system and start with an existing Maven archetype for Scala provided by IntelliJ IDEA.  At a high-level, creating a Scala application in IntelliJ IDEA will involve the following steps:\n\n\n* Use Maven as the build system.\n* Update Project Object Model (POM) file to resolve Spark module dependencies\n* Write your application in Scala.\n* Generate a jar file that can be submitted to HDInsight Spark clusters.\n* Run the application on Spark cluster using spark-submit.\n\n**Prerequisites**\n\n* An Azure subscription. See [Get Azure free trial](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/).\n* An Apache Spark cluster on HDInsight Linux. For instructions, see [Create Apache Spark clusters in Azure HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).\n* Oracle Java Development kit. You can install it from [here](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html).\n* A Java IDE. This article uses IntelliJ IDEA 15.0.1. You can install it from [here](https://www.jetbrains.com/idea/download/). \n\n\n## Install Scala plugin for IntelliJ IDEA\n\nIf IntelliJ IDEA installation did not not prompt for enabling Scala plugin, launch IntelliJ IDEA and go through the following steps to install the plugin:\n\n1. Start IntelliJ IDEA and from welcome screen click **Configure** and then click **Plugins**.\n\n    ![Enable scala plugin](./media/hdinsight-apache-spark-create-standalone-application/enable-scala-plugin.png)\n\n2. In the next screen, click **Install JetBrains plugin** from the lower left corner. In the **Browse JetBrains Plugins** dialog box that opens, search for Scala and then click **Install**.\n\n    ![Install scala plugin](./media/hdinsight-apache-spark-create-standalone-application/install-scala-plugin.png)\n\n3. After the plugin installs successfully, click the **Restart IntelliJ IDEA button** to restart the IDE.\n\n## Create a standalone Scala project\n\n1. Launch IntelliJ IDEA and create a new project. In the new project dialog box, make the following choices, and then click **Next**.\n\n    ![Create Maven project](./media/hdinsight-apache-spark-create-standalone-application/create-maven-project.png)\n\n    * Select **Maven** as the project type.\n    * Specify a **Project SDK**. Click New and navigate to the Java installation directory, typically `C:\\Program Files\\Java\\jdk1.8.0_66`.\n    * Select the **Create from archetype** option.\n    * From the list of archetypes, select **org.scala-tools.archetypes:scala-archetype-simple**. This will create the right directory structure and download the required default dependencies to write Scala program.\n\n2. Provide relevant values for **GroupId**, **ArtifactId**, and **Version**. Click **Next**.\n\n3. In the next dialog box, where you specify Maven home directory and other user settings, accept the defaults and click **Next**.\n\n4. In the last dialog box, specify a project name and location and then click **Finish**.\n\n5. Delete the **MySpec.Scala** file at **src\\test\\scala\\com\\microsoft\\spark\\example**. You do not need this for the application.\n\n6. If required, rename the default source and test files. From the left pane in the IntelliJ IDEA, navigate to **src\\main\\scala\\com.microsoft.spark.example**. Right-click **App.scala**, click **Refactor**, click Rename file, and in the dialog box, provide the new name for the application and then click **Refactor**.\n\n    ![Rename files](./media/hdinsight-apache-spark-create-standalone-application/rename-scala-files.png)  \n\n7. In the subsequent steps, you will update the pom.xml to define the dependencies for the Spark Scala application. For those dependencies to be downloaded and resolved automatically, you must configure Maven accordingly.\n\n    ![Configure Maven for automatic downloads](./media/hdinsight-apache-spark-create-standalone-application/configure-maven.png)\n\n    1. From the **File** menu, click **Settings**.\n    2. In the **Settings** dialog box, navigate to **Build, Execution, Deployment** > **Build Tools** > **Maven** > **Importing**.\n    3. Select the option to **Import Maven projects automatically**.\n    4. Click **Apply**, and then click **OK**. \n\n\n8. Update the Scala source file to include your application code. Open and replace the existing sample code with the following code and save the changes. This code reads the data from the HVAC.csv (available on all HDInsight Spark clusters), retrieves the rows that only have one digit in the sixth column, and writes the output to **/HVACOut** under the default storage container for the cluster.\n\n        package com.microsoft.spark.example\n\n        import org.apache.spark.SparkConf\n        import org.apache.spark.SparkContext\n        \n        /**\n          * Test IO to wasb\n          */\n        object WasbIOTest {\n          def main (arg: Array[String]): Unit = {\n            val conf = new SparkConf().setAppName(\"WASBIOTest\")\n            val sc = new SparkContext(conf)\n        \n            val rdd = sc.textFile(\"wasb:///HdiSamples/HdiSamples/SensorSampleData/hvac/HVAC.csv\")\n        \n            //find the rows which have only one digit in the 7th column in the CSV\n            val rdd1 = rdd.filter(s => s.split(\",\")(6).length() == 1)\n        \n            rdd1.saveAsTextFile(\"wasb:///HVACout\")\n          }\n        }\n\n\n9. Update the pom.xml.\n\n    1.  Within `<project>\\<properties>` add the following:\n\n            <scala.version>2.10.4</scala.version>\n            <scala.compat.version>2.10.4</scala.compat.version>\n            <scala.binary.version>2.10</scala.binary.version>\n\n    2. Within `<project>\\<dependencies>` add the following:\n\n            <dependency>\n              <groupId>org.apache.spark</groupId>\n              <artifactId>spark-core_${scala.binary.version}</artifactId>\n              <version>1.4.1</version>\n            </dependency>\n\n    Save changes to pom.xml.\n\n10. Create the .jar file. IntelliJ IDEA enables creation of JAR as an artifact of a project. Perform the following steps.\n\n    1. From the **File** menu, click **Project Structure**.\n    2. In the **Project Structure** dialog box, click **Artifacts** and then click the plus symbol. From the pop-up dialog box, click **JAR**, and then click **From modules with dependencies**.\n\n        ![Create JAR](./media/hdinsight-apache-spark-create-standalone-application/create-jar-1.png)\n\n    3. In the **Create JAR from Modules** dialog box, click the ellipsis (![ellipsis](./media/hdinsight-apache-spark-create-standalone-application/ellipsis.png) ) against the **Main Class**.\n\n    4. In the **Select Main Class** dialog box, select the class that appears by default and then click **OK**.\n\n        ![Create JAR](./media/hdinsight-apache-spark-create-standalone-application/create-jar-2.png)\n\n    5. In the **Create JAR from Modules** dialog box, make sure that the option to **extract to the target JAR** is selected, and then click **OK**. This creates a single JAR with all dependencies.\n\n        ![Create JAR](./media/hdinsight-apache-spark-create-standalone-application/create-jar-3.png)\n\n    6. The output layout tab lists all the jars that are included as part of the Maven project. You can select and delete the ones on which the Scala application has no direct dependency. For the application we are creating here, you can remove all but the last one (**SparkSimpleApp compile output**). Select the jars to delete and then click the **Delete** icon.\n\n        ![Create JAR](./media/hdinsight-apache-spark-create-standalone-application/delete-output-jars.png)\n\n        Make sure **Build on make** box is selected, which ensures that the jar is created every time the project is built or updated. Click **Apply** and then **OK**.\n\n    7. From the menu bar, click **Build**, and then click **Make Project**. You can also click **Build Artifacts** to create the jar. The output jar is created under **\\out\\artifacts**.\n\n        ![Create JAR](./media/hdinsight-apache-spark-create-standalone-application/output.png)\n\n## Run the application on the Spark cluster\n\nTo run the application on the cluster, you must do the following:\n\n* **Copy the application jar to the Azure storage blob** associated with the cluster. You can use [**AzCopy**](storage/storage-use-azcopy.md), a command line utility, to do so. There are a lot of other clients as well that you can use to upload data. You can find more about them at [Upload data for Hadoop jobs in HDInsight](hdinsight-upload-data.md).\n\n* **Use Livy to submit an application job remotely** to the Spark cluster. Spark clusters on HDInsight includes Livy that exposes REST endpoints to remotely submit Spark jobs. For more information, see [Submit Spark jobs remotely using Livy with Spark clusters on HDInsight](hdinsight-apache-spark-livy-rest-interface.md).\n\n\n## <a name=\"seealso\"></a>See also\n\n\n* [Overview: Apache Spark on Azure HDInsight](hdinsight-apache-spark-overview.md)\n\n### Scenarios\n\n* [Spark with BI: Perform interactive data analysis using Spark in HDInsight with BI tools](hdinsight-apache-spark-use-bi-tools.md)\n\n* [Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data](hdinsight-apache-spark-ipython-notebook-machine-learning.md)\n\n* [Spark with Machine Learning: Use Spark in HDInsight to predict food inspection results](hdinsight-apache-spark-machine-learning-mllib-ipython.md)\n\n* [Spark Streaming: Use Spark in HDInsight for building real-time streaming applications](hdinsight-apache-spark-eventhub-streaming.md)\n\n* [Website log analysis using Spark in HDInsight](hdinsight-apache-spark-custom-library-website-log-analysis.md)\n\n### Create and run applications\n\n* [Run jobs remotely on a Spark cluster using Livy](hdinsight-apache-spark-livy-rest-interface.md)\n\n### Tools and extensions\n\n* [Use HDInsight Tools Plugin for IntelliJ IDEA to create and submit Spark Scala applicatons](hdinsight-apache-spark-intellij-tool-plugin.md)\n\n* [Use Zeppelin notebooks with a Spark cluster on HDInsight](hdinsight-apache-spark-use-zeppelin-notebook.md)\n\n* [Kernels available for Jupyter notebook in Spark cluster for HDInsight](hdinsight-apache-spark-jupyter-notebook-kernels.md)\n\n### Manage resources\n\n* [Manage resources for the Apache Spark cluster in Azure HDInsight](hdinsight-apache-spark-resource-manager.md)\n"
}