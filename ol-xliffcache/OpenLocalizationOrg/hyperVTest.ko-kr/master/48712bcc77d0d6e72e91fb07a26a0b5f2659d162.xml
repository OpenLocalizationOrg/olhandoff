{
  "nodes": [
    {
      "pos": [
        27,
        98
      ],
      "content": "Run Hadoop MapReduce samples on Linux-based HDInsight | Microsoft Azure"
    },
    {
      "pos": [
        117,
        263
      ],
      "content": "Get started using MapReduce samples with Linux-based HDInsight. Use SSH to connect to the cluster, then use the Hadoop command to run sample jobs.",
      "nodes": [
        {
          "content": "Get started using MapReduce samples with Linux-based HDInsight.",
          "pos": [
            0,
            63
          ]
        },
        {
          "content": "Use SSH to connect to the cluster, then use the Hadoop command to run sample jobs.",
          "pos": [
            64,
            146
          ]
        }
      ]
    },
    {
      "pos": [
        595,
        630
      ],
      "content": "Run the Hadoop samples in HDInsight"
    },
    {
      "pos": [
        718,
        961
      ],
      "content": "Linux-based HDInsight clusters provide a set of MapReduce samples that can be used to familiarize yourself with running Hadoop MapReduce jobs. In this document, you will learn about the available samples and walk through running a few of them.",
      "nodes": [
        {
          "content": "Linux-based HDInsight clusters provide a set of MapReduce samples that can be used to familiarize yourself with running Hadoop MapReduce jobs.",
          "pos": [
            0,
            142
          ]
        },
        {
          "content": "In this document, you will learn about the available samples and walk through running a few of them.",
          "pos": [
            143,
            243
          ]
        }
      ]
    },
    {
      "pos": [
        965,
        978
      ],
      "content": "Prerequisites"
    },
    {
      "pos": [
        982,
        1139
      ],
      "content": "<bpt id=\"p1\">**</bpt>An Azure subscription<ept id=\"p1\">**</ept>: See <bpt id=\"p2\">[</bpt>Get Azure free trial<ept id=\"p2\">](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/)</ept>"
    },
    {
      "pos": [
        1143,
        1290
      ],
      "content": "<bpt id=\"p3\">**</bpt>A Linux-based HDInsight cluster<ept id=\"p3\">**</ept>: See <bpt id=\"p4\">[</bpt>Get started using Hadoop with Hive in HDInsight on Linux<ept id=\"p4\">](hdinsight-hadoop-linux-tutorial-get-started.md)</ept>"
    },
    {
      "pos": [
        1294,
        1385
      ],
      "content": "<bpt id=\"p5\">**</bpt>An SSH client<ept id=\"p5\">**</ept>: For information on using SSH with HDInsight, see the following articles:"
    },
    {
      "pos": [
        1393,
        1505
      ],
      "content": "<bpt id=\"p6\">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X<ept id=\"p6\">](hdinsight-hadoop-linux-use-ssh-unix.md)</ept>"
    },
    {
      "pos": [
        1513,
        1615
      ],
      "content": "<bpt id=\"p7\">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Windows<ept id=\"p7\">](hdinsight-hadoop-linux-use-ssh-windows.md)</ept>"
    },
    {
      "pos": [
        1620,
        1631
      ],
      "content": "The samples"
    },
    {
      "pos": [
        1636,
        1777
      ],
      "content": "<bpt id=\"p8\">**</bpt>Location<ept id=\"p8\">**</ept>: The samples are located on the HDInsight cluster at  <bpt id=\"p9\">**</bpt>/usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar<ept id=\"p9\">**</ept>"
    },
    {
      "pos": [
        1779,
        1845
      ],
      "content": "<bpt id=\"p10\">**</bpt>Contents<ept id=\"p10\">**</ept>: The following samples are contained in this archive:"
    },
    {
      "pos": [
        1849,
        1951
      ],
      "content": "<bpt id=\"p11\">**</bpt>aggregatewordcount<ept id=\"p11\">**</ept>: An Aggregate based map/reduce program that counts the words in the input files"
    },
    {
      "pos": [
        1954,
        2074
      ],
      "content": "<bpt id=\"p12\">**</bpt>aggregatewordhist<ept id=\"p12\">**</ept>: An Aggregate based map/reduce program that computes the histogram of the words in the input files"
    },
    {
      "pos": [
        2077,
        2169
      ],
      "content": "<bpt id=\"p13\">**</bpt>bbp<ept id=\"p13\">**</ept>: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi"
    },
    {
      "pos": [
        2172,
        2246
      ],
      "content": "<bpt id=\"p14\">**</bpt>dbcount<ept id=\"p14\">**</ept>: An example job that count the pageview counts from a database"
    },
    {
      "pos": [
        2249,
        2339
      ],
      "content": "<bpt id=\"p15\">**</bpt>distbbp<ept id=\"p15\">**</ept>: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi"
    },
    {
      "pos": [
        2342,
        2420
      ],
      "content": "<bpt id=\"p16\">**</bpt>grep<ept id=\"p16\">**</ept>: A map/reduce program that counts the matches of a regex in the input"
    },
    {
      "pos": [
        2423,
        2500
      ],
      "content": "<bpt id=\"p17\">**</bpt>join<ept id=\"p17\">**</ept>: A job that effects a join over sorted, equally partitioned datasets"
    },
    {
      "pos": [
        2503,
        2562
      ],
      "content": "<bpt id=\"p18\">**</bpt>multifilewc<ept id=\"p18\">**</ept>: A job that counts words from several files"
    },
    {
      "pos": [
        2565,
        2652
      ],
      "content": "<bpt id=\"p19\">**</bpt>pentomino<ept id=\"p19\">**</ept>: A map/reduce tile laying program to find solutions to pentomino problems"
    },
    {
      "pos": [
        2655,
        2734
      ],
      "content": "<bpt id=\"p20\">**</bpt>pi<ept id=\"p20\">**</ept>: A map/reduce program that estimates Pi using a quasi-Monte Carlo method"
    },
    {
      "pos": [
        2737,
        2828
      ],
      "content": "<bpt id=\"p21\">**</bpt>randomtextwriter<ept id=\"p21\">**</ept>: A map/reduce program that writes 10GB of random textual data per node"
    },
    {
      "pos": [
        2831,
        2910
      ],
      "content": "<bpt id=\"p22\">**</bpt>randomwriter<ept id=\"p22\">**</ept>: A map/reduce program that writes 10GB of random data per node"
    },
    {
      "pos": [
        2913,
        2982
      ],
      "content": "<bpt id=\"p23\">**</bpt>secondarysort<ept id=\"p23\">**</ept>: An example defining a secondary sort to the reduce"
    },
    {
      "pos": [
        2985,
        3064
      ],
      "content": "<bpt id=\"p24\">**</bpt>sort<ept id=\"p24\">**</ept>: A map/reduce program that sorts the data written by the random writer"
    },
    {
      "pos": [
        3067,
        3094
      ],
      "content": "<bpt id=\"p25\">**</bpt>sudoku<ept id=\"p25\">**</ept>: A sudoku solver"
    },
    {
      "pos": [
        3097,
        3140
      ],
      "content": "<bpt id=\"p26\">**</bpt>teragen<ept id=\"p26\">**</ept>: Generate data for the terasort"
    },
    {
      "pos": [
        3143,
        3173
      ],
      "content": "<bpt id=\"p27\">**</bpt>terasort<ept id=\"p27\">**</ept>: Run the terasort"
    },
    {
      "pos": [
        3176,
        3222
      ],
      "content": "<bpt id=\"p28\">**</bpt>teravalidate<ept id=\"p28\">**</ept>: Checking results of terasort"
    },
    {
      "pos": [
        3225,
        3301
      ],
      "content": "<bpt id=\"p29\">**</bpt>wordcount<ept id=\"p29\">**</ept>: A map/reduce program that counts the words in the input files"
    },
    {
      "pos": [
        3304,
        3401
      ],
      "content": "<bpt id=\"p30\">**</bpt>wordmean<ept id=\"p30\">**</ept>: A map/reduce program that counts the average length of the words in the input files"
    },
    {
      "pos": [
        3404,
        3502
      ],
      "content": "<bpt id=\"p31\">**</bpt>wordmedian<ept id=\"p31\">**</ept>: A map/reduce program that counts the median length of the words in the input files"
    },
    {
      "pos": [
        3505,
        3633
      ],
      "content": "<bpt id=\"p32\">**</bpt>wordstandarddeviation<ept id=\"p32\">**</ept>: A map/reduce program that counts the standard deviation of the length of the words in the input files"
    },
    {
      "pos": [
        3635,
        3806
      ],
      "content": "<bpt id=\"p33\">**</bpt>Source code<ept id=\"p33\">**</ept>: Source code for these samples is included on the HDInsight cluster at <bpt id=\"p34\">**</bpt>/usr/hdp/2.2.4.9-1/hadoop/src/hadoop-mapreduce-project/hadoop-mapreduce-examples<ept id=\"p34\">**</ept>"
    },
    {
      "pos": [
        3810,
        3965
      ],
      "content": "<ph id=\"ph3\">[AZURE.NOTE]</ph><ph id=\"ph4\"/> The <ph id=\"ph5\">`2.2.4.9-1`</ph><ph id=\"ph6\"/> in the path is the version of the Hortonworks Data Platform for the HDInsight cluster, and may change as HDInsight is updated."
    },
    {
      "pos": [
        3970,
        3992
      ],
      "content": "How to run the samples"
    },
    {
      "pos": [
        4000,
        4070
      ],
      "content": "Connect to HDInsight using SSH as described in the following articles:"
    },
    {
      "pos": [
        4078,
        4190
      ],
      "content": "<bpt id=\"p35\">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X<ept id=\"p35\">](hdinsight-hadoop-linux-use-ssh-unix.md)</ept>"
    },
    {
      "pos": [
        4198,
        4300
      ],
      "content": "<bpt id=\"p36\">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Windows<ept id=\"p36\">](hdinsight-hadoop-linux-use-ssh-windows.md)</ept>"
    },
    {
      "pos": [
        4305,
        4390
      ],
      "content": "From the <ph id=\"ph7\">`username@#######:~$`</ph><ph id=\"ph8\"/> prompt, use the following command to list the samples:"
    },
    {
      "pos": [
        4485,
        4566
      ],
      "content": "This will generate the list of sample from the previous section of this document."
    },
    {
      "pos": [
        4571,
        4670
      ],
      "content": "Use the following command to get help on a specific sample. In this case, the <bpt id=\"p37\">**</bpt>wordcount<ept id=\"p37\">**</ept><ph id=\"ph9\"/> sample:",
      "nodes": [
        {
          "content": "Use the following command to get help on a specific sample.",
          "pos": [
            0,
            59
          ]
        },
        {
          "content": "In this case, the <bpt id=\"p37\">**</bpt>wordcount<ept id=\"p37\">**</ept><ph id=\"ph9\"/> sample:",
          "pos": [
            60,
            153
          ]
        }
      ]
    },
    {
      "pos": [
        4775,
        4816
      ],
      "content": "You should receive the following message:"
    },
    {
      "pos": [
        4869,
        5048
      ],
      "content": "This indicates that you can provide several input paths for the source documents, and the final path is where the output (count of words in the source documents,) will be located."
    },
    {
      "pos": [
        5053,
        5181
      ],
      "content": "Use the following to count all words in the Notebooks of Leonardo Da Vinci, which are provided as sample data with your cluster:"
    },
    {
      "pos": [
        5353,
        5432
      ],
      "content": "Input for this job is read from <bpt id=\"p38\">**</bpt>wasb:///example/data/gutenberg/davinci.txt<ept id=\"p38\">**</ept>."
    },
    {
      "pos": [
        5438,
        5522
      ],
      "content": "Output for this example will be stored in <bpt id=\"p39\">**</bpt>wasb:///example/data/davinciwordcount<ept id=\"p39\">**</ept>."
    },
    {
      "pos": [
        5530,
        5895
      ],
      "content": "<ph id=\"ph10\">[AZURE.NOTE]</ph><ph id=\"ph11\"/> As noted in the help for the wordcount sample, you could also specify multiple input files. For example, <ph id=\"ph12\">`hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar wordcount /example/data/gutenberg/davinci.txt /example/data/gutenberg/ulysses.txt /example/data/twowordcount`</ph><ph id=\"ph13\"/> would count words in both davinci.txt and ulysses.txt.",
      "nodes": [
        {
          "content": "<ph id=\"ph10\">[AZURE.NOTE]</ph><ph id=\"ph11\"/> As noted in the help for the wordcount sample, you could also specify multiple input files.",
          "pos": [
            0,
            138
          ]
        },
        {
          "content": "For example, <ph id=\"ph12\">`hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar wordcount /example/data/gutenberg/davinci.txt /example/data/gutenberg/ulysses.txt /example/data/twowordcount`</ph><ph id=\"ph13\"/> would count words in both davinci.txt and ulysses.txt.",
          "pos": [
            139,
            433
          ]
        }
      ]
    },
    {
      "pos": [
        5900,
        5969
      ],
      "content": "Once the job completes, use the following command to view the output:"
    },
    {
      "pos": [
        6031,
        6231
      ],
      "content": "This will concatenate all the output files produced by the job, and display them. For this basic example there is only one file, however if there were more this command would iterate over all of them.",
      "nodes": [
        {
          "content": "This will concatenate all the output files produced by the job, and display them.",
          "pos": [
            0,
            81
          ]
        },
        {
          "content": "For this basic example there is only one file, however if there were more this command would iterate over all of them.",
          "pos": [
            82,
            200
          ]
        }
      ]
    },
    {
      "pos": [
        6237,
        6300
      ],
      "content": "You will see output similar to the following from this command:"
    },
    {
      "pos": [
        6379,
        6455
      ],
      "content": "Each line represents a word and how many times it occured in the input data."
    },
    {
      "pos": [
        6460,
        6466
      ],
      "content": "Sudoku"
    },
    {
      "pos": [
        6468,
        6569
      ],
      "content": "The Sudoku example has somewhat unhelpful usage instructions; \"Include a puzzle on the command line.\""
    },
    {
      "pos": [
        6571,
        6947
      ],
      "content": "<bpt id=\"p40\">[</bpt>Sudoku<ept id=\"p40\">](https://en.wikipedia.org/wiki/Sudoku)</ept><ph id=\"ph14\"/> is a logic puzzle made up of nine 3x3 grids. Some cells in the grid have numbers, while others are blank, and the goal is to solve for the blank cells. The link above has more information on the puzzle, but the purpose of this sample is to solve for the blank cells. So our input should be a file that is in the following format:",
      "nodes": [
        {
          "content": "<bpt id=\"p40\">[</bpt>Sudoku<ept id=\"p40\">](https://en.wikipedia.org/wiki/Sudoku)</ept><ph id=\"ph14\"/> is a logic puzzle made up of nine 3x3 grids.",
          "pos": [
            0,
            146
          ]
        },
        {
          "content": "Some cells in the grid have numbers, while others are blank, and the goal is to solve for the blank cells.",
          "pos": [
            147,
            253
          ]
        },
        {
          "content": "The link above has more information on the puzzle, but the purpose of this sample is to solve for the blank cells.",
          "pos": [
            254,
            368
          ]
        },
        {
          "content": "So our input should be a file that is in the following format:",
          "pos": [
            369,
            431
          ]
        }
      ]
    },
    {
      "pos": [
        6951,
        6976
      ],
      "content": "Nine rows of nine columns"
    },
    {
      "pos": [
        6980,
        7057
      ],
      "content": "Each column can contain either a number or <ph id=\"ph15\">`?`</ph><ph id=\"ph16\"/> (which indicates a blank cell)"
    },
    {
      "pos": [
        7061,
        7091
      ],
      "content": "Cells are separated by a space"
    },
    {
      "pos": [
        7093,
        7477
      ],
      "content": "Now, there's a certain way to construct Sudoku puzzles in that you can't repeat a number in a column or row. Thankfully there's an example on the HDInsight cluster that is properly constructed. It is located at <bpt id=\"p41\">**</bpt>/usr/hdp/2.2.4.9-1/hadoop/src/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/puzzle1.dta<ept id=\"p41\">**</ept><ph id=\"ph17\"/> and contains the following:",
      "nodes": [
        {
          "content": "Now, there's a certain way to construct Sudoku puzzles in that you can't repeat a number in a column or row.",
          "pos": [
            0,
            108
          ]
        },
        {
          "content": "Thankfully there's an example on the HDInsight cluster that is properly constructed.",
          "pos": [
            109,
            193
          ]
        },
        {
          "content": "It is located at <bpt id=\"p41\">**</bpt>/usr/hdp/2.2.4.9-1/hadoop/src/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/puzzle1.dta<ept id=\"p41\">**</ept><ph id=\"ph17\"/> and contains the following:",
          "pos": [
            194,
            439
          ]
        }
      ]
    },
    {
      "pos": [
        7680,
        7785
      ],
      "content": "<ph id=\"ph18\">[AZURE.NOTE]</ph><ph id=\"ph19\"/> The <ph id=\"ph20\">`2.2.4.9-1`</ph><ph id=\"ph21\"/> portion of the path may change as updates are made to the HDInsight cluster."
    },
    {
      "pos": [
        7787,
        7853
      ],
      "content": "To run this through the Sudoku example, use the following command:"
    },
    {
      "pos": [
        8089,
        8140
      ],
      "content": "The results should appear similar to the following:"
    },
    {
      "pos": [
        8344,
        8350
      ],
      "content": "Pi (π)"
    },
    {
      "pos": [
        8352,
        8852
      ],
      "content": "The pi sample uses a statistical (quasi-Monte Carlo) method to estimate the value of pi. Points placed at random inside of a unit square also fall within a circle inscribed within that square with a probability equal to the area of the circle, pi/4. The value of pi can be estimated from the value of 4R, where R is the ratio of the number of points that are inside the circle to the total number of points that are within the square. The larger the sample of points used, the better the estimate is.",
      "nodes": [
        {
          "content": "The pi sample uses a statistical (quasi-Monte Carlo) method to estimate the value of pi.",
          "pos": [
            0,
            88
          ]
        },
        {
          "content": "Points placed at random inside of a unit square also fall within a circle inscribed within that square with a probability equal to the area of the circle, pi/4.",
          "pos": [
            89,
            249
          ]
        },
        {
          "content": "The value of pi can be estimated from the value of 4R, where R is the ratio of the number of points that are inside the circle to the total number of points that are within the square.",
          "pos": [
            250,
            434
          ]
        },
        {
          "content": "The larger the sample of points used, the better the estimate is.",
          "pos": [
            435,
            500
          ]
        }
      ]
    },
    {
      "pos": [
        8854,
        9014
      ],
      "content": "The mapper for this sample generates a number of points at random inside of a unit square and then counts the number of those points that are inside the circle."
    },
    {
      "pos": [
        9016,
        9254
      ],
      "content": "The reducer then accumulates points counted by the mappers and estimates the value of pi from the formula 4R, where R is the ratio of the number of points counted inside the circle to the total number of points that are within the square."
    },
    {
      "pos": [
        9256,
        9377
      ],
      "content": "Use the following command to run this sample. This uses 16 maps with 10,000,000 samples each to estimate the value of pi:",
      "nodes": [
        {
          "content": "Use the following command to run this sample.",
          "pos": [
            0,
            45
          ]
        },
        {
          "content": "This uses 16 maps with 10,000,000 samples each to estimate the value of pi:",
          "pos": [
            46,
            121
          ]
        }
      ]
    },
    {
      "pos": [
        9479,
        9622
      ],
      "content": "The value returned by this should be similar to <bpt id=\"p42\">**</bpt>3.14159155000000000000<ept id=\"p42\">**</ept>. For references, the first 10 decimal places of pi are 3.1415926535.",
      "nodes": [
        {
          "content": "The value returned by this should be similar to <bpt id=\"p42\">**</bpt>3.14159155000000000000<ept id=\"p42\">**</ept>.",
          "pos": [
            0,
            115
          ]
        },
        {
          "content": "For references, the first 10 decimal places of pi are 3.1415926535.",
          "pos": [
            116,
            183
          ]
        }
      ]
    },
    {
      "pos": [
        9626,
        9639
      ],
      "content": "10GB Greysort"
    },
    {
      "pos": [
        9641,
        9795
      ],
      "content": "GraySort is a benchmark sort whose metric is the sort rate (TB/minute) that is achieved while sorting very large amounts of data, usually a 100TB minimum."
    },
    {
      "pos": [
        9797,
        10202
      ],
      "content": "This sample uses a modest 10GB of data so that it can be run relatively quickly. It uses the MapReduce applications developed by Owen O'Malley and Arun Murthy that won the annual general-purpose (\"daytona\") terabyte sort benchmark in 2009 with a rate of 0.578TB/min (100TB in 173 minutes). For more information on this and other sorting benchmarks, see the <bpt id=\"p43\">[</bpt>Sortbenchmark<ept id=\"p43\">](http://sortbenchmark.org/)</ept><ph id=\"ph22\"/> site.",
      "nodes": [
        {
          "content": "This sample uses a modest 10GB of data so that it can be run relatively quickly.",
          "pos": [
            0,
            80
          ]
        },
        {
          "content": "It uses the MapReduce applications developed by Owen O'Malley and Arun Murthy that won the annual general-purpose (\"daytona\") terabyte sort benchmark in 2009 with a rate of 0.578TB/min (100TB in 173 minutes).",
          "pos": [
            81,
            289
          ]
        },
        {
          "content": "For more information on this and other sorting benchmarks, see the <bpt id=\"p43\">[</bpt>Sortbenchmark<ept id=\"p43\">](http://sortbenchmark.org/)</ept><ph id=\"ph22\"/> site.",
          "pos": [
            290,
            460
          ]
        }
      ]
    },
    {
      "pos": [
        10204,
        10254
      ],
      "content": "This sample uses three sets of MapReduce programs:"
    },
    {
      "pos": [
        10258,
        10326
      ],
      "content": "<bpt id=\"p44\">**</bpt>TeraGen<ept id=\"p44\">**</ept>: A MapReduce program that generates rows of data to sort"
    },
    {
      "pos": [
        10330,
        10421
      ],
      "content": "<bpt id=\"p45\">**</bpt>TeraSort<ept id=\"p45\">**</ept>: Samples the input data and uses MapReduce to sort the data into a total order"
    },
    {
      "pos": [
        10427,
        10773
      ],
      "content": "TeraSort is a standard sort of MapReduce functions, except for a custom partitioner that uses a sorted list of N-1 sampled keys that define the key range for each reduce. In particular, all keys such that sample[i-1] &lt;= key &lt; sample[i] are sent to reduce i. This guarantees that the outputs of reduce i are all less than the output of reduce i+1.",
      "nodes": [
        {
          "content": "TeraSort is a standard sort of MapReduce functions, except for a custom partitioner that uses a sorted list of N-1 sampled keys that define the key range for each reduce. In pa",
          "pos": [
            0,
            176
          ]
        },
        {
          "content": "rticular, all keys such that sample[i-1] &lt;= key &lt; sample[i] are sent to reduce i.",
          "pos": [
            176,
            263
          ]
        },
        {
          "content": "This guarantees that the outputs of reduce i are all less than the output of reduce i+1.",
          "pos": [
            264,
            352
          ]
        }
      ]
    },
    {
      "pos": [
        10777,
        10864
      ],
      "content": "<bpt id=\"p46\">**</bpt>TeraValidate<ept id=\"p46\">**</ept>: A MapReduce program that validates that the output is globally sorted"
    },
    {
      "pos": [
        10870,
        11274
      ],
      "content": "It creates one map per file in the output directory, and each map ensures that each key is less than or equal to the previous one. The map function also generates records of the first and last keys of each file, and the reduce function ensures that the first key of file i is greater than the last key of file i-1. Any problems are reported as an output of the reduce with the keys that are out of order.",
      "nodes": [
        {
          "content": "It creates one map per file in the output directory, and each map ensures that each key is less than or equal to the previous one.",
          "pos": [
            0,
            130
          ]
        },
        {
          "content": "The map function also generates records of the first and last keys of each file, and the reduce function ensures that the first key of file i is greater than the last key of file i-1.",
          "pos": [
            131,
            314
          ]
        },
        {
          "content": "Any problems are reported as an output of the reduce with the keys that are out of order.",
          "pos": [
            315,
            404
          ]
        }
      ]
    },
    {
      "pos": [
        11276,
        11353
      ],
      "content": "Use the following steps to generate data, sort, and then validate the output:"
    },
    {
      "pos": [
        11358,
        11489
      ],
      "content": "Generate 10GB of data, which will be stored to the HDInsight cluster's default storage at <bpt id=\"p47\">**</bpt>wasb:///example/data/10GB-sort-input<ept id=\"p47\">**</ept>:"
    },
    {
      "pos": [
        11654,
        11863
      ],
      "content": "The <ph id=\"ph23\">`-Dmapred.map.tasks`</ph><ph id=\"ph24\"/> tells Hadoop how many map tasks to use for this job. The final two parameters instruct the job to create 10GB worth of data and to store it at <bpt id=\"p48\">**</bpt>wasb:///example/data/10GB-sort-input<ept id=\"p48\">**</ept>.",
      "nodes": [
        {
          "content": "The <ph id=\"ph23\">`-Dmapred.map.tasks`</ph><ph id=\"ph24\"/> tells Hadoop how many map tasks to use for this job.",
          "pos": [
            0,
            111
          ]
        },
        {
          "content": "The final two parameters instruct the job to create 10GB worth of data and to store it at <bpt id=\"p48\">**</bpt>wasb:///example/data/10GB-sort-input<ept id=\"p48\">**</ept>.",
          "pos": [
            112,
            283
          ]
        }
      ]
    },
    {
      "pos": [
        11868,
        11911
      ],
      "content": "Use the following command to sort the data:"
    },
    {
      "pos": [
        12123,
        12280
      ],
      "content": "The <ph id=\"ph25\">`-Dmapred.reduce.tasks`</ph><ph id=\"ph26\"/> tells Hadoop how many reduce tasks to use for the job. The final two parameters are just the input and output locations for data.",
      "nodes": [
        {
          "content": "The <ph id=\"ph25\">`-Dmapred.reduce.tasks`</ph><ph id=\"ph26\"/> tells Hadoop how many reduce tasks to use for the job.",
          "pos": [
            0,
            116
          ]
        },
        {
          "content": "The final two parameters are just the input and output locations for data.",
          "pos": [
            117,
            191
          ]
        }
      ]
    },
    {
      "pos": [
        12285,
        12346
      ],
      "content": "Use the following to validate the data generated by the sort:"
    },
    {
      "pos": [
        12563,
        12573
      ],
      "content": "Next steps"
    },
    {
      "pos": [
        12578,
        12774
      ],
      "content": "From this article, you learned how to run the samples included with the Linux-based HDInsight clusters. For tutorials about using Pig, Hive, and MapReduce with HDInsight, see the following topics:",
      "nodes": [
        {
          "content": "From this article, you learned how to run the samples included with the Linux-based HDInsight clusters.",
          "pos": [
            0,
            103
          ]
        },
        {
          "content": "For tutorials about using Pig, Hive, and MapReduce with HDInsight, see the following topics:",
          "pos": [
            104,
            196
          ]
        }
      ]
    },
    {
      "pos": [
        12778,
        12831
      ],
      "content": "<bpt id=\"p49\">[</bpt>Use Pig with Hadoop on HDInsight<ept id=\"p49\">][hdinsight-use-pig]</ept>"
    },
    {
      "pos": [
        12834,
        12889
      ],
      "content": "<bpt id=\"p50\">[</bpt>Use Hive with Hadoop on HDInsight<ept id=\"p50\">][hdinsight-use-hive]</ept>"
    },
    {
      "pos": [
        12892,
        12958
      ],
      "content": "<bpt id=\"p51\">[</bpt>Use MapReduce with Hadoop on HDInsight<ept id=\"p51\">] [hdinsight-use-mapreduce]</ept>"
    }
  ],
  "content": "<properties\n    pageTitle=\"Run Hadoop MapReduce samples on Linux-based HDInsight | Microsoft Azure\"\n    description=\"Get started using MapReduce samples with Linux-based HDInsight. Use SSH to connect to the cluster, then use the Hadoop command to run sample jobs.\"\n    services=\"hdinsight\"\n    documentationCenter=\"\"\n    authors=\"Blackmist\"\n    manager=\"paulettm\"\n    editor=\"cgronlun\"\n    tags=\"azure-portal\"/>\n\n<tags\n    ms.service=\"hdinsight\"\n    ms.workload=\"big-data\"\n    ms.tgt_pltfrm=\"na\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\"\n    ms.date=\"02/05/2016\"\n    ms.author=\"larryfr\"/>\n\n\n\n\n#Run the Hadoop samples in HDInsight\n\n[AZURE.INCLUDE [samples-selector](../../includes/hdinsight-run-samples-selector.md)]\n\nLinux-based HDInsight clusters provide a set of MapReduce samples that can be used to familiarize yourself with running Hadoop MapReduce jobs. In this document, you will learn about the available samples and walk through running a few of them.\n\n##Prerequisites\n\n- **An Azure subscription**: See [Get Azure free trial](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/)\n\n- **A Linux-based HDInsight cluster**: See [Get started using Hadoop with Hive in HDInsight on Linux](hdinsight-hadoop-linux-tutorial-get-started.md)\n\n- **An SSH client**: For information on using SSH with HDInsight, see the following articles:\n\n    - [Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X](hdinsight-hadoop-linux-use-ssh-unix.md)\n\n    - [Use SSH with Linux-based Hadoop on HDInsight from Windows](hdinsight-hadoop-linux-use-ssh-windows.md)\n\n## The samples ##\n\n**Location**: The samples are located on the HDInsight cluster at  **/usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar**\n\n**Contents**: The following samples are contained in this archive:\n\n- **aggregatewordcount**: An Aggregate based map/reduce program that counts the words in the input files\n- **aggregatewordhist**: An Aggregate based map/reduce program that computes the histogram of the words in the input files\n- **bbp**: A map/reduce program that uses Bailey-Borwein-Plouffe to compute exact digits of Pi\n- **dbcount**: An example job that count the pageview counts from a database\n- **distbbp**: A map/reduce program that uses a BBP-type formula to compute exact bits of Pi\n- **grep**: A map/reduce program that counts the matches of a regex in the input\n- **join**: A job that effects a join over sorted, equally partitioned datasets\n- **multifilewc**: A job that counts words from several files\n- **pentomino**: A map/reduce tile laying program to find solutions to pentomino problems\n- **pi**: A map/reduce program that estimates Pi using a quasi-Monte Carlo method\n- **randomtextwriter**: A map/reduce program that writes 10GB of random textual data per node\n- **randomwriter**: A map/reduce program that writes 10GB of random data per node\n- **secondarysort**: An example defining a secondary sort to the reduce\n- **sort**: A map/reduce program that sorts the data written by the random writer\n- **sudoku**: A sudoku solver\n- **teragen**: Generate data for the terasort\n- **terasort**: Run the terasort\n- **teravalidate**: Checking results of terasort\n- **wordcount**: A map/reduce program that counts the words in the input files\n- **wordmean**: A map/reduce program that counts the average length of the words in the input files\n- **wordmedian**: A map/reduce program that counts the median length of the words in the input files\n- **wordstandarddeviation**: A map/reduce program that counts the standard deviation of the length of the words in the input files\n\n**Source code**: Source code for these samples is included on the HDInsight cluster at **/usr/hdp/2.2.4.9-1/hadoop/src/hadoop-mapreduce-project/hadoop-mapreduce-examples**\n\n> [AZURE.NOTE] The `2.2.4.9-1` in the path is the version of the Hortonworks Data Platform for the HDInsight cluster, and may change as HDInsight is updated.\n\n## How to run the samples ##\n\n1. Connect to HDInsight using SSH as described in the following articles:\n\n    - [Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X](hdinsight-hadoop-linux-use-ssh-unix.md)\n\n    - [Use SSH with Linux-based Hadoop on HDInsight from Windows](hdinsight-hadoop-linux-use-ssh-windows.md)\n\n2. From the `username@#######:~$` prompt, use the following command to list the samples:\n\n        yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar\n\n    This will generate the list of sample from the previous section of this document.\n\n3. Use the following command to get help on a specific sample. In this case, the **wordcount** sample:\n\n        yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar wordcount\n\n    You should receive the following message:\n\n        Usage: wordcount <in> [<in>...] <out>\n\n    This indicates that you can provide several input paths for the source documents, and the final path is where the output (count of words in the source documents,) will be located.\n\n4. Use the following to count all words in the Notebooks of Leonardo Da Vinci, which are provided as sample data with your cluster:\n\n        yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar wordcount /example/data/gutenberg/davinci.txt /example/data/davinciwordcount\n\n    Input for this job is read from **wasb:///example/data/gutenberg/davinci.txt**.\n\n    Output for this example will be stored in **wasb:///example/data/davinciwordcount**.\n\n    > [AZURE.NOTE] As noted in the help for the wordcount sample, you could also specify multiple input files. For example, `hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar wordcount /example/data/gutenberg/davinci.txt /example/data/gutenberg/ulysses.txt /example/data/twowordcount` would count words in both davinci.txt and ulysses.txt.\n\n5. Once the job completes, use the following command to view the output:\n\n        hdfs dfs -cat /example/data/davinciwordcount/*\n\n    This will concatenate all the output files produced by the job, and display them. For this basic example there is only one file, however if there were more this command would iterate over all of them.\n\n    You will see output similar to the following from this command:\n\n        zum     1\n        zur     1\n        zwanzig 1\n        zweite  1\n\n    Each line represents a word and how many times it occured in the input data.\n\n## Sudoku\n\nThe Sudoku example has somewhat unhelpful usage instructions; \"Include a puzzle on the command line.\"\n\n[Sudoku](https://en.wikipedia.org/wiki/Sudoku) is a logic puzzle made up of nine 3x3 grids. Some cells in the grid have numbers, while others are blank, and the goal is to solve for the blank cells. The link above has more information on the puzzle, but the purpose of this sample is to solve for the blank cells. So our input should be a file that is in the following format:\n\n- Nine rows of nine columns\n\n- Each column can contain either a number or `?` (which indicates a blank cell)\n\n- Cells are separated by a space\n\nNow, there's a certain way to construct Sudoku puzzles in that you can't repeat a number in a column or row. Thankfully there's an example on the HDInsight cluster that is properly constructed. It is located at **/usr/hdp/2.2.4.9-1/hadoop/src/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/puzzle1.dta** and contains the following:\n\n    8 5 ? 3 9 ? ? ? ?\n    ? ? 2 ? ? ? ? ? ?\n    ? ? 6 ? 1 ? ? ? 2\n    ? ? 4 ? ? 3 ? 5 9\n    ? ? 8 9 ? 1 4 ? ?\n    3 2 ? 4 ? ? 8 ? ?\n    9 ? ? ? 8 ? 5 ? ?\n    ? ? ? ? ? ? 2 ? ?\n    ? ? ? ? 4 5 ? 7 8\n\n> [AZURE.NOTE] The `2.2.4.9-1` portion of the path may change as updates are made to the HDInsight cluster.\n\nTo run this through the Sudoku example, use the following command:\n\n    yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar sudoku /usr/hdp/2.2.9.1-1/hadoop/src/hadoop-mapreduce-project/hadoop-mapreduce-examples/src/main/java/org/apache/hadoop/examples/dancing/puzzle1.dta\n\nThe results should appear similar to the following:\n\n    8 5 1 3 9 2 6 4 7\n    4 3 2 6 7 8 1 9 5\n    7 9 6 5 1 4 3 8 2\n    6 1 4 8 2 3 7 5 9\n    5 7 8 9 6 1 4 2 3\n    3 2 9 4 5 7 8 1 6\n    9 4 7 2 8 6 5 3 1\n    1 8 5 7 3 9 2 6 4\n    2 6 3 1 4 5 9 7 8\n\n## Pi (π)\n\nThe pi sample uses a statistical (quasi-Monte Carlo) method to estimate the value of pi. Points placed at random inside of a unit square also fall within a circle inscribed within that square with a probability equal to the area of the circle, pi/4. The value of pi can be estimated from the value of 4R, where R is the ratio of the number of points that are inside the circle to the total number of points that are within the square. The larger the sample of points used, the better the estimate is.\n\nThe mapper for this sample generates a number of points at random inside of a unit square and then counts the number of those points that are inside the circle.\n\nThe reducer then accumulates points counted by the mappers and estimates the value of pi from the formula 4R, where R is the ratio of the number of points counted inside the circle to the total number of points that are within the square.\n\nUse the following command to run this sample. This uses 16 maps with 10,000,000 samples each to estimate the value of pi:\n\n    yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar pi 16 10000000\n\nThe value returned by this should be similar to **3.14159155000000000000**. For references, the first 10 decimal places of pi are 3.1415926535.\n\n##10GB Greysort\n\nGraySort is a benchmark sort whose metric is the sort rate (TB/minute) that is achieved while sorting very large amounts of data, usually a 100TB minimum.\n\nThis sample uses a modest 10GB of data so that it can be run relatively quickly. It uses the MapReduce applications developed by Owen O'Malley and Arun Murthy that won the annual general-purpose (\"daytona\") terabyte sort benchmark in 2009 with a rate of 0.578TB/min (100TB in 173 minutes). For more information on this and other sorting benchmarks, see the [Sortbenchmark](http://sortbenchmark.org/) site.\n\nThis sample uses three sets of MapReduce programs:\n\n- **TeraGen**: A MapReduce program that generates rows of data to sort\n\n- **TeraSort**: Samples the input data and uses MapReduce to sort the data into a total order\n\n    TeraSort is a standard sort of MapReduce functions, except for a custom partitioner that uses a sorted list of N-1 sampled keys that define the key range for each reduce. In particular, all keys such that sample[i-1] <= key < sample[i] are sent to reduce i. This guarantees that the outputs of reduce i are all less than the output of reduce i+1.\n\n- **TeraValidate**: A MapReduce program that validates that the output is globally sorted\n\n    It creates one map per file in the output directory, and each map ensures that each key is less than or equal to the previous one. The map function also generates records of the first and last keys of each file, and the reduce function ensures that the first key of file i is greater than the last key of file i-1. Any problems are reported as an output of the reduce with the keys that are out of order.\n\nUse the following steps to generate data, sort, and then validate the output:\n\n1. Generate 10GB of data, which will be stored to the HDInsight cluster's default storage at **wasb:///example/data/10GB-sort-input**:\n\n        yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar teragen -Dmapred.map.tasks=50 100000000 /example/data/10GB-sort-input\n\n    The `-Dmapred.map.tasks` tells Hadoop how many map tasks to use for this job. The final two parameters instruct the job to create 10GB worth of data and to store it at **wasb:///example/data/10GB-sort-input**.\n\n2. Use the following command to sort the data:\n\n        yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar terasort -Dmapred.map.tasks=50 -Dmapred.reduce.tasks=25 /example/data/10GB-sort-input /example/data/10GB-sort-output\n\n    The `-Dmapred.reduce.tasks` tells Hadoop how many reduce tasks to use for the job. The final two parameters are just the input and output locations for data.\n\n3. Use the following to validate the data generated by the sort:\n\n        yarn jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar teravalidate -Dmapred.map.tasks=50 -Dmapred.reduce.tasks=25 /example/data/10GB-sort-output /example/data/10GB-sort-validate\n\n##Next steps ##\n\nFrom this article, you learned how to run the samples included with the Linux-based HDInsight clusters. For tutorials about using Pig, Hive, and MapReduce with HDInsight, see the following topics:\n\n* [Use Pig with Hadoop on HDInsight][hdinsight-use-pig]\n* [Use Hive with Hadoop on HDInsight][hdinsight-use-hive]\n* [Use MapReduce with Hadoop on HDInsight] [hdinsight-use-mapreduce]\n\n\n\n[hdinsight-errors]: hdinsight-debug-jobs.md\n[hdinsight-use-mapreduce]: hdinsight-use-mapreduce.md\n[hdinsight-sdk-documentation]: https://msdn.microsoft.com/library/azure/dn479185.aspx\n\n[hdinsight-submit-jobs]: hdinsight-submit-hadoop-jobs-programmatically.md\n[hdinsight-introduction]: hdinsight-hadoop-introduction.md\n\n\n\n[hdinsight-samples]: hdinsight-run-samples.md\n\n[hdinsight-use-hive]: hdinsight-use-hive.md\n[hdinsight-use-pig]: hdinsight-use-pig.md\n"
}