{
  "nodes": [
    {
      "pos": [
        26,
        93
      ],
      "content": "Use Hadoop Hive on the Query Console in HDInsight | Microsoft Azure"
    },
    {
      "pos": [
        111,
        225
      ],
      "content": "Learn how to use the web-based Query Console to run Hive queries on an HDInsight Hadoop cluster from your browser."
    },
    {
      "pos": [
        543,
        583
      ],
      "content": "Run Hive queries using the Query Console"
    },
    {
      "pos": [
        665,
        805
      ],
      "content": "In this article, you will learn how to use the HDInsight Query Console to run Hive queries on an HDInsight Hadoop cluster from your browser."
    },
    {
      "pos": [
        809,
        894
      ],
      "content": "<ph id=\"ph3\">[AZURE.NOTE]</ph><ph id=\"ph4\"/> The Query Console is only available on Windows-based HDInsight clusters."
    },
    {
      "pos": [
        897,
        899
      ],
      "content": "##"
    },
    {
      "pos": [
        918,
        931
      ],
      "content": "Prerequisites"
    },
    {
      "pos": [
        933,
        1000
      ],
      "content": "To complete the steps in this article, you will need the following."
    },
    {
      "pos": [
        1004,
        1044
      ],
      "content": "A Windows-based HDInsight Hadoop cluster"
    },
    {
      "pos": [
        1048,
        1068
      ],
      "content": "A modern web browser"
    },
    {
      "pos": [
        1070,
        1072
      ],
      "content": "##"
    },
    {
      "pos": [
        1089,
        1129
      ],
      "content": "Run Hive queries using the Query Console"
    },
    {
      "pos": [
        1134,
        1363
      ],
      "content": "Open a web browser and navigate to <bpt id=\"p1\">__</bpt>https://CLUSTERNAME.azurehdinsight.net<ept id=\"p1\">__</ept>, where <bpt id=\"p2\">__</bpt>CLUSTERNAME<ept id=\"p2\">__</ept><ph id=\"ph5\"/> is the name of your HDInsight cluster. If prompted, enter the user name and password that you used when you created the cluster.",
      "nodes": [
        {
          "content": "Open a web browser and navigate to <bpt id=\"p1\">__</bpt>https://CLUSTERNAME.azurehdinsight.net<ept id=\"p1\">__</ept>, where <bpt id=\"p2\">__</bpt>CLUSTERNAME<ept id=\"p2\">__</ept><ph id=\"ph5\"/> is the name of your HDInsight cluster.",
          "pos": [
            0,
            229
          ]
        },
        {
          "content": "If prompted, enter the user name and password that you used when you created the cluster.",
          "pos": [
            230,
            319
          ]
        }
      ]
    },
    {
      "pos": [
        1369,
        1547
      ],
      "content": "From the links at the top of the page, select <bpt id=\"p3\">**</bpt>Hive Editor<ept id=\"p3\">**</ept>. This displays a form that can be used to enter the HiveQL statements that you want to run in the HDInsight cluster.",
      "nodes": [
        {
          "content": "From the links at the top of the page, select <bpt id=\"p3\">**</bpt>Hive Editor<ept id=\"p3\">**</ept>.",
          "pos": [
            0,
            100
          ]
        },
        {
          "content": "This displays a form that can be used to enter the HiveQL statements that you want to run in the HDInsight cluster.",
          "pos": [
            101,
            216
          ]
        }
      ]
    },
    {
      "pos": [
        1553,
        1637
      ],
      "content": "<ph id=\"ph6\">![</ph>the hive editor<ph id=\"ph7\">](./media/hdinsight-hadoop-use-hive-query-console/queryconsole.png)</ph>"
    },
    {
      "pos": [
        1643,
        1729
      ],
      "content": "Replace the text <ph id=\"ph8\">`Select * from hivesampletable`</ph><ph id=\"ph9\"/> with the following HiveQL statements:"
    },
    {
      "pos": [
        2166,
        2213
      ],
      "content": "These statements perform the following actions:"
    },
    {
      "pos": [
        2221,
        2301
      ],
      "content": "<bpt id=\"p4\">**</bpt>DROP TABLE<ept id=\"p4\">**</ept>: Deletes the table and the data file if the table already exists."
    },
    {
      "pos": [
        2308,
        2474
      ],
      "content": "<bpt id=\"p5\">**</bpt>CREATE EXTERNAL TABLE<ept id=\"p5\">**</ept>: Creates a new 'external' table in Hive. External tables store only the table definition in Hive; the data is left in the original location.",
      "nodes": [
        {
          "content": "<bpt id=\"p5\">**</bpt>CREATE EXTERNAL TABLE<ept id=\"p5\">**</ept>: Creates a new 'external' table in Hive.",
          "pos": [
            0,
            104
          ]
        },
        {
          "content": "External tables store only the table definition in Hive; the data is left in the original location.",
          "pos": [
            105,
            204
          ]
        }
      ]
    },
    {
      "pos": [
        2482,
        2732
      ],
      "content": "<ph id=\"ph10\">[AZURE.NOTE]</ph><ph id=\"ph11\"/> External tables should be used when you expect the underlying data to be updated by an external source (such as an automated data upload process) or by another MapReduce operation, but you always want Hive queries to use the latest data."
    },
    {
      "pos": [
        2745,
        2828
      ],
      "content": "Dropping an external table does <bpt id=\"p6\">**</bpt>not<ept id=\"p6\">**</ept><ph id=\"ph12\"/> delete the data, only the table definition."
    },
    {
      "pos": [
        2836,
        2952
      ],
      "content": "<bpt id=\"p7\">**</bpt>ROW FORMAT<ept id=\"p7\">**</ept>: Tells Hive how the data is formatted. In this case, the fields in each log are separated by a space.",
      "nodes": [
        {
          "content": "<bpt id=\"p7\">**</bpt>ROW FORMAT<ept id=\"p7\">**</ept>: Tells Hive how the data is formatted.",
          "pos": [
            0,
            91
          ]
        },
        {
          "content": "In this case, the fields in each log are separated by a space.",
          "pos": [
            92,
            154
          ]
        }
      ]
    },
    {
      "pos": [
        2959,
        3086
      ],
      "content": "<bpt id=\"p8\">**</bpt>STORED AS TEXTFILE LOCATION<ept id=\"p8\">**</ept>: Tells Hive where the data is stored (the example/data directory) and that it is stored as text"
    },
    {
      "pos": [
        3093,
        3272
      ],
      "content": "<bpt id=\"p9\">**</bpt>SELECT<ept id=\"p9\">**</ept>: Select a count of all rows where column <bpt id=\"p10\">**</bpt>t4<ept id=\"p10\">**</ept><ph id=\"ph13\"/> contain the value <bpt id=\"p11\">**</bpt>[ERROR]<ept id=\"p11\">**</ept>. This should return a value of <bpt id=\"p12\">**</bpt>3<ept id=\"p12\">**</ept><ph id=\"ph14\"/> because there are three rows that contain this value.",
      "nodes": [
        {
          "content": "<bpt id=\"p9\">**</bpt>SELECT<ept id=\"p9\">**</ept>: Select a count of all rows where column <bpt id=\"p10\">**</bpt>t4<ept id=\"p10\">**</ept><ph id=\"ph13\"/> contain the value <bpt id=\"p11\">**</bpt>[ERROR]<ept id=\"p11\">**</ept>.",
          "pos": [
            0,
            222
          ]
        },
        {
          "content": "This should return a value of <bpt id=\"p12\">**</bpt>3<ept id=\"p12\">**</ept><ph id=\"ph14\"/> because there are three rows that contain this value.",
          "pos": [
            223,
            367
          ]
        }
      ]
    },
    {
      "pos": [
        3279,
        3562
      ],
      "content": "<bpt id=\"p13\">**</bpt>INPUT__FILE__NAME LIKE '%.log'<ept id=\"p13\">**</ept><ph id=\"ph15\"/> - Tells Hive that we should only return data from files ending in .log. This restricts the search to the sample.log file that contains the data, and keeps it from returning data from other example data files that do not match the schema we defined.",
      "nodes": [
        {
          "content": "<bpt id=\"p13\">**</bpt>INPUT__FILE__NAME LIKE '%.log'<ept id=\"p13\">**</ept><ph id=\"ph15\"/> - Tells Hive that we should only return data from files ending in .log.",
          "pos": [
            0,
            161
          ]
        },
        {
          "content": "This restricts the search to the sample.log file that contains the data, and keeps it from returning data from other example data files that do not match the schema we defined.",
          "pos": [
            162,
            338
          ]
        }
      ]
    },
    {
      "pos": [
        3567,
        3666
      ],
      "content": "Click <bpt id=\"p14\">**</bpt>Submit<ept id=\"p14\">**</ept>. The <bpt id=\"p15\">**</bpt>Job Session<ept id=\"p15\">**</ept><ph id=\"ph16\"/> at the bottom of the page should display details for the job.",
      "nodes": [
        {
          "content": "Click <bpt id=\"p14\">**</bpt>Submit<ept id=\"p14\">**</ept>.",
          "pos": [
            0,
            57
          ]
        },
        {
          "content": "The <bpt id=\"p15\">**</bpt>Job Session<ept id=\"p15\">**</ept><ph id=\"ph16\"/> at the bottom of the page should display details for the job.",
          "pos": [
            58,
            194
          ]
        }
      ]
    },
    {
      "pos": [
        3671,
        3932
      ],
      "content": "When the <bpt id=\"p16\">**</bpt>Status<ept id=\"p16\">**</ept><ph id=\"ph17\"/> field changes to <bpt id=\"p17\">**</bpt>Completed<ept id=\"p17\">**</ept>, select <bpt id=\"p18\">**</bpt>View Details<ept id=\"p18\">**</ept><ph id=\"ph18\"/> for the job. On the details page, the <bpt id=\"p19\">**</bpt>Job Output<ept id=\"p19\">**</ept><ph id=\"ph19\"/> contains <ph id=\"ph20\">`[ERROR]   3`</ph>. You can use the <bpt id=\"p20\">**</bpt>Download<ept id=\"p20\">**</ept><ph id=\"ph21\"/> button under this field to download a file that contains the output of the job.",
      "nodes": [
        {
          "content": "When the <bpt id=\"p16\">**</bpt>Status<ept id=\"p16\">**</ept><ph id=\"ph17\"/> field changes to <bpt id=\"p17\">**</bpt>Completed<ept id=\"p17\">**</ept>, select <bpt id=\"p18\">**</bpt>View Details<ept id=\"p18\">**</ept><ph id=\"ph18\"/> for the job.",
          "pos": [
            0,
            238
          ]
        },
        {
          "content": "On the details page, the <bpt id=\"p19\">**</bpt>Job Output<ept id=\"p19\">**</ept><ph id=\"ph19\"/> contains <ph id=\"ph20\">`[ERROR]   3`</ph>.",
          "pos": [
            239,
            376
          ]
        },
        {
          "content": "You can use the <bpt id=\"p20\">**</bpt>Download<ept id=\"p20\">**</ept><ph id=\"ph21\"/> button under this field to download a file that contains the output of the job.",
          "pos": [
            377,
            540
          ]
        }
      ]
    },
    {
      "pos": [
        3935,
        3937
      ],
      "content": "##"
    },
    {
      "pos": [
        3957,
        3964
      ],
      "content": "Summary"
    },
    {
      "pos": [
        3966,
        4114
      ],
      "content": "As you can see, the Query Console provides an easy way to run Hive queries in an HDInsight cluster, monitor the job status, and retrieve the output."
    },
    {
      "pos": [
        4116,
        4418
      ],
      "content": "To learn more about using Hive Query Console to run Hive jobs, select <bpt id=\"p21\">**</bpt>Getting Started<ept id=\"p21\">**</ept><ph id=\"ph22\"/> at the top of the Query Console, then use the samples that are provided. Each sample walks through the process of using Hive to analyze data, including explanations about the HiveQL statements used in the sample.",
      "nodes": [
        {
          "content": "To learn more about using Hive Query Console to run Hive jobs, select <bpt id=\"p21\">**</bpt>Getting Started<ept id=\"p21\">**</ept><ph id=\"ph22\"/> at the top of the Query Console, then use the samples that are provided.",
          "pos": [
            0,
            217
          ]
        },
        {
          "content": "Each sample walks through the process of using Hive to analyze data, including explanations about the HiveQL statements used in the sample.",
          "pos": [
            218,
            357
          ]
        }
      ]
    },
    {
      "pos": [
        4420,
        4422
      ],
      "content": "##"
    },
    {
      "pos": [
        4444,
        4454
      ],
      "content": "Next steps"
    },
    {
      "pos": [
        4456,
        4504
      ],
      "content": "For general information about Hive in HDInsight:"
    },
    {
      "pos": [
        4508,
        4566
      ],
      "content": "<bpt id=\"p22\">[</bpt>Use Hive with Hadoop on HDInsight<ept id=\"p22\">](hdinsight-use-hive.md)</ept>"
    },
    {
      "pos": [
        4568,
        4639
      ],
      "content": "For information about other ways you can work with Hadoop on HDInsight:"
    },
    {
      "pos": [
        4643,
        4699
      ],
      "content": "<bpt id=\"p23\">[</bpt>Use Pig with Hadoop on HDInsight<ept id=\"p23\">](hdinsight-use-pig.md)</ept>"
    },
    {
      "pos": [
        4703,
        4771
      ],
      "content": "<bpt id=\"p24\">[</bpt>Use MapReduce with Hadoop on HDInsight<ept id=\"p24\">](hdinsight-use-mapreduce.md)</ept>"
    },
    {
      "pos": [
        4773,
        4859
      ],
      "content": "If you are using Tez with Hive, see the following documents for debugging information:"
    },
    {
      "pos": [
        4863,
        4933
      ],
      "content": "<bpt id=\"p25\">[</bpt>Use the Tez UI on Windows-based HDInsight<ept id=\"p25\">](hdinsight-debug-tez-ui.md)</ept>"
    },
    {
      "pos": [
        4937,
        5023
      ],
      "content": "<bpt id=\"p26\">[</bpt>Use the Ambari Tez view on Linux-based HDInsight<ept id=\"p26\">](hdinsight-debug-ambari-tez-view.md)</ept>"
    }
  ],
  "content": "<properties\n   pageTitle=\"Use Hadoop Hive on the Query Console in HDInsight | Microsoft Azure\"\n   description=\"Learn how to use the web-based Query Console to run Hive queries on an HDInsight Hadoop cluster from your browser.\"\n   services=\"hdinsight\"\n   documentationCenter=\"\"\n   authors=\"Blackmist\"\n   manager=\"paulettm\"\n   editor=\"cgronlun\"\n    tags=\"azure-portal\"/>\n\n<tags\n   ms.service=\"hdinsight\"\n   ms.devlang=\"na\"\n   ms.topic=\"article\"\n   ms.tgt_pltfrm=\"na\"\n   ms.workload=\"big-data\"\n   ms.date=\"02/16/2016\"\n   ms.author=\"larryfr\"/>\n\n# Run Hive queries using the Query Console\n\n[AZURE.INCLUDE [hive-selector](../../includes/hdinsight-selector-use-hive.md)]\n\nIn this article, you will learn how to use the HDInsight Query Console to run Hive queries on an HDInsight Hadoop cluster from your browser.\n\n> [AZURE.NOTE] The Query Console is only available on Windows-based HDInsight clusters.\n\n\n##<a id=\"prereq\"></a>Prerequisites\n\nTo complete the steps in this article, you will need the following.\n\n* A Windows-based HDInsight Hadoop cluster\n\n* A modern web browser\n\n##<a id=\"run\"></a> Run Hive queries using the Query Console\n\n1. Open a web browser and navigate to __https://CLUSTERNAME.azurehdinsight.net__, where __CLUSTERNAME__ is the name of your HDInsight cluster. If prompted, enter the user name and password that you used when you created the cluster.\n\n\n2. From the links at the top of the page, select **Hive Editor**. This displays a form that can be used to enter the HiveQL statements that you want to run in the HDInsight cluster.\n\n    ![the hive editor](./media/hdinsight-hadoop-use-hive-query-console/queryconsole.png)\n\n    Replace the text `Select * from hivesampletable` with the following HiveQL statements:\n\n        set hive.execution.engine=tez;\n        DROP TABLE log4jLogs;\n        CREATE EXTERNAL TABLE log4jLogs (t1 string, t2 string, t3 string, t4 string, t5 string, t6 string, t7 string)\n        ROW FORMAT DELIMITED FIELDS TERMINATED BY ' '\n        STORED AS TEXTFILE LOCATION 'wasb:///example/data/';\n        SELECT t4 AS sev, COUNT(*) AS count FROM log4jLogs WHERE t4 = '[ERROR]' AND INPUT__FILE__NAME LIKE '%.log' GROUP BY t4;\n\n    These statements perform the following actions:\n\n    * **DROP TABLE**: Deletes the table and the data file if the table already exists.\n    * **CREATE EXTERNAL TABLE**: Creates a new 'external' table in Hive. External tables store only the table definition in Hive; the data is left in the original location.\n\n    > [AZURE.NOTE] External tables should be used when you expect the underlying data to be updated by an external source (such as an automated data upload process) or by another MapReduce operation, but you always want Hive queries to use the latest data.\n    >\n    > Dropping an external table does **not** delete the data, only the table definition.\n\n    * **ROW FORMAT**: Tells Hive how the data is formatted. In this case, the fields in each log are separated by a space.\n    * **STORED AS TEXTFILE LOCATION**: Tells Hive where the data is stored (the example/data directory) and that it is stored as text\n    * **SELECT**: Select a count of all rows where column **t4** contain the value **[ERROR]**. This should return a value of **3** because there are three rows that contain this value.\n    * **INPUT__FILE__NAME LIKE '%.log'** - Tells Hive that we should only return data from files ending in .log. This restricts the search to the sample.log file that contains the data, and keeps it from returning data from other example data files that do not match the schema we defined.\n\n2. Click **Submit**. The **Job Session** at the bottom of the page should display details for the job.\n\n3. When the **Status** field changes to **Completed**, select **View Details** for the job. On the details page, the **Job Output** contains `[ERROR]   3`. You can use the **Download** button under this field to download a file that contains the output of the job.\n\n\n##<a id=\"summary\"></a>Summary\n\nAs you can see, the Query Console provides an easy way to run Hive queries in an HDInsight cluster, monitor the job status, and retrieve the output.\n\nTo learn more about using Hive Query Console to run Hive jobs, select **Getting Started** at the top of the Query Console, then use the samples that are provided. Each sample walks through the process of using Hive to analyze data, including explanations about the HiveQL statements used in the sample.\n\n##<a id=\"nextsteps\"></a>Next steps\n\nFor general information about Hive in HDInsight:\n\n* [Use Hive with Hadoop on HDInsight](hdinsight-use-hive.md)\n\nFor information about other ways you can work with Hadoop on HDInsight:\n\n* [Use Pig with Hadoop on HDInsight](hdinsight-use-pig.md)\n\n* [Use MapReduce with Hadoop on HDInsight](hdinsight-use-mapreduce.md)\n\nIf you are using Tez with Hive, see the following documents for debugging information:\n\n* [Use the Tez UI on Windows-based HDInsight](hdinsight-debug-tez-ui.md)\n\n* [Use the Ambari Tez view on Linux-based HDInsight](hdinsight-debug-ambari-tez-view.md)\n\n[1]: ../HDInsight/hdinsight-hadoop-visual-studio-tools-get-started.md\n\n[hdinsight-sdk-documentation]: http://msdnstage.redmond.corp.microsoft.com/library/dn479185.aspx\n\n[azure-purchase-options]: http://azure.microsoft.com/pricing/purchase-options/\n[azure-member-offers]: http://azure.microsoft.com/pricing/member-offers/\n[azure-free-trial]: http://azure.microsoft.com/pricing/free-trial/\n\n[apache-tez]: http://tez.apache.org\n[apache-hive]: http://hive.apache.org/\n[apache-log4j]: http://en.wikipedia.org/wiki/Log4j\n[hive-on-tez-wiki]: https://cwiki.apache.org/confluence/display/Hive/Hive+on+Tez\n[import-to-excel]: http://azure.microsoft.com/documentation/articles/hdinsight-connect-excel-power-query/\n\n\n[hdinsight-use-oozie]: hdinsight-use-oozie.md\n[hdinsight-analyze-flight-data]: hdinsight-analyze-flight-delay-data.md\n\n\n\n[hdinsight-storage]: hdinsight-hadoop-use-blob-storage.md\n\n[hdinsight-provision]: hdinsight-provision-clusters.md\n[hdinsight-submit-jobs]: hdinsight-submit-hadoop-jobs-programmatically.md\n[hdinsight-upload-data]: hdinsight-upload-data.md\n[hdinsight-get-started]: hdinsight-hadoop-linux-tutorial-get-started.md\n\n[Powershell-install-configure]: powershell-install-configure.md\n[powershell-here-strings]: http://technet.microsoft.com/library/ee692792.aspx\n\n\n[img-hdi-hive-powershell-output]: ./media/hdinsight-use-hive/HDI.Hive.PowerShell.Output.png\n"
}