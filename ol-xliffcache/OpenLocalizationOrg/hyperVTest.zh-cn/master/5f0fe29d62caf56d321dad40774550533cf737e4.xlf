<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" source-language="en-US" target-language="zh-cn">
    <body>
      <group id="main" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Use Script Action to install Spark on Hadoop cluster | Microsoft Azure</source>
          <target state="new">Use Script Action to install Spark on Hadoop cluster | Microsoft Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Learn how to customize an HDInsight cluster with Spark using Script Action.</source>
          <target state="new">Learn how to customize an HDInsight cluster with Spark using Script Action.</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Install and use Spark on HDInsight Hadoop clusters using Script Action</source>
          <target state="new">Install and use Spark on HDInsight Hadoop clusters using Script Action</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source><ph id="ph2">[AZURE.IMPORTANT]</ph><ph id="ph3" /> This article is now deprecated.</source>
          <target state="new"><ph id="ph2">[AZURE.IMPORTANT]</ph><ph id="ph3" /> This article is now deprecated.</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>HDInsight now provides Spark as a first-class cluster type for Windows-based clusters, which means you can now directly create a Spark cluster without modifying a Hadoop cluster using Script action.</source>
          <target state="new">HDInsight now provides Spark as a first-class cluster type for Windows-based clusters, which means you can now directly create a Spark cluster without modifying a Hadoop cluster using Script action.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>Using the Spark cluster type, you get an HDInsight version 3.2 cluster with Spark version 1.3.1.</source>
          <target state="new">Using the Spark cluster type, you get an HDInsight version 3.2 cluster with Spark version 1.3.1.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>To install different versions of Spark, you can use Script action.</source>
          <target state="new">To install different versions of Spark, you can use Script action.</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>HDInsight provides a sample Script Action script.</source>
          <target state="new">HDInsight provides a sample Script Action script.</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>Learn how to install Spark on Windows based HDInsight using Script Action, and how to run Spark queries on HDInsight clusters.</source>
          <target state="new">Learn how to install Spark on Windows based HDInsight using Script Action, and how to run Spark queries on HDInsight clusters.</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Related articles<ept id="p1">**</ept></source>
          <target state="new"><bpt id="p1">**</bpt>Related articles<ept id="p1">**</ept></target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source><bpt id="p2">[</bpt>Install Spark on Linux-based HDInsight clusters<ept id="p2">](hdinsight-hadoop-spark-install-linux.md)</ept>.</source>
          <target state="new"><bpt id="p2">[</bpt>Install Spark on Linux-based HDInsight clusters<ept id="p2">](hdinsight-hadoop-spark-install-linux.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source><bpt id="p3">[</bpt>Create Hadoop clusters in HDInsight<ept id="p3">](hdinsight-provision-clusters.md)</ept>: general information on creating HDInsight clusters.</source>
          <target state="new"><bpt id="p3">[</bpt>Create Hadoop clusters in HDInsight<ept id="p3">](hdinsight-provision-clusters.md)</ept>: general information on creating HDInsight clusters.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source><bpt id="p4">[</bpt>Get Started with Apache Spark on HDInsight<ept id="p4">](hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql.md)</ept>: create a Spark type cluster on Windows OS.</source>
          <target state="new"><bpt id="p4">[</bpt>Get Started with Apache Spark on HDInsight<ept id="p4">](hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql.md)</ept>: create a Spark type cluster on Windows OS.</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source><bpt id="p5">[</bpt>Customize HDInsight cluster using Script Action<ept id="p5">][hdinsight-cluster-customize]</ept>: general information on customizing HDInsight clusters using Script Action.</source>
          <target state="new"><bpt id="p5">[</bpt>Customize HDInsight cluster using Script Action<ept id="p5">][hdinsight-cluster-customize]</ept>: general information on customizing HDInsight clusters using Script Action.</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source><bpt id="p6">[</bpt>Develop Script Action scripts for HDInsight<ept id="p6">](hdinsight-hadoop-script-actions.md)</ept>.</source>
          <target state="new"><bpt id="p6">[</bpt>Develop Script Action scripts for HDInsight<ept id="p6">](hdinsight-hadoop-script-actions.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>What is Spark?</source>
          <target state="new">What is Spark?</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>Apache Spark</source>
          <target state="new">Apache Spark</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>is an open-source parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications.</source>
          <target state="new">is an open-source parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications.</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>Spark's in-memory computation capabilities make it a good choice for iterative algorithms in machine learning and graph computations.</source>
          <target state="new">Spark's in-memory computation capabilities make it a good choice for iterative algorithms in machine learning and graph computations.</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>Spark can also be used to perform conventional disk-based data processing.</source>
          <target state="new">Spark can also be used to perform conventional disk-based data processing.</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>Spark improves the traditional MapReduce framework by avoiding writes to disk in the intermediate stages.</source>
          <target state="new">Spark improves the traditional MapReduce framework by avoiding writes to disk in the intermediate stages.</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>Also, Spark is compatible with the Hadoop Distributed File System (HDFS) and Azure Blob storage so the existing data can easily be processed via Spark.</source>
          <target state="new">Also, Spark is compatible with the Hadoop Distributed File System (HDFS) and Azure Blob storage so the existing data can easily be processed via Spark.</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>This topic provides instructions on how to customize an HDInsight cluster to install Spark.</source>
          <target state="new">This topic provides instructions on how to customize an HDInsight cluster to install Spark.</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>Install Spark using the Azure Portal</source>
          <target state="new">Install Spark using the Azure Portal</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>A sample script to install Spark on an HDInsight cluster is available from a read-only Azure storage blob at <bpt id="p7">[</bpt>https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv03/spark-installer-v03.ps1<ept id="p7">](https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv03/spark-installer-v03.ps1)</ept>.</source>
          <target state="new">A sample script to install Spark on an HDInsight cluster is available from a read-only Azure storage blob at <bpt id="p7">[</bpt>https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv03/spark-installer-v03.ps1<ept id="p7">](https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv03/spark-installer-v03.ps1)</ept>.</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>This script can install Spark 1.2.0 or Spark 1.0.2 depending on the version of the HDInsight cluster you create.</source>
          <target state="new">This script can install Spark 1.2.0 or Spark 1.0.2 depending on the version of the HDInsight cluster you create.</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>If you use the script while creating an <bpt id="p8">**</bpt>HDInsight 3.2<ept id="p8">**</ept><ph id="ph4" /> cluster, it installs <bpt id="p9">**</bpt>Spark 1.2.0<ept id="p9">**</ept>.</source>
          <target state="new">If you use the script while creating an <bpt id="p8">**</bpt>HDInsight 3.2<ept id="p8">**</ept><ph id="ph4" /> cluster, it installs <bpt id="p9">**</bpt>Spark 1.2.0<ept id="p9">**</ept>.</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>If you use the script while creating an <bpt id="p10">**</bpt>HDInsight 3.1<ept id="p10">**</ept><ph id="ph5" /> cluster, it installs <bpt id="p11">**</bpt>Spark 1.0.2<ept id="p11">**</ept>.</source>
          <target state="new">If you use the script while creating an <bpt id="p10">**</bpt>HDInsight 3.1<ept id="p10">**</ept><ph id="ph5" /> cluster, it installs <bpt id="p11">**</bpt>Spark 1.0.2<ept id="p11">**</ept>.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>You can modify this script or create your own script to install other versions of Spark.</source>
          <target state="new">You can modify this script or create your own script to install other versions of Spark.</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source><ph id="ph6">[AZURE.NOTE]</ph><ph id="ph7" /> The sample script works only with HDInsight 3.1 and 3.2 clusters.</source>
          <target state="new"><ph id="ph6">[AZURE.NOTE]</ph><ph id="ph7" /> The sample script works only with HDInsight 3.1 and 3.2 clusters.</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>For more information on HDInsight cluster versions, see <bpt id="p12">[</bpt>HDInsight cluster versions<ept id="p12">](hdinsight-component-versioning.md)</ept>.</source>
          <target state="new">For more information on HDInsight cluster versions, see <bpt id="p12">[</bpt>HDInsight cluster versions<ept id="p12">](hdinsight-component-versioning.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>Start creating a cluster by using the <bpt id="p13">**</bpt>CUSTOM CREATE<ept id="p13">**</ept><ph id="ph8" /> option, as described at <bpt id="p14">[</bpt>Create Hadoop clusters in HDInsight<ept id="p14">](hdinsight-provision-clusters.md#portal)</ept>.</source>
          <target state="new">Start creating a cluster by using the <bpt id="p13">**</bpt>CUSTOM CREATE<ept id="p13">**</ept><ph id="ph8" /> option, as described at <bpt id="p14">[</bpt>Create Hadoop clusters in HDInsight<ept id="p14">](hdinsight-provision-clusters.md#portal)</ept>.</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>Pick the cluster version depending on the following:</source>
          <target state="new">Pick the cluster version depending on the following:</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>If you want to install <bpt id="p15">**</bpt>Spark 1.2.0<ept id="p15">**</ept>, create an HDInsight 3.2 cluster.</source>
          <target state="new">If you want to install <bpt id="p15">**</bpt>Spark 1.2.0<ept id="p15">**</ept>, create an HDInsight 3.2 cluster.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>If you want to install <bpt id="p16">**</bpt>Spark 1.0.2<ept id="p16">**</ept>, create an HDInsight 3.1 cluster.</source>
          <target state="new">If you want to install <bpt id="p16">**</bpt>Spark 1.0.2<ept id="p16">**</ept>, create an HDInsight 3.1 cluster.</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>On the <bpt id="p17">**</bpt>Script Actions<ept id="p17">**</ept><ph id="ph9" /> page of the wizard, click <bpt id="p18">**</bpt>add script action<ept id="p18">**</ept><ph id="ph10" /> to provide details about the script action, as shown below:</source>
          <target state="new">On the <bpt id="p17">**</bpt>Script Actions<ept id="p17">**</ept><ph id="ph9" /> page of the wizard, click <bpt id="p18">**</bpt>add script action<ept id="p18">**</ept><ph id="ph10" /> to provide details about the script action, as shown below:</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source><ph id="ph11">![</ph>Use Script Action to customize a cluster<ph id="ph12">](./media/hdinsight-hadoop-spark-install/HDI.CustomProvision.Page6.png "Use Script Action to customize a cluster")</ph></source>
          <target state="new"><ph id="ph11">![</ph>Use Script Action to customize a cluster<ph id="ph12">](./media/hdinsight-hadoop-spark-install/HDI.CustomProvision.Page6.png "Use Script Action to customize a cluster")</ph></target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source><ph id="ph13">&lt;table border='1'&gt;</ph><ph id="ph14">
     &lt;tr&gt;</ph><ph id="ph15">&lt;th&gt;</ph>Property<ph id="ph16">&lt;/th&gt;</ph><ph id="ph17">&lt;th&gt;</ph>Value<ph id="ph18">&lt;/th&gt;</ph><ph id="ph19">&lt;/tr&gt;</ph><ph id="ph20">
     &lt;tr&gt;</ph><ph id="ph21">&lt;td&gt;</ph>Name<ph id="ph22">&lt;/td&gt;</ph><ph id="ph23">
         &lt;td&gt;</ph>Specify a name for the script action.</source>
          <target state="new"><ph id="ph13">&lt;table border='1'&gt;</ph><ph id="ph14">
     &lt;tr&gt;</ph><ph id="ph15">&lt;th&gt;</ph>Property<ph id="ph16">&lt;/th&gt;</ph><ph id="ph17">&lt;th&gt;</ph>Value<ph id="ph18">&lt;/th&gt;</ph><ph id="ph19">&lt;/tr&gt;</ph><ph id="ph20">
     &lt;tr&gt;</ph><ph id="ph21">&lt;td&gt;</ph>Name<ph id="ph22">&lt;/td&gt;</ph><ph id="ph23">
         &lt;td&gt;</ph>Specify a name for the script action.</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>For example, <ph id="ph24">&lt;b&gt;</ph>Install Spark<ph id="ph25">&lt;/b&gt;</ph>.<ph id="ph26">&lt;/td&gt;</ph><ph id="ph27">&lt;/tr&gt;</ph><ph id="ph28">
     &lt;tr&gt;</ph><ph id="ph29">&lt;td&gt;</ph>Script URI<ph id="ph30">&lt;/td&gt;</ph><ph id="ph31">
         &lt;td&gt;</ph>Specify the Uniform Resource Identifier (URI) to the script that is invoked to customize the cluster.</source>
          <target state="new">For example, <ph id="ph24">&lt;b&gt;</ph>Install Spark<ph id="ph25">&lt;/b&gt;</ph>.<ph id="ph26">&lt;/td&gt;</ph><ph id="ph27">&lt;/tr&gt;</ph><ph id="ph28">
     &lt;tr&gt;</ph><ph id="ph29">&lt;td&gt;</ph>Script URI<ph id="ph30">&lt;/td&gt;</ph><ph id="ph31">
         &lt;td&gt;</ph>Specify the Uniform Resource Identifier (URI) to the script that is invoked to customize the cluster.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>For example, <ph id="ph32">&lt;i&gt;</ph>https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv03/spark-installer-v03.ps1<ph id="ph33">&lt;/i&gt;</ph><ph id="ph34">&lt;/td&gt;</ph><ph id="ph35">&lt;/tr&gt;</ph><ph id="ph36">
     &lt;tr&gt;</ph><ph id="ph37">&lt;td&gt;</ph>Node Type<ph id="ph38">&lt;/td&gt;</ph><ph id="ph39">
         &lt;td&gt;</ph>Specify the nodes on which the customization script is run.</source>
          <target state="new">For example, <ph id="ph32">&lt;i&gt;</ph>https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv03/spark-installer-v03.ps1<ph id="ph33">&lt;/i&gt;</ph><ph id="ph34">&lt;/td&gt;</ph><ph id="ph35">&lt;/tr&gt;</ph><ph id="ph36">
     &lt;tr&gt;</ph><ph id="ph37">&lt;td&gt;</ph>Node Type<ph id="ph38">&lt;/td&gt;</ph><ph id="ph39">
         &lt;td&gt;</ph>Specify the nodes on which the customization script is run.</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>You can choose <ph id="ph40">&lt;b&gt;</ph>All nodes<ph id="ph41">&lt;/b&gt;</ph>, <ph id="ph42">&lt;b&gt;</ph>Head nodes only<ph id="ph43">&lt;/b&gt;</ph>, or <ph id="ph44">&lt;b&gt;</ph>Worker nodes only<ph id="ph45">&lt;/b&gt;</ph>.</source>
          <target state="new">You can choose <ph id="ph40">&lt;b&gt;</ph>All nodes<ph id="ph41">&lt;/b&gt;</ph>, <ph id="ph42">&lt;b&gt;</ph>Head nodes only<ph id="ph43">&lt;/b&gt;</ph>, or <ph id="ph44">&lt;b&gt;</ph>Worker nodes only<ph id="ph45">&lt;/b&gt;</ph>.</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source><ph id="ph46">&lt;tr&gt;</ph><ph id="ph47">&lt;td&gt;</ph>Parameters<ph id="ph48">&lt;/td&gt;</ph><ph id="ph49">
         &lt;td&gt;</ph>Specify the parameters, if required by the script.</source>
          <target state="new"><ph id="ph46">&lt;tr&gt;</ph><ph id="ph47">&lt;td&gt;</ph>Parameters<ph id="ph48">&lt;/td&gt;</ph><ph id="ph49">
         &lt;td&gt;</ph>Specify the parameters, if required by the script.</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>The script to install Spark does not require any parameters so you can leave this blank.<ph id="ph50">&lt;/td&gt;</ph><ph id="ph51">&lt;/tr&gt;</ph><ph id="ph52">
 &lt;/table&gt;</ph></source>
          <target state="new">The script to install Spark does not require any parameters so you can leave this blank.<ph id="ph50">&lt;/td&gt;</ph><ph id="ph51">&lt;/tr&gt;</ph><ph id="ph52">
 &lt;/table&gt;</ph></target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>You can add more than one script action to install multiple components on the cluster.</source>
          <target state="new">You can add more than one script action to install multiple components on the cluster.</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>After you have added the scripts, click the checkmark to start creating the cluster.</source>
          <target state="new">After you have added the scripts, click the checkmark to start creating the cluster.</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>You can also use the script to install Spark on HDInsight by using Azure PowerShell or the HDInsight .NET SDK.</source>
          <target state="new">You can also use the script to install Spark on HDInsight by using Azure PowerShell or the HDInsight .NET SDK.</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>Instructions for these procedures are provided later in this topic.</source>
          <target state="new">Instructions for these procedures are provided later in this topic.</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>Use Spark in HDInsight</source>
          <target state="new">Use Spark in HDInsight</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>Spark provides APIs in Scala, Python, and Java.</source>
          <target state="new">Spark provides APIs in Scala, Python, and Java.</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>You can also use the interactive Spark shell to run Spark queries.</source>
          <target state="new">You can also use the interactive Spark shell to run Spark queries.</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>This section provides instructions on how to use the different approaches to work with Spark:</source>
          <target state="new">This section provides instructions on how to use the different approaches to work with Spark:</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source><bpt id="p19">[</bpt>Use the Spark shell to run interactive queries<ept id="p19">](#sparkshell)</ept></source>
          <target state="new"><bpt id="p19">[</bpt>Use the Spark shell to run interactive queries<ept id="p19">](#sparkshell)</ept></target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source><bpt id="p20">[</bpt>Use the Spark shell to run Spark SQL queries<ept id="p20">](#sparksql)</ept></source>
          <target state="new"><bpt id="p20">[</bpt>Use the Spark shell to run Spark SQL queries<ept id="p20">](#sparksql)</ept></target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source><bpt id="p21">[</bpt>Use a standalone Scala program<ept id="p21">](#standalone)</ept></source>
          <target state="new"><bpt id="p21">[</bpt>Use a standalone Scala program<ept id="p21">](#standalone)</ept></target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>Use the Spark shell to run interactive queries</source>
          <target state="new">Use the Spark shell to run interactive queries</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>Perform the following steps to run Spark queries from an interactive Spark shell.</source>
          <target state="new">Perform the following steps to run Spark queries from an interactive Spark shell.</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>In this section, we run a Spark query on a sample data file (/example/data/gutenberg/davinci.txt) that is available on HDInsight clusters by default.</source>
          <target state="new">In this section, we run a Spark query on a sample data file (/example/data/gutenberg/davinci.txt) that is available on HDInsight clusters by default.</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>From the Azure portal, enable Remote Desktop for the cluster you created with Spark installed, and then remote into the cluster.</source>
          <target state="new">From the Azure portal, enable Remote Desktop for the cluster you created with Spark installed, and then remote into the cluster.</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source>For instructions, see <bpt id="p22">[</bpt>Connect to HDInsight clusters using RDP<ept id="p22">](hdinsight-administer-use-management-portal.md#rdp)</ept>.</source>
          <target state="new">For instructions, see <bpt id="p22">[</bpt>Connect to HDInsight clusters using RDP<ept id="p22">](hdinsight-administer-use-management-portal.md#rdp)</ept>.</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>In the Remote Desktop Protocol (RDP) session, from the desktop, open the Hadoop command line (from a desktop shortcut), and navigate to the location where Spark is installed; for example, <bpt id="p23">**</bpt>C:\apps\dist\spark-1.2.0<ept id="p23">**</ept>.</source>
          <target state="new">In the Remote Desktop Protocol (RDP) session, from the desktop, open the Hadoop command line (from a desktop shortcut), and navigate to the location where Spark is installed; for example, <bpt id="p23">**</bpt>C:\apps\dist\spark-1.2.0<ept id="p23">**</ept>.</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>Run the following command to start the Spark shell:</source>
          <target state="new">Run the following command to start the Spark shell:</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>After the command finishes running, you should get a Scala prompt:</source>
          <target state="new">After the command finishes running, you should get a Scala prompt:</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>On the Scala prompt, enter the Spark query shown below.</source>
          <target state="new">On the Scala prompt, enter the Spark query shown below.</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>This query counts the occurrence of each word in the davinci.txt file that is available at the /example/data/gutenberg/ location on the Azure Blob storage associated with the cluster.</source>
          <target state="new">This query counts the occurrence of each word in the davinci.txt file that is available at the /example/data/gutenberg/ location on the Azure Blob storage associated with the cluster.</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>The output should resemble the following:</source>
          <target state="new">The output should resemble the following:</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source><ph id="ph53">![</ph>Output from running Scala interactive shell in an HDInsight cluster<ph id="ph54">](./media/hdinsight-hadoop-spark-install/hdi-scala-interactive.png)</ph></source>
          <target state="new"><ph id="ph53">![</ph>Output from running Scala interactive shell in an HDInsight cluster<ph id="ph54">](./media/hdinsight-hadoop-spark-install/hdi-scala-interactive.png)</ph></target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source>Enter :q to exit the Scala prompt.</source>
          <target state="new">Enter :q to exit the Scala prompt.</target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>Use the Spark shell to run Spark SQL queries</source>
          <target state="new">Use the Spark shell to run Spark SQL queries</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>Spark SQL allows you to use Spark to run relational queries expressed in Structured Query Language (SQL), HiveQL, or Scala.</source>
          <target state="new">Spark SQL allows you to use Spark to run relational queries expressed in Structured Query Language (SQL), HiveQL, or Scala.</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source>In this section, we look at using Spark to run a Hive query on a sample Hive table.</source>
          <target state="new">In this section, we look at using Spark to run a Hive query on a sample Hive table.</target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source>The Hive table used in this section (called <bpt id="p24">**</bpt>hivesampletable<ept id="p24">**</ept>) is available by default when you create a cluster.</source>
          <target state="new">The Hive table used in this section (called <bpt id="p24">**</bpt>hivesampletable<ept id="p24">**</ept>) is available by default when you create a cluster.</target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source><ph id="ph55">[AZURE.NOTE]</ph><ph id="ph56" /> The sample below was created against <bpt id="p25">**</bpt>Spark 1.2.0<ept id="p25">**</ept>, which is installed if you run the script action while creating HDInsight 3.2 cluster.</source>
          <target state="new"><ph id="ph55">[AZURE.NOTE]</ph><ph id="ph56" /> The sample below was created against <bpt id="p25">**</bpt>Spark 1.2.0<ept id="p25">**</ept>, which is installed if you run the script action while creating HDInsight 3.2 cluster.</target>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source>From the Azure portal, enable Remote Desktop for the cluster you created with Spark installed, and then remote into the cluster.</source>
          <target state="new">From the Azure portal, enable Remote Desktop for the cluster you created with Spark installed, and then remote into the cluster.</target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source>For instructions, see <bpt id="p26">[</bpt>Connect to HDInsight clusters using RDP<ept id="p26">](hdinsight-administer-use-management-portal.md#rdp)</ept>.</source>
          <target state="new">For instructions, see <bpt id="p26">[</bpt>Connect to HDInsight clusters using RDP<ept id="p26">](hdinsight-administer-use-management-portal.md#rdp)</ept>.</target>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source>In the RDP session, from the desktop, open the Hadoop command line (from a desktop shortcut), and navigate to the location where Spark is installed; for example, <bpt id="p27">**</bpt>C:\apps\dist\spark-1.2.0<ept id="p27">**</ept>.</source>
          <target state="new">In the RDP session, from the desktop, open the Hadoop command line (from a desktop shortcut), and navigate to the location where Spark is installed; for example, <bpt id="p27">**</bpt>C:\apps\dist\spark-1.2.0<ept id="p27">**</ept>.</target>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source>Run the following command to start the Spark shell:</source>
          <target state="new">Run the following command to start the Spark shell:</target>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source>After the command finishes running, you should get a Scala prompt:</source>
          <target state="new">After the command finishes running, you should get a Scala prompt:</target>
        </trans-unit>
        <trans-unit id="180" translate="yes" xml:space="preserve">
          <source>On the Scala prompt, set the Hive context.</source>
          <target state="new">On the Scala prompt, set the Hive context.</target>
        </trans-unit>
        <trans-unit id="181" translate="yes" xml:space="preserve">
          <source>This is required to work with Hive queries by using Spark.</source>
          <target state="new">This is required to work with Hive queries by using Spark.</target>
        </trans-unit>
        <trans-unit id="182" translate="yes" xml:space="preserve">
          <source>Note that <bpt id="p28">**</bpt>sc<ept id="p28">**</ept><ph id="ph57" /> is default Spark context that is set when you start the Spark shell.</source>
          <target state="new">Note that <bpt id="p28">**</bpt>sc<ept id="p28">**</ept><ph id="ph57" /> is default Spark context that is set when you start the Spark shell.</target>
        </trans-unit>
        <trans-unit id="183" translate="yes" xml:space="preserve">
          <source>Run a Hive query by using the Hive context and print the output to the console.</source>
          <target state="new">Run a Hive query by using the Hive context and print the output to the console.</target>
        </trans-unit>
        <trans-unit id="184" translate="yes" xml:space="preserve">
          <source>The query retrieves data on devices of a specific make and limits the number of records retrieved to 20.</source>
          <target state="new">The query retrieves data on devices of a specific make and limits the number of records retrieved to 20.</target>
        </trans-unit>
        <trans-unit id="185" translate="yes" xml:space="preserve">
          <source>You should see an output like the following:</source>
          <target state="new">You should see an output like the following:</target>
        </trans-unit>
        <trans-unit id="186" translate="yes" xml:space="preserve">
          <source><ph id="ph58">![</ph>Output from running Spark SQL on an HDInsight cluster<ph id="ph59">](./media/hdinsight-hadoop-spark-install/hdi-spark-sql.png)</ph></source>
          <target state="new"><ph id="ph58">![</ph>Output from running Spark SQL on an HDInsight cluster<ph id="ph59">](./media/hdinsight-hadoop-spark-install/hdi-spark-sql.png)</ph></target>
        </trans-unit>
        <trans-unit id="187" translate="yes" xml:space="preserve">
          <source>Enter :q to exit the Scala prompt.</source>
          <target state="new">Enter :q to exit the Scala prompt.</target>
        </trans-unit>
        <trans-unit id="188" translate="yes" xml:space="preserve">
          <source>Use a standalone Scala program</source>
          <target state="new">Use a standalone Scala program</target>
        </trans-unit>
        <trans-unit id="189" translate="yes" xml:space="preserve">
          <source>In this section, we write a Scala application that counts the number of lines containing the letters 'a' and 'b' in a sample data file (/example/data/gutenberg/davinci.txt) that is available on HDInsight clusters by default.</source>
          <target state="new">In this section, we write a Scala application that counts the number of lines containing the letters 'a' and 'b' in a sample data file (/example/data/gutenberg/davinci.txt) that is available on HDInsight clusters by default.</target>
        </trans-unit>
        <trans-unit id="190" translate="yes" xml:space="preserve">
          <source>To write and use a standalone Scala program with a cluster customized with Spark installation, you must perform the following steps:</source>
          <target state="new">To write and use a standalone Scala program with a cluster customized with Spark installation, you must perform the following steps:</target>
        </trans-unit>
        <trans-unit id="191" translate="yes" xml:space="preserve">
          <source>Write a Scala program</source>
          <target state="new">Write a Scala program</target>
        </trans-unit>
        <trans-unit id="192" translate="yes" xml:space="preserve">
          <source>Build the Scala program to get the .jar file</source>
          <target state="new">Build the Scala program to get the .jar file</target>
        </trans-unit>
        <trans-unit id="193" translate="yes" xml:space="preserve">
          <source>Run the job on the cluster</source>
          <target state="new">Run the job on the cluster</target>
        </trans-unit>
        <trans-unit id="194" translate="yes" xml:space="preserve">
          <source>Write a Scala program</source>
          <target state="new">Write a Scala program</target>
        </trans-unit>
        <trans-unit id="195" translate="yes" xml:space="preserve">
          <source>In this section, you write a Scala program that counts the number of lines containing 'a' and 'b' in the sample data file.</source>
          <target state="new">In this section, you write a Scala program that counts the number of lines containing 'a' and 'b' in the sample data file.</target>
        </trans-unit>
        <trans-unit id="196" translate="yes" xml:space="preserve">
          <source>Open a text editor and paste the following code:</source>
          <target state="new">Open a text editor and paste the following code:</target>
        </trans-unit>
        <trans-unit id="197" translate="yes" xml:space="preserve">
          <source>Save the file with the name <bpt id="p29">**</bpt>SimpleApp.scala<ept id="p29">**</ept>.</source>
          <target state="new">Save the file with the name <bpt id="p29">**</bpt>SimpleApp.scala<ept id="p29">**</ept>.</target>
        </trans-unit>
        <trans-unit id="198" translate="yes" xml:space="preserve">
          <source>Build the Scala program</source>
          <target state="new">Build the Scala program</target>
        </trans-unit>
        <trans-unit id="199" translate="yes" xml:space="preserve">
          <source>In this section, you use the</source>
          <target state="new">In this section, you use the</target>
        </trans-unit>
        <trans-unit id="200" translate="yes" xml:space="preserve">
          <source>Simple Build Tool</source>
          <target state="new">Simple Build Tool</target>
        </trans-unit>
        <trans-unit id="201" translate="yes" xml:space="preserve">
          <source>(or sbt) to build the Scala program.</source>
          <target state="new">(or sbt) to build the Scala program.</target>
        </trans-unit>
        <trans-unit id="202" translate="yes" xml:space="preserve">
          <source>sbt requires Java 1.6 or later, so make sure you have the right version of Java installed before continuing with this section.</source>
          <target state="new">sbt requires Java 1.6 or later, so make sure you have the right version of Java installed before continuing with this section.</target>
        </trans-unit>
        <trans-unit id="203" translate="yes" xml:space="preserve">
          <source>Install sbt from http://www.scala-sbt.org/0.13/tutorial/Installing-sbt-on-Windows.html.</source>
          <target state="new">Install sbt from http://www.scala-sbt.org/0.13/tutorial/Installing-sbt-on-Windows.html.</target>
        </trans-unit>
        <trans-unit id="204" translate="yes" xml:space="preserve">
          <source>Create a folder called <bpt id="p30">**</bpt>SimpleScalaApp<ept id="p30">**</ept>, and within this folder create a file called <bpt id="p31">**</bpt>simple.sbt<ept id="p31">**</ept>.</source>
          <target state="new">Create a folder called <bpt id="p30">**</bpt>SimpleScalaApp<ept id="p30">**</ept>, and within this folder create a file called <bpt id="p31">**</bpt>simple.sbt<ept id="p31">**</ept>.</target>
        </trans-unit>
        <trans-unit id="205" translate="yes" xml:space="preserve">
          <source>This is a configuration file that contains information about the Scala version, library dependencies, etc. Paste the following into the simple.sbt file and save it:</source>
          <target state="new">This is a configuration file that contains information about the Scala version, library dependencies, etc. Paste the following into the simple.sbt file and save it:</target>
        </trans-unit>
        <trans-unit id="206" translate="yes" xml:space="preserve">
          <source>Under the <bpt id="p32">**</bpt>SimpleScalaApp<ept id="p32">**</ept><ph id="ph60" /> folder, create a directory structure <bpt id="p33">**</bpt>\src\main\scala<ept id="p33">**</ept><ph id="ph61" /> and paste the Scala program (<bpt id="p34">**</bpt>SimpleApp.scala<ept id="p34">**</ept>) you created earlier under the \src\main\scala folder.</source>
          <target state="new">Under the <bpt id="p32">**</bpt>SimpleScalaApp<ept id="p32">**</ept><ph id="ph60" /> folder, create a directory structure <bpt id="p33">**</bpt>\src\main\scala<ept id="p33">**</ept><ph id="ph61" /> and paste the Scala program (<bpt id="p34">**</bpt>SimpleApp.scala<ept id="p34">**</ept>) you created earlier under the \src\main\scala folder.</target>
        </trans-unit>
        <trans-unit id="207" translate="yes" xml:space="preserve">
          <source>Open a command prompt, navigate to the SimpleScalaApp directory, and enter the following command:</source>
          <target state="new">Open a command prompt, navigate to the SimpleScalaApp directory, and enter the following command:</target>
        </trans-unit>
        <trans-unit id="208" translate="yes" xml:space="preserve">
          <source>Run the job on the cluster</source>
          <target state="new">Run the job on the cluster</target>
        </trans-unit>
        <trans-unit id="209" translate="yes" xml:space="preserve">
          <source>In this section, you remote into the cluster that has Spark installed and then copy the SimpleScalaApp project's target folder.</source>
          <target state="new">In this section, you remote into the cluster that has Spark installed and then copy the SimpleScalaApp project's target folder.</target>
        </trans-unit>
        <trans-unit id="210" translate="yes" xml:space="preserve">
          <source>You then use the <bpt id="p35">**</bpt>spark-submit<ept id="p35">**</ept><ph id="ph62" /> command to submit the job on the cluster.</source>
          <target state="new">You then use the <bpt id="p35">**</bpt>spark-submit<ept id="p35">**</ept><ph id="ph62" /> command to submit the job on the cluster.</target>
        </trans-unit>
        <trans-unit id="211" translate="yes" xml:space="preserve">
          <source>Remote into the cluster that has Spark installed.</source>
          <target state="new">Remote into the cluster that has Spark installed.</target>
        </trans-unit>
        <trans-unit id="212" translate="yes" xml:space="preserve">
          <source>From the computer where you wrote and built the SimpleApp.scala program, copy the <bpt id="p36">**</bpt>SimpleScalaApp\target<ept id="p36">**</ept><ph id="ph63" /> folder and paste it to a location on the cluster.</source>
          <target state="new">From the computer where you wrote and built the SimpleApp.scala program, copy the <bpt id="p36">**</bpt>SimpleScalaApp\target<ept id="p36">**</ept><ph id="ph63" /> folder and paste it to a location on the cluster.</target>
        </trans-unit>
        <trans-unit id="213" translate="yes" xml:space="preserve">
          <source>In the RDP session, from the desktop, open the Hadoop command line, and navigate to the location where you pasted the <bpt id="p37">**</bpt>target<ept id="p37">**</ept><ph id="ph64" /> folder.</source>
          <target state="new">In the RDP session, from the desktop, open the Hadoop command line, and navigate to the location where you pasted the <bpt id="p37">**</bpt>target<ept id="p37">**</ept><ph id="ph64" /> folder.</target>
        </trans-unit>
        <trans-unit id="214" translate="yes" xml:space="preserve">
          <source>Enter the following command to run the SimpleApp.scala program:</source>
          <target state="new">Enter the following command to run the SimpleApp.scala program:</target>
        </trans-unit>
        <trans-unit id="215" translate="yes" xml:space="preserve">
          <source>When the program finishes running, the output is displayed on the console.</source>
          <target state="new">When the program finishes running, the output is displayed on the console.</target>
        </trans-unit>
        <trans-unit id="216" translate="yes" xml:space="preserve">
          <source>Install Spark using Azure PowerShell</source>
          <target state="new">Install Spark using Azure PowerShell</target>
        </trans-unit>
        <trans-unit id="217" translate="yes" xml:space="preserve">
          <source>In this section, we use the <bpt id="p38">**</bpt><ph id="ph65">&lt;a href = "http://msdn.microsoft.com/library/dn858088.aspx" target="_blank"&gt;</ph>Add-AzureHDInsightScriptAction<ph id="ph66">&lt;/a&gt;</ph><ept id="p38">**</ept><ph id="ph67" /> cmdlet to invoke scripts by using Script Action to customize a cluster.</source>
          <target state="new">In this section, we use the <bpt id="p38">**</bpt><ph id="ph65">&lt;a href = "http://msdn.microsoft.com/library/dn858088.aspx" target="_blank"&gt;</ph>Add-AzureHDInsightScriptAction<ph id="ph66">&lt;/a&gt;</ph><ept id="p38">**</ept><ph id="ph67" /> cmdlet to invoke scripts by using Script Action to customize a cluster.</target>
        </trans-unit>
        <trans-unit id="218" translate="yes" xml:space="preserve">
          <source>Before proceeding, make sure you have installed and configured Azure PowerShell.</source>
          <target state="new">Before proceeding, make sure you have installed and configured Azure PowerShell.</target>
        </trans-unit>
        <trans-unit id="219" translate="yes" xml:space="preserve">
          <source>For information on configuring a workstation to run Azure PowerShell cmdlets for HDInsight, see <bpt id="p39">[</bpt>Install and configure Azure PowerShell<ept id="p39">][powershell-install-configure]</ept>.</source>
          <target state="new">For information on configuring a workstation to run Azure PowerShell cmdlets for HDInsight, see <bpt id="p39">[</bpt>Install and configure Azure PowerShell<ept id="p39">][powershell-install-configure]</ept>.</target>
        </trans-unit>
        <trans-unit id="220" translate="yes" xml:space="preserve">
          <source>Perform the following steps:</source>
          <target state="new">Perform the following steps:</target>
        </trans-unit>
        <trans-unit id="221" translate="yes" xml:space="preserve">
          <source>Open an Azure PowerShell window and declare the following variables:</source>
          <target state="new">Open an Azure PowerShell window and declare the following variables:</target>
        </trans-unit>
        <trans-unit id="222" translate="yes" xml:space="preserve">
          <source>Specify the configuration values such as nodes in the cluster and the default storage to be used.</source>
          <target state="new">Specify the configuration values such as nodes in the cluster and the default storage to be used.</target>
        </trans-unit>
        <trans-unit id="223" translate="yes" xml:space="preserve">
          <source>Use the <bpt id="p40">**</bpt>Add-AzureHDInsightScriptAction<ept id="p40">**</ept><ph id="ph68" /> cmdlet to add a script action to cluster configuration.</source>
          <target state="new">Use the <bpt id="p40">**</bpt>Add-AzureHDInsightScriptAction<ept id="p40">**</ept><ph id="ph68" /> cmdlet to add a script action to cluster configuration.</target>
        </trans-unit>
        <trans-unit id="224" translate="yes" xml:space="preserve">
          <source>Later, when the cluster is being created, the script action gets executed.</source>
          <target state="new">Later, when the cluster is being created, the script action gets executed.</target>
        </trans-unit>
        <trans-unit id="225" translate="yes" xml:space="preserve">
          <source><bpt id="p41">**</bpt>Add-AzureHDInsightScriptAction<ept id="p41">**</ept><ph id="ph69" /> cmdlet takes the following parameters:</source>
          <target state="new"><bpt id="p41">**</bpt>Add-AzureHDInsightScriptAction<ept id="p41">**</ept><ph id="ph69" /> cmdlet takes the following parameters:</target>
        </trans-unit>
        <trans-unit id="226" translate="yes" xml:space="preserve">
          <source><ph id="ph70">&lt;table style="border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse;"&gt;</ph><ph id="ph71">
 &lt;tr&gt;</ph><ph id="ph72">
 &lt;th style="border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; width:90px; padding-left:5px; padding-right:5px;"&gt;</ph>Parameter<ph id="ph73">&lt;/th&gt;</ph><ph id="ph74">
 &lt;th style="border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; width:550px; padding-left:5px; padding-right:5px;"&gt;</ph>Definition<ph id="ph75">&lt;/th&gt;</ph><ph id="ph76">&lt;/tr&gt;</ph><ph id="ph77">
 &lt;tr&gt;</ph><ph id="ph78">
 &lt;td style="border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;"&gt;</ph>Config<ph id="ph79">&lt;/td&gt;</ph><ph id="ph80">
 &lt;td style="border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px; padding-right:5px;"&gt;</ph>The configuration object to which script action information is added.<ph id="ph81">&lt;/td&gt;</ph><ph id="ph82">&lt;/tr&gt;</ph><ph id="ph83">
 &lt;tr&gt;</ph><ph id="ph84">
 &lt;td style="border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;"&gt;</ph>Name<ph id="ph85">&lt;/td&gt;</ph><ph id="ph86">
 &lt;td style="border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;"&gt;</ph>Name of the script action.<ph id="ph87">&lt;/td&gt;</ph><ph id="ph88">&lt;/tr&gt;</ph><ph id="ph89">
 &lt;tr&gt;</ph><ph id="ph90">
 &lt;td style="border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;"&gt;</ph>ClusterRoleCollection<ph id="ph91">&lt;/td&gt;</ph><ph id="ph92">
 &lt;td style="border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;"&gt;</ph>Specifies the nodes on which the customization script is run.</source>
          <target state="new"><ph id="ph70">&lt;table style="border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse;"&gt;</ph><ph id="ph71">
 &lt;tr&gt;</ph><ph id="ph72">
 &lt;th style="border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; width:90px; padding-left:5px; padding-right:5px;"&gt;</ph>Parameter<ph id="ph73">&lt;/th&gt;</ph><ph id="ph74">
 &lt;th style="border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; width:550px; padding-left:5px; padding-right:5px;"&gt;</ph>Definition<ph id="ph75">&lt;/th&gt;</ph><ph id="ph76">&lt;/tr&gt;</ph><ph id="ph77">
 &lt;tr&gt;</ph><ph id="ph78">
 &lt;td style="border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;"&gt;</ph>Config<ph id="ph79">&lt;/td&gt;</ph><ph id="ph80">
 &lt;td style="border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px; padding-right:5px;"&gt;</ph>The configuration object to which script action information is added.<ph id="ph81">&lt;/td&gt;</ph><ph id="ph82">&lt;/tr&gt;</ph><ph id="ph83">
 &lt;tr&gt;</ph><ph id="ph84">
 &lt;td style="border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;"&gt;</ph>Name<ph id="ph85">&lt;/td&gt;</ph><ph id="ph86">
 &lt;td style="border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;"&gt;</ph>Name of the script action.<ph id="ph87">&lt;/td&gt;</ph><ph id="ph88">&lt;/tr&gt;</ph><ph id="ph89">
 &lt;tr&gt;</ph><ph id="ph90">
 &lt;td style="border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;"&gt;</ph>ClusterRoleCollection<ph id="ph91">&lt;/td&gt;</ph><ph id="ph92">
 &lt;td style="border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;"&gt;</ph>Specifies the nodes on which the customization script is run.</target>
        </trans-unit>
        <trans-unit id="227" translate="yes" xml:space="preserve">
          <source>The valid values are HeadNode (to install on the head node) or DataNode (to install on all the data nodes).</source>
          <target state="new">The valid values are HeadNode (to install on the head node) or DataNode (to install on all the data nodes).</target>
        </trans-unit>
        <trans-unit id="228" translate="yes" xml:space="preserve">
          <source>You can use either or both values.<ph id="ph93">&lt;/td&gt;</ph><ph id="ph94">&lt;/tr&gt;</ph><ph id="ph95">
 &lt;tr&gt;</ph><ph id="ph96">
 &lt;td style="border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;"&gt;</ph>Uri<ph id="ph97">&lt;/td&gt;</ph><ph id="ph98">
 &lt;td style="border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;"&gt;</ph>Specifies the URI to the script that is executed.<ph id="ph99">&lt;/td&gt;</ph><ph id="ph100">&lt;/tr&gt;</ph><ph id="ph101">
 &lt;tr&gt;</ph><ph id="ph102">
 &lt;td style="border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;"&gt;</ph>Parameters<ph id="ph103">&lt;/td&gt;</ph><ph id="ph104">
 &lt;td style="border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;"&gt;</ph>Parameters required by the script.</source>
          <target state="new">You can use either or both values.<ph id="ph93">&lt;/td&gt;</ph><ph id="ph94">&lt;/tr&gt;</ph><ph id="ph95">
 &lt;tr&gt;</ph><ph id="ph96">
 &lt;td style="border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;"&gt;</ph>Uri<ph id="ph97">&lt;/td&gt;</ph><ph id="ph98">
 &lt;td style="border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;"&gt;</ph>Specifies the URI to the script that is executed.<ph id="ph99">&lt;/td&gt;</ph><ph id="ph100">&lt;/tr&gt;</ph><ph id="ph101">
 &lt;tr&gt;</ph><ph id="ph102">
 &lt;td style="border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;"&gt;</ph>Parameters<ph id="ph103">&lt;/td&gt;</ph><ph id="ph104">
 &lt;td style="border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;"&gt;</ph>Parameters required by the script.</target>
        </trans-unit>
        <trans-unit id="229" translate="yes" xml:space="preserve">
          <source>The sample script used in this topic does not require any parameters, and hence you do not see this parameter in the snippet above.</source>
          <target state="new">The sample script used in this topic does not require any parameters, and hence you do not see this parameter in the snippet above.</target>
        </trans-unit>
        <trans-unit id="230" translate="yes" xml:space="preserve">
          <source><ph id="ph105">&lt;/td&gt;</ph><ph id="ph106">&lt;/tr&gt;</ph><ph id="ph107">
 &lt;/table&gt;</ph></source>
          <target state="new"><ph id="ph105">&lt;/td&gt;</ph><ph id="ph106">&lt;/tr&gt;</ph><ph id="ph107">
 &lt;/table&gt;</ph></target>
        </trans-unit>
        <trans-unit id="231" translate="yes" xml:space="preserve">
          <source>Finally, start creating a customized cluster with Spark installed.</source>
          <target state="new">Finally, start creating a customized cluster with Spark installed.</target>
        </trans-unit>
        <trans-unit id="232" translate="yes" xml:space="preserve">
          <source>When prompted, enter the credentials for the cluster.</source>
          <target state="new">When prompted, enter the credentials for the cluster.</target>
        </trans-unit>
        <trans-unit id="233" translate="yes" xml:space="preserve">
          <source>It can take several minutes before the cluster is created.</source>
          <target state="new">It can take several minutes before the cluster is created.</target>
        </trans-unit>
        <trans-unit id="234" translate="yes" xml:space="preserve">
          <source>Install Spark using PowerShell</source>
          <target state="new">Install Spark using PowerShell</target>
        </trans-unit>
        <trans-unit id="235" translate="yes" xml:space="preserve">
          <source>See <bpt id="p42">[</bpt>Customize HDInsight clusters using Script Action<ept id="p42">](hdinsight-hadoop-customize-cluster.md#call_scripts_using_powershell)</ept>.</source>
          <target state="new">See <bpt id="p42">[</bpt>Customize HDInsight clusters using Script Action<ept id="p42">](hdinsight-hadoop-customize-cluster.md#call_scripts_using_powershell)</ept>.</target>
        </trans-unit>
        <trans-unit id="236" translate="yes" xml:space="preserve">
          <source>Install Spark using .NET SDK</source>
          <target state="new">Install Spark using .NET SDK</target>
        </trans-unit>
        <trans-unit id="237" translate="yes" xml:space="preserve">
          <source>See <bpt id="p43">[</bpt>Customize HDInsight clusters using Script Action<ept id="p43">](hdinsight-hadoop-customize-cluster.md#call_scripts_using_azure_powershell)</ept>.</source>
          <target state="new">See <bpt id="p43">[</bpt>Customize HDInsight clusters using Script Action<ept id="p43">](hdinsight-hadoop-customize-cluster.md#call_scripts_using_azure_powershell)</ept>.</target>
        </trans-unit>
        <trans-unit id="238" translate="yes" xml:space="preserve">
          <source>See also</source>
          <target state="new">See also</target>
        </trans-unit>
        <trans-unit id="239" translate="yes" xml:space="preserve">
          <source><bpt id="p44">[</bpt>Install Spark on Linux-based HDInsight clusters<ept id="p44">](hdinsight-hadoop-spark-install-linux.md)</ept>: install Spark on Linux based HDInsight clusters.</source>
          <target state="new"><bpt id="p44">[</bpt>Install Spark on Linux-based HDInsight clusters<ept id="p44">](hdinsight-hadoop-spark-install-linux.md)</ept>: install Spark on Linux based HDInsight clusters.</target>
        </trans-unit>
        <trans-unit id="240" translate="yes" xml:space="preserve">
          <source><bpt id="p45">[</bpt>Create Hadoop clusters in HDInsight<ept id="p45">](hdinsight-provision-clusters.md)</ept>: create HDInsight clusters.</source>
          <target state="new"><bpt id="p45">[</bpt>Create Hadoop clusters in HDInsight<ept id="p45">](hdinsight-provision-clusters.md)</ept>: create HDInsight clusters.</target>
        </trans-unit>
        <trans-unit id="241" translate="yes" xml:space="preserve">
          <source><bpt id="p46">[</bpt>Get Started with Apache Spark on HDInsight<ept id="p46">](hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql.md)</ept>: get started with Spark on HDInsight.</source>
          <target state="new"><bpt id="p46">[</bpt>Get Started with Apache Spark on HDInsight<ept id="p46">](hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql.md)</ept>: get started with Spark on HDInsight.</target>
        </trans-unit>
        <trans-unit id="242" translate="yes" xml:space="preserve">
          <source><bpt id="p47">[</bpt>Customize HDInsight cluster using Script Action<ept id="p47">][hdinsight-cluster-customize]</ept>: customize HDInsight clusters using Script Action.</source>
          <target state="new"><bpt id="p47">[</bpt>Customize HDInsight cluster using Script Action<ept id="p47">][hdinsight-cluster-customize]</ept>: customize HDInsight clusters using Script Action.</target>
        </trans-unit>
        <trans-unit id="243" translate="yes" xml:space="preserve">
          <source><bpt id="p48">[</bpt>Develop Script Action scripts for HDInsight<ept id="p48">](hdinsight-hadoop-script-actions.md)</ept>: develop Script Action scripts.</source>
          <target state="new"><bpt id="p48">[</bpt>Develop Script Action scripts for HDInsight<ept id="p48">](hdinsight-hadoop-script-actions.md)</ept>: develop Script Action scripts.</target>
        </trans-unit>
        <trans-unit id="244" translate="yes" xml:space="preserve">
          <source><bpt id="p49">[</bpt>Install R on HDInsight clusters<ept id="p49">][hdinsight-install-r]</ept><ph id="ph108" /> provides instructions on how to use cluster customization to install and use R on HDInsight Hadoop clusters.</source>
          <target state="new"><bpt id="p49">[</bpt>Install R on HDInsight clusters<ept id="p49">][hdinsight-install-r]</ept><ph id="ph108" /> provides instructions on how to use cluster customization to install and use R on HDInsight Hadoop clusters.</target>
        </trans-unit>
        <trans-unit id="245" translate="yes" xml:space="preserve">
          <source>R is an open-source language and environment for statistical computing.</source>
          <target state="new">R is an open-source language and environment for statistical computing.</target>
        </trans-unit>
        <trans-unit id="246" translate="yes" xml:space="preserve">
          <source>It provides hundreds of built-in statistical functions and its own programming language that combines aspects of functional and object-oriented programming.</source>
          <target state="new">It provides hundreds of built-in statistical functions and its own programming language that combines aspects of functional and object-oriented programming.</target>
        </trans-unit>
        <trans-unit id="247" translate="yes" xml:space="preserve">
          <source>It also provides extensive graphical capabilities.</source>
          <target state="new">It also provides extensive graphical capabilities.</target>
        </trans-unit>
        <trans-unit id="248" translate="yes" xml:space="preserve">
          <source><bpt id="p50">[</bpt>Install Giraph on HDInsight clusters<ept id="p50">](hdinsight-hadoop-giraph-install.md)</ept>.</source>
          <target state="new"><bpt id="p50">[</bpt>Install Giraph on HDInsight clusters<ept id="p50">](hdinsight-hadoop-giraph-install.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="249" translate="yes" xml:space="preserve">
          <source>Use cluster customization to install Giraph on HDInsight Hadoop clusters.</source>
          <target state="new">Use cluster customization to install Giraph on HDInsight Hadoop clusters.</target>
        </trans-unit>
        <trans-unit id="250" translate="yes" xml:space="preserve">
          <source>Giraph allows you to perform graph processing by using Hadoop, and can be used with Azure HDInsight.</source>
          <target state="new">Giraph allows you to perform graph processing by using Hadoop, and can be used with Azure HDInsight.</target>
        </trans-unit>
        <trans-unit id="251" translate="yes" xml:space="preserve">
          <source><bpt id="p51">[</bpt>Install Solr on HDInsight clusters<ept id="p51">](hdinsight-hadoop-solr-install.md)</ept>.</source>
          <target state="new"><bpt id="p51">[</bpt>Install Solr on HDInsight clusters<ept id="p51">](hdinsight-hadoop-solr-install.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="252" translate="yes" xml:space="preserve">
          <source>Use cluster customization to install Solr on HDInsight Hadoop clusters.</source>
          <target state="new">Use cluster customization to install Solr on HDInsight Hadoop clusters.</target>
        </trans-unit>
        <trans-unit id="253" translate="yes" xml:space="preserve">
          <source>Solr allows you to perform powerful search operations on data stored.</source>
          <target state="new">Solr allows you to perform powerful search operations on data stored.</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">5f0fe29d62caf56d321dad40774550533cf737e4</xliffext:olfilehash>
  </header>
</xliff>