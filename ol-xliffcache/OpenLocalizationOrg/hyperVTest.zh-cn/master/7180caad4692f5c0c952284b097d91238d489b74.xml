{
  "nodes": [
    {
      "pos": [
        27,
        89
      ],
      "content": "Query data from HDFS-compatible Blob storage | Microsoft Azure"
    },
    {
      "pos": [
        108,
        251
      ],
      "content": "HDInsight uses Azure Blob storage as the big data store for HDFS. Learn how to query data from Blob storage and store results of your analysis.",
      "nodes": [
        {
          "content": "HDInsight uses Azure Blob storage as the big data store for HDFS.",
          "pos": [
            0,
            65
          ]
        },
        {
          "content": "Learn how to query data from Blob storage and store results of your analysis.",
          "pos": [
            66,
            143
          ]
        }
      ]
    },
    {
      "pos": [
        651,
        714
      ],
      "content": "Use HDFS-compatible Azure Blob storage with Hadoop in HDInsight"
    },
    {
      "pos": [
        716,
        867
      ],
      "content": "Learn how to use low-cost Azure Blob storage with HDInsight, create Azure storage account and Blob storage container, and then address the data inside."
    },
    {
      "pos": [
        869,
        1148
      ],
      "content": "Azure Blob storage is a robust, general-purpose storage solution that integrates seamlessly with HDInsight. Through a Hadoop distributed file system (HDFS) interface, the full set of components in HDInsight can operate directly on structured or unstructured data in Blob storage.",
      "nodes": [
        {
          "content": "Azure Blob storage is a robust, general-purpose storage solution that integrates seamlessly with HDInsight.",
          "pos": [
            0,
            107
          ]
        },
        {
          "content": "Through a Hadoop distributed file system (HDFS) interface, the full set of components in HDInsight can operate directly on structured or unstructured data in Blob storage.",
          "pos": [
            108,
            279
          ]
        }
      ]
    },
    {
      "pos": [
        1150,
        1286
      ],
      "content": "Storing data in Blob storage enables you to safely delete the HDInsight clusters that are used for computation without losing user data."
    },
    {
      "pos": [
        1290,
        1816
      ],
      "content": "<ph id=\"ph2\">[AZURE.NOTE]</ph><ph id=\"ph3\"/>  The <bpt id=\"p1\">*</bpt>asv://<ept id=\"p1\">*</ept><ph id=\"ph4\"/> syntax is not supported in HDInsight version 3.0 clusters. This means that any jobs submitted to an HDInsight version 3.0 cluster that explicitly use the <bpt id=\"p2\">*</bpt>asv://<ept id=\"p2\">*</ept><ph id=\"ph5\"/> syntax will fail. The <bpt id=\"p3\">*</bpt>wasb://<ept id=\"p3\">*</ept><ph id=\"ph6\"/> syntax should be used instead. Also, jobs submitted to any HDInsight version 3.0 clusters that are created with an existing metastore that contains explicit references to resources that use the asv:// syntax will fail. These metastores need to be re-created using the wasb:// syntax to address resources.",
      "nodes": [
        {
          "content": "<ph id=\"ph2\">[AZURE.NOTE]</ph><ph id=\"ph3\"/>  The <bpt id=\"p1\">*</bpt>asv://<ept id=\"p1\">*</ept><ph id=\"ph4\"/> syntax is not supported in HDInsight version 3.0 clusters.",
          "pos": [
            0,
            169
          ]
        },
        {
          "content": "This means that any jobs submitted to an HDInsight version 3.0 cluster that explicitly use the <bpt id=\"p2\">*</bpt>asv://<ept id=\"p2\">*</ept><ph id=\"ph5\"/> syntax will fail.",
          "pos": [
            170,
            343
          ]
        },
        {
          "content": "The <bpt id=\"p3\">*</bpt>wasb://<ept id=\"p3\">*</ept><ph id=\"ph6\"/> syntax should be used instead.",
          "pos": [
            344,
            440
          ]
        },
        {
          "content": "Also, jobs submitted to any HDInsight version 3.0 clusters that are created with an existing metastore that contains explicit references to resources that use the asv:// syntax will fail.",
          "pos": [
            441,
            628
          ]
        },
        {
          "content": "These metastores need to be re-created using the wasb:// syntax to address resources.",
          "pos": [
            629,
            714
          ]
        }
      ]
    },
    {
      "pos": [
        1820,
        1866
      ],
      "content": "HDInsight currently only supports block blobs."
    },
    {
      "pos": [
        1870,
        2171
      ],
      "content": "Most HDFS commands (for example, <ph id=\"ph7\">&lt;b&gt;</ph>ls<ph id=\"ph8\">&lt;/b&gt;</ph>, <ph id=\"ph9\">&lt;b&gt;</ph>copyFromLocal<ph id=\"ph10\">&lt;/b&gt;</ph><ph id=\"ph11\"/> and <ph id=\"ph12\">&lt;b&gt;</ph>mkdir<ph id=\"ph13\">&lt;/b&gt;</ph>) still work as expected. Only the commands that are specific to the native HDFS implementation (which is referred to as DFS), such as <ph id=\"ph14\">&lt;b&gt;</ph>fschk<ph id=\"ph15\">&lt;/b&gt;</ph><ph id=\"ph16\"/> and <ph id=\"ph17\">&lt;b&gt;</ph>dfsadmin<ph id=\"ph18\">&lt;/b&gt;</ph>, will show different behavior in Azure Blob storage.",
      "nodes": [
        {
          "content": "Most HDFS commands (for example, <ph id=\"ph7\">&lt;b&gt;</ph>ls<ph id=\"ph8\">&lt;/b&gt;</ph>, <ph id=\"ph9\">&lt;b&gt;</ph>copyFromLocal<ph id=\"ph10\">&lt;/b&gt;</ph><ph id=\"ph11\"/> and <ph id=\"ph12\">&lt;b&gt;</ph>mkdir<ph id=\"ph13\">&lt;/b&gt;</ph>) still work as expected.",
          "pos": [
            0,
            268
          ]
        },
        {
          "content": "Only the commands that are specific to the native HDFS implementation (which is referred to as DFS), such as <ph id=\"ph14\">&lt;b&gt;</ph>fschk<ph id=\"ph15\">&lt;/b&gt;</ph><ph id=\"ph16\"/> and <ph id=\"ph17\">&lt;b&gt;</ph>dfsadmin<ph id=\"ph18\">&lt;/b&gt;</ph>, will show different behavior in Azure Blob storage.",
          "pos": [
            269,
            578
          ]
        }
      ]
    },
    {
      "pos": [
        2173,
        2333
      ],
      "content": "For information about creating an HDInsight cluster, see <bpt id=\"p4\">[</bpt>Get Started with HDInsight<ept id=\"p4\">][hdinsight-get-started]</ept><ph id=\"ph19\"/> or <bpt id=\"p5\">[</bpt>Create HDInsight clusters<ept id=\"p5\">][hdinsight-creation]</ept>."
    },
    {
      "pos": [
        2339,
        2369
      ],
      "content": "HDInsight storage architecture"
    },
    {
      "pos": [
        2370,
        2456
      ],
      "content": "The following diagram provides an abstract view of the HDInsight storage architecture:"
    },
    {
      "pos": [
        2458,
        2657
      ],
      "content": "<ph id=\"ph20\">![</ph>Hadoop clusters use the HDFS API to access and store structured and unstructured data in Blob storage.<ph id=\"ph21\">](./media/hdinsight-hadoop-use-blob-storage/HDI.WASB.Arch.png \"HDInsight Storage Architecture\")</ph>"
    },
    {
      "pos": [
        2659,
        2842
      ],
      "content": "HDInsight provides access to the distributed file system that is locally attached to the compute nodes. This file system can be accessed by using the fully qualified URI, for example:",
      "nodes": [
        {
          "content": "HDInsight provides access to the distributed file system that is locally attached to the compute nodes.",
          "pos": [
            0,
            103
          ]
        },
        {
          "content": "This file system can be accessed by using the fully qualified URI, for example:",
          "pos": [
            104,
            183
          ]
        }
      ]
    },
    {
      "pos": [
        2878,
        2989
      ],
      "content": "In addition, HDInsight provides the ability to access data that is stored in Azure Blob storage. The syntax is:",
      "nodes": [
        {
          "content": "In addition, HDInsight provides the ability to access data that is stored in Azure Blob storage.",
          "pos": [
            0,
            96
          ]
        },
        {
          "content": "The syntax is:",
          "pos": [
            97,
            111
          ]
        }
      ]
    },
    {
      "pos": [
        3066,
        3397
      ],
      "content": "Hadoop supports a notion of the default file system. The default file system implies a default scheme and authority. It can also be used to resolve relative paths. During the HDInsight creation process, an Azure Storage account and a specific Azure Blob storage container from that account is designated as the default file system.",
      "nodes": [
        {
          "content": "Hadoop supports a notion of the default file system.",
          "pos": [
            0,
            52
          ]
        },
        {
          "content": "The default file system implies a default scheme and authority.",
          "pos": [
            53,
            116
          ]
        },
        {
          "content": "It can also be used to resolve relative paths.",
          "pos": [
            117,
            163
          ]
        },
        {
          "content": "During the HDInsight creation process, an Azure Storage account and a specific Azure Blob storage container from that account is designated as the default file system.",
          "pos": [
            164,
            331
          ]
        }
      ]
    },
    {
      "pos": [
        3399,
        3682
      ],
      "content": "In addition to this storage account, you can add additional storage accounts from the same Azure subscription or different Azure subscriptions during the creation process. For instructions about adding additional storage accounts, see <bpt id=\"p6\">[</bpt>Create HDInsight clusters<ept id=\"p6\">][hdinsight-creation]</ept>.",
      "nodes": [
        {
          "content": "In addition to this storage account, you can add additional storage accounts from the same Azure subscription or different Azure subscriptions during the creation process.",
          "pos": [
            0,
            171
          ]
        },
        {
          "content": "For instructions about adding additional storage accounts, see <bpt id=\"p6\">[</bpt>Create HDInsight clusters<ept id=\"p6\">][hdinsight-creation]</ept>.",
          "pos": [
            172,
            321
          ]
        }
      ]
    },
    {
      "pos": [
        3686,
        3894
      ],
      "content": "<bpt id=\"p7\">**</bpt>Containers in the storage accounts that are connected to a cluster:<ept id=\"p7\">**</ept><ph id=\"ph22\"/> Because the account name and key are associated with the cluster during creation, you have full access to the blobs in those containers."
    },
    {
      "pos": [
        3898,
        4054
      ],
      "content": "<bpt id=\"p8\">**</bpt>Public containers or public blobs in storage accounts that are NOT connected to a cluster:<ept id=\"p8\">**</ept><ph id=\"ph23\"/> You have read-only permission to the blobs in the containers."
    },
    {
      "pos": [
        4062,
        4421
      ],
      "leadings": [
        "",
        "        > "
      ],
      "content": "<ph id=\"ph24\">[AZURE.NOTE]</ph>\nPublic containers allow you to get a list of all blobs that are available in that container and get container metadata. Public blobs allow you to access the blobs only if you know the exact URL. For more information, see <ph id=\"ph25\">&lt;a href=\"http://msdn.microsoft.com/library/windowsazure/dd179354.aspx\"&gt;</ph>Restrict access to containers and blobs<ph id=\"ph26\">&lt;/a&gt;</ph>.",
      "nodes": [
        {
          "content": "<ph id=\"ph24\">[AZURE.NOTE]</ph>\nPublic containers allow you to get a list of all blobs that are available in that container and get container metadata.",
          "pos": [
            0,
            151
          ]
        },
        {
          "content": "Public blobs allow you to access the blobs only if you know the exact URL.",
          "pos": [
            152,
            226
          ]
        },
        {
          "content": "For more information, see <ph id=\"ph25\">&lt;a href=\"http://msdn.microsoft.com/library/windowsazure/dd179354.aspx\"&gt;</ph>Restrict access to containers and blobs<ph id=\"ph26\">&lt;/a&gt;</ph>.",
          "pos": [
            227,
            418
          ]
        }
      ]
    },
    {
      "pos": [
        4425,
        4662
      ],
      "content": "<bpt id=\"p9\">**</bpt>Private containers in storage accounts that are NOT connected to a cluster:<ept id=\"p9\">**</ept><ph id=\"ph27\"/> You can't access the blobs in the containers unless you define the storage account when you submit the WebHCat jobs. This is explained later in this article.",
      "nodes": [
        {
          "content": "<bpt id=\"p9\">**</bpt>Private containers in storage accounts that are NOT connected to a cluster:<ept id=\"p9\">**</ept><ph id=\"ph27\"/> You can't access the blobs in the containers unless you define the storage account when you submit the WebHCat jobs.",
          "pos": [
            0,
            249
          ]
        },
        {
          "content": "This is explained later in this article.",
          "pos": [
            250,
            290
          ]
        }
      ]
    },
    {
      "pos": [
        4665,
        5087
      ],
      "content": "The storage accounts that are defined in the creation process and their keys are stored in %HADOOP_HOME%/conf/core-site.xml on the cluster nodes. The default behavior of HDInsight is to use the storage accounts defined in the core-site.xml file. It is not recommended to edit the core-site.xml file because the cluster head node(master) may be reimaged or migrated at any time, and any changes to those files will be lost.",
      "nodes": [
        {
          "content": "The storage accounts that are defined in the creation process and their keys are stored in %HADOOP_HOME%/conf/core-site.xml on the cluster nodes.",
          "pos": [
            0,
            145
          ]
        },
        {
          "content": "The default behavior of HDInsight is to use the storage accounts defined in the core-site.xml file.",
          "pos": [
            146,
            245
          ]
        },
        {
          "content": "It is not recommended to edit the core-site.xml file because the cluster head node(master) may be reimaged or migrated at any time, and any changes to those files will be lost.",
          "pos": [
            246,
            422
          ]
        }
      ]
    },
    {
      "pos": [
        5089,
        5675
      ],
      "content": "Multiple WebHCat jobs, including Hive, MapReduce, Hadoop streaming, and Pig, can carry a description of storage accounts and metadata with them. (This currently works for Pig with storage accounts, but not for metadata.) In the <bpt id=\"p10\">[</bpt>Access blobs using Azure PowerShell<ept id=\"p10\">](#powershell)</ept><ph id=\"ph28\"/> section of this article, there is a sample of this feature. For more information, see <bpt id=\"p11\">[</bpt>Using an HDInsight Cluster with Alternate Storage Accounts and Metastores<ept id=\"p11\">](http://social.technet.microsoft.com/wiki/contents/articles/23256.using-an-hdinsight-cluster-with-alternate-storage-accounts-and-metastores.aspx)</ept>.",
      "nodes": [
        {
          "content": "Multiple WebHCat jobs, including Hive, MapReduce, Hadoop streaming, and Pig, can carry a description of storage accounts and metadata with them.",
          "pos": [
            0,
            144
          ]
        },
        {
          "content": "(This currently works for Pig with storage accounts, but not for metadata.) In the <bpt id=\"p10\">[</bpt>Access blobs using Azure PowerShell<ept id=\"p10\">](#powershell)</ept><ph id=\"ph28\"/> section of this article, there is a sample of this feature.",
          "pos": [
            145,
            393
          ]
        },
        {
          "content": "For more information, see <bpt id=\"p11\">[</bpt>Using an HDInsight Cluster with Alternate Storage Accounts and Metastores<ept id=\"p11\">](http://social.technet.microsoft.com/wiki/contents/articles/23256.using-an-hdinsight-cluster-with-alternate-storage-accounts-and-metastores.aspx)</ept>.",
          "pos": [
            394,
            681
          ]
        }
      ]
    },
    {
      "pos": [
        5677,
        6157
      ],
      "content": "Blob storage can be used for structured and unstructured data. Blob storage containers store data as key/value pairs, and there is no directory hierarchy. However the slash character ( / ) can be used within the key name to make it appear as if a file is stored within a directory structure. For example, a blob's key may be <bpt id=\"p12\">*</bpt>input/log1.txt<ept id=\"p12\">*</ept>. No actual <bpt id=\"p13\">*</bpt>input<ept id=\"p13\">*</ept><ph id=\"ph29\"/> directory exists, but due to the presence of the slash character in the key name, it has the appearance of a file path.",
      "nodes": [
        {
          "content": "Blob storage can be used for structured and unstructured data.",
          "pos": [
            0,
            62
          ]
        },
        {
          "content": "Blob storage containers store data as key/value pairs, and there is no directory hierarchy.",
          "pos": [
            63,
            154
          ]
        },
        {
          "content": "However the slash character ( / ) can be used within the key name to make it appear as if a file is stored within a directory structure.",
          "pos": [
            155,
            291
          ]
        },
        {
          "content": "For example, a blob's key may be <bpt id=\"p12\">*</bpt>input/log1.txt<ept id=\"p12\">*</ept>.",
          "pos": [
            292,
            382
          ]
        },
        {
          "content": "No actual <bpt id=\"p13\">*</bpt>input<ept id=\"p13\">*</ept><ph id=\"ph29\"/> directory exists, but due to the presence of the slash character in the key name, it has the appearance of a file path.",
          "pos": [
            383,
            575
          ]
        }
      ]
    },
    {
      "pos": [
        6159,
        6162
      ],
      "content": "###"
    },
    {
      "pos": [
        6183,
        6207
      ],
      "content": "Benefits of Blob storage"
    },
    {
      "pos": [
        6208,
        6536
      ],
      "content": "The implied performance cost of not co-locating compute clusters and storage resources is mitigated by the way the compute clusters are created close to the storage account resources inside the Azure region, where the high-speed network makes it very efficient for the compute nodes to access the data inside Azure Blob storage."
    },
    {
      "pos": [
        6538,
        6636
      ],
      "content": "There are several benefits associated with storing the data in Azure Blob storage instead of HDFS:"
    },
    {
      "pos": [
        6640,
        7088
      ],
      "content": "<bpt id=\"p14\">**</bpt>Data reuse and sharing:<ept id=\"p14\">**</ept><ph id=\"ph30\"/> The data in HDFS is located inside the compute cluster. Only the applications that have access to the compute cluster can use the data by using HDFS APIs. The data in Azure Blob storage can be accessed either through the HDFS APIs or through the <bpt id=\"p15\">[</bpt>Blob Storage REST APIs<ept id=\"p15\">][blob-storage-restAPI]</ept>. Thus, a larger set of applications (including other HDInsight clusters) and tools can be used to produce and consume the data.",
      "nodes": [
        {
          "content": "<bpt id=\"p14\">**</bpt>Data reuse and sharing:<ept id=\"p14\">**</ept><ph id=\"ph30\"/> The data in HDFS is located inside the compute cluster.",
          "pos": [
            0,
            138
          ]
        },
        {
          "content": "Only the applications that have access to the compute cluster can use the data by using HDFS APIs.",
          "pos": [
            139,
            237
          ]
        },
        {
          "content": "The data in Azure Blob storage can be accessed either through the HDFS APIs or through the <bpt id=\"p15\">[</bpt>Blob Storage REST APIs<ept id=\"p15\">][blob-storage-restAPI]</ept>.",
          "pos": [
            238,
            416
          ]
        },
        {
          "content": "Thus, a larger set of applications (including other HDInsight clusters) and tools can be used to produce and consume the data.",
          "pos": [
            417,
            543
          ]
        }
      ]
    },
    {
      "pos": [
        7091,
        7244
      ],
      "content": "<bpt id=\"p16\">**</bpt>Data archiving:<ept id=\"p16\">**</ept><ph id=\"ph31\"/> Storing data in Azure Blob storage enables the HDInsight clusters used for computation to be safely deleted without losing user data."
    },
    {
      "pos": [
        7247,
        7601
      ],
      "content": "<bpt id=\"p17\">**</bpt>Data storage cost:<ept id=\"p17\">**</ept><ph id=\"ph32\"/> Storing data in DFS for the long term is more costly than storing the data in Azure Blob storage because the cost of a compute cluster is higher than the cost of an Azure Blob storage container. In addition, because the data does not have to be reloaded for every compute cluster generation, you are also saving data loading costs.",
      "nodes": [
        {
          "content": "<bpt id=\"p17\">**</bpt>Data storage cost:<ept id=\"p17\">**</ept><ph id=\"ph32\"/> Storing data in DFS for the long term is more costly than storing the data in Azure Blob storage because the cost of a compute cluster is higher than the cost of an Azure Blob storage container.",
          "pos": [
            0,
            272
          ]
        },
        {
          "content": "In addition, because the data does not have to be reloaded for every compute cluster generation, you are also saving data loading costs.",
          "pos": [
            273,
            409
          ]
        }
      ]
    },
    {
      "pos": [
        7604,
        7922
      ],
      "content": "<bpt id=\"p18\">**</bpt>Elastic scale-out:<ept id=\"p18\">**</ept><ph id=\"ph33\"/> Although HDFS provides you with a scaled-out file system, the scale is determined by the number of nodes that you create for your cluster. Changing the scale can become a more complicated process than relying on the elastic scaling capabilities that you get automatically in Azure  Blob storage.",
      "nodes": [
        {
          "content": "<bpt id=\"p18\">**</bpt>Elastic scale-out:<ept id=\"p18\">**</ept><ph id=\"ph33\"/> Although HDFS provides you with a scaled-out file system, the scale is determined by the number of nodes that you create for your cluster.",
          "pos": [
            0,
            216
          ]
        },
        {
          "content": "Changing the scale can become a more complicated process than relying on the elastic scaling capabilities that you get automatically in Azure  Blob storage.",
          "pos": [
            217,
            373
          ]
        }
      ]
    },
    {
      "pos": [
        7925,
        8306
      ],
      "content": "<bpt id=\"p19\">**</bpt>Geo-replication:<ept id=\"p19\">**</ept><ph id=\"ph34\"/> Your Azure Blob storage containers can be geo-replicated. Although this gives you geographic recovery and data redundancy, a failover to the geo-replicated location severely impacts your performance, and it may incur additional costs. So our recommendation is to choose the geo-replication wisely and only if the value of the data is worth the additional cost.",
      "nodes": [
        {
          "content": "<bpt id=\"p19\">**</bpt>Geo-replication:<ept id=\"p19\">**</ept><ph id=\"ph34\"/> Your Azure Blob storage containers can be geo-replicated.",
          "pos": [
            0,
            133
          ]
        },
        {
          "content": "Although this gives you geographic recovery and data redundancy, a failover to the geo-replicated location severely impacts your performance, and it may incur additional costs.",
          "pos": [
            134,
            310
          ]
        },
        {
          "content": "So our recommendation is to choose the geo-replication wisely and only if the value of the data is worth the additional cost.",
          "pos": [
            311,
            436
          ]
        }
      ]
    },
    {
      "pos": [
        8308,
        8603
      ],
      "content": "Certain MapReduce jobs and packages may create intermediate results that you don't really want to store in Azure Blob storage. In that case, you can elect to store the data in the local HDFS. In fact, HDInsight uses DFS for several of these intermediate results in Hive jobs and other processes.",
      "nodes": [
        {
          "content": "Certain MapReduce jobs and packages may create intermediate results that you don't really want to store in Azure Blob storage.",
          "pos": [
            0,
            126
          ]
        },
        {
          "content": "In that case, you can elect to store the data in the local HDFS.",
          "pos": [
            127,
            191
          ]
        },
        {
          "content": "In fact, HDInsight uses DFS for several of these intermediate results in Hive jobs and other processes.",
          "pos": [
            192,
            295
          ]
        }
      ]
    },
    {
      "pos": [
        8610,
        8632
      ],
      "content": "Create Blob containers"
    },
    {
      "pos": [
        8634,
        9009
      ],
      "content": "To use blobs, you first create an <bpt id=\"p20\">[</bpt>Azure Storage account<ept id=\"p20\">][azure-storage-create]</ept>. As part of this, you specify an Azure region that will store the objects you create using this account. The cluster and the storage account must be hosted in the same region. The Hive metastore SQL Server database and Oozie metastore SQL Server database must also be located in the same region.",
      "nodes": [
        {
          "content": "To use blobs, you first create an <bpt id=\"p20\">[</bpt>Azure Storage account<ept id=\"p20\">][azure-storage-create]</ept>.",
          "pos": [
            0,
            120
          ]
        },
        {
          "content": "As part of this, you specify an Azure region that will store the objects you create using this account.",
          "pos": [
            121,
            224
          ]
        },
        {
          "content": "The cluster and the storage account must be hosted in the same region.",
          "pos": [
            225,
            295
          ]
        },
        {
          "content": "The Hive metastore SQL Server database and Oozie metastore SQL Server database must also be located in the same region.",
          "pos": [
            296,
            415
          ]
        }
      ]
    },
    {
      "pos": [
        9011,
        9249
      ],
      "content": "Wherever it lives, each blob you create belongs to a container in your Azure Storage account. This container may be an existing blob that was created outside of HDInsight, or it may be a container that is created for an HDInsight cluster.",
      "nodes": [
        {
          "content": "Wherever it lives, each blob you create belongs to a container in your Azure Storage account.",
          "pos": [
            0,
            93
          ]
        },
        {
          "content": "This container may be an existing blob that was created outside of HDInsight, or it may be a container that is created for an HDInsight cluster.",
          "pos": [
            94,
            238
          ]
        }
      ]
    },
    {
      "pos": [
        9252,
        10103
      ],
      "content": "The default Blob container stores cluster specific information such as job history and logs. Don't share a default Blob container with multiple HDInsight clusters. This might corrupt job history, and the cluster will misbehave. It is recommended to use a different container for each cluster and put shared data on a linked storage account specified in deployment of all relevant clusters rather than the default storage account. For more information on configuring linked storage accounts, see <bpt id=\"p21\">[</bpt>Create HDInsight clusters<ept id=\"p21\">][hdinsight-creation]</ept>. However you can reuse a default storage container after the original HDInsight cluster has been deleted. For HBase clusters, you can actually retain the HBase table schema and data by create a new HBase cluster using the default blob storage container that is used by an HBase cluster that has been deleted.",
      "nodes": [
        {
          "content": "The default Blob container stores cluster specific information such as job history and logs.",
          "pos": [
            0,
            92
          ]
        },
        {
          "content": "Don't share a default Blob container with multiple HDInsight clusters.",
          "pos": [
            93,
            163
          ]
        },
        {
          "content": "This might corrupt job history, and the cluster will misbehave.",
          "pos": [
            164,
            227
          ]
        },
        {
          "content": "It is recommended to use a different container for each cluster and put shared data on a linked storage account specified in deployment of all relevant clusters rather than the default storage account.",
          "pos": [
            228,
            429
          ]
        },
        {
          "content": "For more information on configuring linked storage accounts, see <bpt id=\"p21\">[</bpt>Create HDInsight clusters<ept id=\"p21\">][hdinsight-creation]</ept>.",
          "pos": [
            430,
            583
          ]
        },
        {
          "content": "However you can reuse a default storage container after the original HDInsight cluster has been deleted.",
          "pos": [
            584,
            688
          ]
        },
        {
          "content": "For HBase clusters, you can actually retain the HBase table schema and data by create a new HBase cluster using the default blob storage container that is used by an HBase cluster that has been deleted.",
          "pos": [
            689,
            891
          ]
        }
      ]
    },
    {
      "pos": [
        10110,
        10132
      ],
      "content": "Using the Azure Portal"
    },
    {
      "pos": [
        10134,
        10274
      ],
      "content": "When creating an HDInsight cluster from the Portal, you have the options to use an existing storage account or create a new storage account:"
    },
    {
      "pos": [
        10276,
        10395
      ],
      "content": "<ph id=\"ph35\">![</ph>hdinsight hadoop creation data source<ph id=\"ph36\">](./media/hdinsight-hadoop-use-blob-storage/hdinsight.provision.data.source.png)</ph>"
    },
    {
      "pos": [
        10400,
        10415
      ],
      "content": "Using Azure CLI"
    },
    {
      "pos": [
        10417,
        10565
      ],
      "content": "If you have <bpt id=\"p22\">[</bpt>installed and configured the Azure CLI<ept id=\"p22\">](../xplat-cli-install.md)</ept>, the following command can be used to a storage account and container."
    },
    {
      "pos": [
        10635,
        10883
      ],
      "content": "<ph id=\"ph37\">[AZURE.NOTE]</ph><ph id=\"ph38\"/> The <ph id=\"ph39\">`--type`</ph><ph id=\"ph40\"/> parameter indicates how the storage account will be replicated. For more information, see <bpt id=\"p23\">[</bpt>Azure Storage Replication<ept id=\"p23\">](../storage/storage-redundancy.md)</ept>. Don't use ZRS as ZRS doesn't support page blob, file, table or queue.",
      "nodes": [
        {
          "content": "<ph id=\"ph37\">[AZURE.NOTE]</ph><ph id=\"ph38\"/> The <ph id=\"ph39\">`--type`</ph><ph id=\"ph40\"/> parameter indicates how the storage account will be replicated.",
          "pos": [
            0,
            157
          ]
        },
        {
          "content": "For more information, see <bpt id=\"p23\">[</bpt>Azure Storage Replication<ept id=\"p23\">](../storage/storage-redundancy.md)</ept>.",
          "pos": [
            158,
            286
          ]
        },
        {
          "content": "Don't use ZRS as ZRS doesn't support page blob, file, table or queue.",
          "pos": [
            287,
            356
          ]
        }
      ]
    },
    {
      "pos": [
        10885,
        11090
      ],
      "content": "You will be prompted to specify the geographic region that the storage account will be located in. You should create the storage account in the same region that you plan on creating your HDInsight cluster.",
      "nodes": [
        {
          "content": "You will be prompted to specify the geographic region that the storage account will be located in.",
          "pos": [
            0,
            98
          ]
        },
        {
          "content": "You should create the storage account in the same region that you plan on creating your HDInsight cluster.",
          "pos": [
            99,
            205
          ]
        }
      ]
    },
    {
      "pos": [
        11092,
        11192
      ],
      "content": "Once the storage account is created, use the following command to retrieve the storage account keys:"
    },
    {
      "pos": [
        11252,
        11301
      ],
      "content": "To create a container, use the following command:"
    },
    {
      "pos": [
        11429,
        11451
      ],
      "content": "Using Azure PowerShell"
    },
    {
      "pos": [
        11453,
        11626
      ],
      "content": "If you <bpt id=\"p24\">[</bpt>installed and configured Azure PowerShell<ept id=\"p24\">][powershell-install]</ept>, you can use the following from the Azure PowerShell prompt to create a storage account and container:"
    },
    {
      "pos": [
        12653,
        12682
      ],
      "content": "Address files in Blob storage"
    },
    {
      "pos": [
        12684,
        12753
      ],
      "content": "The URI scheme for accessing files in Blob storage from HDInsight is:"
    },
    {
      "pos": [
        12850,
        13010
      ],
      "content": "<ph id=\"ph41\">[AZURE.NOTE]</ph><ph id=\"ph42\"/> The syntax for addressing the files on the storage emulator (running on HDInsight emulator) is <ph id=\"ph43\">&lt;i&gt;</ph>wasb://&amp;lt;ContainerName&amp;gt;@storageemulator<ph id=\"ph44\">&lt;/i&gt;</ph>."
    },
    {
      "pos": [
        13014,
        13238
      ],
      "content": "The URI scheme provides unencrypted access (with the <bpt id=\"p25\">*</bpt>wasb:<ept id=\"p25\">*</ept><ph id=\"ph45\"/> prefix) and SSL encrypted access (with <bpt id=\"p26\">*</bpt>wasbs<ept id=\"p26\">*</ept>). We recommend using <bpt id=\"p27\">*</bpt>wasbs<ept id=\"p27\">*</ept><ph id=\"ph46\"/> wherever possible, even when accessing data that lives inside the same region in Azure.",
      "nodes": [
        {
          "content": "The URI scheme provides unencrypted access (with the <bpt id=\"p25\">*</bpt>wasb:<ept id=\"p25\">*</ept><ph id=\"ph45\"/> prefix) and SSL encrypted access (with <bpt id=\"p26\">*</bpt>wasbs<ept id=\"p26\">*</ept>).",
          "pos": [
            0,
            204
          ]
        },
        {
          "content": "We recommend using <bpt id=\"p27\">*</bpt>wasbs<ept id=\"p27\">*</ept><ph id=\"ph46\"/> wherever possible, even when accessing data that lives inside the same region in Azure.",
          "pos": [
            205,
            374
          ]
        }
      ]
    },
    {
      "pos": [
        13240,
        13337
      ],
      "content": "The &amp;lt;BlobStorageContainerName&amp;gt; identifies the name of the container in Azure Blob storage.\n",
      "nodes": [
        {
          "content": "The &amp;lt;BlobStorageContainerName&amp;gt; identifies the name of the container in Azure Blob storage.",
          "pos": [
            0,
            104
          ]
        },
        {
          "content": "\n",
          "pos": [
            104,
            105
          ]
        }
      ]
    },
    {
      "pos": [
        13337,
        13460
      ],
      "content": "The &amp;lt;StorageAccountName&amp;gt; identifies the Azure Storage account name. A fully qualified domain name (FQDN) is required.",
      "nodes": [
        {
          "content": "The &amp;lt;StorageAccountName&amp;gt; identifies the Azure Storage account name.",
          "pos": [
            0,
            81
          ]
        },
        {
          "content": "A fully qualified domain name (FQDN) is required.",
          "pos": [
            82,
            131
          ]
        }
      ]
    },
    {
      "pos": [
        13462,
        13819
      ],
      "content": "If neither &amp;lt;BlobStorageContainerName&amp;gt; nor &amp;lt;StorageAccountName&amp;gt; has been specified, the default file system is used. For the files on the default file system, you can use a relative path or an absolute path. For example, the <bpt id=\"p28\">*</bpt>hadoop-mapreduce-examples.jar<ept id=\"p28\">*</ept><ph id=\"ph47\"/> file that comes with HDInsight clusters can be referred to by using one of the following:",
      "nodes": [
        {
          "content": "If neither &amp;lt;BlobStorageContainerName&amp;gt; nor &amp;lt;StorageAccountName&amp;gt; has been specified, the default file system is used.",
          "pos": [
            0,
            143
          ]
        },
        {
          "content": "For the files on the default file system, you can use a relative path or an absolute path.",
          "pos": [
            144,
            234
          ]
        },
        {
          "content": "For example, the <bpt id=\"p28\">*</bpt>hadoop-mapreduce-examples.jar<ept id=\"p28\">*</ept><ph id=\"ph47\"/> file that comes with HDInsight clusters can be referred to by using one of the following:",
          "pos": [
            235,
            428
          ]
        }
      ]
    },
    {
      "pos": [
        14025,
        14125
      ],
      "content": "<ph id=\"ph48\">[AZURE.NOTE]</ph><ph id=\"ph49\"/> The file name is <ph id=\"ph50\">&lt;i&gt;</ph>hadoop-examples.jar<ph id=\"ph51\">&lt;/i&gt;</ph><ph id=\"ph52\"/> in HDInsight versions 2.1 and 1.6 clusters."
    },
    {
      "pos": [
        14128,
        14448
      ],
      "content": "The &amp;lt;path&amp;gt; is the file or directory HDFS path name. Because containers in Azure Blob storage are simply key-value stores, there is no true hierarchical file system. A slash character ( / ) inside a blob key is interpreted as a directory separator. For example, the blob name for <bpt id=\"p29\">*</bpt>hadoop-mapreduce-examples.jar<ept id=\"p29\">*</ept><ph id=\"ph53\"/> is:",
      "nodes": [
        {
          "content": "The &amp;lt;path&amp;gt; is the file or directory HDFS path name.",
          "pos": [
            0,
            65
          ]
        },
        {
          "content": "Because containers in Azure Blob storage are simply key-value stores, there is no true hierarchical file system.",
          "pos": [
            66,
            178
          ]
        },
        {
          "content": "A slash character ( / ) inside a blob key is interpreted as a directory separator.",
          "pos": [
            179,
            261
          ]
        },
        {
          "content": "For example, the blob name for <bpt id=\"p29\">*</bpt>hadoop-mapreduce-examples.jar<ept id=\"p29\">*</ept><ph id=\"ph53\"/> is:",
          "pos": [
            262,
            383
          ]
        }
      ]
    },
    {
      "pos": [
        14500,
        14700
      ],
      "content": "<ph id=\"ph54\">[AZURE.NOTE]</ph><ph id=\"ph55\"/> When working with blobs outside of HDInsight, most utilities do not recognize the WASB format and instead expect a basic path format, such as <ph id=\"ph56\">`example/jars/hadoop-mapreduce-examples.jar`</ph>."
    },
    {
      "pos": [
        14705,
        14733
      ],
      "content": "Access blobs using Azure CLI"
    },
    {
      "pos": [
        14735,
        14795
      ],
      "content": "Use the following command to list the blob-related commands:"
    },
    {
      "pos": [
        14821,
        14868
      ],
      "content": "<bpt id=\"p30\">**</bpt>Example of using Azure CLI to upload a file<ept id=\"p30\">**</ept>"
    },
    {
      "pos": [
        15015,
        15064
      ],
      "content": "<bpt id=\"p31\">**</bpt>Example of using Azure CLI to download a file<ept id=\"p31\">**</ept>"
    },
    {
      "pos": [
        15218,
        15265
      ],
      "content": "<bpt id=\"p32\">**</bpt>Example of using Azure CLI to delete a file<ept id=\"p32\">**</ept>"
    },
    {
      "pos": [
        15395,
        15439
      ],
      "content": "<bpt id=\"p33\">**</bpt>Example of using Azure CLI to list files<ept id=\"p33\">**</ept>"
    },
    {
      "pos": [
        15577,
        15612
      ],
      "content": "Access blobs using Azure PowerShell"
    },
    {
      "pos": [
        15616,
        15886
      ],
      "content": "<ph id=\"ph57\">[AZURE.NOTE]</ph><ph id=\"ph58\"/> The commands in this section provide a basic example of using PowerShell to access data stored in blobs. For a more full-featured example that is customized for working with HDInsight, see the <bpt id=\"p34\">[</bpt>HDInsight Tools<ept id=\"p34\">](https://github.com/Blackmist/hdinsight-tools)</ept>.",
      "nodes": [
        {
          "content": "<ph id=\"ph57\">[AZURE.NOTE]</ph><ph id=\"ph58\"/> The commands in this section provide a basic example of using PowerShell to access data stored in blobs.",
          "pos": [
            0,
            151
          ]
        },
        {
          "content": "For a more full-featured example that is customized for working with HDInsight, see the <bpt id=\"p34\">[</bpt>HDInsight Tools<ept id=\"p34\">](https://github.com/Blackmist/hdinsight-tools)</ept>.",
          "pos": [
            152,
            344
          ]
        }
      ]
    },
    {
      "pos": [
        15888,
        15947
      ],
      "content": "Use the following command to list the blob-related cmdlets:"
    },
    {
      "pos": [
        15973,
        16049
      ],
      "content": "<ph id=\"ph59\">![</ph>List of blob-related PowerShell cmdlets.<ph id=\"ph60\">][img-hdi-powershell-blobcommands]</ph>"
    },
    {
      "pos": [
        16054,
        16066
      ],
      "content": "Upload files"
    },
    {
      "pos": [
        16068,
        16122
      ],
      "content": "See <bpt id=\"p35\">[</bpt>Upload data to HDInsight<ept id=\"p35\">][hdinsight-upload-data]</ept>."
    },
    {
      "pos": [
        16127,
        16141
      ],
      "content": "Download files"
    },
    {
      "pos": [
        16143,
        16302
      ],
      "content": "The following scrip downloads a block blob to the current folder. Before running the script, change the directory to a folder where you have write permissions.",
      "nodes": [
        {
          "content": "The following scrip downloads a block blob to the current folder.",
          "pos": [
            0,
            65
          ]
        },
        {
          "content": "Before running the script, change the directory to a folder where you have write permissions.",
          "pos": [
            66,
            159
          ]
        }
      ]
    },
    {
      "pos": [
        17473,
        17560
      ],
      "content": "Providing the resource group name and the cluster name, you can use the following code:"
    },
    {
      "pos": [
        18469,
        18481
      ],
      "content": "Delete files"
    },
    {
      "pos": [
        18579,
        18589
      ],
      "content": "List files"
    },
    {
      "pos": [
        18695,
        18746
      ],
      "content": "Run Hive queries using an undefined storage account"
    },
    {
      "pos": [
        18748,
        18858
      ],
      "content": "This example shows how to list a folder from storage account that is not defined during the creating process.\n",
      "nodes": [
        {
          "content": "This example shows how to list a folder from storage account that is not defined during the creating process.",
          "pos": [
            0,
            109
          ]
        },
        {
          "content": "\n",
          "pos": [
            109,
            110
          ]
        }
      ]
    },
    {
      "pos": [
        18862,
        18878
      ],
      "content": "$clusterName = \""
    },
    {
      "pos": [
        18900,
        18901
      ],
      "content": "\""
    },
    {
      "pos": [
        19488,
        19498
      ],
      "content": "Next steps"
    },
    {
      "pos": [
        19500,
        19875
      ],
      "content": "In this article, you learned how to use HDFS-compatible Azure Blob storage with HDInsight, and you learned that Azure Blob storage is a fundamental component of HDInsight. This allows you to build scalable, long-term, archiving data acquisition solutions with Azure Blob storage and use HDInsight to unlock the information inside the stored  structured and unstructured data.",
      "nodes": [
        {
          "content": "In this article, you learned how to use HDFS-compatible Azure Blob storage with HDInsight, and you learned that Azure Blob storage is a fundamental component of HDInsight.",
          "pos": [
            0,
            171
          ]
        },
        {
          "content": "This allows you to build scalable, long-term, archiving data acquisition solutions with Azure Blob storage and use HDInsight to unlock the information inside the stored  structured and unstructured data.",
          "pos": [
            172,
            375
          ]
        }
      ]
    },
    {
      "pos": [
        19877,
        19903
      ],
      "content": "For more information, see:"
    },
    {
      "pos": [
        19907,
        19964
      ],
      "content": "<bpt id=\"p36\">[</bpt>Get Started with Azure HDInsight<ept id=\"p36\">][hdinsight-get-started]</ept>"
    },
    {
      "pos": [
        19967,
        20016
      ],
      "content": "<bpt id=\"p37\">[</bpt>Upload data to HDInsight<ept id=\"p37\">][hdinsight-upload-data]</ept>"
    },
    {
      "pos": [
        20019,
        20064
      ],
      "content": "<bpt id=\"p38\">[</bpt>Use Hive with HDInsight<ept id=\"p38\">][hdinsight-use-hive]</ept>"
    },
    {
      "pos": [
        20067,
        20110
      ],
      "content": "<bpt id=\"p39\">[</bpt>Use Pig with HDInsight<ept id=\"p39\">][hdinsight-use-pig]</ept>"
    },
    {
      "pos": [
        20113,
        20218
      ],
      "content": "<bpt id=\"p40\">[</bpt>Use Azure Storage Shared Access Signatures to restrict access to data with HDInsight<ept id=\"p40\">][hdinsight-use-sas]</ept>"
    }
  ],
  "content": "<properties\n    pageTitle=\"Query data from HDFS-compatible Blob storage | Microsoft Azure\"\n    description=\"HDInsight uses Azure Blob storage as the big data store for HDFS. Learn how to query data from Blob storage and store results of your analysis.\"\n    keywords=\"blob storage,hdfs,structured data,unstructured data\"\n    services=\"hdinsight,storage\"\n    documentationCenter=\"\"\n    tags=\"azure-portal\"\n    authors=\"mumian\"\n    manager=\"paulettm\"\n    editor=\"cgronlun\"/>\n\n<tags\n    ms.service=\"hdinsight\"\n    ms.workload=\"big-data\"\n    ms.tgt_pltfrm=\"na\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\"\n    ms.date=\"01/29/2016\"\n    ms.author=\"jgao\"/>\n\n\n# Use HDFS-compatible Azure Blob storage with Hadoop in HDInsight\n\nLearn how to use low-cost Azure Blob storage with HDInsight, create Azure storage account and Blob storage container, and then address the data inside.\n\nAzure Blob storage is a robust, general-purpose storage solution that integrates seamlessly with HDInsight. Through a Hadoop distributed file system (HDFS) interface, the full set of components in HDInsight can operate directly on structured or unstructured data in Blob storage.\n\nStoring data in Blob storage enables you to safely delete the HDInsight clusters that are used for computation without losing user data.\n\n> [AZURE.NOTE]  The *asv://* syntax is not supported in HDInsight version 3.0 clusters. This means that any jobs submitted to an HDInsight version 3.0 cluster that explicitly use the *asv://* syntax will fail. The *wasb://* syntax should be used instead. Also, jobs submitted to any HDInsight version 3.0 clusters that are created with an existing metastore that contains explicit references to resources that use the asv:// syntax will fail. These metastores need to be re-created using the wasb:// syntax to address resources.\n\n> HDInsight currently only supports block blobs.\n\n> Most HDFS commands (for example, <b>ls</b>, <b>copyFromLocal</b> and <b>mkdir</b>) still work as expected. Only the commands that are specific to the native HDFS implementation (which is referred to as DFS), such as <b>fschk</b> and <b>dfsadmin</b>, will show different behavior in Azure Blob storage.\n\nFor information about creating an HDInsight cluster, see [Get Started with HDInsight][hdinsight-get-started] or [Create HDInsight clusters][hdinsight-creation].\n\n\n## HDInsight storage architecture\nThe following diagram provides an abstract view of the HDInsight storage architecture:\n\n![Hadoop clusters use the HDFS API to access and store structured and unstructured data in Blob storage.](./media/hdinsight-hadoop-use-blob-storage/HDI.WASB.Arch.png \"HDInsight Storage Architecture\")\n\nHDInsight provides access to the distributed file system that is locally attached to the compute nodes. This file system can be accessed by using the fully qualified URI, for example:\n\n    hdfs://<namenodehost>/<path>\n\nIn addition, HDInsight provides the ability to access data that is stored in Azure Blob storage. The syntax is:\n\n    wasb[s]://<containername>@<accountname>.blob.core.windows.net/<path>\n\n\nHadoop supports a notion of the default file system. The default file system implies a default scheme and authority. It can also be used to resolve relative paths. During the HDInsight creation process, an Azure Storage account and a specific Azure Blob storage container from that account is designated as the default file system.\n\nIn addition to this storage account, you can add additional storage accounts from the same Azure subscription or different Azure subscriptions during the creation process. For instructions about adding additional storage accounts, see [Create HDInsight clusters][hdinsight-creation].\n\n- **Containers in the storage accounts that are connected to a cluster:** Because the account name and key are associated with the cluster during creation, you have full access to the blobs in those containers.\n\n- **Public containers or public blobs in storage accounts that are NOT connected to a cluster:** You have read-only permission to the blobs in the containers.\n\n    > [AZURE.NOTE]\n        > Public containers allow you to get a list of all blobs that are available in that container and get container metadata. Public blobs allow you to access the blobs only if you know the exact URL. For more information, see <a href=\"http://msdn.microsoft.com/library/windowsazure/dd179354.aspx\">Restrict access to containers and blobs</a>.\n\n- **Private containers in storage accounts that are NOT connected to a cluster:** You can't access the blobs in the containers unless you define the storage account when you submit the WebHCat jobs. This is explained later in this article.\n\n\nThe storage accounts that are defined in the creation process and their keys are stored in %HADOOP_HOME%/conf/core-site.xml on the cluster nodes. The default behavior of HDInsight is to use the storage accounts defined in the core-site.xml file. It is not recommended to edit the core-site.xml file because the cluster head node(master) may be reimaged or migrated at any time, and any changes to those files will be lost.\n\nMultiple WebHCat jobs, including Hive, MapReduce, Hadoop streaming, and Pig, can carry a description of storage accounts and metadata with them. (This currently works for Pig with storage accounts, but not for metadata.) In the [Access blobs using Azure PowerShell](#powershell) section of this article, there is a sample of this feature. For more information, see [Using an HDInsight Cluster with Alternate Storage Accounts and Metastores](http://social.technet.microsoft.com/wiki/contents/articles/23256.using-an-hdinsight-cluster-with-alternate-storage-accounts-and-metastores.aspx).\n\nBlob storage can be used for structured and unstructured data. Blob storage containers store data as key/value pairs, and there is no directory hierarchy. However the slash character ( / ) can be used within the key name to make it appear as if a file is stored within a directory structure. For example, a blob's key may be *input/log1.txt*. No actual *input* directory exists, but due to the presence of the slash character in the key name, it has the appearance of a file path.\n\n###<a id=\"benefits\"></a>Benefits of Blob storage\nThe implied performance cost of not co-locating compute clusters and storage resources is mitigated by the way the compute clusters are created close to the storage account resources inside the Azure region, where the high-speed network makes it very efficient for the compute nodes to access the data inside Azure Blob storage.\n\nThere are several benefits associated with storing the data in Azure Blob storage instead of HDFS:\n\n* **Data reuse and sharing:** The data in HDFS is located inside the compute cluster. Only the applications that have access to the compute cluster can use the data by using HDFS APIs. The data in Azure Blob storage can be accessed either through the HDFS APIs or through the [Blob Storage REST APIs][blob-storage-restAPI]. Thus, a larger set of applications (including other HDInsight clusters) and tools can be used to produce and consume the data.\n* **Data archiving:** Storing data in Azure Blob storage enables the HDInsight clusters used for computation to be safely deleted without losing user data.\n* **Data storage cost:** Storing data in DFS for the long term is more costly than storing the data in Azure Blob storage because the cost of a compute cluster is higher than the cost of an Azure Blob storage container. In addition, because the data does not have to be reloaded for every compute cluster generation, you are also saving data loading costs.\n* **Elastic scale-out:** Although HDFS provides you with a scaled-out file system, the scale is determined by the number of nodes that you create for your cluster. Changing the scale can become a more complicated process than relying on the elastic scaling capabilities that you get automatically in Azure  Blob storage.\n* **Geo-replication:** Your Azure Blob storage containers can be geo-replicated. Although this gives you geographic recovery and data redundancy, a failover to the geo-replicated location severely impacts your performance, and it may incur additional costs. So our recommendation is to choose the geo-replication wisely and only if the value of the data is worth the additional cost.\n\nCertain MapReduce jobs and packages may create intermediate results that you don't really want to store in Azure Blob storage. In that case, you can elect to store the data in the local HDFS. In fact, HDInsight uses DFS for several of these intermediate results in Hive jobs and other processes.\n\n\n\n## Create Blob containers\n\nTo use blobs, you first create an [Azure Storage account][azure-storage-create]. As part of this, you specify an Azure region that will store the objects you create using this account. The cluster and the storage account must be hosted in the same region. The Hive metastore SQL Server database and Oozie metastore SQL Server database must also be located in the same region.\n\nWherever it lives, each blob you create belongs to a container in your Azure Storage account. This container may be an existing blob that was created outside of HDInsight, or it may be a container that is created for an HDInsight cluster.\n\n\nThe default Blob container stores cluster specific information such as job history and logs. Don't share a default Blob container with multiple HDInsight clusters. This might corrupt job history, and the cluster will misbehave. It is recommended to use a different container for each cluster and put shared data on a linked storage account specified in deployment of all relevant clusters rather than the default storage account. For more information on configuring linked storage accounts, see [Create HDInsight clusters][hdinsight-creation]. However you can reuse a default storage container after the original HDInsight cluster has been deleted. For HBase clusters, you can actually retain the HBase table schema and data by create a new HBase cluster using the default blob storage container that is used by an HBase cluster that has been deleted.\n\n\n### Using the Azure Portal\n\nWhen creating an HDInsight cluster from the Portal, you have the options to use an existing storage account or create a new storage account:\n\n![hdinsight hadoop creation data source](./media/hdinsight-hadoop-use-blob-storage/hdinsight.provision.data.source.png)\n\n###Using Azure CLI\n\nIf you have [installed and configured the Azure CLI](../xplat-cli-install.md), the following command can be used to a storage account and container.\n\n    azure storage account create <storageaccountname> --type LRS\n\n> [AZURE.NOTE] The `--type` parameter indicates how the storage account will be replicated. For more information, see [Azure Storage Replication](../storage/storage-redundancy.md). Don't use ZRS as ZRS doesn't support page blob, file, table or queue.\n\nYou will be prompted to specify the geographic region that the storage account will be located in. You should create the storage account in the same region that you plan on creating your HDInsight cluster.\n\nOnce the storage account is created, use the following command to retrieve the storage account keys:\n\n    azure storage account keys list <storageaccountname>\n\nTo create a container, use the following command:\n\n    azure storage container create <containername> --account-name <storageaccountname> --account-key <storageaccountkey>\n\n### Using Azure PowerShell\n\nIf you [installed and configured Azure PowerShell][powershell-install], you can use the following from the Azure PowerShell prompt to create a storage account and container:\n\n    $SubscriptionID = \"<Your Azure Subscription ID>\"\n    $ResourceGroupName = \"<New Azure Resource Group Name>\"\n    $Location = \"EAST US 2\"\n    \n    $StorageAccountName = \"<New Azure Storage Account Name>\"\n    $containerName = \"<New Azure Blob Container Name>\"\n    \n    Add-AzureRmAccount\n    Select-AzureRmSubscription -SubscriptionId $SubscriptionID\n    \n    # Create resource group\n    New-AzureRmResourceGroup -name $ResourceGroupName -Location $Location\n    \n    # Create default storage account\n    New-AzureRmStorageAccount -ResourceGroupName $ResourceGroupName -Name $StorageAccountName -Location $Location -Type Standard_LRS \n    \n    # Create default blob containers\n    $storageAccountKey = Get-AzureRmStorageAccountKey -ResourceGroupName $resourceGroupName -StorageAccountName $StorageAccountName |  %{ $_.Key1 }\n    $destContext = New-AzureStorageContext -StorageAccountName $storageAccountName -StorageAccountKey $storageAccountKey  \n    New-AzureStorageContainer -Name $containerName -Context $destContext\n\n## Address files in Blob storage\n\nThe URI scheme for accessing files in Blob storage from HDInsight is:\n\n    wasb[s]://<BlobStorageContainerName>@<StorageAccountName>.blob.core.windows.net/<path>\n\n\n> [AZURE.NOTE] The syntax for addressing the files on the storage emulator (running on HDInsight emulator) is <i>wasb://&lt;ContainerName&gt;@storageemulator</i>.\n\n\n\nThe URI scheme provides unencrypted access (with the *wasb:* prefix) and SSL encrypted access (with *wasbs*). We recommend using *wasbs* wherever possible, even when accessing data that lives inside the same region in Azure.\n\nThe &lt;BlobStorageContainerName&gt; identifies the name of the container in Azure Blob storage.\nThe &lt;StorageAccountName&gt; identifies the Azure Storage account name. A fully qualified domain name (FQDN) is required.\n\nIf neither &lt;BlobStorageContainerName&gt; nor &lt;StorageAccountName&gt; has been specified, the default file system is used. For the files on the default file system, you can use a relative path or an absolute path. For example, the *hadoop-mapreduce-examples.jar* file that comes with HDInsight clusters can be referred to by using one of the following:\n\n    wasb://mycontainer@myaccount.blob.core.windows.net/example/jars/hadoop-mapreduce-examples.jar\n    wasb:///example/jars/hadoop-mapreduce-examples.jar\n    /example/jars/hadoop-mapreduce-examples.jar\n\n> [AZURE.NOTE] The file name is <i>hadoop-examples.jar</i> in HDInsight versions 2.1 and 1.6 clusters.\n\n\nThe &lt;path&gt; is the file or directory HDFS path name. Because containers in Azure Blob storage are simply key-value stores, there is no true hierarchical file system. A slash character ( / ) inside a blob key is interpreted as a directory separator. For example, the blob name for *hadoop-mapreduce-examples.jar* is:\n\n    example/jars/hadoop-mapreduce-examples.jar\n\n> [AZURE.NOTE] When working with blobs outside of HDInsight, most utilities do not recognize the WASB format and instead expect a basic path format, such as `example/jars/hadoop-mapreduce-examples.jar`.\n\n## Access blobs using Azure CLI\n\nUse the following command to list the blob-related commands:\n\n    azure storage blob\n\n**Example of using Azure CLI to upload a file**\n\n    azure storage blob upload <sourcefilename> <containername> <blobname> --account-name <storageaccountname> --account-key <storageaccountkey>\n\n**Example of using Azure CLI to download a file**\n\n    azure storage blob download <containername> <blobname> <destinationfilename> --account-name <storageaccountname> --account-key <storageaccountkey>\n\n**Example of using Azure CLI to delete a file**\n\n    azure storage blob delete <containername> <blobname> --account-name <storageaccountname> --account-key <storageaccountkey>\n\n**Example of using Azure CLI to list files**\n\n    azure storage blob list <containername> <blobname|prefix> --account-name <storageaccountname> --account-key <storageaccountkey>\n\n## Access blobs using Azure PowerShell\n\n> [AZURE.NOTE] The commands in this section provide a basic example of using PowerShell to access data stored in blobs. For a more full-featured example that is customized for working with HDInsight, see the [HDInsight Tools](https://github.com/Blackmist/hdinsight-tools).\n\nUse the following command to list the blob-related cmdlets:\n\n    Get-Command *blob*\n\n![List of blob-related PowerShell cmdlets.][img-hdi-powershell-blobcommands]\n\n###Upload files\n\nSee [Upload data to HDInsight][hdinsight-upload-data].\n\n###Download files\n\nThe following scrip downloads a block blob to the current folder. Before running the script, change the directory to a folder where you have write permissions.\n\n    $resourceGroupName = \"<AzureResourceGroupName>\"\n    $storageAccountName = \"<AzureStorageAccountName>\"   # The storage account used for the default file system specified at creation.\n    $containerName = \"<BlobStorageContainerName>\"  # The default file system container has the same name as the cluster.\n    $blob = \"example/data/sample.log\" # The name of the blob to be downloaded.\n    \n    # Use Add-AzureAccount if you haven't connected to your Azure subscription\n    Login-AzureRmAccount \n    Select-AzureRmSubscription -SubscriptionID \"<Your Azure Subscription ID>\"\n    \n    Write-Host \"Create a context object ... \" -ForegroundColor Green\n    $storageAccountKey = Get-AzureRmStorageAccountKey -ResourceGroupName $resourceGroupName -Name $storageAccountName | %{ $_.key1 }\n    $storageContext = New-AzureStorageContext -StorageAccountName $storageAccountName -StorageAccountKey $storageAccountKey  \n    \n    Write-Host \"Download the blob ...\" -ForegroundColor Green\n    Get-AzureStorageBlobContent -Container $ContainerName -Blob $blob -Context $storageContext -Force\n    \n    Write-Host \"List the downloaded file ...\" -ForegroundColor Green\n    cat \"./$blob\"\n\nProviding the resource group name and the cluster name, you can use the following code:\n\n    $resourceGroupName = \"<AzureResourceGroupName>\"\n    $clusterName = \"<HDInsightClusterName>\"\n    $blob = \"example/data/sample.log\" # The name of the blob to be downloaded.\n    \n    $cluster = Get-AzureRmHDInsightCluster -ResourceGroupName $resourceGroupName -ClusterName $clusterName\n    $defaultStorageAccount = $cluster.DefaultStorageAccount -replace '.blob.core.windows.net'\n    $defaultStorageAccountKey = Get-AzureRmStorageAccountKey -ResourceGroupName $resourceGroupName -Name $defaultStorageAccount |  %{ $_.Key1 }\n    $defaultStorageContainer = $cluster.DefaultStorageContainer\n    $storageContext = New-AzureStorageContext -StorageAccountName $defaultStorageAccount -StorageAccountKey $defaultStorageAccountKey \n    \n    Write-Host \"Download the blob ...\" -ForegroundColor Green\n    Get-AzureStorageBlobContent -Container $defaultStorageContainer -Blob $blob -Context $storageContext -Force\n\n###Delete files\n\n\n    Remove-AzureStorageBlob -Container $containerName -Context $storageContext -blob $blob\n\n###List files\n\n    Get-AzureStorageBlob -Container $containerName -Context $storageContext -prefix \"example/data/\"\n\n###Run Hive queries using an undefined storage account\n\nThis example shows how to list a folder from storage account that is not defined during the creating process.\n    $clusterName = \"<HDInsightClusterName>\"\n\n    $undefinedStorageAccount = \"<UnboundedStorageAccountUnderTheSameSubscription>\"\n    $undefinedContainer = \"<UnboundedBlobContainerAssociatedWithTheStorageAccount>\"\n\n    $undefinedStorageKey = Get-AzureStorageKey $undefinedStorageAccount | %{ $_.Primary }\n\n    Use-AzureRmHDInsightCluster $clusterName\n\n    $defines = @{}\n    $defines.Add(\"fs.azure.account.key.$undefinedStorageAccount.blob.core.windows.net\", $undefinedStorageKey)\n\n    Invoke-AzureRmHDInsightHiveJob -Defines $defines -Query \"dfs -ls wasb://$undefinedContainer@$undefinedStorageAccount.blob.core.windows.net/;\"\n\n## Next steps\n\nIn this article, you learned how to use HDFS-compatible Azure Blob storage with HDInsight, and you learned that Azure Blob storage is a fundamental component of HDInsight. This allows you to build scalable, long-term, archiving data acquisition solutions with Azure Blob storage and use HDInsight to unlock the information inside the stored  structured and unstructured data.\n\nFor more information, see:\n\n* [Get Started with Azure HDInsight][hdinsight-get-started]\n* [Upload data to HDInsight][hdinsight-upload-data]\n* [Use Hive with HDInsight][hdinsight-use-hive]\n* [Use Pig with HDInsight][hdinsight-use-pig]\n* [Use Azure Storage Shared Access Signatures to restrict access to data with HDInsight][hdinsight-use-sas]\n\n[hdinsight-use-sas]: hdinsight-storage-sharedaccesssignature-permissions.md\n[powershell-install]: powershell-install-configure.md\n[hdinsight-creation]: hdinsight-provision-clusters.md\n[hdinsight-get-started]: hdinsight-hadoop-tutorial-get-started-windows.md\n[hdinsight-upload-data]: hdinsight-upload-data.md\n[hdinsight-use-hive]: hdinsight-use-hive.md\n[hdinsight-use-pig]: hdinsight-use-pig.md\n\n[blob-storage-restAPI]: http://msdn.microsoft.com/library/windowsazure/dd135733.aspx\n[azure-storage-create]: ../storage-create-storage-account.md\n\n[img-hdi-powershell-blobcommands]: ./media/hdinsight-hadoop-use-blob-storage/HDI.PowerShell.BlobCommands.png\n[img-hdi-quick-create]: ./media/hdinsight-hadoop-use-blob-storage/HDI.QuickCreateCluster.png\n[img-hdi-custom-create-storage-account]: ./media/hdinsight-hadoop-use-blob-storage/HDI.CustomCreateStorageAccount.png  \n"
}