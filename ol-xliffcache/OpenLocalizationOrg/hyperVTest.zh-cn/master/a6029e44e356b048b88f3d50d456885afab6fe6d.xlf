<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" source-language="en-US" target-language="zh-cn">
    <body>
      <group id="main" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Use Azure Data Lake Store with Apache Storm on Azure HDInsight</source>
          <target state="new">Use Azure Data Lake Store with Apache Storm on Azure HDInsight</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Learn how to write data to Azure Data Lake Store from an Apache Storm topology on HDInsight.</source>
          <target state="new">Learn how to write data to Azure Data Lake Store from an Apache Storm topology on HDInsight.</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>This document, and the associated example, demonstrate how the HdfsBolt component can be used to write to Data Lake Store.</source>
          <target state="new">This document, and the associated example, demonstrate how the HdfsBolt component can be used to write to Data Lake Store.</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>Use Azure Data Lake Store with Apache Storm with HDInsight</source>
          <target state="new">Use Azure Data Lake Store with Apache Storm with HDInsight</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>Azure Data Lake Store is an HDFS compatible cloud storage service that provides high throughput, availability, durability, and reliability for your data.</source>
          <target state="new">Azure Data Lake Store is an HDFS compatible cloud storage service that provides high throughput, availability, durability, and reliability for your data.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>In this document, you will learn how to use a Java-based Storm topology to write data to Azure Data Lake Store using the <bpt id="p1">[</bpt>HdfsBolt<ept id="p1">](http://storm.apache.org/javadoc/apidocs/org/apache/storm/hdfs/bolt/HdfsBolt.html)</ept><ph id="ph2" /> component, which is provided as part of Apache Storm.</source>
          <target state="new">In this document, you will learn how to use a Java-based Storm topology to write data to Azure Data Lake Store using the <bpt id="p1">[</bpt>HdfsBolt<ept id="p1">](http://storm.apache.org/javadoc/apidocs/org/apache/storm/hdfs/bolt/HdfsBolt.html)</ept><ph id="ph2" /> component, which is provided as part of Apache Storm.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source><ph id="ph3">[AZURE.IMPORTANT]</ph><ph id="ph4" /> The example topology used in this document relies on components that are included with Storm on HDInsight clusters, and may require modification to work with Azure Data Lake Store when used with other Apache Storm clusters.</source>
          <target state="new"><ph id="ph3">[AZURE.IMPORTANT]</ph><ph id="ph4" /> The example topology used in this document relies on components that are included with Storm on HDInsight clusters, and may require modification to work with Azure Data Lake Store when used with other Apache Storm clusters.</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>Prerequisites</source>
          <target state="new">Prerequisites</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source><bpt id="p2">[</bpt>Java JDK 1.7<ept id="p2">](https://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html)</ept><ph id="ph5" /> or higher</source>
          <target state="new"><bpt id="p2">[</bpt>Java JDK 1.7<ept id="p2">](https://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html)</ept><ph id="ph5" /> or higher</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source><bpt id="p3">[</bpt>Maven 3.x<ept id="p3">](https://maven.apache.org/download.cgi)</ept></source>
          <target state="new"><bpt id="p3">[</bpt>Maven 3.x<ept id="p3">](https://maven.apache.org/download.cgi)</ept></target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>An Azure subscription</source>
          <target state="new">An Azure subscription</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>A Storm on HDInsight cluster.</source>
          <target state="new">A Storm on HDInsight cluster.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>Information on creating a cluster that can use Azure Data Lake Store are included in this document.</source>
          <target state="new">Information on creating a cluster that can use Azure Data Lake Store are included in this document.</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>Configure environment variables</source>
          <target state="new">Configure environment variables</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>The following environment variables may be set when you install Java and the JDK on your development workstation.</source>
          <target state="new">The following environment variables may be set when you install Java and the JDK on your development workstation.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>However, you should check that they exist and that they contain the correct values for your system.</source>
          <target state="new">However, you should check that they exist and that they contain the correct values for your system.</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source><bpt id="p4">__</bpt>JAVA_HOME<ept id="p4">__</ept><ph id="ph6" /> - should point to the directory where the Java runtime environment (JRE) is installed.</source>
          <target state="new"><bpt id="p4">__</bpt>JAVA_HOME<ept id="p4">__</ept><ph id="ph6" /> - should point to the directory where the Java runtime environment (JRE) is installed.</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>For example, in a Unix or Linux distribution, it should have a value similar to <ph id="ph7">`/usr/lib/jvm/java-7-oracle`</ph>.</source>
          <target state="new">For example, in a Unix or Linux distribution, it should have a value similar to <ph id="ph7">`/usr/lib/jvm/java-7-oracle`</ph>.</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>In Windows, it would have a value similar to <ph id="ph8">`c:\Program Files (x86)\Java\jre1.7`</ph>.</source>
          <target state="new">In Windows, it would have a value similar to <ph id="ph8">`c:\Program Files (x86)\Java\jre1.7`</ph>.</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source><bpt id="p5">__</bpt>PATH<ept id="p5">__</ept><ph id="ph9" /> - should contain the following paths:</source>
          <target state="new"><bpt id="p5">__</bpt>PATH<ept id="p5">__</ept><ph id="ph9" /> - should contain the following paths:</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source><bpt id="p6">__</bpt>JAVA\_HOME<ept id="p6">__</ept><ph id="ph10" /> (or the equivalent path)</source>
          <target state="new"><bpt id="p6">__</bpt>JAVA\_HOME<ept id="p6">__</ept><ph id="ph10" /> (or the equivalent path)</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source><bpt id="p7">__</bpt>JAVA\_HOME\bin<ept id="p7">__</ept><ph id="ph11" /> (or the equivalent path)</source>
          <target state="new"><bpt id="p7">__</bpt>JAVA\_HOME\bin<ept id="p7">__</ept><ph id="ph11" /> (or the equivalent path)</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>The directory where Maven is installed</source>
          <target state="new">The directory where Maven is installed</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>Topology implementation</source>
          <target state="new">Topology implementation</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>The example used in this document is written in Java, and uses the following components:</source>
          <target state="new">The example used in this document is written in Java, and uses the following components:</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source><bpt id="p8">__</bpt>TickSpout<ept id="p8">__</ept>: Generates the data used by other components in the topology.</source>
          <target state="new"><bpt id="p8">__</bpt>TickSpout<ept id="p8">__</ept>: Generates the data used by other components in the topology.</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source><bpt id="p9">__</bpt>PartialCount<ept id="p9">__</ept>: Counts events generated by TickSpout.</source>
          <target state="new"><bpt id="p9">__</bpt>PartialCount<ept id="p9">__</ept>: Counts events generated by TickSpout.</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source><bpt id="p10">__</bpt>FinalCount<ept id="p10">__</ept>: Aggregates count data from PartialCount.</source>
          <target state="new"><bpt id="p10">__</bpt>FinalCount<ept id="p10">__</ept>: Aggregates count data from PartialCount.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source><bpt id="p11">__</bpt>ADLStoreBolt<ept id="p11">__</ept>: Writes data to Azure Data Lake Store using the <bpt id="p12">[</bpt>HdfsBolt<ept id="p12">](http://storm.apache.org/javadoc/apidocs/org/apache/storm/hdfs/bolt/HdfsBolt.html)</ept><ph id="ph12" /> component.</source>
          <target state="new"><bpt id="p11">__</bpt>ADLStoreBolt<ept id="p11">__</ept>: Writes data to Azure Data Lake Store using the <bpt id="p12">[</bpt>HdfsBolt<ept id="p12">](http://storm.apache.org/javadoc/apidocs/org/apache/storm/hdfs/bolt/HdfsBolt.html)</ept><ph id="ph12" /> component.</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>The project containing this topology is available as a download from <bpt id="p13">[</bpt>https://github.com/Azure-Samples/hdinsight-storm-azure-data-lake-store<ept id="p13">](https://github.com/Azure-Samples/hdinsight-storm-azure-data-lake-store)</ept>.</source>
          <target state="new">The project containing this topology is available as a download from <bpt id="p13">[</bpt>https://github.com/Azure-Samples/hdinsight-storm-azure-data-lake-store<ept id="p13">](https://github.com/Azure-Samples/hdinsight-storm-azure-data-lake-store)</ept>.</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>Understanding ADLStoreBolt</source>
          <target state="new">Understanding ADLStoreBolt</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>The ADLStoreBolt is the name used for the HdfsBolt instance in the topology that writes to Azure Data Lake.</source>
          <target state="new">The ADLStoreBolt is the name used for the HdfsBolt instance in the topology that writes to Azure Data Lake.</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>This is not a special version of HdfsBolt created by Microsoft; however it does rely on core-site configuration values, as well as Hadoop components that are included with Azure HDInsight to communication with Data Lake.</source>
          <target state="new">This is not a special version of HdfsBolt created by Microsoft; however it does rely on core-site configuration values, as well as Hadoop components that are included with Azure HDInsight to communication with Data Lake.</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>Specifically, when you create an HDInsight cluster, you can associate it with an Azure Data Lake Store.</source>
          <target state="new">Specifically, when you create an HDInsight cluster, you can associate it with an Azure Data Lake Store.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>This writes entries into core-site for the Data Lake Store you selected, which are used by components such as hadoop-client and hadoop-hdfs to enable communication with Data Lake Store.</source>
          <target state="new">This writes entries into core-site for the Data Lake Store you selected, which are used by components such as hadoop-client and hadoop-hdfs to enable communication with Data Lake Store.</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source><ph id="ph13">[AZURE.NOTE]</ph><ph id="ph14" /> Microsoft has contributed code to the Apache Hadoop and Storm projects that enables communication with Azure Data Lake Store and Azure Blob storage, but this functionality may not be included by default in other Hadoop and Storm distributions.</source>
          <target state="new"><ph id="ph13">[AZURE.NOTE]</ph><ph id="ph14" /> Microsoft has contributed code to the Apache Hadoop and Storm projects that enables communication with Azure Data Lake Store and Azure Blob storage, but this functionality may not be included by default in other Hadoop and Storm distributions.</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>The configuration for HdfsBolt in the topology is as follows:</source>
          <target state="new">The configuration for HdfsBolt in the topology is as follows:</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>If you are familiar with using HdfsBolt, you will notice that this is all pretty standard configuration except for the URL.</source>
          <target state="new">If you are familiar with using HdfsBolt, you will notice that this is all pretty standard configuration except for the URL.</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>The URL provides the path to the root of your Azure Data Lake Store.</source>
          <target state="new">The URL provides the path to the root of your Azure Data Lake Store.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>Since writing to Data Lake Store uses HdfsBolt, and is just a URL change, you should be able to take any existing topology that writes to HDFS or WASB using HdfsBolt, and easily change it to use Azure Data Lake Store.</source>
          <target state="new">Since writing to Data Lake Store uses HdfsBolt, and is just a URL change, you should be able to take any existing topology that writes to HDFS or WASB using HdfsBolt, and easily change it to use Azure Data Lake Store.</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>Create an HDInsight cluster and Data Lake Store</source>
          <target state="new">Create an HDInsight cluster and Data Lake Store</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>Create a new Storm on HDInsight cluster using the steps in the <bpt id="p14">[</bpt>Use HDInsight with Data Lake Store using Azure<ept id="p14">](../data-lake-store/data-lake-store-hdinsight-hadoop-use-portal.md)</ept><ph id="ph15" /> document.</source>
          <target state="new">Create a new Storm on HDInsight cluster using the steps in the <bpt id="p14">[</bpt>Use HDInsight with Data Lake Store using Azure<ept id="p14">](../data-lake-store/data-lake-store-hdinsight-hadoop-use-portal.md)</ept><ph id="ph15" /> document.</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>The steps in this document will walk you through creating a new HDInsight cluster and Azure Data Lake Store.</source>
          <target state="new">The steps in this document will walk you through creating a new HDInsight cluster and Azure Data Lake Store.</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source><ph id="ph16">[AZURE.IMPORTANT]</ph><ph id="ph17" /> When you create the HDInsight cluster, you must select <bpt id="p15">__</bpt>Storm<ept id="p15">__</ept><ph id="ph18" /> as the cluster type.</source>
          <target state="new"><ph id="ph16">[AZURE.IMPORTANT]</ph><ph id="ph17" /> When you create the HDInsight cluster, you must select <bpt id="p15">__</bpt>Storm<ept id="p15">__</ept><ph id="ph18" /> as the cluster type.</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>The OS can be either Windows or Linux.</source>
          <target state="new">The OS can be either Windows or Linux.</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>Build and package the topology</source>
          <target state="new">Build and package the topology</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>Download the example project from <bpt id="p16">[</bpt>https://github.com/Azure-Samples/hdinsight-storm-azure-data-lake-store
<ept id="p16">](https://github.com/Azure-Samples/hdinsight-storm-azure-data-lake-store
)</ept><ph id="ph19" /> to your development environment.</source>
          <target state="new">Download the example project from <bpt id="p16">[</bpt>https://github.com/Azure-Samples/hdinsight-storm-azure-data-lake-store
<ept id="p16">](https://github.com/Azure-Samples/hdinsight-storm-azure-data-lake-store
)</ept><ph id="ph19" /> to your development environment.</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>Open the <ph id="ph20">`StormToDataLake\src\main\java\com\microsoft\example\StormToDataLakeStore.java`</ph><ph id="ph21" /> file in an editor and find the line that contains <ph id="ph22">`.withFsUrl("adl://MYDATALAKE.azuredatalakestore.net/")`</ph>.</source>
          <target state="new">Open the <ph id="ph20">`StormToDataLake\src\main\java\com\microsoft\example\StormToDataLakeStore.java`</ph><ph id="ph21" /> file in an editor and find the line that contains <ph id="ph22">`.withFsUrl("adl://MYDATALAKE.azuredatalakestore.net/")`</ph>.</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>Change <bpt id="p17">__</bpt>MYDATALAKE<ept id="p17">__</ept><ph id="ph23" /> to the name of the Azure Data Lake Store you used when creating your HDInsight server.</source>
          <target state="new">Change <bpt id="p17">__</bpt>MYDATALAKE<ept id="p17">__</ept><ph id="ph23" /> to the name of the Azure Data Lake Store you used when creating your HDInsight server.</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>From a command prompt, terminal, or shell session, change directories to the root of the downloaded project, and run the following commands to build and package the topology.</source>
          <target state="new">From a command prompt, terminal, or shell session, change directories to the root of the downloaded project, and run the following commands to build and package the topology.</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>Once the build and packaging completes, there will be a new directory named <ph id="ph24">`target`</ph>, that contains a file named <ph id="ph25">`StormToDataLakeStore-1.0-SNAPSHOT.jar`</ph>.</source>
          <target state="new">Once the build and packaging completes, there will be a new directory named <ph id="ph24">`target`</ph>, that contains a file named <ph id="ph25">`StormToDataLakeStore-1.0-SNAPSHOT.jar`</ph>.</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>This contains the compiled topology.</source>
          <target state="new">This contains the compiled topology.</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>Deploy and run on Linux-based HDInsight</source>
          <target state="new">Deploy and run on Linux-based HDInsight</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>If you created a Linux-based Storm on HDInsight cluster, use the steps below to deploy and run the topology.</source>
          <target state="new">If you created a Linux-based Storm on HDInsight cluster, use the steps below to deploy and run the topology.</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source>Use the following command to copy the topology to the HDInsight cluster.</source>
          <target state="new">Use the following command to copy the topology to the HDInsight cluster.</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>Replace <bpt id="p18">__</bpt>USER<ept id="p18">__</ept><ph id="ph26" /> with the SSH user name you used when creating the cluster.</source>
          <target state="new">Replace <bpt id="p18">__</bpt>USER<ept id="p18">__</ept><ph id="ph26" /> with the SSH user name you used when creating the cluster.</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>Replace <bpt id="p19">__</bpt>CLUSTERNAME<ept id="p19">__</ept><ph id="ph27" /> with the name of the cluster.</source>
          <target state="new">Replace <bpt id="p19">__</bpt>CLUSTERNAME<ept id="p19">__</ept><ph id="ph27" /> with the name of the cluster.</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>When prompted, enter the password used when creating the SSH user for the cluster.</source>
          <target state="new">When prompted, enter the password used when creating the SSH user for the cluster.</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>If you used a public key instead of a password, you may need to use the <ph id="ph28">`-i`</ph><ph id="ph29" /> parameter to specify the path to the matching private key.</source>
          <target state="new">If you used a public key instead of a password, you may need to use the <ph id="ph28">`-i`</ph><ph id="ph29" /> parameter to specify the path to the matching private key.</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source><ph id="ph30">[AZURE.NOTE]</ph><ph id="ph31" /> If you are using a Windows client for development, you may not have an <ph id="ph32">`scp`</ph><ph id="ph33" /> command.</source>
          <target state="new"><ph id="ph30">[AZURE.NOTE]</ph><ph id="ph31" /> If you are using a Windows client for development, you may not have an <ph id="ph32">`scp`</ph><ph id="ph33" /> command.</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>If so, you can use <ph id="ph34">`pscp`</ph>, which is available from <bpt id="p20">[</bpt>http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html<ept id="p20">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.</source>
          <target state="new">If so, you can use <ph id="ph34">`pscp`</ph>, which is available from <bpt id="p20">[</bpt>http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html<ept id="p20">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>Once the upload completes, use the following to connect to the HDInsight cluster using SSH.</source>
          <target state="new">Once the upload completes, use the following to connect to the HDInsight cluster using SSH.</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>Replace <bpt id="p21">__</bpt>USER<ept id="p21">__</ept><ph id="ph35" /> with the SSH user name you used when creating the cluster.</source>
          <target state="new">Replace <bpt id="p21">__</bpt>USER<ept id="p21">__</ept><ph id="ph35" /> with the SSH user name you used when creating the cluster.</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>Replace <bpt id="p22">__</bpt>CLUSTERNAME<ept id="p22">__</ept><ph id="ph36" /> with the name of the cluster.</source>
          <target state="new">Replace <bpt id="p22">__</bpt>CLUSTERNAME<ept id="p22">__</ept><ph id="ph36" /> with the name of the cluster.</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>When prompted, enter the password used when creating the SSH user for the cluster.</source>
          <target state="new">When prompted, enter the password used when creating the SSH user for the cluster.</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>If you used a public key instead of a password, you may need to use the <ph id="ph37">`-i`</ph><ph id="ph38" /> parameter to specify the path to the matching private key.</source>
          <target state="new">If you used a public key instead of a password, you may need to use the <ph id="ph37">`-i`</ph><ph id="ph38" /> parameter to specify the path to the matching private key.</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source><ph id="ph39">[AZURE.NOTE]</ph><ph id="ph40" /> If you are using a Windows client for development, follow the information in <bpt id="p23">[</bpt>Connect to Linux-based HDInsight with SSH from Windows<ept id="p23">](hdinsight-hadoop-linux-use-ssh-windows.md)</ept><ph id="ph41" /> for information on using the PuTTY client to connect to the cluster.</source>
          <target state="new"><ph id="ph39">[AZURE.NOTE]</ph><ph id="ph40" /> If you are using a Windows client for development, follow the information in <bpt id="p23">[</bpt>Connect to Linux-based HDInsight with SSH from Windows<ept id="p23">](hdinsight-hadoop-linux-use-ssh-windows.md)</ept><ph id="ph41" /> for information on using the PuTTY client to connect to the cluster.</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source>Once connected, use the following to start the topology:</source>
          <target state="new">Once connected, use the following to start the topology:</target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source>This will start the topology with a friendly name of <ph id="ph42">`datalakewriter`</ph>.</source>
          <target state="new">This will start the topology with a friendly name of <ph id="ph42">`datalakewriter`</ph>.</target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>Deploy and run on Windows-based HDInsight</source>
          <target state="new">Deploy and run on Windows-based HDInsight</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>Open a web browser and go to HTTPS://CLUSTERNAME.azurehdinsight.net, where <bpt id="p24">__</bpt>CLUSTERNAME<ept id="p24">__</ept><ph id="ph43" /> is the name of your HDInsight cluster.</source>
          <target state="new">Open a web browser and go to HTTPS://CLUSTERNAME.azurehdinsight.net, where <bpt id="p24">__</bpt>CLUSTERNAME<ept id="p24">__</ept><ph id="ph43" /> is the name of your HDInsight cluster.</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source>When prompted, provide the admin user name (<ph id="ph44">`admin`</ph>) and the password that you used for this account when the cluster was created.</source>
          <target state="new">When prompted, provide the admin user name (<ph id="ph44">`admin`</ph>) and the password that you used for this account when the cluster was created.</target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source>From the Storm Dashboard, select <bpt id="p25">__</bpt>Browse<ept id="p25">__</ept><ph id="ph45" /> from the <bpt id="p26">__</bpt>Jar File<ept id="p26">__</ept><ph id="ph46" /> drop-down, then select the StormToDataLakeStore-1.0-SNAPSHOT.jar file from the <ph id="ph47">`target`</ph><ph id="ph48" /> directory.</source>
          <target state="new">From the Storm Dashboard, select <bpt id="p25">__</bpt>Browse<ept id="p25">__</ept><ph id="ph45" /> from the <bpt id="p26">__</bpt>Jar File<ept id="p26">__</ept><ph id="ph46" /> drop-down, then select the StormToDataLakeStore-1.0-SNAPSHOT.jar file from the <ph id="ph47">`target`</ph><ph id="ph48" /> directory.</target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source>Use the following values for the other entries on the form:</source>
          <target state="new">Use the following values for the other entries on the form:</target>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source>Class Name: com.microsoft.example.StormToDataLakeStore</source>
          <target state="new">Class Name: com.microsoft.example.StormToDataLakeStore</target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source>Additional Parameters: datalakewriter</source>
          <target state="new">Additional Parameters: datalakewriter</target>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source><ph id="ph49">![</ph>image of storm dashboard<ph id="ph50">](./media/hdinsight-storm-write-data-lake-store/submit.png)</ph></source>
          <target state="new"><ph id="ph49">![</ph>image of storm dashboard<ph id="ph50">](./media/hdinsight-storm-write-data-lake-store/submit.png)</ph></target>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source>Select the <bpt id="p27">__</bpt>Submit<ept id="p27">__</ept><ph id="ph51" /> button to upload and start the topology.</source>
          <target state="new">Select the <bpt id="p27">__</bpt>Submit<ept id="p27">__</ept><ph id="ph51" /> button to upload and start the topology.</target>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source>The result field below the <bpt id="p28">__</bpt>Submit<ept id="p28">__</ept><ph id="ph52" /> button should display information similar to the following once the topology has started:</source>
          <target state="new">The result field below the <bpt id="p28">__</bpt>Submit<ept id="p28">__</ept><ph id="ph52" /> button should display information similar to the following once the topology has started:</target>
        </trans-unit>
        <trans-unit id="180" translate="yes" xml:space="preserve">
          <source>View output data</source>
          <target state="new">View output data</target>
        </trans-unit>
        <trans-unit id="181" translate="yes" xml:space="preserve">
          <source>There are several ways to view the data.</source>
          <target state="new">There are several ways to view the data.</target>
        </trans-unit>
        <trans-unit id="182" translate="yes" xml:space="preserve">
          <source>In this section we use the Azure Portal and the <ph id="ph53">`hdfs`</ph><ph id="ph54" /> command to view the data.</source>
          <target state="new">In this section we use the Azure Portal and the <ph id="ph53">`hdfs`</ph><ph id="ph54" /> command to view the data.</target>
        </trans-unit>
        <trans-unit id="183" translate="yes" xml:space="preserve">
          <source><ph id="ph55">[AZURE.NOTE]</ph><ph id="ph56" /> You should allow the topologies to run for several minutes before checking the output data, so that data has been synched to several files on Azure Data Lake Store.</source>
          <target state="new"><ph id="ph55">[AZURE.NOTE]</ph><ph id="ph56" /> You should allow the topologies to run for several minutes before checking the output data, so that data has been synched to several files on Azure Data Lake Store.</target>
        </trans-unit>
        <trans-unit id="184" translate="yes" xml:space="preserve">
          <source><bpt id="p29">__</bpt>From the <bpt id="p30">[</bpt>Azure Portal<ept id="p30">](https://portal.azure.com)</ept><ept id="p29">__</ept>: In the portal, select the Azure Data Lake Store that you used with HDInsight.</source>
          <target state="new"><bpt id="p29">__</bpt>From the <bpt id="p30">[</bpt>Azure Portal<ept id="p30">](https://portal.azure.com)</ept><ept id="p29">__</ept>: In the portal, select the Azure Data Lake Store that you used with HDInsight.</target>
        </trans-unit>
        <trans-unit id="185" translate="yes" xml:space="preserve">
          <source><ph id="ph57">[AZURE.NOTE]</ph><ph id="ph58" /> If you did not pin the Data Lake Store to the Azure portal dashboard, you can find it by selecting <bpt id="p31">__</bpt>Browse<ept id="p31">__</ept><ph id="ph59" /> at the bottom of the list on the left, then <bpt id="p32">__</bpt>Data Lake Store<ept id="p32">__</ept>, and finally selecting the store.</source>
          <target state="new"><ph id="ph57">[AZURE.NOTE]</ph><ph id="ph58" /> If you did not pin the Data Lake Store to the Azure portal dashboard, you can find it by selecting <bpt id="p31">__</bpt>Browse<ept id="p31">__</ept><ph id="ph59" /> at the bottom of the list on the left, then <bpt id="p32">__</bpt>Data Lake Store<ept id="p32">__</ept>, and finally selecting the store.</target>
        </trans-unit>
        <trans-unit id="186" translate="yes" xml:space="preserve">
          <source>From the icons at the top of the Data Lake Store, select <bpt id="p33">__</bpt>Data Explorer<ept id="p33">__</ept>.</source>
          <target state="new">From the icons at the top of the Data Lake Store, select <bpt id="p33">__</bpt>Data Explorer<ept id="p33">__</ept>.</target>
        </trans-unit>
        <trans-unit id="187" translate="yes" xml:space="preserve">
          <source><ph id="ph60">![</ph>data explore icon<ph id="ph61">](./media/hdinsight-storm-write-data-lake-store/dataexplorer.png)</ph></source>
          <target state="new"><ph id="ph60">![</ph>data explore icon<ph id="ph61">](./media/hdinsight-storm-write-data-lake-store/dataexplorer.png)</ph></target>
        </trans-unit>
        <trans-unit id="188" translate="yes" xml:space="preserve">
          <source>Next, select the <bpt id="p34">__</bpt>stormdata<ept id="p34">__</ept><ph id="ph62" /> folder.</source>
          <target state="new">Next, select the <bpt id="p34">__</bpt>stormdata<ept id="p34">__</ept><ph id="ph62" /> folder.</target>
        </trans-unit>
        <trans-unit id="189" translate="yes" xml:space="preserve">
          <source>A list of text files should be displayed.</source>
          <target state="new">A list of text files should be displayed.</target>
        </trans-unit>
        <trans-unit id="190" translate="yes" xml:space="preserve">
          <source><ph id="ph63">![</ph>text files<ph id="ph64">](./media/hdinsight-storm-write-data-lake-store/stormoutput.png)</ph></source>
          <target state="new"><ph id="ph63">![</ph>text files<ph id="ph64">](./media/hdinsight-storm-write-data-lake-store/stormoutput.png)</ph></target>
        </trans-unit>
        <trans-unit id="191" translate="yes" xml:space="preserve">
          <source>Select one of the files to view its contents.</source>
          <target state="new">Select one of the files to view its contents.</target>
        </trans-unit>
        <trans-unit id="192" translate="yes" xml:space="preserve">
          <source><bpt id="p35">__</bpt>From the cluster<ept id="p35">__</ept>: If you have connected to the HDInsight cluster using SSH (Linux cluster,) or Remote Desktop (Windows cluster,) you can use the following to view the data.</source>
          <target state="new"><bpt id="p35">__</bpt>From the cluster<ept id="p35">__</ept>: If you have connected to the HDInsight cluster using SSH (Linux cluster,) or Remote Desktop (Windows cluster,) you can use the following to view the data.</target>
        </trans-unit>
        <trans-unit id="193" translate="yes" xml:space="preserve">
          <source>Replace <bpt id="p36">__</bpt>DATALAKE<ept id="p36">__</ept><ph id="ph65" /> with the name of your Data Lake Store</source>
          <target state="new">Replace <bpt id="p36">__</bpt>DATALAKE<ept id="p36">__</ept><ph id="ph65" /> with the name of your Data Lake Store</target>
        </trans-unit>
        <trans-unit id="194" translate="yes" xml:space="preserve">
          <source>This will concatenate the text files stored in the directory, and display information similar to the following:</source>
          <target state="new">This will concatenate the text files stored in the directory, and display information similar to the following:</target>
        </trans-unit>
        <trans-unit id="195" translate="yes" xml:space="preserve">
          <source>Stop the topology</source>
          <target state="new">Stop the topology</target>
        </trans-unit>
        <trans-unit id="196" translate="yes" xml:space="preserve">
          <source>Storm topologies will run until stopped, or the cluster is deleted.</source>
          <target state="new">Storm topologies will run until stopped, or the cluster is deleted.</target>
        </trans-unit>
        <trans-unit id="197" translate="yes" xml:space="preserve">
          <source>To stop the topologies, use the following information.</source>
          <target state="new">To stop the topologies, use the following information.</target>
        </trans-unit>
        <trans-unit id="198" translate="yes" xml:space="preserve">
          <source><bpt id="p37">__</bpt>For Linux-based HDInsight<ept id="p37">__</ept>:</source>
          <target state="new"><bpt id="p37">__</bpt>For Linux-based HDInsight<ept id="p37">__</ept>:</target>
        </trans-unit>
        <trans-unit id="199" translate="yes" xml:space="preserve">
          <source>From an SSH session to the cluster, use the following command:</source>
          <target state="new">From an SSH session to the cluster, use the following command:</target>
        </trans-unit>
        <trans-unit id="200" translate="yes" xml:space="preserve">
          <source><bpt id="p38">__</bpt>For Windows-based HDInsight<ept id="p38">__</ept>:</source>
          <target state="new"><bpt id="p38">__</bpt>For Windows-based HDInsight<ept id="p38">__</ept>:</target>
        </trans-unit>
        <trans-unit id="201" translate="yes" xml:space="preserve">
          <source>From the Storm Dashboard (https://CLUSTERNAME.azurehdinsight.net,) select the <bpt id="p39">__</bpt>Storm UI<ept id="p39">__</ept><ph id="ph66" /> link at the top of the page.</source>
          <target state="new">From the Storm Dashboard (https://CLUSTERNAME.azurehdinsight.net,) select the <bpt id="p39">__</bpt>Storm UI<ept id="p39">__</ept><ph id="ph66" /> link at the top of the page.</target>
        </trans-unit>
        <trans-unit id="202" translate="yes" xml:space="preserve">
          <source>Once the Storm UI loads, select the <bpt id="p40">__</bpt>datalakewriter<ept id="p40">__</ept><ph id="ph67" /> link.</source>
          <target state="new">Once the Storm UI loads, select the <bpt id="p40">__</bpt>datalakewriter<ept id="p40">__</ept><ph id="ph67" /> link.</target>
        </trans-unit>
        <trans-unit id="203" translate="yes" xml:space="preserve">
          <source><ph id="ph68">![</ph>link to datalakewriter<ph id="ph69">](./media/hdinsight-storm-write-data-lake-store/selecttopology.png)</ph></source>
          <target state="new"><ph id="ph68">![</ph>link to datalakewriter<ph id="ph69">](./media/hdinsight-storm-write-data-lake-store/selecttopology.png)</ph></target>
        </trans-unit>
        <trans-unit id="204" translate="yes" xml:space="preserve">
          <source>In the <bpt id="p41">__</bpt>Topology Actions<ept id="p41">__</ept><ph id="ph70" /> section, select <bpt id="p42">__</bpt>Kill<ept id="p42">__</ept><ph id="ph71" /> and then select OK on the dialog box that appears.</source>
          <target state="new">In the <bpt id="p41">__</bpt>Topology Actions<ept id="p41">__</ept><ph id="ph70" /> section, select <bpt id="p42">__</bpt>Kill<ept id="p42">__</ept><ph id="ph71" /> and then select OK on the dialog box that appears.</target>
        </trans-unit>
        <trans-unit id="205" translate="yes" xml:space="preserve">
          <source><ph id="ph72">![</ph>topology actions<ph id="ph73">](./media/hdinsight-storm-write-data-lake-store/topologyactions.png)</ph></source>
          <target state="new"><ph id="ph72">![</ph>topology actions<ph id="ph73">](./media/hdinsight-storm-write-data-lake-store/topologyactions.png)</ph></target>
        </trans-unit>
        <trans-unit id="206" translate="yes" xml:space="preserve">
          <source>Next steps</source>
          <target state="new">Next steps</target>
        </trans-unit>
        <trans-unit id="207" translate="yes" xml:space="preserve">
          <source>Now that you have learned how to use Storm to write to Azure Data Lake Store, discover other <bpt id="p43">[</bpt>Storm examples for HDInsight<ept id="p43">](hdinsight-storm-example-topology.md)</ept>.</source>
          <target state="new">Now that you have learned how to use Storm to write to Azure Data Lake Store, discover other <bpt id="p43">[</bpt>Storm examples for HDInsight<ept id="p43">](hdinsight-storm-example-topology.md)</ept>.</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">a6029e44e356b048b88f3d50d456885afab6fe6d</xliffext:olfilehash>
  </header>
</xliff>