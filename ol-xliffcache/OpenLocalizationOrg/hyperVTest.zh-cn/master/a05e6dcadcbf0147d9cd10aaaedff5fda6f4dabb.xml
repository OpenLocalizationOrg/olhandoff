{
  "nodes": [
    {
      "pos": [
        27,
        102
      ],
      "content": "The Cortana Analytics Process in action: using SQL Server | Microsoft Azure"
    },
    {
      "pos": [
        121,
        172
      ],
      "content": "Advanced Analytics Process and Technology in Action"
    },
    {
      "pos": [
        515,
        572
      ],
      "content": "The Cortana Analytics Process in action: using SQL Server"
    },
    {
      "pos": [
        574,
        813
      ],
      "content": "In this tutorial, you walkthrough building and deploying a model using a publicly available dataset -- the <bpt id=\"p1\">[</bpt>NYC Taxi Trips<ept id=\"p1\">](http://www.andresmh.com/nyctaxitrips/)</ept><ph id=\"ph2\"/> dataset. The procedure follows the Cortana Analytics Process (CAP) workflow.",
      "nodes": [
        {
          "content": "In this tutorial, you walkthrough building and deploying a model using a publicly available dataset -- the <bpt id=\"p1\">[</bpt>NYC Taxi Trips<ept id=\"p1\">](http://www.andresmh.com/nyctaxitrips/)</ept><ph id=\"ph2\"/> dataset.",
          "pos": [
            0,
            223
          ]
        },
        {
          "content": "The procedure follows the Cortana Analytics Process (CAP) workflow.",
          "pos": [
            224,
            291
          ]
        }
      ]
    },
    {
      "pos": [
        841,
        875
      ],
      "content": "NYC Taxi Trips Dataset Description"
    },
    {
      "pos": [
        877,
        1300
      ],
      "content": "The NYC Taxi Trip data is about 20GB of compressed CSV files (~48GB uncompressed), comprising more than 173 million individual trips and the fares paid for each trip. Each trip record includes the pickup and drop-off location and time, anonymized hack (driver's) license number and medallion (taxi’s unique id) number. The data covers all trips in the year 2013 and is provided in the following two datasets for each month:",
      "nodes": [
        {
          "content": "The NYC Taxi Trip data is about 20GB of compressed CSV files (~48GB uncompressed), comprising more than 173 million individual trips and the fares paid for each trip.",
          "pos": [
            0,
            166
          ]
        },
        {
          "content": "Each trip record includes the pickup and drop-off location and time, anonymized hack (driver's) license number and medallion (taxi’s unique id) number.",
          "pos": [
            167,
            318
          ]
        },
        {
          "content": "The data covers all trips in the year 2013 and is provided in the following two datasets for each month:",
          "pos": [
            319,
            423
          ]
        }
      ]
    },
    {
      "pos": [
        1305,
        1467
      ],
      "content": "The 'trip_data' CSV contains trip details, such as number of passengers, pickup and dropoff points, trip duration, and trip length. Here are a few sample records:",
      "nodes": [
        {
          "content": "The 'trip_data' CSV contains trip details, such as number of passengers, pickup and dropoff points, trip duration, and trip length.",
          "pos": [
            0,
            131
          ]
        },
        {
          "content": "Here are a few sample records:",
          "pos": [
            132,
            162
          ]
        }
      ]
    },
    {
      "pos": [
        2563,
        2761
      ],
      "content": "The 'trip_fare' CSV contains details of the fare paid for each trip, such as payment type, fare amount, surcharge and taxes, tips and tolls, and the total amount paid. Here are a few sample records:",
      "nodes": [
        {
          "content": "The 'trip_fare' CSV contains details of the fare paid for each trip, such as payment type, fare amount, surcharge and taxes, tips and tolls, and the total amount paid.",
          "pos": [
            0,
            167
          ]
        },
        {
          "content": "Here are a few sample records:",
          "pos": [
            168,
            198
          ]
        }
      ]
    },
    {
      "pos": [
        3506,
        3628
      ],
      "content": "The unique key to join trip\\_data and trip\\_fare is composed of the fields: medallion, hack\\_licence and pickup\\_datetime."
    },
    {
      "pos": [
        3655,
        3683
      ],
      "content": "Examples of Prediction Tasks"
    },
    {
      "pos": [
        3685,
        3764
      ],
      "content": "We will formulate three prediction problems based on the <bpt id=\"p2\">*</bpt>tip\\_amount<ept id=\"p2\">*</ept>, namely:"
    },
    {
      "pos": [
        3769,
        3960
      ],
      "content": "Binary classification: Predict whether or not a tip was paid for a trip, i.e. a <bpt id=\"p3\">*</bpt>tip\\_amount<ept id=\"p3\">*</ept><ph id=\"ph3\"/> that is greater than $0 is a positive example, while a <bpt id=\"p4\">*</bpt>tip\\_amount<ept id=\"p4\">*</ept><ph id=\"ph4\"/> of $0 is a negative example."
    },
    {
      "pos": [
        3965,
        4093
      ],
      "content": "Multiclass classification: To predict the range of tip paid for the trip. We divide the <bpt id=\"p5\">*</bpt>tip\\_amount<ept id=\"p5\">*</ept><ph id=\"ph5\"/> into five bins or classes:",
      "nodes": [
        {
          "content": "Multiclass classification: To predict the range of tip paid for the trip.",
          "pos": [
            0,
            73
          ]
        },
        {
          "content": "We divide the <bpt id=\"p5\">*</bpt>tip\\_amount<ept id=\"p5\">*</ept><ph id=\"ph5\"/> into five bins or classes:",
          "pos": [
            74,
            180
          ]
        }
      ]
    },
    {
      "pos": [
        4336,
        4398
      ],
      "content": "Regression task: To predict the amount of tip paid for a trip."
    },
    {
      "pos": [
        4426,
        4494
      ],
      "content": "Setting Up the Azure data science environment for advanced analytics"
    },
    {
      "pos": [
        4496,
        4682
      ],
      "content": "As you can see from the <bpt id=\"p6\">[</bpt>Plan Your Environment<ept id=\"p6\">](machine-learning-data-science-plan-your-environment.md)</ept><ph id=\"ph6\"/> guide, there are several options to work with the NYC Taxi Trips dataset in Azure:"
    },
    {
      "pos": [
        4686,
        4756
      ],
      "content": "Work with the data in Azure blobs then model in Azure Machine Learning"
    },
    {
      "pos": [
        4759,
        4836
      ],
      "content": "Load the data into a SQL Server database then model in Azure Machine Learning"
    },
    {
      "pos": [
        4838,
        5444
      ],
      "content": "In this tutorial we will demonstrate parallel bulk import of the data to a SQL Server, data exploration, feature engineering and down sampling using SQL Server Management Studio as well as using IPython Notebook. <bpt id=\"p7\">[</bpt>Sample scripts<ept id=\"p7\">](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/DataScienceProcess/DataScienceScripts)</ept><ph id=\"ph7\"/> and <bpt id=\"p8\">[</bpt>IPython notebooks<ept id=\"p8\">](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/DataScienceProcess/iPythonNotebooks)</ept><ph id=\"ph8\"/> are shared in GitHub. A sample IPython notebook to work with the data in Azure blobs is also available in the same location.",
      "nodes": [
        {
          "content": "In this tutorial we will demonstrate parallel bulk import of the data to a SQL Server, data exploration, feature engineering and down sampling using SQL Server Management Studio as well as using IPython Notebook.",
          "pos": [
            0,
            212
          ]
        },
        {
          "content": "<bpt id=\"p7\">[</bpt>Sample scripts<ept id=\"p7\">](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/DataScienceProcess/DataScienceScripts)</ept><ph id=\"ph7\"/> and <bpt id=\"p8\">[</bpt>IPython notebooks<ept id=\"p8\">](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/DataScienceProcess/iPythonNotebooks)</ept><ph id=\"ph8\"/> are shared in GitHub.",
          "pos": [
            213,
            607
          ]
        },
        {
          "content": "A sample IPython notebook to work with the data in Azure blobs is also available in the same location.",
          "pos": [
            608,
            710
          ]
        }
      ]
    },
    {
      "pos": [
        5446,
        5492
      ],
      "content": "To set up your Azure Data Science environment:"
    },
    {
      "pos": [
        5497,
        5561
      ],
      "content": "<bpt id=\"p9\">[</bpt>Create a storage account<ept id=\"p9\">](../storage-create-storage-account.md)</ept>"
    },
    {
      "pos": [
        5566,
        5634
      ],
      "content": "<bpt id=\"p10\">[</bpt>Create an Azure ML workspace<ept id=\"p10\">](machine-learning-create-workspace.md)</ept>"
    },
    {
      "pos": [
        5639,
        5818
      ],
      "content": "<bpt id=\"p11\">[</bpt>Provision a Data Science Virtual Machine<ept id=\"p11\">](machine-learning-data-science-setup-sql-server-virtual-machine.md)</ept>, which will serve as a SQL Server as well an IPython Notebook server."
    },
    {
      "pos": [
        5826,
        6061
      ],
      "content": "<ph id=\"ph9\">[AZURE.NOTE]</ph><ph id=\"ph10\"/> The sample scripts and IPython notebooks will be downloaded to your Data Science virtual machine during the setup process. When the VM post-installation script completes, the samples will be in your VM's Documents library:",
      "nodes": [
        {
          "content": "<ph id=\"ph9\">[AZURE.NOTE]</ph><ph id=\"ph10\"/> The sample scripts and IPython notebooks will be downloaded to your Data Science virtual machine during the setup process.",
          "pos": [
            0,
            168
          ]
        },
        {
          "content": "When the VM post-installation script completes, the samples will be in your VM's Documents library:",
          "pos": [
            169,
            268
          ]
        }
      ]
    },
    {
      "pos": [
        6072,
        6141
      ],
      "content": "Sample Scripts: <ph id=\"ph11\">`C:\\Users\\&lt;user_name&gt;\\Documents\\Data Science Scripts`</ph>"
    },
    {
      "pos": [
        6152,
        6400
      ],
      "leadings": [
        "",
        "    > "
      ],
      "content": "Sample IPython Notebooks: <ph id=\"ph12\">`C:\\Users\\&lt;user_name&gt;\\Documents\\IPython Notebooks\\DataScienceSamples`</ph><ph id=\"ph13\"/>  \nwhere <ph id=\"ph14\">`&lt;user_name&gt;`</ph><ph id=\"ph15\"/> is your VM's Windows login name. We will refer to the sample folders as <bpt id=\"p12\">**</bpt>Sample Scripts<ept id=\"p12\">**</ept><ph id=\"ph16\"/> and <bpt id=\"p13\">**</bpt>Sample IPython Notebooks<ept id=\"p13\">**</ept>.",
      "nodes": [
        {
          "content": "Sample IPython Notebooks: <ph id=\"ph12\">`C:\\Users\\&lt;user_name&gt;\\Documents\\IPython Notebooks\\DataScienceSamples`</ph><ph id=\"ph13\"/>  \nwhere <ph id=\"ph14\">`&lt;user_name&gt;`</ph><ph id=\"ph15\"/> is your VM's Windows login name.",
          "pos": [
            0,
            230
          ]
        },
        {
          "content": "We will refer to the sample folders as <bpt id=\"p12\">**</bpt>Sample Scripts<ept id=\"p12\">**</ept><ph id=\"ph16\"/> and <bpt id=\"p13\">**</bpt>Sample IPython Notebooks<ept id=\"p13\">**</ept>.",
          "pos": [
            231,
            417
          ]
        }
      ]
    },
    {
      "pos": [
        6403,
        6675
      ],
      "content": "Based on the dataset size, data source location, and the selected Azure target environment, this scenario is similar to <bpt id=\"p14\">[</bpt>Scenario \\#5: Large dataset in a local files, target SQL Server in Azure VM<ept id=\"p14\">](../machine-learning-data-science-plan-sample-scenarios.md#largelocaltodb)</ept>."
    },
    {
      "pos": [
        6702,
        6733
      ],
      "content": "Get the Data from Public Source"
    },
    {
      "pos": [
        6735,
        7018
      ],
      "content": "To get the <bpt id=\"p15\">[</bpt>NYC Taxi Trips<ept id=\"p15\">](http://www.andresmh.com/nyctaxitrips/)</ept><ph id=\"ph17\"/> dataset from its public location, you may use any of the methods described in <bpt id=\"p16\">[</bpt>Move Data to and from Azure Blob Storage<ept id=\"p16\">](machine-learning-data-science-move-azure-blob.md)</ept><ph id=\"ph18\"/> to copy the data to your new virtual machine."
    },
    {
      "pos": [
        7020,
        7050
      ],
      "content": "To copy the data using AzCopy:"
    },
    {
      "pos": [
        7055,
        7090
      ],
      "content": "Log in to your virtual machine (VM)"
    },
    {
      "pos": [
        7095,
        7217
      ],
      "content": "Create a new directory in the VM's data disk (Note: Do not use the Temporary Disk which comes with the VM as a Data Disk)."
    },
    {
      "pos": [
        7222,
        7358
      ],
      "content": "In a Command Prompt window, run the following Azcopy command line, replacing &lt;path_to_data_folder&gt; with your data folder created in (2):"
    },
    {
      "pos": [
        7520,
        7649
      ],
      "content": "When the AzCopy completes, a total of 24 zipped CSV files (12 for trip\\_data and 12 for trip\\_fare) should be in the data folder."
    },
    {
      "pos": [
        7654,
        7799
      ],
      "content": "Unzip the downloaded files. Note the folder where the uncompressed files reside. This folder will be referred to as the &lt;path\\_to\\_data\\_files\\&gt;.",
      "nodes": [
        {
          "content": "Unzip the downloaded files. Note",
          "pos": [
            0,
            32
          ]
        },
        {
          "content": "the folder where the uncompressed files reside. This",
          "pos": [
            33,
            85
          ]
        },
        {
          "content": "folder will be referred to as the &lt;path\\_to\\_data\\_files\\&gt;.",
          "pos": [
            86,
            151
          ]
        }
      ]
    },
    {
      "pos": [
        7825,
        7866
      ],
      "content": "Bulk Import Data into SQL Server Database"
    },
    {
      "pos": [
        7868,
        8296
      ],
      "content": "The performance of loading/transferring large amounts of data to an SQL database and subsequent queries can be improved by using <bpt id=\"p17\">_</bpt>Partitioned Tables and Views<ept id=\"p17\">_</ept>. In this section, we will follow the instructions described in <bpt id=\"p18\">[</bpt>Parallel Bulk Data Import Using SQL Partition Tables<ept id=\"p18\">](machine-learning-data-science-parallel-load-sql-partitioned-tables.md)</ept><ph id=\"ph19\"/> to create a new database and load the data into partitioned tables in parallel.",
      "nodes": [
        {
          "content": "The performance of loading/transferring large amounts of data to an SQL database and subsequent queries can be improved by using <bpt id=\"p17\">_</bpt>Partitioned Tables and Views<ept id=\"p17\">_</ept>.",
          "pos": [
            0,
            200
          ]
        },
        {
          "content": "In this section, we will follow the instructions described in <bpt id=\"p18\">[</bpt>Parallel Bulk Data Import Using SQL Partition Tables<ept id=\"p18\">](machine-learning-data-science-parallel-load-sql-partitioned-tables.md)</ept><ph id=\"ph19\"/> to create a new database and load the data into partitioned tables in parallel.",
          "pos": [
            201,
            523
          ]
        }
      ]
    },
    {
      "pos": [
        8301,
        8368
      ],
      "content": "While logged in to your VM, start <bpt id=\"p19\">**</bpt>SQL Server Management Studio<ept id=\"p19\">**</ept>."
    },
    {
      "pos": [
        8373,
        8410
      ],
      "content": "Connect using Windows Authentication."
    },
    {
      "pos": [
        8416,
        8435
      ],
      "content": "<ph id=\"ph20\">![</ph>SSMS Connect<ph id=\"ph21\">][12]</ph>"
    },
    {
      "pos": [
        8440,
        8715
      ],
      "content": "If you have not yet changed the SQL Server authentication mode and created a new SQL login user, open the script file named <bpt id=\"p20\">**</bpt>change\\_auth.sql<ept id=\"p20\">**</ept><ph id=\"ph22\"/> in the <bpt id=\"p21\">**</bpt>Sample Scripts<ept id=\"p21\">**</ept><ph id=\"ph23\"/> folder. Change the  default user name and password. Click <bpt id=\"p22\">**</bpt>!Execute<ept id=\"p22\">**</ept><ph id=\"ph24\"/> in the toolbar to run the script.",
      "nodes": [
        {
          "content": "If you have not yet changed the SQL Server authentication mode and created a new SQL login user, open the script file named <bpt id=\"p20\">**</bpt>change\\_auth.sql<ept id=\"p20\">**</ept><ph id=\"ph22\"/> in the <bpt id=\"p21\">**</bpt>Sample Scripts<ept id=\"p21\">**</ept><ph id=\"ph23\"/> folder.",
          "pos": [
            0,
            288
          ]
        },
        {
          "content": "Change the  default user name and password.",
          "pos": [
            289,
            332
          ]
        },
        {
          "content": "Click <bpt id=\"p22\">**</bpt>!Execute<ept id=\"p22\">**</ept><ph id=\"ph24\"/> in the toolbar to run the script.",
          "pos": [
            333,
            440
          ]
        }
      ]
    },
    {
      "pos": [
        8721,
        8742
      ],
      "content": "<ph id=\"ph25\">![</ph>Execute Script<ph id=\"ph26\">][13]</ph>"
    },
    {
      "pos": [
        8747,
        9140
      ],
      "content": "Verify and/or change the SQL Server default database and log folders to ensure that newly created databases will be stored in a Data Disk. The SQL Server VM image that is optimized for datawarehousing loads is pre-configured with data and log disks. If your VM did not include a Data Disk and you added new virtual hard disks during the VM setup process, change the default folders as follows:",
      "nodes": [
        {
          "content": "Verify and/or change the SQL Server default database and log folders to ensure that newly created databases will be stored in a Data Disk.",
          "pos": [
            0,
            138
          ]
        },
        {
          "content": "The SQL Server VM image that is optimized for datawarehousing loads is pre-configured with data and log disks.",
          "pos": [
            139,
            249
          ]
        },
        {
          "content": "If your VM did not include a Data Disk and you added new virtual hard disks during the VM setup process, change the default folders as follows:",
          "pos": [
            250,
            393
          ]
        }
      ]
    },
    {
      "pos": [
        9148,
        9223
      ],
      "content": "Right-click the SQL Server name in the left panel and click <bpt id=\"p23\">**</bpt>Properties<ept id=\"p23\">**</ept>."
    },
    {
      "pos": [
        9233,
        9261
      ],
      "content": "<ph id=\"ph27\">![</ph>SQL Server Properties<ph id=\"ph28\">][14]</ph>"
    },
    {
      "pos": [
        9269,
        9342
      ],
      "content": "Select <bpt id=\"p24\">**</bpt>Database Settings<ept id=\"p24\">**</ept><ph id=\"ph29\"/> from the <bpt id=\"p25\">**</bpt>Select a page<ept id=\"p25\">**</ept><ph id=\"ph30\"/> list to the left."
    },
    {
      "pos": [
        9350,
        9534
      ],
      "content": "Verify and/or change the <bpt id=\"p26\">**</bpt>Database default locations<ept id=\"p26\">**</ept><ph id=\"ph31\"/> to the <bpt id=\"p27\">**</bpt>Data Disk<ept id=\"p27\">**</ept><ph id=\"ph32\"/> locations of your choice. This is where new databases reside if created with the default location settings.",
      "nodes": [
        {
          "content": "Verify and/or change the <bpt id=\"p26\">**</bpt>Database default locations<ept id=\"p26\">**</ept><ph id=\"ph31\"/> to the <bpt id=\"p27\">**</bpt>Data Disk<ept id=\"p27\">**</ept><ph id=\"ph32\"/> locations of your choice.",
          "pos": [
            0,
            212
          ]
        },
        {
          "content": "This is where new databases reside if created with the default location settings.",
          "pos": [
            213,
            294
          ]
        }
      ]
    },
    {
      "pos": [
        9544,
        9572
      ],
      "content": "<ph id=\"ph33\">![</ph>SQL Database Defaults<ph id=\"ph34\">][15]</ph>"
    },
    {
      "pos": [
        9579,
        9961
      ],
      "content": "To create a new database and a set of filegroups to hold the partitioned tables, open the sample script <bpt id=\"p28\">**</bpt>create\\_db\\_default.sql<ept id=\"p28\">**</ept>. The script will create a new database named <bpt id=\"p29\">**</bpt>TaxiNYC<ept id=\"p29\">**</ept><ph id=\"ph35\"/> and 12 filegroups in the default data location. Each filegroup will hold one month of trip\\_data and trip\\_fare data. Modify the database name, if desired. Click <bpt id=\"p30\">**</bpt>!Execute<ept id=\"p30\">**</ept><ph id=\"ph36\"/> to run the script.",
      "nodes": [
        {
          "content": "To create a new database and a set of filegroups to hold the partitioned tables, open the sample script <bpt id=\"p28\">**</bpt>create\\_db\\_default.sql<ept id=\"p28\">**</ept>.",
          "pos": [
            0,
            172
          ]
        },
        {
          "content": "The script will create a new database named <bpt id=\"p29\">**</bpt>TaxiNYC<ept id=\"p29\">**</ept><ph id=\"ph35\"/> and 12 filegroups in the default data location.",
          "pos": [
            173,
            331
          ]
        },
        {
          "content": "Each filegroup will hold one month of trip\\_data and trip\\_fare data.",
          "pos": [
            332,
            401
          ]
        },
        {
          "content": "Modify the database name, if desired.",
          "pos": [
            402,
            439
          ]
        },
        {
          "content": "Click <bpt id=\"p30\">**</bpt>!Execute<ept id=\"p30\">**</ept><ph id=\"ph36\"/> to run the script.",
          "pos": [
            440,
            532
          ]
        }
      ]
    },
    {
      "pos": [
        9966,
        10126
      ],
      "content": "Next, create two partition tables, one for the trip\\_data and another for the trip\\_fare. Open the sample script <bpt id=\"p31\">**</bpt>create\\_partitioned\\_table.sql<ept id=\"p31\">**</ept>, which will:",
      "nodes": [
        {
          "content": "Next, create two partition tables, one for the trip\\_data and another for the trip\\_fare.",
          "pos": [
            0,
            89
          ]
        },
        {
          "content": "Open the sample script <bpt id=\"p31\">**</bpt>create\\_partitioned\\_table.sql<ept id=\"p31\">**</ept>, which will:",
          "pos": [
            90,
            200
          ]
        }
      ]
    },
    {
      "pos": [
        10134,
        10189
      ],
      "content": "Create a partition function to split the data by month."
    },
    {
      "pos": [
        10196,
        10272
      ],
      "content": "Create a partition scheme to map each month's data to a different filegroup."
    },
    {
      "pos": [
        10279,
        10436
      ],
      "content": "Create two partitioned tables mapped to the partition scheme: <bpt id=\"p32\">**</bpt>nyctaxi\\_trip<ept id=\"p32\">**</ept><ph id=\"ph37\"/> will hold the trip\\_data and <bpt id=\"p33\">**</bpt>nyctaxi\\_fare<ept id=\"p33\">**</ept><ph id=\"ph38\"/> will hold the trip\\_fare data."
    },
    {
      "pos": [
        10442,
        10513
      ],
      "content": "Click <bpt id=\"p34\">**</bpt>!Execute<ept id=\"p34\">**</ept><ph id=\"ph39\"/> to run the script and create the partitioned tables."
    },
    {
      "pos": [
        10518,
        10667
      ],
      "content": "In the <bpt id=\"p35\">**</bpt>Sample Scripts<ept id=\"p35\">**</ept><ph id=\"ph40\"/> folder, there are two sample PowerShell scripts provided to demonstrate parallel bulk imports of data to SQL Server tables."
    },
    {
      "pos": [
        10675,
        10874
      ],
      "content": "<bpt id=\"p36\">**</bpt>bcp\\_parallel\\_generic.ps1<ept id=\"p36\">**</ept><ph id=\"ph41\"/> is a generic script to parallel bulk import data into a table. Modify this script to set the input and target variables as indicated in the comment lines in the script.",
      "nodes": [
        {
          "content": "<bpt id=\"p36\">**</bpt>bcp\\_parallel\\_generic.ps1<ept id=\"p36\">**</ept><ph id=\"ph41\"/> is a generic script to parallel bulk import data into a table.",
          "pos": [
            0,
            148
          ]
        },
        {
          "content": "Modify this script to set the input and target variables as indicated in the comment lines in the script.",
          "pos": [
            149,
            254
          ]
        }
      ]
    },
    {
      "pos": [
        10881,
        11029
      ],
      "content": "<bpt id=\"p37\">**</bpt>bcp\\_parallel\\_nyctaxi.ps1<ept id=\"p37\">**</ept><ph id=\"ph42\"/> is a pre-configured version of the generic script and can be used to to load both tables for the NYC Taxi Trips data."
    },
    {
      "pos": [
        11036,
        11388
      ],
      "content": "Right-click the <bpt id=\"p38\">**</bpt>bcp\\_parallel\\_nyctaxi.ps1<ept id=\"p38\">**</ept><ph id=\"ph43\"/> script name and click <bpt id=\"p39\">**</bpt>Edit<ept id=\"p39\">**</ept><ph id=\"ph44\"/> to open it in PowerShell. Review the preset variables and modify according to your selected database name, input data folder, target log folder, and paths to the  sample format files <bpt id=\"p40\">**</bpt>nyctaxi_trip.xml<ept id=\"p40\">**</ept><ph id=\"ph45\"/> and <bpt id=\"p41\">**</bpt>nyctaxi\\_fare.xml<ept id=\"p41\">**</ept><ph id=\"ph46\"/> (provided in the <bpt id=\"p42\">**</bpt>Sample Scripts<ept id=\"p42\">**</ept><ph id=\"ph47\"/> folder).",
      "nodes": [
        {
          "content": "Right-click the <bpt id=\"p38\">**</bpt>bcp\\_parallel\\_nyctaxi.ps1<ept id=\"p38\">**</ept><ph id=\"ph43\"/> script name and click <bpt id=\"p39\">**</bpt>Edit<ept id=\"p39\">**</ept><ph id=\"ph44\"/> to open it in PowerShell.",
          "pos": [
            0,
            213
          ]
        },
        {
          "content": "Review the preset variables and modify according to your selected database name, input data folder, target log folder, and paths to the  sample format files <bpt id=\"p40\">**</bpt>nyctaxi_trip.xml<ept id=\"p40\">**</ept><ph id=\"ph45\"/> and <bpt id=\"p41\">**</bpt>nyctaxi\\_fare.xml<ept id=\"p41\">**</ept><ph id=\"ph46\"/> (provided in the <bpt id=\"p42\">**</bpt>Sample Scripts<ept id=\"p42\">**</ept><ph id=\"ph47\"/> folder).",
          "pos": [
            214,
            627
          ]
        }
      ]
    },
    {
      "pos": [
        11394,
        11417
      ],
      "content": "<ph id=\"ph48\">![</ph>Bulk Import Data<ph id=\"ph49\">][16]</ph>"
    },
    {
      "pos": [
        11423,
        11741
      ],
      "content": "You may also select the authentication mode, default is Windows Authentication. Click the green arrow in the toolbar to run. The script will launch 24 bulk import operations in parallel, 12 for each partitioned table. You may monitor the data import progress by opening the SQL Server default data folder as set above.",
      "nodes": [
        {
          "content": "You may also select the authentication mode, default is Windows Authentication.",
          "pos": [
            0,
            79
          ]
        },
        {
          "content": "Click the green arrow in the toolbar to run.",
          "pos": [
            80,
            124
          ]
        },
        {
          "content": "The script will launch 24 bulk import operations in parallel, 12 for each partitioned table.",
          "pos": [
            125,
            217
          ]
        },
        {
          "content": "You may monitor the data import progress by opening the SQL Server default data folder as set above.",
          "pos": [
            218,
            318
          ]
        }
      ]
    },
    {
      "pos": [
        11746,
        11995
      ],
      "content": "The PowerShell script reports the starting and ending times. When all bulk imports complete, the ending time is reported. Check the target log folder to verify that the bulk imports were successful, i.e., no errors reported in the target log folder.",
      "nodes": [
        {
          "content": "The PowerShell script reports the starting and ending times.",
          "pos": [
            0,
            60
          ]
        },
        {
          "content": "When all bulk imports complete, the ending time is reported.",
          "pos": [
            61,
            121
          ]
        },
        {
          "content": "Check the target log folder to verify that the bulk imports were successful, i.e., no errors reported in the target log folder.",
          "pos": [
            122,
            249
          ]
        }
      ]
    },
    {
      "pos": [
        12001,
        12294
      ],
      "content": "Your database is now ready for exploration, feature engineering, and other operations as desired. Since the tables are partitioned according to the <bpt id=\"p43\">**</bpt>pickup\\_datetime<ept id=\"p43\">**</ept><ph id=\"ph50\"/> field, queries which include <bpt id=\"p44\">**</bpt>pickup\\_datetime<ept id=\"p44\">**</ept><ph id=\"ph51\"/> conditions in the <bpt id=\"p45\">**</bpt>WHERE<ept id=\"p45\">**</ept><ph id=\"ph52\"/> clause will benefit from the partition scheme.",
      "nodes": [
        {
          "content": "Your database is now ready for exploration, feature engineering, and other operations as desired.",
          "pos": [
            0,
            97
          ]
        },
        {
          "content": "Since the tables are partitioned according to the <bpt id=\"p43\">**</bpt>pickup\\_datetime<ept id=\"p43\">**</ept><ph id=\"ph50\"/> field, queries which include <bpt id=\"p44\">**</bpt>pickup\\_datetime<ept id=\"p44\">**</ept><ph id=\"ph51\"/> conditions in the <bpt id=\"p45\">**</bpt>WHERE<ept id=\"p45\">**</ept><ph id=\"ph52\"/> clause will benefit from the partition scheme.",
          "pos": [
            98,
            458
          ]
        }
      ]
    },
    {
      "pos": [
        12300,
        12496
      ],
      "content": "In <bpt id=\"p46\">**</bpt>SQL Server Management Studio<ept id=\"p46\">**</ept>, explore the provided sample script <bpt id=\"p47\">**</bpt>sample\\_queries.sql<ept id=\"p47\">**</ept>. To run any of the sample queries, highlight the query lines then click <bpt id=\"p48\">**</bpt>!Execute<ept id=\"p48\">**</ept><ph id=\"ph53\"/> in the toolbar.",
      "nodes": [
        {
          "content": "In <bpt id=\"p46\">**</bpt>SQL Server Management Studio<ept id=\"p46\">**</ept>, explore the provided sample script <bpt id=\"p47\">**</bpt>sample\\_queries.sql<ept id=\"p47\">**</ept>.",
          "pos": [
            0,
            176
          ]
        },
        {
          "content": "To run any of the sample queries, highlight the query lines then click <bpt id=\"p48\">**</bpt>!Execute<ept id=\"p48\">**</ept><ph id=\"ph53\"/> in the toolbar.",
          "pos": [
            177,
            331
          ]
        }
      ]
    },
    {
      "pos": [
        12502,
        12792
      ],
      "content": "The NYC Taxi Trips data is loaded in two separate tables. To improve join operations, it is highly recommended to index the tables. The sample script <bpt id=\"p49\">**</bpt>create\\_partitioned\\_index.sql<ept id=\"p49\">**</ept><ph id=\"ph54\"/> creates partitioned indexes on the composite join key <bpt id=\"p50\">**</bpt>medallion, hack\\_license, and pickup\\_datetime<ept id=\"p50\">**</ept>.",
      "nodes": [
        {
          "content": "The NYC Taxi Trips data is loaded in two separate tables.",
          "pos": [
            0,
            57
          ]
        },
        {
          "content": "To improve join operations, it is highly recommended to index the tables.",
          "pos": [
            58,
            131
          ]
        },
        {
          "content": "The sample script <bpt id=\"p49\">**</bpt>create\\_partitioned\\_index.sql<ept id=\"p49\">**</ept><ph id=\"ph54\"/> creates partitioned indexes on the composite join key <bpt id=\"p50\">**</bpt>medallion, hack\\_license, and pickup\\_datetime<ept id=\"p50\">**</ept>.",
          "pos": [
            132,
            385
          ]
        }
      ]
    },
    {
      "pos": [
        12821,
        12875
      ],
      "content": "Data Exploration and Feature Engineering in SQL Server"
    },
    {
      "pos": [
        12877,
        13257
      ],
      "content": "In this section, we will perform data exploration and feature generation by running SQL queries directly in the <bpt id=\"p51\">**</bpt>SQL Server Management Studio<ept id=\"p51\">**</ept><ph id=\"ph55\"/> using the SQL Server database created earlier. A sample script named <bpt id=\"p52\">**</bpt>sample\\_queries.sql<ept id=\"p52\">**</ept><ph id=\"ph56\"/> is provided in the <bpt id=\"p53\">**</bpt>Sample Scripts<ept id=\"p53\">**</ept><ph id=\"ph57\"/> folder. Modify the script to change the database name, if it is different from the default: <bpt id=\"p54\">**</bpt>TaxiNYC<ept id=\"p54\">**</ept>.",
      "nodes": [
        {
          "content": "In this section, we will perform data exploration and feature generation by running SQL queries directly in the <bpt id=\"p51\">**</bpt>SQL Server Management Studio<ept id=\"p51\">**</ept><ph id=\"ph55\"/> using the SQL Server database created earlier.",
          "pos": [
            0,
            246
          ]
        },
        {
          "content": "A sample script named <bpt id=\"p52\">**</bpt>sample\\_queries.sql<ept id=\"p52\">**</ept><ph id=\"ph56\"/> is provided in the <bpt id=\"p53\">**</bpt>Sample Scripts<ept id=\"p53\">**</ept><ph id=\"ph57\"/> folder.",
          "pos": [
            247,
            448
          ]
        },
        {
          "content": "Modify the script to change the database name, if it is different from the default: <bpt id=\"p54\">**</bpt>TaxiNYC<ept id=\"p54\">**</ept>.",
          "pos": [
            449,
            585
          ]
        }
      ]
    },
    {
      "pos": [
        13259,
        13285
      ],
      "content": "In this exercise, we will:"
    },
    {
      "pos": [
        13289,
        13433
      ],
      "content": "Connect to <bpt id=\"p55\">**</bpt>SQL Server Management Studio<ept id=\"p55\">**</ept><ph id=\"ph58\"/> using either Windows Authentication or using SQL Authentication and the SQL login name and password."
    },
    {
      "pos": [
        13436,
        13503
      ],
      "content": "Explore data distributions of a few fields in varying time windows."
    },
    {
      "pos": [
        13506,
        13568
      ],
      "content": "Investigate data quality of the longitude and latitude fields."
    },
    {
      "pos": [
        13571,
        13653
      ],
      "content": "Generate binary and multiclass classification labels based on the <bpt id=\"p56\">**</bpt>tip\\_amount<ept id=\"p56\">**</ept>."
    },
    {
      "pos": [
        13656,
        13709
      ],
      "content": "Generate features and compute/compare trip distances."
    },
    {
      "pos": [
        13712,
        13794
      ],
      "content": "Join the two tables and extract a random sample that will be used to build models."
    },
    {
      "pos": [
        13796,
        13868
      ],
      "content": "When you are ready to proceed to Azure Machine Learning, you may either:"
    },
    {
      "pos": [
        13875,
        14025
      ],
      "content": "Save the final SQL query to extract and sample the data and copy-paste the query directly into a [Reader][reader] module in Azure Machine Learning, or"
    },
    {
      "pos": [
        14029,
        14207
      ],
      "content": "Persist the sampled and engineered data you plan to use for model building in a new database table and use the new table in the [Reader][reader] module in Azure Machine Learning."
    },
    {
      "pos": [
        14209,
        14405
      ],
      "content": "In this section we will save the final query to extract and sample the data. The second method is demonstrated in the <bpt id=\"p57\">[</bpt>Data Exploration and Feature Engineering in IPython Notebook<ept id=\"p57\">](#ipnb)</ept><ph id=\"ph59\"/> section.",
      "nodes": [
        {
          "content": "In this section we will save the final query to extract and sample the data.",
          "pos": [
            0,
            76
          ]
        },
        {
          "content": "The second method is demonstrated in the <bpt id=\"p57\">[</bpt>Data Exploration and Feature Engineering in IPython Notebook<ept id=\"p57\">](#ipnb)</ept><ph id=\"ph59\"/> section.",
          "pos": [
            77,
            251
          ]
        }
      ]
    },
    {
      "pos": [
        14407,
        14525
      ],
      "content": "For a quick verification of the number of rows and columns in the tables populated earlier using parallel bulk import,"
    },
    {
      "pos": [
        14829,
        14872
      ],
      "content": "Exploration: Trip distribution by medallion"
    },
    {
      "pos": [
        14874,
        15199
      ],
      "content": "This example identifies the medallion (taxi numbers) with more than 100 trips within a given time period. The query would benefit from the partitioned table access since it is conditioned by the partition scheme of <bpt id=\"p58\">**</bpt>pickup\\_datetime<ept id=\"p58\">**</ept>. Querying the full dataset will also make use of the partitioned table and/or index scan.",
      "nodes": [
        {
          "content": "This example identifies the medallion (taxi numbers) with more than 100 trips within a given time period.",
          "pos": [
            0,
            105
          ]
        },
        {
          "content": "The query would benefit from the partitioned table access since it is conditioned by the partition scheme of <bpt id=\"p58\">**</bpt>pickup\\_datetime<ept id=\"p58\">**</ept>.",
          "pos": [
            106,
            276
          ]
        },
        {
          "content": "Querying the full dataset will also make use of the partitioned table and/or index scan.",
          "pos": [
            277,
            365
          ]
        }
      ]
    },
    {
      "pos": [
        15369,
        15429
      ],
      "content": "Exploration: Trip distribution by medallion and hack_license"
    },
    {
      "pos": [
        15627,
        15707
      ],
      "content": "Data Quality Assessment: Verify records with incorrect longitude and/or latitude"
    },
    {
      "pos": [
        15709,
        15888
      ],
      "content": "This example investigates if any of the longitude and/or latitude fields either contain an invalid value (radian degrees should be between -90 and 90), or have (0, 0) coordinates."
    },
    {
      "pos": [
        16379,
        16432
      ],
      "content": "Exploration: Tipped vs. Not Tipped Trips distribution"
    },
    {
      "pos": [
        16434,
        16689
      ],
      "content": "This example finds the number of trips that were tipped vs. not tipped in a given time period (or in the full dataset if covering the full year). This distribution reflects the binary label distribution to be later used for binary classification modeling.",
      "nodes": [
        {
          "content": "This example finds the number of trips that were tipped vs. not tipped in a given time period (or in the full dataset if covering the full year).",
          "pos": [
            0,
            145
          ]
        },
        {
          "content": "This distribution reflects the binary label distribution to be later used for binary classification modeling.",
          "pos": [
            146,
            255
          ]
        }
      ]
    },
    {
      "pos": [
        16934,
        16975
      ],
      "content": "Exploration: Tip Class/Range Distribution"
    },
    {
      "pos": [
        16977,
        17214
      ],
      "content": "This example computes the distribution of tip ranges in a given time period (or in the full dataset if covering the full year). This is the distribution of the label classes that will be used later for multiclass classification modeling.",
      "nodes": [
        {
          "content": "This example computes the distribution of tip ranges in a given time period (or in the full dataset if covering the full year).",
          "pos": [
            0,
            127
          ]
        },
        {
          "content": "This is the distribution of the label classes that will be used later for multiclass classification modeling.",
          "pos": [
            128,
            237
          ]
        }
      ]
    },
    {
      "pos": [
        17672,
        17718
      ],
      "content": "Exploration: Compute and Compare Trip Distance"
    },
    {
      "pos": [
        17720,
        18052
      ],
      "content": "This example converts the pickup and drop-off longitude and latitude to SQL geography points, computes the trip distance using SQL geography points difference, and returns a random sample of the results for comparison. The example limits the results to valid coordinates only using the data quality assessment query covered earlier.",
      "nodes": [
        {
          "content": "This example converts the pickup and drop-off longitude and latitude to SQL geography points, computes the trip distance using SQL geography points difference, and returns a random sample of the results for comparison.",
          "pos": [
            0,
            218
          ]
        },
        {
          "content": "The example limits the results to valid coordinates only using the data quality assessment query covered earlier.",
          "pos": [
            219,
            332
          ]
        }
      ]
    },
    {
      "pos": [
        18784,
        18818
      ],
      "content": "Feature Engineering in SQL Queries"
    },
    {
      "pos": [
        18820,
        19450
      ],
      "content": "The label generation and geography conversion exploration queries can also be used to generate labels/features by removing the counting part. Additional feature engineering SQL examples are provided in the <bpt id=\"p59\">[</bpt>Data Exploration and Feature Engineering in IPython Notebook<ept id=\"p59\">](#ipnb)</ept><ph id=\"ph60\"/> section. It is more efficient to run the feature generation queries on the full dataset or a large subset of it using SQL queries which run directly on the SQL Server database instance. The queries may be executed in <bpt id=\"p60\">**</bpt>SQL Server Management Studio<ept id=\"p60\">**</ept>, IPython Notebook or any development tool/environment which can access the database locally or remotely.",
      "nodes": [
        {
          "content": "The label generation and geography conversion exploration queries can also be used to generate labels/features by removing the counting part.",
          "pos": [
            0,
            141
          ]
        },
        {
          "content": "Additional feature engineering SQL examples are provided in the <bpt id=\"p59\">[</bpt>Data Exploration and Feature Engineering in IPython Notebook<ept id=\"p59\">](#ipnb)</ept><ph id=\"ph60\"/> section.",
          "pos": [
            142,
            339
          ]
        },
        {
          "content": "It is more efficient to run the feature generation queries on the full dataset or a large subset of it using SQL queries which run directly on the SQL Server database instance.",
          "pos": [
            340,
            516
          ]
        },
        {
          "content": "The queries may be executed in <bpt id=\"p60\">**</bpt>SQL Server Management Studio<ept id=\"p60\">**</ept>, IPython Notebook or any development tool/environment which can access the database locally or remotely.",
          "pos": [
            517,
            725
          ]
        }
      ]
    },
    {
      "pos": [
        19457,
        19490
      ],
      "content": "Preparing Data for Model Building"
    },
    {
      "pos": [
        19492,
        20007
      ],
      "content": "The following query joins the <bpt id=\"p61\">**</bpt>nyctaxi\\_trip<ept id=\"p61\">**</ept><ph id=\"ph61\"/> and <bpt id=\"p62\">**</bpt>nyctaxi\\_fare<ept id=\"p62\">**</ept><ph id=\"ph62\"/> tables, generates a binary classification label <bpt id=\"p63\">**</bpt>tipped<ept id=\"p63\">**</ept>, a multi-class classification label <bpt id=\"p64\">**</bpt>tip\\_class<ept id=\"p64\">**</ept>, and extracts a 1% random sample from the full joined dataset. This query can be copied then pasted directly in the <bpt id=\"p65\">[</bpt>Azure Machine Learning Studio<ept id=\"p65\">](https://studio.azureml.net)</ept><ph id=\"ph63\"/> [Reader][reader] module for direct data ingestion from the SQL Server database instance in Azure. The query excludes records with incorrect (0, 0) coordinates.",
      "nodes": [
        {
          "content": "The following query joins the <bpt id=\"p61\">**</bpt>nyctaxi\\_trip<ept id=\"p61\">**</ept><ph id=\"ph61\"/> and <bpt id=\"p62\">**</bpt>nyctaxi\\_fare<ept id=\"p62\">**</ept><ph id=\"ph62\"/> tables, generates a binary classification label <bpt id=\"p63\">**</bpt>tipped<ept id=\"p63\">**</ept>, a multi-class classification label <bpt id=\"p64\">**</bpt>tip\\_class<ept id=\"p64\">**</ept>, and extracts a 1% random sample from the full joined dataset.",
          "pos": [
            0,
            432
          ]
        },
        {
          "content": "This query can be copied then pasted directly in the <bpt id=\"p65\">[</bpt>Azure Machine Learning Studio<ept id=\"p65\">](https://studio.azureml.net)</ept><ph id=\"ph63\"/> [Reader][reader] module for direct data ingestion from the SQL Server database instance in Azure.",
          "pos": [
            433,
            698
          ]
        },
        {
          "content": "The query excludes records with incorrect (0, 0) coordinates.",
          "pos": [
            699,
            760
          ]
        }
      ]
    },
    {
      "pos": [
        20744,
        20804
      ],
      "content": "Data Exploration and Feature Engineering in IPython Notebook"
    },
    {
      "pos": [
        20806,
        21265
      ],
      "content": "In this section, we will perform data exploration and feature generation\nusing both Python and SQL queries against the SQL Server database created earlier. A sample IPython notebook named <bpt id=\"p66\">**</bpt>machine-Learning-data-science-process-sql-story.ipynb<ept id=\"p66\">**</ept><ph id=\"ph64\"/> is provided in the <bpt id=\"p67\">**</bpt>Sample IPython Notebooks<ept id=\"p67\">**</ept><ph id=\"ph65\"/> folder. This notebook is also available on <bpt id=\"p68\">[</bpt>GitHub<ept id=\"p68\">](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/DataScienceProcess/iPythonNotebooks)</ept>.",
      "nodes": [
        {
          "content": "In this section, we will perform data exploration and feature generation\nusing both Python and SQL queries against the SQL Server database created earlier.",
          "pos": [
            0,
            155
          ]
        },
        {
          "content": "A sample IPython notebook named <bpt id=\"p66\">**</bpt>machine-Learning-data-science-process-sql-story.ipynb<ept id=\"p66\">**</ept><ph id=\"ph64\"/> is provided in the <bpt id=\"p67\">**</bpt>Sample IPython Notebooks<ept id=\"p67\">**</ept><ph id=\"ph65\"/> folder.",
          "pos": [
            156,
            411
          ]
        },
        {
          "content": "This notebook is also available on <bpt id=\"p68\">[</bpt>GitHub<ept id=\"p68\">](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/DataScienceProcess/iPythonNotebooks)</ept>.",
          "pos": [
            412,
            609
          ]
        }
      ]
    },
    {
      "pos": [
        21267,
        21336
      ],
      "content": "The recommended sequence when working with big data is the following:"
    },
    {
      "pos": [
        21340,
        21404
      ],
      "content": "Read in a small sample of the data into an in-memory data frame."
    },
    {
      "pos": [
        21407,
        21475
      ],
      "content": "Perform some visualizations and explorations using the sampled data."
    },
    {
      "pos": [
        21478,
        21537
      ],
      "content": "Experiment with feature engineering using the sampled data."
    },
    {
      "pos": [
        21540,
        21701
      ],
      "content": "For larger data exploration, data manipulation and feature engineering, use Python to issue SQL Queries directly against the SQL Server database in the Azure VM."
    },
    {
      "pos": [
        21704,
        21776
      ],
      "content": "Decide the sample size to use for Azure Machine Learning model building."
    },
    {
      "pos": [
        21778,
        21842
      ],
      "content": "When ready to proceed to Azure Machine Learning, you may either:"
    },
    {
      "pos": [
        21849,
        22094
      ],
      "content": "Save the final SQL query to extract and sample the data and copy-paste the query directly into a [Reader][reader] module in Azure Machine Learning. This method is demonstrated in the <bpt id=\"p69\">[</bpt>Building Models in Azure Machine Learning<ept id=\"p69\">](#mlmodel)</ept><ph id=\"ph66\"/> section.",
      "nodes": [
        {
          "content": "Save the final SQL query to extract and sample the data and copy-paste the query directly into a [Reader][reader] module in Azure Machine Learning.",
          "pos": [
            0,
            147
          ]
        },
        {
          "content": "This method is demonstrated in the <bpt id=\"p69\">[</bpt>Building Models in Azure Machine Learning<ept id=\"p69\">](#mlmodel)</ept><ph id=\"ph66\"/> section.",
          "pos": [
            148,
            300
          ]
        }
      ]
    },
    {
      "pos": [
        22102,
        22256
      ],
      "content": "Persist the sampled and engineered data you plan to use for model building in a new database table, then use the new table in the [Reader][reader] module."
    },
    {
      "pos": [
        22258,
        22452
      ],
      "content": "The following are a few data exploration, data visualization, and feature engineering examples. For more examples, see the sample SQL IPython notebook in the <bpt id=\"p70\">**</bpt>Sample IPython Notebooks<ept id=\"p70\">**</ept><ph id=\"ph67\"/> folder.",
      "nodes": [
        {
          "content": "The following are a few data exploration, data visualization, and feature engineering examples.",
          "pos": [
            0,
            95
          ]
        },
        {
          "content": "For more examples, see the sample SQL IPython notebook in the <bpt id=\"p70\">**</bpt>Sample IPython Notebooks<ept id=\"p70\">**</ept><ph id=\"ph67\"/> folder.",
          "pos": [
            96,
            249
          ]
        }
      ]
    },
    {
      "pos": [
        22459,
        22490
      ],
      "content": "Initialize Database Credentials"
    },
    {
      "pos": [
        22492,
        22564
      ],
      "content": "Initialize your database connection settings in the following variables:"
    },
    {
      "pos": [
        22717,
        22743
      ],
      "content": "Create Database Connection"
    },
    {
      "pos": [
        22921,
        22976
      ],
      "content": "Report number of rows and columns in table nyctaxi_trip"
    },
    {
      "pos": [
        23382,
        23414
      ],
      "content": "Total number of rows = 173179759"
    },
    {
      "pos": [
        23419,
        23447
      ],
      "content": "Total number of columns = 14"
    },
    {
      "pos": [
        23454,
        23510
      ],
      "content": "Read-in a small data sample from the SQL Server Database"
    },
    {
      "pos": [
        24124,
        24226
      ],
      "content": "Time to read the sample table is 6.492000 seconds  \n<ph id=\"ph68\"/>Number of rows and columns retrieved = (84952, 21)"
    },
    {
      "pos": [
        24233,
        24255
      ],
      "content": "Descriptive Statistics"
    },
    {
      "pos": [
        24257,
        24399
      ],
      "content": "Now are ready to explore the sampled data. We start with\nlooking at descriptive statistics for the <bpt id=\"p71\">**</bpt>trip\\_distance<ept id=\"p71\">**</ept><ph id=\"ph69\"/> (or any other) field(s):",
      "nodes": [
        {
          "content": "Now are ready to explore the sampled data.",
          "pos": [
            0,
            42
          ]
        },
        {
          "content": "We start with\nlooking at descriptive statistics for the <bpt id=\"p71\">**</bpt>trip\\_distance<ept id=\"p71\">**</ept><ph id=\"ph69\"/> (or any other) field(s):",
          "pos": [
            43,
            197
          ]
        }
      ]
    },
    {
      "pos": [
        24443,
        24474
      ],
      "content": "Visualization: Box Plot Example"
    },
    {
      "pos": [
        24476,
        24553
      ],
      "content": "Next we look at the box plot for the trip distance to visualize the quantiles"
    },
    {
      "pos": [
        24615,
        24628
      ],
      "content": "<ph id=\"ph70\">![</ph>Plot #1<ph id=\"ph71\">][1]</ph>"
    },
    {
      "pos": [
        24635,
        24675
      ],
      "content": "Visualization: Distribution Plot Example"
    },
    {
      "pos": [
        24887,
        24900
      ],
      "content": "<ph id=\"ph72\">![</ph>Plot #2<ph id=\"ph73\">][2]</ph>"
    },
    {
      "pos": [
        24907,
        24940
      ],
      "content": "Visualization: Bar and Line Plots"
    },
    {
      "pos": [
        24942,
        25033
      ],
      "content": "In this example, we bin the trip distance into five bins and visualize the binning results."
    },
    {
      "pos": [
        25194,
        25263
      ],
      "content": "We can plot the above bin distribution in a bar or line plot as below"
    },
    {
      "pos": [
        25330,
        25343
      ],
      "content": "<ph id=\"ph74\">![</ph>Plot #3<ph id=\"ph75\">][3]</ph>"
    },
    {
      "pos": [
        25411,
        25424
      ],
      "content": "<ph id=\"ph76\">![</ph>Plot #4<ph id=\"ph77\">][4]</ph>"
    },
    {
      "pos": [
        25431,
        25465
      ],
      "content": "Visualization: Scatterplot Example"
    },
    {
      "pos": [
        25467,
        25578
      ],
      "content": "We show scatter plot between <bpt id=\"p72\">**</bpt>trip\\_time\\_in\\_secs<ept id=\"p72\">**</ept><ph id=\"ph78\"/> and <bpt id=\"p73\">**</bpt>trip\\_distance<ept id=\"p73\">**</ept><ph id=\"ph79\"/> to see if there\nis any correlation"
    },
    {
      "pos": [
        25645,
        25658
      ],
      "content": "<ph id=\"ph80\">![</ph>Plot #6<ph id=\"ph81\">][6]</ph>"
    },
    {
      "pos": [
        25660,
        25746
      ],
      "content": "Similarly we can check the relationship between <bpt id=\"p74\">**</bpt>rate\\_code<ept id=\"p74\">**</ept><ph id=\"ph82\"/> and <bpt id=\"p75\">**</bpt>trip\\_distance<ept id=\"p75\">**</ept>."
    },
    {
      "pos": [
        25811,
        25824
      ],
      "content": "<ph id=\"ph83\">![</ph>Plot #8<ph id=\"ph84\">][8]</ph>"
    },
    {
      "pos": [
        25830,
        25858
      ],
      "content": "Sub-Sampling the Data in SQL"
    },
    {
      "pos": [
        25860,
        26210
      ],
      "content": "When preparing data for model building in <bpt id=\"p76\">[</bpt>Azure Machine Learning Studio<ept id=\"p76\">](https://studio.azureml.net)</ept>, you may either decide on the <bpt id=\"p77\">**</bpt>SQL query to use directly in the Reader module<ept id=\"p77\">**</ept><ph id=\"ph85\"/> or persist the engineered and sampled data in a new table, which you could use in the [Reader][reader] module with a simple <bpt id=\"p78\">**</bpt>SELECT * FROM &lt;your\\_new\\_table\\_name&gt;<ept id=\"p78\">**</ept>."
    },
    {
      "pos": [
        26212,
        26444
      ],
      "content": "In this section we will create a new table to hold the sampled and engineered data. An example of a direct SQL query for model building is provided in the <bpt id=\"p79\">[</bpt>Data Exploration and Feature Engineering in SQL Server<ept id=\"p79\">](#dbexplore)</ept><ph id=\"ph86\"/> section.",
      "nodes": [
        {
          "content": "In this section we will create a new table to hold the sampled and engineered data.",
          "pos": [
            0,
            83
          ]
        },
        {
          "content": "An example of a direct SQL query for model building is provided in the <bpt id=\"p79\">[</bpt>Data Exploration and Feature Engineering in SQL Server<ept id=\"p79\">](#dbexplore)</ept><ph id=\"ph86\"/> section.",
          "pos": [
            84,
            287
          ]
        }
      ]
    },
    {
      "pos": [
        26451,
        26546
      ],
      "content": "Create a Sample Table and Populate with 1% of the Joined Tables. Drop Table First if it Exists.",
      "nodes": [
        {
          "content": "Create a Sample Table and Populate with 1% of the Joined Tables.",
          "pos": [
            0,
            64
          ]
        },
        {
          "content": "Drop Table First if it Exists.",
          "pos": [
            65,
            95
          ]
        }
      ]
    },
    {
      "pos": [
        26548,
        26728
      ],
      "content": "In this section, we join the tables <bpt id=\"p80\">**</bpt>nyctaxi\\_trip<ept id=\"p80\">**</ept><ph id=\"ph87\"/> and <bpt id=\"p81\">**</bpt>nyctaxi\\_fare<ept id=\"p81\">**</ept>, extract a 1% random sample, and persist the sampled data in a new table name <bpt id=\"p82\">**</bpt>nyctaxi\\_one\\_percent<ept id=\"p82\">**</ept>:"
    },
    {
      "pos": [
        27483,
        27537
      ],
      "content": "Data Exploration using SQL Queries in IPython Notebook"
    },
    {
      "pos": [
        27539,
        27998
      ],
      "content": "In this section, we explore data distributions using the 1% sampled data which is persisted in the new table we created above. Note that similar explorations can be performed using the original tables, optionally using <bpt id=\"p83\">**</bpt>TABLESAMPLE<ept id=\"p83\">**</ept><ph id=\"ph88\"/> to limit the exploration sample or by limiting the results to a given time period using the <bpt id=\"p84\">**</bpt>pickup\\_datetime<ept id=\"p84\">**</ept><ph id=\"ph89\"/> partitions, as illustrated in the <bpt id=\"p85\">[</bpt>Data Exploration and Feature Engineering in SQL Server<ept id=\"p85\">](#dbexplore)</ept><ph id=\"ph90\"/> section.",
      "nodes": [
        {
          "content": "In this section, we explore data distributions using the 1% sampled data which is persisted in the new table we created above.",
          "pos": [
            0,
            126
          ]
        },
        {
          "content": "Note that similar explorations can be performed using the original tables, optionally using <bpt id=\"p83\">**</bpt>TABLESAMPLE<ept id=\"p83\">**</ept><ph id=\"ph88\"/> to limit the exploration sample or by limiting the results to a given time period using the <bpt id=\"p84\">**</bpt>pickup\\_datetime<ept id=\"p84\">**</ept><ph id=\"ph89\"/> partitions, as illustrated in the <bpt id=\"p85\">[</bpt>Data Exploration and Feature Engineering in SQL Server<ept id=\"p85\">](#dbexplore)</ept><ph id=\"ph90\"/> section.",
          "pos": [
            127,
            624
          ]
        }
      ]
    },
    {
      "pos": [
        28005,
        28045
      ],
      "content": "Exploration: Daily distribution of trips"
    },
    {
      "pos": [
        28258,
        28302
      ],
      "content": "Exploration: Trip distribution per medallion"
    },
    {
      "pos": [
        28461,
        28517
      ],
      "content": "Feature Generation Using SQL Queries in IPython Notebook"
    },
    {
      "pos": [
        28519,
        28672
      ],
      "content": "In this section we will generate new labels and features directly using SQL queries, operating on the 1% sample table we created in the previous section."
    },
    {
      "pos": [
        28679,
        28718
      ],
      "content": "Label Generation: Generate Class Labels"
    },
    {
      "pos": [
        28720,
        28797
      ],
      "content": "In the following example, we generate two sets of labels to use for modeling:"
    },
    {
      "pos": [
        28802,
        28868
      ],
      "content": "Binary Class Labels <bpt id=\"p86\">**</bpt>tipped<ept id=\"p86\">**</ept><ph id=\"ph91\"/> (predicting if a tip will be given)"
    },
    {
      "pos": [
        28872,
        28938
      ],
      "content": "Multiclass Labels <bpt id=\"p87\">**</bpt>tip\\_class<ept id=\"p87\">**</ept><ph id=\"ph92\"/> (predicting the tip bin or range)"
    },
    {
      "pos": [
        29792,
        29851
      ],
      "content": "Feature Engineering: Count Features for Categorical Columns"
    },
    {
      "pos": [
        29853,
        29991
      ],
      "content": "This example transforms a categorical field into a numeric field by replacing each category with the count of its occurrences in the data."
    },
    {
      "pos": [
        30910,
        30965
      ],
      "content": "Feature Engineering: Bin features for Numerical Columns"
    },
    {
      "pos": [
        30967,
        31102
      ],
      "content": "This example transforms a continuous numeric field into preset category ranges, i.e., transform numeric field into a categorical field."
    },
    {
      "pos": [
        31948,
        32026
      ],
      "content": "Feature Engineering: Extract Location Features from Decimal Latitude/Longitude"
    },
    {
      "pos": [
        32028,
        32404
      ],
      "content": "This example breaks down the decimal representation of a latitude and/or longitude field into multiple region fields of different granularity, such as, country, city, town, block, etc. Note that the new geo-fields are not mapped to actual locations. For information on mapping geocode locations, see <bpt id=\"p88\">[</bpt>Bing Maps REST Services<ept id=\"p88\">](https://msdn.microsoft.com/library/ff701710.aspx)</ept>.",
      "nodes": [
        {
          "content": "This example breaks down the decimal representation of a latitude and/or longitude field into multiple region fields of different granularity, such as, country, city, town, block, etc. Note that the new geo-fields are not mapped to actual locations.",
          "pos": [
            0,
            249
          ]
        },
        {
          "content": "For information on mapping geocode locations, see <bpt id=\"p88\">[</bpt>Bing Maps REST Services<ept id=\"p88\">](https://msdn.microsoft.com/library/ff701710.aspx)</ept>.",
          "pos": [
            250,
            416
          ]
        }
      ]
    },
    {
      "pos": [
        34262,
        34307
      ],
      "content": "Verify the final form of the featurized table"
    },
    {
      "pos": [
        34398,
        34602
      ],
      "content": "We are now ready to proceed to model building and model deployment in <bpt id=\"p89\">[</bpt>Azure Machine Learning<ept id=\"p89\">](https://studio.azureml.net)</ept>. The data is ready for any of the prediction problems identified earlier, namely:",
      "nodes": [
        {
          "content": "We are now ready to proceed to model building and model deployment in <bpt id=\"p89\">[</bpt>Azure Machine Learning<ept id=\"p89\">](https://studio.azureml.net)</ept>.",
          "pos": [
            0,
            163
          ]
        },
        {
          "content": "The data is ready for any of the prediction problems identified earlier, namely:",
          "pos": [
            164,
            244
          ]
        }
      ]
    },
    {
      "pos": [
        34607,
        34682
      ],
      "content": "Binary classification: To predict whether or not a tip was paid for a trip."
    },
    {
      "pos": [
        34687,
        34792
      ],
      "content": "Multiclass classification: To predict the range of tip paid, according to the previously defined classes."
    },
    {
      "pos": [
        34797,
        34859
      ],
      "content": "Regression task: To predict the amount of tip paid for a trip."
    },
    {
      "pos": [
        34889,
        34930
      ],
      "content": "Building Models in Azure Machine Learning"
    },
    {
      "pos": [
        34932,
        35144
      ],
      "content": "To begin the modeling exercise, log in to your Azure Machine Learning workspace. If you have not yet created a machine learning workspace, see <bpt id=\"p90\">[</bpt>Create an Azure ML workspace<ept id=\"p90\">](machine-learning-create-workspace.md)</ept>.",
      "nodes": [
        {
          "content": "To begin the modeling exercise, log in to your Azure Machine Learning workspace.",
          "pos": [
            0,
            80
          ]
        },
        {
          "content": "If you have not yet created a machine learning workspace, see <bpt id=\"p90\">[</bpt>Create an Azure ML workspace<ept id=\"p90\">](machine-learning-create-workspace.md)</ept>.",
          "pos": [
            81,
            252
          ]
        }
      ]
    },
    {
      "pos": [
        35149,
        35276
      ],
      "content": "To get started with Azure Machine Learning, see <bpt id=\"p91\">[</bpt>What is Azure Machine Learning Studio?<ept id=\"p91\">](machine-learning-what-is-ml-studio.md)</ept>"
    },
    {
      "pos": [
        35281,
        35351
      ],
      "content": "Log in to <bpt id=\"p92\">[</bpt>Azure Machine Learning Studio<ept id=\"p92\">](https://studio.azureml.net)</ept>."
    },
    {
      "pos": [
        35356,
        35663
      ],
      "content": "The Studio Home page provides a wealth of information, videos, tutorials, links to the Modules Reference, and other resources. Fore more information about Azure Machine Learning, consult the <bpt id=\"p93\">[</bpt>Azure Machine Learning Documentation Center<ept id=\"p93\">](https://azure.microsoft.com/documentation/services/machine-learning/)</ept>.",
      "nodes": [
        {
          "content": "The Studio Home page provides a wealth of information, videos, tutorials, links to the Modules Reference, and other resources.",
          "pos": [
            0,
            126
          ]
        },
        {
          "content": "Fore more information about Azure Machine Learning, consult the <bpt id=\"p93\">[</bpt>Azure Machine Learning Documentation Center<ept id=\"p93\">](https://azure.microsoft.com/documentation/services/machine-learning/)</ept>.",
          "pos": [
            127,
            347
          ]
        }
      ]
    },
    {
      "pos": [
        35665,
        35721
      ],
      "content": "A typical training experiment consists of the following:"
    },
    {
      "pos": [
        35726,
        35755
      ],
      "content": "Create a <bpt id=\"p94\">**</bpt>+NEW<ept id=\"p94\">**</ept><ph id=\"ph93\"/> experiment."
    },
    {
      "pos": [
        35759,
        35784
      ],
      "content": "Get the data to Azure ML."
    },
    {
      "pos": [
        35788,
        35845
      ],
      "content": "Pre-process, transform and manipulate the data as needed."
    },
    {
      "pos": [
        35849,
        35877
      ],
      "content": "Generate features as needed."
    },
    {
      "pos": [
        35881,
        35974
      ],
      "content": "Split the data into training/validation/testing datasets(or have separate datasets for each)."
    },
    {
      "pos": [
        35978,
        36136
      ],
      "content": "Select one or more machine learning algorithms depending on the learning problem to solve. E.g., binary classification, multiclass classification, regression.",
      "nodes": [
        {
          "content": "Select one or more machine learning algorithms depending on the learning problem to solve.",
          "pos": [
            0,
            90
          ]
        },
        {
          "content": "E.g., binary classification, multiclass classification, regression.",
          "pos": [
            91,
            158
          ]
        }
      ]
    },
    {
      "pos": [
        36140,
        36192
      ],
      "content": "Train one or more models using the training dataset."
    },
    {
      "pos": [
        36196,
        36252
      ],
      "content": "Score the validation dataset using the trained model(s)."
    },
    {
      "pos": [
        36256,
        36335
      ],
      "content": "Evaluate the model(s) to compute the relevant metrics for the learning problem."
    },
    {
      "pos": [
        36340,
        36399
      ],
      "content": "Fine tune the model(s) and select the best model to deploy."
    },
    {
      "pos": [
        36401,
        36594
      ],
      "content": "In this exercise, we have already explored and engineered the data in SQL Server, and decided on the sample size to ingest in Azure ML. To build one or more of the prediction models we decided:",
      "nodes": [
        {
          "content": "In this exercise, we have already explored and engineered the data in SQL Server, and decided on the sample size to ingest in Azure ML.",
          "pos": [
            0,
            135
          ]
        },
        {
          "content": "To build one or more of the prediction models we decided:",
          "pos": [
            136,
            193
          ]
        }
      ]
    },
    {
      "pos": [
        36599,
        36780
      ],
      "content": "Get the data to Azure ML using the [Reader][reader] module, available in the <bpt id=\"p95\">**</bpt>Data Input and Output<ept id=\"p95\">**</ept><ph id=\"ph94\"/> section. For more information, see the [Reader][reader] module reference page.",
      "nodes": [
        {
          "content": "Get the data to Azure ML using the [Reader][reader] module, available in the <bpt id=\"p95\">**</bpt>Data Input and Output<ept id=\"p95\">**</ept><ph id=\"ph94\"/> section.",
          "pos": [
            0,
            166
          ]
        },
        {
          "content": "For more information, see the [Reader][reader] module reference page.",
          "pos": [
            167,
            236
          ]
        }
      ]
    },
    {
      "pos": [
        36786,
        36808
      ],
      "content": "<ph id=\"ph95\">![</ph>Azure ML Reader<ph id=\"ph96\">][17]</ph>"
    },
    {
      "pos": [
        36813,
        36894
      ],
      "content": "Select <bpt id=\"p96\">**</bpt>Azure SQL Database<ept id=\"p96\">**</ept><ph id=\"ph97\"/> as the <bpt id=\"p97\">**</bpt>Data source<ept id=\"p97\">**</ept><ph id=\"ph98\"/> in the <bpt id=\"p98\">**</bpt>Properties<ept id=\"p98\">**</ept><ph id=\"ph99\"/> panel."
    },
    {
      "pos": [
        36899,
        37016
      ],
      "content": "Enter the database DNS name in the <bpt id=\"p99\">**</bpt>Database server name<ept id=\"p99\">**</ept><ph id=\"ph100\"/> field. Format: <ph id=\"ph101\">`tcp:&lt;your_virtual_machine_DNS_name&gt;,1433`</ph>",
      "nodes": [
        {
          "content": "Enter the database DNS name in the <bpt id=\"p99\">**</bpt>Database server name<ept id=\"p99\">**</ept><ph id=\"ph100\"/> field.",
          "pos": [
            0,
            122
          ]
        },
        {
          "content": "Format: <ph id=\"ph101\">`tcp:&lt;your_virtual_machine_DNS_name&gt;,1433`</ph>",
          "pos": [
            123,
            199
          ]
        }
      ]
    },
    {
      "pos": [
        37021,
        37076
      ],
      "content": "Enter the <bpt id=\"p100\">**</bpt>Database name<ept id=\"p100\">**</ept><ph id=\"ph102\"/> in the corresponding field."
    },
    {
      "pos": [
        37081,
        37202
      ],
      "content": "Enter the <bpt id=\"p101\">**</bpt>SQL user name<ept id=\"p101\">**</ept><ph id=\"ph103\"/> in the **Server user aqccount name, and the password in the <bpt id=\"p102\">**</bpt>Server user account password<ept id=\"p102\">**</ept>."
    },
    {
      "pos": [
        37207,
        37254
      ],
      "content": "Check <bpt id=\"p103\">**</bpt>Accept any server certificate<ept id=\"p103\">**</ept><ph id=\"ph104\"/> option."
    },
    {
      "pos": [
        37259,
        37466
      ],
      "content": "In the <bpt id=\"p104\">**</bpt>Database query<ept id=\"p104\">**</ept><ph id=\"ph105\"/> edit text area, paste the query which extracts the necessary database fields (including any computed fields such as the labels) and down samples the data to the desired sample size."
    },
    {
      "pos": [
        37468,
        37685
      ],
      "content": "An example of a binary classification experiment reading data directly from the SQL Server database is in the figure below. Similar experiments can be constructed for multiclass classification and regression problems.",
      "nodes": [
        {
          "content": "An example of a binary classification experiment reading data directly from the SQL Server database is in the figure below.",
          "pos": [
            0,
            123
          ]
        },
        {
          "content": "Similar experiments can be constructed for multiclass classification and regression problems.",
          "pos": [
            124,
            217
          ]
        }
      ]
    },
    {
      "pos": [
        37687,
        37708
      ],
      "content": "<ph id=\"ph106\">![</ph>Azure ML Train<ph id=\"ph107\">][10]</ph>"
    },
    {
      "pos": [
        37712,
        38266
      ],
      "content": "<ph id=\"ph108\">[AZURE.IMPORTANT]</ph><ph id=\"ph109\"/> In the modeling data extraction and sampling query examples provided in previous sections, <bpt id=\"p105\">**</bpt>all labels for the three modeling exercises are included in the query<ept id=\"p105\">**</ept>. An important (required) step in each of the modeling exercises is to <bpt id=\"p106\">**</bpt>exclude<ept id=\"p106\">**</ept><ph id=\"ph110\"/> the unnecessary labels for the other two problems, and any other <bpt id=\"p107\">**</bpt>target leaks<ept id=\"p107\">**</ept>. For e.g., when using binary classification, use the label <bpt id=\"p108\">**</bpt>tipped<ept id=\"p108\">**</ept><ph id=\"ph111\"/> and exclude the fields <bpt id=\"p109\">**</bpt>tip\\_class<ept id=\"p109\">**</ept>, <bpt id=\"p110\">**</bpt>tip\\_amount<ept id=\"p110\">**</ept>, and <bpt id=\"p111\">**</bpt>total\\_amount<ept id=\"p111\">**</ept>. The latter are target leaks since they imply the tip paid.",
      "nodes": [
        {
          "content": "<ph id=\"ph108\">[AZURE.IMPORTANT]</ph><ph id=\"ph109\"/> In the modeling data extraction and sampling query examples provided in previous sections, <bpt id=\"p105\">**</bpt>all labels for the three modeling exercises are included in the query<ept id=\"p105\">**</ept>.",
          "pos": [
            0,
            261
          ]
        },
        {
          "content": "An important (required) step in each of the modeling exercises is to <bpt id=\"p106\">**</bpt>exclude<ept id=\"p106\">**</ept><ph id=\"ph110\"/> the unnecessary labels for the other two problems, and any other <bpt id=\"p107\">**</bpt>target leaks<ept id=\"p107\">**</ept>.",
          "pos": [
            262,
            525
          ]
        },
        {
          "content": "For e.g., when using binary classification, use the label <bpt id=\"p108\">**</bpt>tipped<ept id=\"p108\">**</ept><ph id=\"ph111\"/> and exclude the fields <bpt id=\"p109\">**</bpt>tip\\_class<ept id=\"p109\">**</ept>, <bpt id=\"p110\">**</bpt>tip\\_amount<ept id=\"p110\">**</ept>, and <bpt id=\"p111\">**</bpt>total\\_amount<ept id=\"p111\">**</ept>.",
          "pos": [
            526,
            857
          ]
        },
        {
          "content": "The latter are target leaks since they imply the tip paid.",
          "pos": [
            858,
            916
          ]
        }
      ]
    },
    {
      "pos": [
        38271,
        38540
      ],
      "content": "To exclude unnecessary columns and/or target leaks, you may use the [Project Columns][project-columns] module or the [Metadata Editor][metadata-editor]. For more information, see [Project Columns][project-columns] and [Metadata Editor][metadata-editor] reference pages.",
      "nodes": [
        {
          "content": "To exclude unnecessary columns and/or target leaks, you may use the [Project Columns][project-columns] module or the [Metadata Editor][metadata-editor].",
          "pos": [
            0,
            152
          ]
        },
        {
          "content": "For more information, see [Project Columns][project-columns] and [Metadata Editor][metadata-editor] reference pages.",
          "pos": [
            153,
            269
          ]
        }
      ]
    },
    {
      "pos": [
        38568,
        38610
      ],
      "content": "Deploying Models in Azure Machine Learning"
    },
    {
      "pos": [
        38612,
        38881
      ],
      "content": "When your model is ready, you can easily deploy it as a web service directly from the experiment. For more information about deploying Azure ML web services, see <bpt id=\"p112\">[</bpt>Deploy an Azure Machine Learning web service<ept id=\"p112\">](machine-learning-publish-a-machine-learning-web-service.md)</ept>.",
      "nodes": [
        {
          "content": "When your model is ready, you can easily deploy it as a web service directly from the experiment.",
          "pos": [
            0,
            97
          ]
        },
        {
          "content": "For more information about deploying Azure ML web services, see <bpt id=\"p112\">[</bpt>Deploy an Azure Machine Learning web service<ept id=\"p112\">](machine-learning-publish-a-machine-learning-web-service.md)</ept>.",
          "pos": [
            98,
            311
          ]
        }
      ]
    },
    {
      "pos": [
        38883,
        38924
      ],
      "content": "To deploy a new web service, you need to:"
    },
    {
      "pos": [
        38929,
        38957
      ],
      "content": "Create a scoring experiment."
    },
    {
      "pos": [
        38961,
        38984
      ],
      "content": "Deploy the web service."
    },
    {
      "pos": [
        38986,
        39118
      ],
      "content": "To create a scoring experiment from a <bpt id=\"p113\">**</bpt>Finished<ept id=\"p113\">**</ept><ph id=\"ph112\"/> training experiment, click <bpt id=\"p114\">**</bpt>CREATE SCORING EXPERIMENT<ept id=\"p114\">**</ept><ph id=\"ph113\"/> in the lower action bar."
    },
    {
      "pos": [
        39120,
        39140
      ],
      "content": "<ph id=\"ph114\">![</ph>Azure Scoring<ph id=\"ph115\">][18]</ph>"
    },
    {
      "pos": [
        39142,
        39284
      ],
      "content": "Azure Machine Learning will attempt to create a scoring experiment based on the components of the training experiment. In particular, it will:",
      "nodes": [
        {
          "content": "Azure Machine Learning will attempt to create a scoring experiment based on the components of the training experiment.",
          "pos": [
            0,
            118
          ]
        },
        {
          "content": "In particular, it will:",
          "pos": [
            119,
            142
          ]
        }
      ]
    },
    {
      "pos": [
        39289,
        39350
      ],
      "content": "Save the trained model and remove the model training modules."
    },
    {
      "pos": [
        39354,
        39432
      ],
      "content": "Identify a logical <bpt id=\"p115\">**</bpt>input port<ept id=\"p115\">**</ept><ph id=\"ph116\"/> to represent the expected input data schema."
    },
    {
      "pos": [
        39436,
        39523
      ],
      "content": "Identify a logical <bpt id=\"p116\">**</bpt>output port<ept id=\"p116\">**</ept><ph id=\"ph117\"/> to represent the expected web service output schema."
    },
    {
      "pos": [
        39525,
        40096
      ],
      "content": "When the scoring experiment is created, review it and adjust as needed. A typical adjustment is to replace the input dataset and/or query with one which excludes label fields, as these will not be available when the service is called. It is also a good practice to reduce the size of the input dataset and/or query to a few records, just enough to indicate the input schema. For the output port, it is common to exclude all input fields and only include the <bpt id=\"p117\">**</bpt>Scored Labels<ept id=\"p117\">**</ept><ph id=\"ph118\"/> and <bpt id=\"p118\">**</bpt>Scored Probabilities<ept id=\"p118\">**</ept><ph id=\"ph119\"/> in the output using the [Project Columns][project-columns] module.",
      "nodes": [
        {
          "content": "When the scoring experiment is created, review it and adjust as needed.",
          "pos": [
            0,
            71
          ]
        },
        {
          "content": "A typical adjustment is to replace the input dataset and/or query with one which excludes label fields, as these will not be available when the service is called.",
          "pos": [
            72,
            234
          ]
        },
        {
          "content": "It is also a good practice to reduce the size of the input dataset and/or query to a few records, just enough to indicate the input schema.",
          "pos": [
            235,
            374
          ]
        },
        {
          "content": "For the output port, it is common to exclude all input fields and only include the <bpt id=\"p117\">**</bpt>Scored Labels<ept id=\"p117\">**</ept><ph id=\"ph118\"/> and <bpt id=\"p118\">**</bpt>Scored Probabilities<ept id=\"p118\">**</ept><ph id=\"ph119\"/> in the output using the [Project Columns][project-columns] module.",
          "pos": [
            375,
            687
          ]
        }
      ]
    },
    {
      "pos": [
        40098,
        40237
      ],
      "content": "A sample scoring experiment is in the figure below. When ready to deploy, click the <bpt id=\"p119\">**</bpt>PUBLISH WEB SERVICE<ept id=\"p119\">**</ept><ph id=\"ph120\"/> button in the lower action bar.",
      "nodes": [
        {
          "content": "A sample scoring experiment is in the figure below.",
          "pos": [
            0,
            51
          ]
        },
        {
          "content": "When ready to deploy, click the <bpt id=\"p119\">**</bpt>PUBLISH WEB SERVICE<ept id=\"p119\">**</ept><ph id=\"ph120\"/> button in the lower action bar.",
          "pos": [
            52,
            197
          ]
        }
      ]
    },
    {
      "pos": [
        40239,
        40262
      ],
      "content": "<ph id=\"ph121\">![</ph>Azure ML Publish<ph id=\"ph122\">][11]</ph>"
    },
    {
      "pos": [
        40264,
        40498
      ],
      "content": "To recap, in this walkthrough tutorial, you have created an Azure data science environment, worked with a large public dataset all the way from data acquisition to model training and deploying of an Azure Machine Learning web service."
    },
    {
      "pos": [
        40504,
        40523
      ],
      "content": "License Information"
    },
    {
      "pos": [
        40525,
        40749
      ],
      "content": "This sample walkthrough and its accompanying scripts and IPython notebook(s) are shared by Microsoft under the MIT license. Please check the LICENSE.txt file in in the directory of the sample code on GitHub for more details.",
      "nodes": [
        {
          "content": "This sample walkthrough and its accompanying scripts and IPython notebook(s) are shared by Microsoft under the MIT license.",
          "pos": [
            0,
            123
          ]
        },
        {
          "content": "Please check the LICENSE.txt file in in the directory of the sample code on GitHub for more details.",
          "pos": [
            124,
            224
          ]
        }
      ]
    },
    {
      "pos": [
        40755,
        40765
      ],
      "content": "References"
    },
    {
      "pos": [
        40767,
        41080
      ],
      "content": "•   <bpt id=\"p120\">[</bpt>Andrés Monroy NYC Taxi Trips Download Page<ept id=\"p120\">](http://www.andresmh.com/nyctaxitrips/)</ept><ph id=\"ph123\"/>  \n•   <bpt id=\"p121\">[</bpt>FOILing NYC’s Taxi Trip Data by Chris Whong<ept id=\"p121\">](http://chriswhong.com/open-data/foil_nyc_taxi/)</ept><ph id=\"ph124\"/>   \n•   <bpt id=\"p122\">[</bpt>NYC Taxi and Limousine Commission Research and Statistics<ept id=\"p122\">](https://www1.nyc.gov/html/tlc/html/about/statistics.shtml)</ept>"
    },
    {
      "pos": [
        42709,
        42993
      ],
      "content": "[metadata-editor]: https://msdn.microsoft.com/library/azure/370b6676-c11c-486f-bf73-35349f842a66/\n[project-columns]: https://msdn.microsoft.com/library/azure/1ec722fa-b623-4e26-a44e-a50c6d726223/\n[reader]: https://msdn.microsoft.com/library/azure/4e1b0fe6-aded-4b3f-a36f-39b8862b9004/"
    }
  ],
  "content": "<properties\n    pageTitle=\"The Cortana Analytics Process in action: using SQL Server | Microsoft Azure\"\n    description=\"Advanced Analytics Process and Technology in Action\"  \n    services=\"machine-learning\"\n    documentationCenter=\"\"\n    authors=\"msolhab\"\n    manager=\"paulettm\"\n    editor=\"cgronlun\" />\n\n<tags\n    ms.service=\"machine-learning\"\n    ms.workload=\"data-services\"\n    ms.tgt_pltfrm=\"na\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\"\n    ms.date=\"02/08/2016\" \n    ms.author=\"mohabib;fashah;bradsev\"/>\n\n\n# The Cortana Analytics Process in action: using SQL Server\n\nIn this tutorial, you walkthrough building and deploying a model using a publicly available dataset -- the [NYC Taxi Trips](http://www.andresmh.com/nyctaxitrips/) dataset. The procedure follows the Cortana Analytics Process (CAP) workflow.\n\n\n## <a name=\"dataset\"></a>NYC Taxi Trips Dataset Description\n\nThe NYC Taxi Trip data is about 20GB of compressed CSV files (~48GB uncompressed), comprising more than 173 million individual trips and the fares paid for each trip. Each trip record includes the pickup and drop-off location and time, anonymized hack (driver's) license number and medallion (taxi’s unique id) number. The data covers all trips in the year 2013 and is provided in the following two datasets for each month:\n\n1. The 'trip_data' CSV contains trip details, such as number of passengers, pickup and dropoff points, trip duration, and trip length. Here are a few sample records:\n\n        medallion,hack_license,vendor_id,rate_code,store_and_fwd_flag,pickup_datetime,dropoff_datetime,passenger_count,trip_time_in_secs,trip_distance,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude\n        89D227B655E5C82AECF13C3F540D4CF4,BA96DE419E711691B9445D6A6307C170,CMT,1,N,2013-01-01 15:11:48,2013-01-01 15:18:10,4,382,1.00,-73.978165,40.757977,-73.989838,40.751171\n        0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,1,N,2013-01-06 00:18:35,2013-01-06 00:22:54,1,259,1.50,-74.006683,40.731781,-73.994499,40.75066\n        0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,1,N,2013-01-05 18:49:41,2013-01-05 18:54:23,1,282,1.10,-74.004707,40.73777,-74.009834,40.726002\n        DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,1,N,2013-01-07 23:54:15,2013-01-07 23:58:20,2,244,.70,-73.974602,40.759945,-73.984734,40.759388\n        DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,1,N,2013-01-07 23:25:03,2013-01-07 23:34:24,1,560,2.10,-73.97625,40.748528,-74.002586,40.747868\n\n2. The 'trip_fare' CSV contains details of the fare paid for each trip, such as payment type, fare amount, surcharge and taxes, tips and tolls, and the total amount paid. Here are a few sample records:\n\n        medallion, hack_license, vendor_id, pickup_datetime, payment_type, fare_amount, surcharge, mta_tax, tip_amount, tolls_amount, total_amount\n        89D227B655E5C82AECF13C3F540D4CF4,BA96DE419E711691B9445D6A6307C170,CMT,2013-01-01 15:11:48,CSH,6.5,0,0.5,0,0,7\n        0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,2013-01-06 00:18:35,CSH,6,0.5,0.5,0,0,7\n        0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,2013-01-05 18:49:41,CSH,5.5,1,0.5,0,0,7\n        DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,2013-01-07 23:54:15,CSH,5,0.5,0.5,0,0,6\n        DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,2013-01-07 23:25:03,CSH,9.5,0.5,0.5,0,0,10.5\n\nThe unique key to join trip\\_data and trip\\_fare is composed of the fields: medallion, hack\\_licence and pickup\\_datetime.\n\n## <a name=\"mltasks\"></a>Examples of Prediction Tasks\n\nWe will formulate three prediction problems based on the *tip\\_amount*, namely:\n\n1. Binary classification: Predict whether or not a tip was paid for a trip, i.e. a *tip\\_amount* that is greater than $0 is a positive example, while a *tip\\_amount* of $0 is a negative example.\n\n2. Multiclass classification: To predict the range of tip paid for the trip. We divide the *tip\\_amount* into five bins or classes:\n\n        Class 0 : tip_amount = $0\n        Class 1 : tip_amount > $0 and tip_amount <= $5\n        Class 2 : tip_amount > $5 and tip_amount <= $10\n        Class 3 : tip_amount > $10 and tip_amount <= $20\n        Class 4 : tip_amount > $20\n\n3. Regression task: To predict the amount of tip paid for a trip.  \n\n\n## <a name=\"setup\"></a>Setting Up the Azure data science environment for advanced analytics\n\nAs you can see from the [Plan Your Environment](machine-learning-data-science-plan-your-environment.md) guide, there are several options to work with the NYC Taxi Trips dataset in Azure:\n\n- Work with the data in Azure blobs then model in Azure Machine Learning\n- Load the data into a SQL Server database then model in Azure Machine Learning\n\nIn this tutorial we will demonstrate parallel bulk import of the data to a SQL Server, data exploration, feature engineering and down sampling using SQL Server Management Studio as well as using IPython Notebook. [Sample scripts](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/DataScienceProcess/DataScienceScripts) and [IPython notebooks](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/DataScienceProcess/iPythonNotebooks) are shared in GitHub. A sample IPython notebook to work with the data in Azure blobs is also available in the same location.\n\nTo set up your Azure Data Science environment:\n\n1. [Create a storage account](../storage-create-storage-account.md)\n\n2. [Create an Azure ML workspace](machine-learning-create-workspace.md)\n\n3. [Provision a Data Science Virtual Machine](machine-learning-data-science-setup-sql-server-virtual-machine.md), which will serve as a SQL Server as well an IPython Notebook server.\n\n    > [AZURE.NOTE] The sample scripts and IPython notebooks will be downloaded to your Data Science virtual machine during the setup process. When the VM post-installation script completes, the samples will be in your VM's Documents library:  \n    > - Sample Scripts: `C:\\Users\\<user_name>\\Documents\\Data Science Scripts`  \n    > - Sample IPython Notebooks: `C:\\Users\\<user_name>\\Documents\\IPython Notebooks\\DataScienceSamples`  \n    > where `<user_name>` is your VM's Windows login name. We will refer to the sample folders as **Sample Scripts** and **Sample IPython Notebooks**.\n\n\nBased on the dataset size, data source location, and the selected Azure target environment, this scenario is similar to [Scenario \\#5: Large dataset in a local files, target SQL Server in Azure VM](../machine-learning-data-science-plan-sample-scenarios.md#largelocaltodb).\n\n## <a name=\"getdata\"></a>Get the Data from Public Source\n\nTo get the [NYC Taxi Trips](http://www.andresmh.com/nyctaxitrips/) dataset from its public location, you may use any of the methods described in [Move Data to and from Azure Blob Storage](machine-learning-data-science-move-azure-blob.md) to copy the data to your new virtual machine.\n\nTo copy the data using AzCopy:\n\n1. Log in to your virtual machine (VM)\n\n2. Create a new directory in the VM's data disk (Note: Do not use the Temporary Disk which comes with the VM as a Data Disk).\n\n3. In a Command Prompt window, run the following Azcopy command line, replacing <path_to_data_folder> with your data folder created in (2):\n\n        \"C:\\Program Files (x86)\\Microsoft SDKs\\Azure\\AzCopy\\azcopy\" /Source:https://nyctaxitrips.blob.core.windows.net/data /Dest:<path_to_data_folder> /S\n\n    When the AzCopy completes, a total of 24 zipped CSV files (12 for trip\\_data and 12 for trip\\_fare) should be in the data folder.\n\n4. Unzip the downloaded files. Note the folder where the uncompressed files reside. This folder will be referred to as the <path\\_to\\_data\\_files\\>.\n\n## <a name=\"dbload\"></a>Bulk Import Data into SQL Server Database\n\nThe performance of loading/transferring large amounts of data to an SQL database and subsequent queries can be improved by using _Partitioned Tables and Views_. In this section, we will follow the instructions described in [Parallel Bulk Data Import Using SQL Partition Tables](machine-learning-data-science-parallel-load-sql-partitioned-tables.md) to create a new database and load the data into partitioned tables in parallel.\n\n1. While logged in to your VM, start **SQL Server Management Studio**.\n\n2. Connect using Windows Authentication.\n\n    ![SSMS Connect][12]\n\n3. If you have not yet changed the SQL Server authentication mode and created a new SQL login user, open the script file named **change\\_auth.sql** in the **Sample Scripts** folder. Change the  default user name and password. Click **!Execute** in the toolbar to run the script.\n\n    ![Execute Script][13]\n\n4. Verify and/or change the SQL Server default database and log folders to ensure that newly created databases will be stored in a Data Disk. The SQL Server VM image that is optimized for datawarehousing loads is pre-configured with data and log disks. If your VM did not include a Data Disk and you added new virtual hard disks during the VM setup process, change the default folders as follows:\n\n    - Right-click the SQL Server name in the left panel and click **Properties**.\n\n        ![SQL Server Properties][14]\n\n    - Select **Database Settings** from the **Select a page** list to the left.\n\n    - Verify and/or change the **Database default locations** to the **Data Disk** locations of your choice. This is where new databases reside if created with the default location settings.\n\n        ![SQL Database Defaults][15]  \n\n5. To create a new database and a set of filegroups to hold the partitioned tables, open the sample script **create\\_db\\_default.sql**. The script will create a new database named **TaxiNYC** and 12 filegroups in the default data location. Each filegroup will hold one month of trip\\_data and trip\\_fare data. Modify the database name, if desired. Click **!Execute** to run the script.\n\n6. Next, create two partition tables, one for the trip\\_data and another for the trip\\_fare. Open the sample script **create\\_partitioned\\_table.sql**, which will:\n\n    - Create a partition function to split the data by month.\n    - Create a partition scheme to map each month's data to a different filegroup.\n    - Create two partitioned tables mapped to the partition scheme: **nyctaxi\\_trip** will hold the trip\\_data and **nyctaxi\\_fare** will hold the trip\\_fare data.\n\n    Click **!Execute** to run the script and create the partitioned tables.\n\n7. In the **Sample Scripts** folder, there are two sample PowerShell scripts provided to demonstrate parallel bulk imports of data to SQL Server tables.\n\n    - **bcp\\_parallel\\_generic.ps1** is a generic script to parallel bulk import data into a table. Modify this script to set the input and target variables as indicated in the comment lines in the script.\n    - **bcp\\_parallel\\_nyctaxi.ps1** is a pre-configured version of the generic script and can be used to to load both tables for the NYC Taxi Trips data.  \n\n8. Right-click the **bcp\\_parallel\\_nyctaxi.ps1** script name and click **Edit** to open it in PowerShell. Review the preset variables and modify according to your selected database name, input data folder, target log folder, and paths to the  sample format files **nyctaxi_trip.xml** and **nyctaxi\\_fare.xml** (provided in the **Sample Scripts** folder).\n\n    ![Bulk Import Data][16]\n\n    You may also select the authentication mode, default is Windows Authentication. Click the green arrow in the toolbar to run. The script will launch 24 bulk import operations in parallel, 12 for each partitioned table. You may monitor the data import progress by opening the SQL Server default data folder as set above.\n\n9. The PowerShell script reports the starting and ending times. When all bulk imports complete, the ending time is reported. Check the target log folder to verify that the bulk imports were successful, i.e., no errors reported in the target log folder.\n\n10. Your database is now ready for exploration, feature engineering, and other operations as desired. Since the tables are partitioned according to the **pickup\\_datetime** field, queries which include **pickup\\_datetime** conditions in the **WHERE** clause will benefit from the partition scheme.\n\n11. In **SQL Server Management Studio**, explore the provided sample script **sample\\_queries.sql**. To run any of the sample queries, highlight the query lines then click **!Execute** in the toolbar.\n\n12. The NYC Taxi Trips data is loaded in two separate tables. To improve join operations, it is highly recommended to index the tables. The sample script **create\\_partitioned\\_index.sql** creates partitioned indexes on the composite join key **medallion, hack\\_license, and pickup\\_datetime**.\n\n## <a name=\"dbexplore\"></a>Data Exploration and Feature Engineering in SQL Server\n\nIn this section, we will perform data exploration and feature generation by running SQL queries directly in the **SQL Server Management Studio** using the SQL Server database created earlier. A sample script named **sample\\_queries.sql** is provided in the **Sample Scripts** folder. Modify the script to change the database name, if it is different from the default: **TaxiNYC**.\n\nIn this exercise, we will:\n\n- Connect to **SQL Server Management Studio** using either Windows Authentication or using SQL Authentication and the SQL login name and password.\n- Explore data distributions of a few fields in varying time windows.\n- Investigate data quality of the longitude and latitude fields.\n- Generate binary and multiclass classification labels based on the **tip\\_amount**.\n- Generate features and compute/compare trip distances.\n- Join the two tables and extract a random sample that will be used to build models.\n\nWhen you are ready to proceed to Azure Machine Learning, you may either:  \n\n1. Save the final SQL query to extract and sample the data and copy-paste the query directly into a [Reader][reader] module in Azure Machine Learning, or\n2. Persist the sampled and engineered data you plan to use for model building in a new database table and use the new table in the [Reader][reader] module in Azure Machine Learning.\n\nIn this section we will save the final query to extract and sample the data. The second method is demonstrated in the [Data Exploration and Feature Engineering in IPython Notebook](#ipnb) section.\n\nFor a quick verification of the number of rows and columns in the tables populated earlier using parallel bulk import,\n\n    -- Report number of rows in table nyctaxi_trip without table scan\n    SELECT SUM(rows) FROM sys.partitions WHERE object_id = OBJECT_ID('nyctaxi_trip')\n\n    -- Report number of columns in table nyctaxi_trip\n    SELECT COUNT(*) FROM information_schema.columns WHERE table_name = 'nyctaxi_trip'\n\n#### Exploration: Trip distribution by medallion\n\nThis example identifies the medallion (taxi numbers) with more than 100 trips within a given time period. The query would benefit from the partitioned table access since it is conditioned by the partition scheme of **pickup\\_datetime**. Querying the full dataset will also make use of the partitioned table and/or index scan.\n\n    SELECT medallion, COUNT(*)\n    FROM nyctaxi_fare\n    WHERE pickup_datetime BETWEEN '20130101' AND '20130331'\n    GROUP BY medallion\n    HAVING COUNT(*) > 100\n\n#### Exploration: Trip distribution by medallion and hack_license\n\n    SELECT medallion, hack_license, COUNT(*)\n    FROM nyctaxi_fare\n    WHERE pickup_datetime BETWEEN '20130101' AND '20130131'\n    GROUP BY medallion, hack_license\n    HAVING COUNT(*) > 100\n\n#### Data Quality Assessment: Verify records with incorrect longitude and/or latitude\n\nThis example investigates if any of the longitude and/or latitude fields either contain an invalid value (radian degrees should be between -90 and 90), or have (0, 0) coordinates.\n\n    SELECT COUNT(*) FROM nyctaxi_trip\n    WHERE pickup_datetime BETWEEN '20130101' AND '20130331'\n    AND  (CAST(pickup_longitude AS float) NOT BETWEEN -90 AND 90\n    OR    CAST(pickup_latitude AS float) NOT BETWEEN -90 AND 90\n    OR    CAST(dropoff_longitude AS float) NOT BETWEEN -90 AND 90\n    OR    CAST(dropoff_latitude AS float) NOT BETWEEN -90 AND 90\n    OR    (pickup_longitude = '0' AND pickup_latitude = '0')\n    OR    (dropoff_longitude = '0' AND dropoff_latitude = '0'))\n\n#### Exploration: Tipped vs. Not Tipped Trips distribution\n\nThis example finds the number of trips that were tipped vs. not tipped in a given time period (or in the full dataset if covering the full year). This distribution reflects the binary label distribution to be later used for binary classification modeling.\n\n    SELECT tipped, COUNT(*) AS tip_freq FROM (\n      SELECT CASE WHEN (tip_amount > 0) THEN 1 ELSE 0 END AS tipped, tip_amount\n      FROM nyctaxi_fare\n      WHERE pickup_datetime BETWEEN '20130101' AND '20131231') tc\n    GROUP BY tipped\n\n#### Exploration: Tip Class/Range Distribution\n\nThis example computes the distribution of tip ranges in a given time period (or in the full dataset if covering the full year). This is the distribution of the label classes that will be used later for multiclass classification modeling.\n\n    SELECT tip_class, COUNT(*) AS tip_freq FROM (\n        SELECT CASE\n            WHEN (tip_amount = 0) THEN 0\n            WHEN (tip_amount > 0 AND tip_amount <= 5) THEN 1\n            WHEN (tip_amount > 5 AND tip_amount <= 10) THEN 2\n            WHEN (tip_amount > 10 AND tip_amount <= 20) THEN 3\n            ELSE 4\n        END AS tip_class\n    FROM nyctaxi_fare\n    WHERE pickup_datetime BETWEEN '20130101' AND '20131231') tc\n    GROUP BY tip_class\n\n#### Exploration: Compute and Compare Trip Distance\n\nThis example converts the pickup and drop-off longitude and latitude to SQL geography points, computes the trip distance using SQL geography points difference, and returns a random sample of the results for comparison. The example limits the results to valid coordinates only using the data quality assessment query covered earlier.\n\n    SELECT\n    pickup_location=geography::STPointFromText('POINT(' + pickup_longitude + ' ' + pickup_latitude + ')', 4326)\n    ,dropoff_location=geography::STPointFromText('POINT(' + dropoff_longitude + ' ' + dropoff_latitude + ')', 4326)\n    ,trip_distance\n    ,computedist=round(geography::STPointFromText('POINT(' + pickup_longitude + ' ' + pickup_latitude + ')', 4326).STDistance(geography::STPointFromText('POINT(' + dropoff_longitude + ' ' + dropoff_latitude + ')', 4326))/1000, 2)\n    FROM nyctaxi_trip\n    tablesample(0.01 percent)\n    WHERE CAST(pickup_latitude AS float) BETWEEN -90 AND 90\n    AND   CAST(dropoff_latitude AS float) BETWEEN -90 AND 90\n    AND   pickup_longitude != '0' AND dropoff_longitude != '0'\n\n#### Feature Engineering in SQL Queries\n\nThe label generation and geography conversion exploration queries can also be used to generate labels/features by removing the counting part. Additional feature engineering SQL examples are provided in the [Data Exploration and Feature Engineering in IPython Notebook](#ipnb) section. It is more efficient to run the feature generation queries on the full dataset or a large subset of it using SQL queries which run directly on the SQL Server database instance. The queries may be executed in **SQL Server Management Studio**, IPython Notebook or any development tool/environment which can access the database locally or remotely.\n\n#### Preparing Data for Model Building\n\nThe following query joins the **nyctaxi\\_trip** and **nyctaxi\\_fare** tables, generates a binary classification label **tipped**, a multi-class classification label **tip\\_class**, and extracts a 1% random sample from the full joined dataset. This query can be copied then pasted directly in the [Azure Machine Learning Studio](https://studio.azureml.net) [Reader][reader] module for direct data ingestion from the SQL Server database instance in Azure. The query excludes records with incorrect (0, 0) coordinates.\n\n    SELECT t.*, f.payment_type, f.fare_amount, f.surcharge, f.mta_tax, f.tolls_amount,  f.total_amount, f.tip_amount,\n        CASE WHEN (tip_amount > 0) THEN 1 ELSE 0 END AS tipped,\n        CASE WHEN (tip_amount = 0) THEN 0\n            WHEN (tip_amount > 0 AND tip_amount <= 5) THEN 1\n            WHEN (tip_amount > 5 AND tip_amount <= 10) THEN 2\n            WHEN (tip_amount > 10 AND tip_amount <= 20) THEN 3\n            ELSE 4\n        END AS tip_class\n    FROM nyctaxi_trip t, nyctaxi_fare f\n    TABLESAMPLE (1 percent)\n    WHERE t.medallion = f.medallion\n    AND   t.hack_license = f.hack_license\n    AND   t.pickup_datetime = f.pickup_datetime\n    AND   pickup_longitude != '0' AND dropoff_longitude != '0'\n\n\n## <a name=\"ipnb\"></a>Data Exploration and Feature Engineering in IPython Notebook\n\nIn this section, we will perform data exploration and feature generation\nusing both Python and SQL queries against the SQL Server database created earlier. A sample IPython notebook named **machine-Learning-data-science-process-sql-story.ipynb** is provided in the **Sample IPython Notebooks** folder. This notebook is also available on [GitHub](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/DataScienceProcess/iPythonNotebooks).\n\nThe recommended sequence when working with big data is the following:\n\n- Read in a small sample of the data into an in-memory data frame.\n- Perform some visualizations and explorations using the sampled data.\n- Experiment with feature engineering using the sampled data.\n- For larger data exploration, data manipulation and feature engineering, use Python to issue SQL Queries directly against the SQL Server database in the Azure VM.\n- Decide the sample size to use for Azure Machine Learning model building.\n\nWhen ready to proceed to Azure Machine Learning, you may either:  \n\n1. Save the final SQL query to extract and sample the data and copy-paste the query directly into a [Reader][reader] module in Azure Machine Learning. This method is demonstrated in the [Building Models in Azure Machine Learning](#mlmodel) section.    \n2. Persist the sampled and engineered data you plan to use for model building in a new database table, then use the new table in the [Reader][reader] module.\n\nThe following are a few data exploration, data visualization, and feature engineering examples. For more examples, see the sample SQL IPython notebook in the **Sample IPython Notebooks** folder.\n\n#### Initialize Database Credentials\n\nInitialize your database connection settings in the following variables:\n\n    SERVER_NAME=<server name>\n    DATABASE_NAME=<database name>\n    USERID=<user name>\n    PASSWORD=<password>\n    DB_DRIVER = <database server>\n\n#### Create Database Connection\n\n    CONNECTION_STRING = 'DRIVER={'+DRIVER+'};SERVER='+SERVER_NAME+';DATABASE='+DATABASE_NAME+';UID='+USERID+';PWD='+PASSWORD\n    conn = pyodbc.connect(CONNECTION_STRING)\n\n#### Report number of rows and columns in table nyctaxi_trip\n\n    nrows = pd.read_sql('''\n        SELECT SUM(rows) FROM sys.partitions\n        WHERE object_id = OBJECT_ID('nyctaxi_trip')\n    ''', conn)\n\n    print 'Total number of rows = %d' % nrows.iloc[0,0]\n\n    ncols = pd.read_sql('''\n        SELECT COUNT(*) FROM information_schema.columns\n        WHERE table_name = ('nyctaxi_trip')\n    ''', conn)\n\n    print 'Total number of columns = %d' % ncols.iloc[0,0]\n\n- Total number of rows = 173179759  \n- Total number of columns = 14\n\n#### Read-in a small data sample from the SQL Server Database\n\n    t0 = time.time()\n\n    query = '''\n        SELECT t.*, f.payment_type, f.fare_amount, f.surcharge, f.mta_tax,\n            f.tolls_amount, f.total_amount, f.tip_amount\n        FROM nyctaxi_trip t, nyctaxi_fare f\n        TABLESAMPLE (0.05 PERCENT)\n        WHERE t.medallion = f.medallion\n        AND   t.hack_license = f.hack_license\n        AND   t.pickup_datetime = f.pickup_datetime\n    '''\n\n    df1 = pd.read_sql(query, conn)\n\n    t1 = time.time()\n    print 'Time to read the sample table is %f seconds' % (t1-t0)\n\n    print 'Number of rows and columns retrieved = (%d, %d)' % (df1.shape[0], df1.shape[1])\n\nTime to read the sample table is 6.492000 seconds  \nNumber of rows and columns retrieved = (84952, 21)\n\n#### Descriptive Statistics\n\nNow are ready to explore the sampled data. We start with\nlooking at descriptive statistics for the **trip\\_distance** (or any other) field(s):\n\n    df1['trip_distance'].describe()\n\n#### Visualization: Box Plot Example\n\nNext we look at the box plot for the trip distance to visualize the quantiles\n\n    df1.boxplot(column='trip_distance',return_type='dict')\n\n![Plot #1][1]\n\n#### Visualization: Distribution Plot Example\n\n    fig = plt.figure()\n    ax1 = fig.add_subplot(1,2,1)\n    ax2 = fig.add_subplot(1,2,2)\n    df1['trip_distance'].plot(ax=ax1,kind='kde', style='b-')\n    df1['trip_distance'].hist(ax=ax2, bins=100, color='k')\n\n![Plot #2][2]\n\n#### Visualization: Bar and Line Plots\n\nIn this example, we bin the trip distance into five bins and visualize the binning results.\n\n    trip_dist_bins = [0, 1, 2, 4, 10, 1000]\n    df1['trip_distance']\n    trip_dist_bin_id = pd.cut(df1['trip_distance'], trip_dist_bins)\n    trip_dist_bin_id\n\nWe can plot the above bin distribution in a bar or line plot as below\n\n    pd.Series(trip_dist_bin_id).value_counts().plot(kind='bar')\n\n![Plot #3][3]\n\n    pd.Series(trip_dist_bin_id).value_counts().plot(kind='line')\n\n![Plot #4][4]\n\n#### Visualization: Scatterplot Example\n\nWe show scatter plot between **trip\\_time\\_in\\_secs** and **trip\\_distance** to see if there\nis any correlation\n\n    plt.scatter(df1['trip_time_in_secs'], df1['trip_distance'])\n\n![Plot #6][6]\n\nSimilarly we can check the relationship between **rate\\_code** and **trip\\_distance**.\n\n    plt.scatter(df1['passenger_count'], df1['trip_distance'])\n\n![Plot #8][8]\n\n### Sub-Sampling the Data in SQL\n\nWhen preparing data for model building in [Azure Machine Learning Studio](https://studio.azureml.net), you may either decide on the **SQL query to use directly in the Reader module** or persist the engineered and sampled data in a new table, which you could use in the [Reader][reader] module with a simple **SELECT * FROM <your\\_new\\_table\\_name>**.\n\nIn this section we will create a new table to hold the sampled and engineered data. An example of a direct SQL query for model building is provided in the [Data Exploration and Feature Engineering in SQL Server](#dbexplore) section.\n\n#### Create a Sample Table and Populate with 1% of the Joined Tables. Drop Table First if it Exists.\n\nIn this section, we join the tables **nyctaxi\\_trip** and **nyctaxi\\_fare**, extract a 1% random sample, and persist the sampled data in a new table name **nyctaxi\\_one\\_percent**:\n\n    cursor = conn.cursor()\n\n    drop_table_if_exists = '''\n        IF OBJECT_ID('nyctaxi_one_percent', 'U') IS NOT NULL DROP TABLE nyctaxi_one_percent\n    '''\n\n    nyctaxi_one_percent_insert = '''\n        SELECT t.*, f.payment_type, f.fare_amount, f.surcharge, f.mta_tax, f.tolls_amount, f.total_amount, f.tip_amount\n        INTO nyctaxi_one_percent\n        FROM nyctaxi_trip t, nyctaxi_fare f\n        TABLESAMPLE (1 PERCENT)\n        WHERE t.medallion = f.medallion\n        AND   t.hack_license = f.hack_license\n        AND   t.pickup_datetime = f.pickup_datetime\n        AND   pickup_longitude <> '0' AND dropoff_longitude <> '0'\n    '''\n\n    cursor.execute(drop_table_if_exists)\n    cursor.execute(nyctaxi_one_percent_insert)\n    cursor.commit()\n\n### Data Exploration using SQL Queries in IPython Notebook\n\nIn this section, we explore data distributions using the 1% sampled data which is persisted in the new table we created above. Note that similar explorations can be performed using the original tables, optionally using **TABLESAMPLE** to limit the exploration sample or by limiting the results to a given time period using the **pickup\\_datetime** partitions, as illustrated in the [Data Exploration and Feature Engineering in SQL Server](#dbexplore) section.\n\n#### Exploration: Daily distribution of trips\n\n    query = '''\n        SELECT CONVERT(date, dropoff_datetime) AS date, COUNT(*) AS c\n        FROM nyctaxi_one_percent\n        GROUP BY CONVERT(date, dropoff_datetime)\n    '''\n\n    pd.read_sql(query,conn)\n\n#### Exploration: Trip distribution per medallion\n\n    query = '''\n        SELECT medallion,count(*) AS c\n        FROM nyctaxi_one_percent\n        GROUP BY medallion\n    '''\n\n    pd.read_sql(query,conn)\n\n### Feature Generation Using SQL Queries in IPython Notebook\n\nIn this section we will generate new labels and features directly using SQL queries, operating on the 1% sample table we created in the previous section.\n\n#### Label Generation: Generate Class Labels\n\nIn the following example, we generate two sets of labels to use for modeling:\n\n1. Binary Class Labels **tipped** (predicting if a tip will be given)\n2. Multiclass Labels **tip\\_class** (predicting the tip bin or range)\n\n        nyctaxi_one_percent_add_col = '''\n            ALTER TABLE nyctaxi_one_percent ADD tipped bit, tip_class int\n        '''\n\n        cursor.execute(nyctaxi_one_percent_add_col)\n        cursor.commit()\n\n        nyctaxi_one_percent_update_col = '''\n            UPDATE nyctaxi_one_percent\n            SET\n               tipped = CASE WHEN (tip_amount > 0) THEN 1 ELSE 0 END,\n               tip_class = CASE WHEN (tip_amount = 0) THEN 0\n                                WHEN (tip_amount > 0 AND tip_amount <= 5) THEN 1\n                                WHEN (tip_amount > 5 AND tip_amount <= 10) THEN 2\n                                WHEN (tip_amount > 10 AND tip_amount <= 20) THEN 3\n                                ELSE 4\n                            END\n        '''\n\n        cursor.execute(nyctaxi_one_percent_update_col)\n        cursor.commit()\n\n#### Feature Engineering: Count Features for Categorical Columns\n\nThis example transforms a categorical field into a numeric field by replacing each category with the count of its occurrences in the data.\n\n    nyctaxi_one_percent_insert_col = '''\n        ALTER TABLE nyctaxi_one_percent ADD cmt_count int, vts_count int\n    '''\n\n    cursor.execute(nyctaxi_one_percent_insert_col)\n    cursor.commit()\n\n    nyctaxi_one_percent_update_col = '''\n        WITH B AS\n        (\n            SELECT medallion, hack_license,\n                SUM(CASE WHEN vendor_id = 'cmt' THEN 1 ELSE 0 END) AS cmt_count,\n                SUM(CASE WHEN vendor_id = 'vts' THEN 1 ELSE 0 END) AS vts_count\n            FROM nyctaxi_one_percent\n            GROUP BY medallion, hack_license\n        )\n\n        UPDATE nyctaxi_one_percent\n        SET nyctaxi_one_percent.cmt_count = B.cmt_count,\n            nyctaxi_one_percent.vts_count = B.vts_count\n        FROM nyctaxi_one_percent A INNER JOIN B\n        ON A.medallion = B.medallion AND A.hack_license = B.hack_license\n    '''\n\n    cursor.execute(nyctaxi_one_percent_update_col)\n    cursor.commit()\n\n#### Feature Engineering: Bin features for Numerical Columns\n\nThis example transforms a continuous numeric field into preset category ranges, i.e., transform numeric field into a categorical field.\n\n    nyctaxi_one_percent_insert_col = '''\n        ALTER TABLE nyctaxi_one_percent ADD trip_time_bin int\n    '''\n\n    cursor.execute(nyctaxi_one_percent_insert_col)\n    cursor.commit()\n\n    nyctaxi_one_percent_update_col = '''\n        WITH B(medallion,hack_license,pickup_datetime,trip_time_in_secs, BinNumber ) AS\n        (\n            SELECT medallion,hack_license,pickup_datetime,trip_time_in_secs,\n            NTILE(5) OVER (ORDER BY trip_time_in_secs) AS BinNumber from nyctaxi_one_percent\n        )\n\n        UPDATE nyctaxi_one_percent\n        SET trip_time_bin = B.BinNumber\n        FROM nyctaxi_one_percent A INNER JOIN B\n        ON A.medallion = B.medallion\n        AND A.hack_license = B.hack_license\n        AND A.pickup_datetime = B.pickup_datetime\n    '''\n\n    cursor.execute(nyctaxi_one_percent_update_col)\n    cursor.commit()\n\n#### Feature Engineering: Extract Location Features from Decimal Latitude/Longitude\n\nThis example breaks down the decimal representation of a latitude and/or longitude field into multiple region fields of different granularity, such as, country, city, town, block, etc. Note that the new geo-fields are not mapped to actual locations. For information on mapping geocode locations, see [Bing Maps REST Services](https://msdn.microsoft.com/library/ff701710.aspx).\n\n    nyctaxi_one_percent_insert_col = '''\n        ALTER TABLE nyctaxi_one_percent\n        ADD l1 varchar(6), l2 varchar(3), l3 varchar(3), l4 varchar(3),\n            l5 varchar(3), l6 varchar(3), l7 varchar(3)\n    '''\n\n    cursor.execute(nyctaxi_one_percent_insert_col)\n    cursor.commit()\n\n    nyctaxi_one_percent_update_col = '''\n        UPDATE nyctaxi_one_percent\n        SET l1=round(pickup_longitude,0)\n            , l2 = CASE WHEN LEN (PARSENAME(ROUND(ABS(pickup_longitude) - FLOOR(ABS(pickup_longitude)),6),1)) >= 1 THEN SUBSTRING(PARSENAME(ROUND(ABS(pickup_longitude) - FLOOR(ABS(pickup_longitude)),6),1),1,1) ELSE '0' END     \n            , l3 = CASE WHEN LEN (PARSENAME(ROUND(ABS(pickup_longitude) - FLOOR(ABS(pickup_longitude)),6),1)) >= 2 THEN SUBSTRING(PARSENAME(ROUND(ABS(pickup_longitude) - FLOOR(ABS(pickup_longitude)),6),1),2,1) ELSE '0' END     \n            , l4 = CASE WHEN LEN (PARSENAME(ROUND(ABS(pickup_longitude) - FLOOR(ABS(pickup_longitude)),6),1)) >= 3 THEN SUBSTRING(PARSENAME(ROUND(ABS(pickup_longitude) - FLOOR(ABS(pickup_longitude)),6),1),3,1) ELSE '0' END     \n            , l5 = CASE WHEN LEN (PARSENAME(ROUND(ABS(pickup_longitude) - FLOOR(ABS(pickup_longitude)),6),1)) >= 4 THEN SUBSTRING(PARSENAME(ROUND(ABS(pickup_longitude) - FLOOR(ABS(pickup_longitude)),6),1),4,1) ELSE '0' END     \n            , l6 = CASE WHEN LEN (PARSENAME(ROUND(ABS(pickup_longitude) - FLOOR(ABS(pickup_longitude)),6),1)) >= 5 THEN SUBSTRING(PARSENAME(ROUND(ABS(pickup_longitude) - FLOOR(ABS(pickup_longitude)),6),1),5,1) ELSE '0' END     \n            , l7 = CASE WHEN LEN (PARSENAME(ROUND(ABS(pickup_longitude) - FLOOR(ABS(pickup_longitude)),6),1)) >= 6 THEN SUBSTRING(PARSENAME(ROUND(ABS(pickup_longitude) - FLOOR(ABS(pickup_longitude)),6),1),6,1) ELSE '0' END\n    '''\n\n    cursor.execute(nyctaxi_one_percent_update_col)\n    cursor.commit()\n\n#### Verify the final form of the featurized table\n\n    query = '''SELECT TOP 100 * FROM nyctaxi_one_percent'''\n    pd.read_sql(query,conn)\n\nWe are now ready to proceed to model building and model deployment in [Azure Machine Learning](https://studio.azureml.net). The data is ready for any of the prediction problems identified earlier, namely:\n\n1. Binary classification: To predict whether or not a tip was paid for a trip.\n\n2. Multiclass classification: To predict the range of tip paid, according to the previously defined classes.\n\n3. Regression task: To predict the amount of tip paid for a trip.  \n\n\n## <a name=\"mlmodel\"></a>Building Models in Azure Machine Learning\n\nTo begin the modeling exercise, log in to your Azure Machine Learning workspace. If you have not yet created a machine learning workspace, see [Create an Azure ML workspace](machine-learning-create-workspace.md).\n\n1. To get started with Azure Machine Learning, see [What is Azure Machine Learning Studio?](machine-learning-what-is-ml-studio.md)\n\n2. Log in to [Azure Machine Learning Studio](https://studio.azureml.net).\n\n3. The Studio Home page provides a wealth of information, videos, tutorials, links to the Modules Reference, and other resources. Fore more information about Azure Machine Learning, consult the [Azure Machine Learning Documentation Center](https://azure.microsoft.com/documentation/services/machine-learning/).\n\nA typical training experiment consists of the following:\n\n1. Create a **+NEW** experiment.\n2. Get the data to Azure ML.\n3. Pre-process, transform and manipulate the data as needed.\n4. Generate features as needed.\n5. Split the data into training/validation/testing datasets(or have separate datasets for each).\n6. Select one or more machine learning algorithms depending on the learning problem to solve. E.g., binary classification, multiclass classification, regression.\n7. Train one or more models using the training dataset.\n8. Score the validation dataset using the trained model(s).\n9. Evaluate the model(s) to compute the relevant metrics for the learning problem.\n10. Fine tune the model(s) and select the best model to deploy.\n\nIn this exercise, we have already explored and engineered the data in SQL Server, and decided on the sample size to ingest in Azure ML. To build one or more of the prediction models we decided:\n\n1. Get the data to Azure ML using the [Reader][reader] module, available in the **Data Input and Output** section. For more information, see the [Reader][reader] module reference page.\n\n    ![Azure ML Reader][17]\n\n2. Select **Azure SQL Database** as the **Data source** in the **Properties** panel.\n\n3. Enter the database DNS name in the **Database server name** field. Format: `tcp:<your_virtual_machine_DNS_name>,1433`\n\n4. Enter the **Database name** in the corresponding field.\n\n5. Enter the **SQL user name** in the **Server user aqccount name, and the password in the **Server user account password**.\n\n6. Check **Accept any server certificate** option.\n\n7. In the **Database query** edit text area, paste the query which extracts the necessary database fields (including any computed fields such as the labels) and down samples the data to the desired sample size.\n\nAn example of a binary classification experiment reading data directly from the SQL Server database is in the figure below. Similar experiments can be constructed for multiclass classification and regression problems.\n\n![Azure ML Train][10]\n\n> [AZURE.IMPORTANT] In the modeling data extraction and sampling query examples provided in previous sections, **all labels for the three modeling exercises are included in the query**. An important (required) step in each of the modeling exercises is to **exclude** the unnecessary labels for the other two problems, and any other **target leaks**. For e.g., when using binary classification, use the label **tipped** and exclude the fields **tip\\_class**, **tip\\_amount**, and **total\\_amount**. The latter are target leaks since they imply the tip paid.\n>\n> To exclude unnecessary columns and/or target leaks, you may use the [Project Columns][project-columns] module or the [Metadata Editor][metadata-editor]. For more information, see [Project Columns][project-columns] and [Metadata Editor][metadata-editor] reference pages.\n\n## <a name=\"mldeploy\"></a>Deploying Models in Azure Machine Learning\n\nWhen your model is ready, you can easily deploy it as a web service directly from the experiment. For more information about deploying Azure ML web services, see [Deploy an Azure Machine Learning web service](machine-learning-publish-a-machine-learning-web-service.md).\n\nTo deploy a new web service, you need to:\n\n1. Create a scoring experiment.\n2. Deploy the web service.\n\nTo create a scoring experiment from a **Finished** training experiment, click **CREATE SCORING EXPERIMENT** in the lower action bar.\n\n![Azure Scoring][18]\n\nAzure Machine Learning will attempt to create a scoring experiment based on the components of the training experiment. In particular, it will:\n\n1. Save the trained model and remove the model training modules.\n2. Identify a logical **input port** to represent the expected input data schema.\n3. Identify a logical **output port** to represent the expected web service output schema.\n\nWhen the scoring experiment is created, review it and adjust as needed. A typical adjustment is to replace the input dataset and/or query with one which excludes label fields, as these will not be available when the service is called. It is also a good practice to reduce the size of the input dataset and/or query to a few records, just enough to indicate the input schema. For the output port, it is common to exclude all input fields and only include the **Scored Labels** and **Scored Probabilities** in the output using the [Project Columns][project-columns] module.\n\nA sample scoring experiment is in the figure below. When ready to deploy, click the **PUBLISH WEB SERVICE** button in the lower action bar.\n\n![Azure ML Publish][11]\n\nTo recap, in this walkthrough tutorial, you have created an Azure data science environment, worked with a large public dataset all the way from data acquisition to model training and deploying of an Azure Machine Learning web service.\n\n### License Information\n\nThis sample walkthrough and its accompanying scripts and IPython notebook(s) are shared by Microsoft under the MIT license. Please check the LICENSE.txt file in in the directory of the sample code on GitHub for more details.\n\n### References\n\n•   [Andrés Monroy NYC Taxi Trips Download Page](http://www.andresmh.com/nyctaxitrips/)  \n•   [FOILing NYC’s Taxi Trip Data by Chris Whong](http://chriswhong.com/open-data/foil_nyc_taxi/)   \n•   [NYC Taxi and Limousine Commission Research and Statistics](https://www1.nyc.gov/html/tlc/html/about/statistics.shtml)\n\n\n[1]: ./media/machine-learning-data-science-process-sql-walkthrough/sql-walkthrough_26_1.png\n[2]: ./media/machine-learning-data-science-process-sql-walkthrough/sql-walkthrough_28_1.png\n[3]: ./media/machine-learning-data-science-process-sql-walkthrough/sql-walkthrough_35_1.png\n[4]: ./media/machine-learning-data-science-process-sql-walkthrough/sql-walkthrough_36_1.png\n[5]: ./media/machine-learning-data-science-process-sql-walkthrough/sql-walkthrough_39_1.png\n[6]: ./media/machine-learning-data-science-process-sql-walkthrough/sql-walkthrough_42_1.png\n[7]: ./media/machine-learning-data-science-process-sql-walkthrough/sql-walkthrough_44_1.png\n[8]: ./media/machine-learning-data-science-process-sql-walkthrough/sql-walkthrough_46_1.png\n[9]: ./media/machine-learning-data-science-process-sql-walkthrough/sql-walkthrough_71_1.png\n[10]: ./media/machine-learning-data-science-process-sql-walkthrough/azuremltrain.png\n[11]: ./media/machine-learning-data-science-process-sql-walkthrough/azuremlpublish.png\n[12]: ./media/machine-learning-data-science-process-sql-walkthrough/ssmsconnect.png\n[13]: ./media/machine-learning-data-science-process-sql-walkthrough/executescript.png\n[14]: ./media/machine-learning-data-science-process-sql-walkthrough/sqlserverproperties.png\n[15]: ./media/machine-learning-data-science-process-sql-walkthrough/sqldefaultdirs.png\n[16]: ./media/machine-learning-data-science-process-sql-walkthrough/bulkimport.png\n[17]: ./media/machine-learning-data-science-process-sql-walkthrough/amlreader.png\n[18]: ./media/machine-learning-data-science-process-sql-walkthrough/amlscoring.png\n\n\n<!-- Module References -->\n[metadata-editor]: https://msdn.microsoft.com/library/azure/370b6676-c11c-486f-bf73-35349f842a66/\n[project-columns]: https://msdn.microsoft.com/library/azure/1ec722fa-b623-4e26-a44e-a50c6d726223/\n[reader]: https://msdn.microsoft.com/library/azure/4e1b0fe6-aded-4b3f-a36f-39b8862b9004/\n"
}