{
  "nodes": [
    {
      "pos": [
        26,
        104
      ],
      "content": "Optimize your Hive queries for faster execution in HDInsight | Microsoft Azure"
    },
    {
      "pos": [
        122,
        174
      ],
      "content": "Learn how to optimize your Hive queries in HDInsight"
    },
    {
      "pos": [
        466,
        511
      ],
      "content": "Optimize Hive queries for Hadoop in HDInsight"
    },
    {
      "pos": [
        513,
        693
      ],
      "content": "By default, Hadoop clusters are not optimized for performance. This article covers a few of the most common Hive performance optimization methods that you can apply to our queries.",
      "nodes": [
        {
          "content": "By default, Hadoop clusters are not optimized for performance.",
          "pos": [
            0,
            62
          ]
        },
        {
          "content": "This article covers a few of the most common Hive performance optimization methods that you can apply to our queries.",
          "pos": [
            63,
            180
          ]
        }
      ]
    },
    {
      "pos": [
        766,
        855
      ],
      "content": "<bpt id=\"p1\">[</bpt>Optimize Hive queries for Hadoop in HDInsight<ept id=\"p1\">](hdinsight-hadoop-optimize-hive-query.md)</ept>."
    },
    {
      "pos": [
        859,
        881
      ],
      "content": "Scale out worker nodes"
    },
    {
      "pos": [
        883,
        1055
      ],
      "content": "Increasing the number of worker nodes in a cluster can leverage more mappers and reducers to be run in parallel. There are two ways you can increase scale out in HDInsight:",
      "nodes": [
        {
          "content": "Increasing the number of worker nodes in a cluster can leverage more mappers and reducers to be run in parallel.",
          "pos": [
            0,
            112
          ]
        },
        {
          "content": "There are two ways you can increase scale out in HDInsight:",
          "pos": [
            113,
            172
          ]
        }
      ]
    },
    {
      "pos": [
        1059,
        1376
      ],
      "content": "At the provision time, you can specify the number of worker nodes using the Azure portal, Azure PowerShell or Cross-platform command line interface.  For more information, see <bpt id=\"p2\">[</bpt>Provision HDInsight clusters<ept id=\"p2\">](hdinsight-provision-clusters.md)</ept>. The following screen show the worker node configuration on the Azure portal:",
      "nodes": [
        {
          "content": "At the provision time, you can specify the number of worker nodes using the Azure portal, Azure PowerShell or Cross-platform command line interface.",
          "pos": [
            0,
            148
          ]
        },
        {
          "content": "For more information, see <bpt id=\"p2\">[</bpt>Provision HDInsight clusters<ept id=\"p2\">](hdinsight-provision-clusters.md)</ept>.",
          "pos": [
            150,
            278
          ]
        },
        {
          "content": "The following screen show the worker node configuration on the Azure portal:",
          "pos": [
            279,
            355
          ]
        }
      ]
    },
    {
      "pos": [
        1382,
        1431
      ],
      "content": "<ph id=\"ph3\">![</ph>scaleout_1<ph id=\"ph4\">][image-hdi-optimize-hive-scaleout_1]</ph>"
    },
    {
      "pos": [
        1434,
        1578
      ],
      "content": "At the run time, you can also scale out a cluster without recreating one. This is shown below.\n<ph id=\"ph5\">![</ph>scaleout_1<ph id=\"ph6\">][image-hdi-optimize-hive-scaleout_2]</ph>",
      "nodes": [
        {
          "content": "At the run time, you can also scale out a cluster without recreating one.",
          "pos": [
            0,
            73
          ]
        },
        {
          "content": "This is shown below.",
          "pos": [
            74,
            94
          ]
        },
        {
          "content": "<ph id=\"ph5\">![</ph>scaleout_1<ph id=\"ph6\">][image-hdi-optimize-hive-scaleout_2]</ph>",
          "pos": [
            95,
            180
          ]
        }
      ]
    },
    {
      "pos": [
        1580,
        1735
      ],
      "content": "For more details on the different virtual machines supported by HDInsight, see <bpt id=\"p3\">[</bpt>HDInsight pricing<ept id=\"p3\">](https://azure.microsoft.com/pricing/details/hdinsight/)</ept>."
    },
    {
      "pos": [
        1739,
        1749
      ],
      "content": "Enable Tez"
    },
    {
      "pos": [
        1751,
        1859
      ],
      "content": "<bpt id=\"p4\">[</bpt>Apache Tez<ept id=\"p4\">](http://hortonworks.com/hadoop/tez/)</ept><ph id=\"ph7\"/> is an alternative execution engine to the MapReduce engine:"
    },
    {
      "pos": [
        1861,
        1900
      ],
      "content": "<ph id=\"ph8\">![</ph>tez_1<ph id=\"ph9\">][image-hdi-optimize-hive-tez_1]</ph>"
    },
    {
      "pos": [
        1903,
        1925
      ],
      "content": "Tez is faster because:"
    },
    {
      "pos": [
        1929,
        2283
      ],
      "content": "Execute Directed Acyclic Graph (DAG) as a single job in the MapReduce engine, the DAG that is expressed requires each set of mappers to be followed by one set of reducers. This causes multiple MapReduce jobs to be spun off for each Hive query. Tez does not have such constraint and can process complex DAG as one job thus minimizing job startup overhead.",
      "nodes": [
        {
          "content": "Execute Directed Acyclic Graph (DAG) as a single job in the MapReduce engine, the DAG that is expressed requires each set of mappers to be followed by one set of reducers.",
          "pos": [
            0,
            171
          ]
        },
        {
          "content": "This causes multiple MapReduce jobs to be spun off for each Hive query.",
          "pos": [
            172,
            243
          ]
        },
        {
          "content": "Tez does not have such constraint and can process complex DAG as one job thus minimizing job startup overhead.",
          "pos": [
            244,
            354
          ]
        }
      ]
    },
    {
      "pos": [
        2286,
        2555
      ],
      "content": "<bpt id=\"p5\">**</bpt>Avoids unnecessary writes<ept id=\"p5\">**</ept><ph id=\"ph10\"/> Due to multiple jobs being spun for the same Hive query in the MapReduce engine, the output of each job is written to HDFS for intermediate data. Since Tez minimizes number of jobs for each Hive query it is able to avoid unnecessary write.",
      "nodes": [
        {
          "content": "<bpt id=\"p5\">**</bpt>Avoids unnecessary writes<ept id=\"p5\">**</ept><ph id=\"ph10\"/> Due to multiple jobs being spun for the same Hive query in the MapReduce engine, the output of each job is written to HDFS for intermediate data.",
          "pos": [
            0,
            228
          ]
        },
        {
          "content": "Since Tez minimizes number of jobs for each Hive query it is able to avoid unnecessary write.",
          "pos": [
            229,
            322
          ]
        }
      ]
    },
    {
      "pos": [
        2558,
        2729
      ],
      "content": "<bpt id=\"p6\">**</bpt>Minimizes start-up delays<ept id=\"p6\">**</ept><ph id=\"ph11\"/> Tez is better able to minimize start-up delay by reducing the number of mappers it needs to start and also improving optimization throughout."
    },
    {
      "pos": [
        2732,
        2868
      ],
      "content": "<bpt id=\"p7\">**</bpt>Reuses containers<ept id=\"p7\">**</ept><ph id=\"ph12\"/> Whenever possible Tez is able to reuse containers to ensure that latency due to starting up containers is reduced."
    },
    {
      "pos": [
        2871,
        3188
      ],
      "content": "<bpt id=\"p8\">**</bpt>Continuous optimization techniques<ept id=\"p8\">**</ept><ph id=\"ph13\"/> Traditionally optimization was done during compilation phase. However more information about the inputs is available that allow for better optimization during runtime. Tez uses continous optimization techniques that allows it to optimize the plan further into the runtime phase.",
      "nodes": [
        {
          "content": "<bpt id=\"p8\">**</bpt>Continuous optimization techniques<ept id=\"p8\">**</ept><ph id=\"ph13\"/> Traditionally optimization was done during compilation phase.",
          "pos": [
            0,
            153
          ]
        },
        {
          "content": "However more information about the inputs is available that allow for better optimization during runtime.",
          "pos": [
            154,
            259
          ]
        },
        {
          "content": "Tez uses continous optimization techniques that allows it to optimize the plan further into the runtime phase.",
          "pos": [
            260,
            370
          ]
        }
      ]
    },
    {
      "pos": [
        3190,
        3274
      ],
      "content": "For more details on these concepts, click <bpt id=\"p9\">[</bpt>here<ept id=\"p9\">](http://hortonworks.com/hadoop/tez/)</ept>"
    },
    {
      "pos": [
        3276,
        3362
      ],
      "content": "You can make any Hive query Tez enabled by prefixing the query with the setting below:"
    },
    {
      "pos": [
        3400,
        3544
      ],
      "content": "Tez must be enabled at the provision time. The following is a sample Azure PowerShell script for provisioning a Hadoop cluster with Tez enabled:",
      "nodes": [
        {
          "content": "Tez must be enabled at the provision time.",
          "pos": [
            0,
            42
          ]
        },
        {
          "content": "The following is a sample Azure PowerShell script for provisioning a Hadoop cluster with Tez enabled:",
          "pos": [
            43,
            144
          ]
        }
      ]
    },
    {
      "pos": [
        4914,
        4931
      ],
      "content": "Hive partitioning"
    },
    {
      "pos": [
        4933,
        5424
      ],
      "content": "I/O operation is the major performance bottleneck for running Hive queries. The performance can be improved if the amount of data that needs to be read can be reduced. By default, Hive queries scan entire Hive tables. This is great for queries like table scans, however for queries that only need to scan a small amount of data (e.g. queries with filtering), this creates unnecessary overhead. Hive partitioning allows Hive queries to access only the necessary amount of data in Hive tables.",
      "nodes": [
        {
          "content": "I/O operation is the major performance bottleneck for running Hive queries.",
          "pos": [
            0,
            75
          ]
        },
        {
          "content": "The performance can be improved if the amount of data that needs to be read can be reduced.",
          "pos": [
            76,
            167
          ]
        },
        {
          "content": "By default, Hive queries scan entire Hive tables.",
          "pos": [
            168,
            217
          ]
        },
        {
          "content": "This is great for queries like table scans, however for queries that only need to scan a small amount of data (e.g. queries with filtering), this creates unnecessary overhead.",
          "pos": [
            218,
            393
          ]
        },
        {
          "content": "Hive partitioning allows Hive queries to access only the necessary amount of data in Hive tables.",
          "pos": [
            394,
            491
          ]
        }
      ]
    },
    {
      "pos": [
        5426,
        5723
      ],
      "content": "Hive partitioning is implemented by reorganizing the raw data into new directories with each partition having its own directory - where the partition is defined by the user. The following diagram illustrates partitioning a Hive table by the column <bpt id=\"p10\">*</bpt>Year<ept id=\"p10\">*</ept>. A new directory is created for each year.",
      "nodes": [
        {
          "content": "Hive partitioning is implemented by reorganizing the raw data into new directories with each partition having its own directory - where the partition is defined by the user.",
          "pos": [
            0,
            173
          ]
        },
        {
          "content": "The following diagram illustrates partitioning a Hive table by the column <bpt id=\"p10\">*</bpt>Year<ept id=\"p10\">*</ept>.",
          "pos": [
            174,
            295
          ]
        },
        {
          "content": "A new directory is created for each year.",
          "pos": [
            296,
            337
          ]
        }
      ]
    },
    {
      "pos": [
        5725,
        5780
      ],
      "content": "<ph id=\"ph14\">![</ph>partitioning<ph id=\"ph15\">][image-hdi-optimize-hive-partitioning_1]</ph>"
    },
    {
      "pos": [
        5782,
        5815
      ],
      "content": "Some partitioning considerations:"
    },
    {
      "pos": [
        5819,
        6077
      ],
      "content": "<bpt id=\"p11\">**</bpt>Do not under-partition<ept id=\"p11\">**</ept><ph id=\"ph16\"/> - Partitioning on columns with only a few values can cause very few partitions. For example, partitioning on gender will only create two partitions to be created (male and female), thus only reduce the latency by a maximum of half.",
      "nodes": [
        {
          "content": "<bpt id=\"p11\">**</bpt>Do not under-partition<ept id=\"p11\">**</ept><ph id=\"ph16\"/> - Partitioning on columns with only a few values can cause very few partitions.",
          "pos": [
            0,
            161
          ]
        },
        {
          "content": "For example, partitioning on gender will only create two partitions to be created (male and female), thus only reduce the latency by a maximum of half.",
          "pos": [
            162,
            313
          ]
        }
      ]
    },
    {
      "pos": [
        6081,
        6335
      ],
      "content": "<bpt id=\"p12\">**</bpt>Do not over-partition<ept id=\"p12\">**</ept><ph id=\"ph17\"/> - On the other extreme, creating a partition on a column with a unique value (e.g. userid) will cause multiple partitions causing a lot of stress on the cluster namenode as it will have to handle the large amount of directories."
    },
    {
      "pos": [
        6339,
        6591
      ],
      "content": "<bpt id=\"p13\">**</bpt>Avoid data skew<ept id=\"p13\">**</ept><ph id=\"ph18\"/> - Choose your partitioning key wisely so that all partitions are even size. An example is partitioning on <bpt id=\"p14\">*</bpt>State<ept id=\"p14\">*</ept><ph id=\"ph19\"/> may cause the number of records under California to be almost 30x that of Vermont due to the difference in population.",
      "nodes": [
        {
          "content": "<bpt id=\"p13\">**</bpt>Avoid data skew<ept id=\"p13\">**</ept><ph id=\"ph18\"/> - Choose your partitioning key wisely so that all partitions are even size.",
          "pos": [
            0,
            150
          ]
        },
        {
          "content": "An example is partitioning on <bpt id=\"p14\">*</bpt>State<ept id=\"p14\">*</ept><ph id=\"ph19\"/> may cause the number of records under California to be almost 30x that of Vermont due to the difference in population.",
          "pos": [
            151,
            362
          ]
        }
      ]
    },
    {
      "pos": [
        6593,
        6654
      ],
      "content": "To create a partition table, use the <bpt id=\"p15\">*</bpt>Partitioned By<ept id=\"p15\">*</ept><ph id=\"ph20\"/> clause:"
    },
    {
      "pos": [
        7159,
        7264
      ],
      "content": "Once the partitioned table is created, you can either create static partitioning or dynamic partitioning."
    },
    {
      "pos": [
        7268,
        7478
      ],
      "content": "<bpt id=\"p16\">**</bpt>Static partitioning<ept id=\"p16\">**</ept><ph id=\"ph21\"/> means that you have already sharded data in the appropriate directories and you can ask Hive partitions manually based on the directory location. This is shown in the code snippet below.",
      "nodes": [
        {
          "content": "<bpt id=\"p16\">**</bpt>Static partitioning<ept id=\"p16\">**</ept><ph id=\"ph21\"/> means that you have already sharded data in the appropriate directories and you can ask Hive partitions manually based on the directory location.",
          "pos": [
            0,
            224
          ]
        },
        {
          "content": "This is shown in the code snippet below.",
          "pos": [
            225,
            265
          ]
        }
      ]
    },
    {
      "pos": [
        7858,
        8102
      ],
      "content": "<bpt id=\"p17\">**</bpt>Dynamic partitioning<ept id=\"p17\">**</ept><ph id=\"ph22\"/> means that you want Hive to create partitions automatically for you. Since we have already created the partitioning table from the staging table, all we need to do is insert data to the partitioned table as shown below:",
      "nodes": [
        {
          "content": "<bpt id=\"p17\">**</bpt>Dynamic partitioning<ept id=\"p17\">**</ept><ph id=\"ph22\"/> means that you want Hive to create partitions automatically for you.",
          "pos": [
            0,
            148
          ]
        },
        {
          "content": "Since we have already created the partitioning table from the staging table, all we need to do is insert data to the partitioned table as shown below:",
          "pos": [
            149,
            299
          ]
        }
      ]
    },
    {
      "pos": [
        8840,
        8988
      ],
      "content": "For more details, see <bpt id=\"p18\">[</bpt>Partitioned Tables<ept id=\"p18\">](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-PartitionedTables)</ept>."
    },
    {
      "pos": [
        8992,
        9014
      ],
      "content": "Use the ORCFile format"
    },
    {
      "pos": [
        9016,
        9066
      ],
      "content": "Hive supports different file formats. For example:",
      "nodes": [
        {
          "content": "Hive supports different file formats.",
          "pos": [
            0,
            37
          ]
        },
        {
          "content": "For example:",
          "pos": [
            38,
            50
          ]
        }
      ]
    },
    {
      "pos": [
        9070,
        9141
      ],
      "content": "<bpt id=\"p19\">**</bpt>Text<ept id=\"p19\">**</ept>: this is the default file format and works with most scenarios"
    },
    {
      "pos": [
        9144,
        9195
      ],
      "content": "<bpt id=\"p20\">**</bpt>Avro<ept id=\"p20\">**</ept>: works well for interoperability scenarios"
    },
    {
      "pos": [
        9198,
        9242
      ],
      "content": "<bpt id=\"p21\">**</bpt>ORC/Parquet<ept id=\"p21\">**</ept>: best suited for performance"
    },
    {
      "pos": [
        9244,
        9386
      ],
      "content": "ORC (Optimized Row Columnar) format is a highly efficient way to store Hive data. Compared to other formats, ORC has the following advantages:",
      "nodes": [
        {
          "content": "ORC (Optimized Row Columnar) format is a highly efficient way to store Hive data.",
          "pos": [
            0,
            81
          ]
        },
        {
          "content": "Compared to other formats, ORC has the following advantages:",
          "pos": [
            82,
            142
          ]
        }
      ]
    },
    {
      "pos": [
        9390,
        9472
      ],
      "content": "support for complex types including DateTime and complex and semi-structured types"
    },
    {
      "pos": [
        9475,
        9496
      ],
      "content": "up to 70% compression"
    },
    {
      "pos": [
        9499,
        9550
      ],
      "content": "indexes every 10,000 rows which allow skipping rows"
    },
    {
      "pos": [
        9553,
        9593
      ],
      "content": "a significant drop in run-time execution"
    },
    {
      "pos": [
        9595,
        9674
      ],
      "content": "To enable ORC format, you first create a table with the clause <bpt id=\"p22\">*</bpt>Stored as ORC<ept id=\"p22\">*</ept>:"
    },
    {
      "pos": [
        10124,
        10199
      ],
      "content": "Next, you insert data to the ORC table from the staging table. For example:",
      "nodes": [
        {
          "content": "Next, you insert data to the ORC table from the staging table.",
          "pos": [
            0,
            62
          ]
        },
        {
          "content": "For example:",
          "pos": [
            63,
            75
          ]
        }
      ]
    },
    {
      "pos": [
        10876,
        10988
      ],
      "content": "You can read more on the ORC format <bpt id=\"p23\">[</bpt>here<ept id=\"p23\">](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC)</ept>."
    },
    {
      "pos": [
        10992,
        11005
      ],
      "content": "Vectorization"
    },
    {
      "pos": [
        11007,
        11205
      ],
      "content": "Vectorization allows Hive to process a batch of 1024 rows together instead of processing one row at a time. This means that simple operations are done faster because less internal code needs to run.",
      "nodes": [
        {
          "content": "Vectorization allows Hive to process a batch of 1024 rows together instead of processing one row at a time.",
          "pos": [
            0,
            107
          ]
        },
        {
          "content": "This means that simple operations are done faster because less internal code needs to run.",
          "pos": [
            108,
            198
          ]
        }
      ]
    },
    {
      "pos": [
        11207,
        11281
      ],
      "content": "To enable vectorization prefix your Hive query with the following setting:"
    },
    {
      "pos": [
        11334,
        11466
      ],
      "content": "For more information, see <bpt id=\"p24\">[</bpt>Vectorized query execution<ept id=\"p24\">](https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution)</ept>."
    },
    {
      "pos": [
        11471,
        11497
      ],
      "content": "Other optimization methods"
    },
    {
      "pos": [
        11499,
        11570
      ],
      "content": "There are more optimization methods that you can consider, for example:"
    },
    {
      "pos": [
        11574,
        11689
      ],
      "content": "<bpt id=\"p25\">**</bpt>Hive bucketing:<ept id=\"p25\">**</ept><ph id=\"ph23\"/> a technique that allows to cluster or segment large sets of data to optimize query performance."
    },
    {
      "pos": [
        11692,
        12010
      ],
      "content": "<bpt id=\"p26\">**</bpt>Join optimization:<ept id=\"p26\">**</ept><ph id=\"ph24\"/> optimization of Hive's query execution planning to improve the efficiency of joins and reduce the need for user hints. For more information, see <bpt id=\"p27\">[</bpt>Join optimization<ept id=\"p27\">](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+JoinOptimization#LanguageManualJoinOptimization-JoinOptimization)</ept>.",
      "nodes": [
        {
          "content": "<bpt id=\"p26\">**</bpt>Join optimization:<ept id=\"p26\">**</ept><ph id=\"ph24\"/> optimization of Hive's query execution planning to improve the efficiency of joins and reduce the need for user hints.",
          "pos": [
            0,
            196
          ]
        },
        {
          "content": "For more information, see <bpt id=\"p27\">[</bpt>Join optimization<ept id=\"p27\">](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+JoinOptimization#LanguageManualJoinOptimization-JoinOptimization)</ept>.",
          "pos": [
            197,
            413
          ]
        }
      ]
    },
    {
      "pos": [
        12013,
        12034
      ],
      "content": "<bpt id=\"p28\">**</bpt>increase Reducers<ept id=\"p28\">**</ept>"
    },
    {
      "pos": [
        12036,
        12038
      ],
      "content": "##"
    },
    {
      "pos": [
        12061,
        12071
      ],
      "content": "Next steps"
    },
    {
      "pos": [
        12072,
        12196
      ],
      "content": "In this article, you have learned several common Hive query optimization methods. To learn more, see the following articles:",
      "nodes": [
        {
          "content": "In this article, you have learned several common Hive query optimization methods.",
          "pos": [
            0,
            81
          ]
        },
        {
          "content": "To learn more, see the following articles:",
          "pos": [
            82,
            124
          ]
        }
      ]
    },
    {
      "pos": [
        12200,
        12253
      ],
      "content": "<bpt id=\"p29\">[</bpt>Use Apache Hive in HDInsight<ept id=\"p29\">](hdinsight-use-hive.md)</ept>"
    },
    {
      "pos": [
        12256,
        12350
      ],
      "content": "<bpt id=\"p30\">[</bpt>Analyze flight delay data by using Hive in HDInsight<ept id=\"p30\">](hdinsight-analyze-flight-delay-data.md)</ept>"
    },
    {
      "pos": [
        12353,
        12434
      ],
      "content": "<bpt id=\"p31\">[</bpt>Analyze Twitter data using Hive in HDInsight<ept id=\"p31\">](hdinsight-analyze-twitter-data.md)</ept>"
    },
    {
      "pos": [
        12437,
        12549
      ],
      "content": "<bpt id=\"p32\">[</bpt>Analyze sensor data using the Hive Query Console on Hadoop in HDInsight<ept id=\"p32\">](hdinsight-hive-analyze-sensor-data.md)</ept>"
    },
    {
      "pos": [
        12552,
        12646
      ],
      "content": "<bpt id=\"p33\">[</bpt>Use Hive with HDInsight to analyze logs from websites<ept id=\"p33\">](hdinsight-hive-analyze-website-log.md)</ept>"
    }
  ],
  "content": "<properties\n   pageTitle=\"Optimize your Hive queries for faster execution in HDInsight | Microsoft Azure\"\n   description=\"Learn how to optimize your Hive queries in HDInsight\"\n   services=\"hdinsight\"\n   documentationCenter=\"\"\n   authors=\"rashimg\"\n   manager=\"mwinkle\"\n   editor=\"cgronlun\"/>\n\n<tags\n   ms.service=\"hdinsight\"\n   ms.devlang=\"na\"\n   ms.topic=\"article\"\n   ms.tgt_pltfrm=\"na\"\n   ms.workload=\"big-data\"\n   ms.date=\"07/28/2015\"\n   ms.author=\"rashimg\"/>\n\n\n# Optimize Hive queries for Hadoop in HDInsight\n\nBy default, Hadoop clusters are not optimized for performance. This article covers a few of the most common Hive performance optimization methods that you can apply to our queries.\n\n[AZURE.INCLUDE [portal](../../includes/hdinsight-azure-portal.md)] \n\n* [Optimize Hive queries for Hadoop in HDInsight](hdinsight-hadoop-optimize-hive-query.md).\n\n##Scale out worker nodes\n\nIncreasing the number of worker nodes in a cluster can leverage more mappers and reducers to be run in parallel. There are two ways you can increase scale out in HDInsight:\n\n- At the provision time, you can specify the number of worker nodes using the Azure portal, Azure PowerShell or Cross-platform command line interface.  For more information, see [Provision HDInsight clusters](hdinsight-provision-clusters.md). The following screen show the worker node configuration on the Azure portal:\n\n    ![scaleout_1][image-hdi-optimize-hive-scaleout_1]\n- At the run time, you can also scale out a cluster without recreating one. This is shown below.\n![scaleout_1][image-hdi-optimize-hive-scaleout_2]\n\nFor more details on the different virtual machines supported by HDInsight, see [HDInsight pricing](https://azure.microsoft.com/pricing/details/hdinsight/).\n\n##Enable Tez\n\n[Apache Tez](http://hortonworks.com/hadoop/tez/) is an alternative execution engine to the MapReduce engine:\n\n![tez_1][image-hdi-optimize-hive-tez_1]\n\n\nTez is faster because:\n\n- Execute Directed Acyclic Graph (DAG) as a single job in the MapReduce engine, the DAG that is expressed requires each set of mappers to be followed by one set of reducers. This causes multiple MapReduce jobs to be spun off for each Hive query. Tez does not have such constraint and can process complex DAG as one job thus minimizing job startup overhead.\n- **Avoids unnecessary writes** Due to multiple jobs being spun for the same Hive query in the MapReduce engine, the output of each job is written to HDFS for intermediate data. Since Tez minimizes number of jobs for each Hive query it is able to avoid unnecessary write.\n- **Minimizes start-up delays** Tez is better able to minimize start-up delay by reducing the number of mappers it needs to start and also improving optimization throughout.\n- **Reuses containers** Whenever possible Tez is able to reuse containers to ensure that latency due to starting up containers is reduced.\n- **Continuous optimization techniques** Traditionally optimization was done during compilation phase. However more information about the inputs is available that allow for better optimization during runtime. Tez uses continous optimization techniques that allows it to optimize the plan further into the runtime phase.\n\nFor more details on these concepts, click [here](http://hortonworks.com/hadoop/tez/)\n\nYou can make any Hive query Tez enabled by prefixing the query with the setting below:\n\n    set hive.execution.engine=tez;\n\nTez must be enabled at the provision time. The following is a sample Azure PowerShell script for provisioning a Hadoop cluster with Tez enabled:\n\n\n    $clusterName = \"[HDInsightClusterName]\"\n    $location = \"[AzureDataCenter]\" #i.e. West US\n    $dataNodes = 32 # number of worker nodes in the cluster\n\n    $defaultStorageAccountName = \"[DefaultStorageAccountName]\"\n    $defaultStorageContainerName = \"[DefaultBlobContainerName]\"\n    $defaultStorageAccountKey = $defaultStorageAccountKey = Get-AzureStorageKey $defaultStorageAccountName.ToLower() | %{ $_.Primary }\n\n    $hdiUserName = \"[HTTPUserName]\"\n    $hdiPassword = \"[HTTPUserPassword]\"\n\n    $hdiSecurePassword = ConvertTo-SecureString $hdiPassword -AsPlainText -Force\n    $hdiCredential = New-Object System.Management.Automation.PSCredential($hdiUserName, $hdiSecurePassword)\n\n    $hiveConfig = new-object 'Microsoft.WindowsAzure.Management.HDInsight.Cmdlet.DataObjects.AzureHDInsightHiveConfiguration'\n    $hiveConfig.Configuration = @{ \"hive.execution.engine\"=\"tez\" }\n\n    New-AzureHDInsightClusterConfig -ClusterSizeInNodes $dataNodes -HeadNodeVMSize Standard_D14 -DataNodeVMSize Standard_D14 |\n    Set-AzureHDInsightDefaultStorage -StorageAccountName \"$defaultStorageAccountName.blob.core.windows.net\" -StorageAccountKey $defaultStorageAccountKey -StorageContainerName $defaultStorageContainerName |\n    Add-AzureHDInsightConfigValues -Hive $hiveConfig |\n    New-AzureHDInsightCluster -Name $clusterName -Location $location -Credential $hdiCredential\n\n## Hive partitioning\n\nI/O operation is the major performance bottleneck for running Hive queries. The performance can be improved if the amount of data that needs to be read can be reduced. By default, Hive queries scan entire Hive tables. This is great for queries like table scans, however for queries that only need to scan a small amount of data (e.g. queries with filtering), this creates unnecessary overhead. Hive partitioning allows Hive queries to access only the necessary amount of data in Hive tables.\n\nHive partitioning is implemented by reorganizing the raw data into new directories with each partition having its own directory - where the partition is defined by the user. The following diagram illustrates partitioning a Hive table by the column *Year*. A new directory is created for each year.\n\n![partitioning][image-hdi-optimize-hive-partitioning_1]\n\nSome partitioning considerations:\n\n- **Do not under-partition** - Partitioning on columns with only a few values can cause very few partitions. For example, partitioning on gender will only create two partitions to be created (male and female), thus only reduce the latency by a maximum of half.\n\n- **Do not over-partition** - On the other extreme, creating a partition on a column with a unique value (e.g. userid) will cause multiple partitions causing a lot of stress on the cluster namenode as it will have to handle the large amount of directories.\n\n- **Avoid data skew** - Choose your partitioning key wisely so that all partitions are even size. An example is partitioning on *State* may cause the number of records under California to be almost 30x that of Vermont due to the difference in population.\n\nTo create a partition table, use the *Partitioned By* clause:\n\n    CREATE TABLE lineitem_part\n        (L_ORDERKEY INT, L_PARTKEY INT, L_SUPPKEY INT,L_LINENUMBER INT,\n         L_QUANTITY DOUBLE, L_EXTENDEDPRICE DOUBLE, L_DISCOUNT DOUBLE,\n         L_TAX DOUBLE, L_RETURNFLAG STRING, L_LINESTATUS STRING,\n         L_SHIPDATE_PS STRING, L_COMMITDATE STRING, L_RECEIPTDATE        STRING, L_SHIPINSTRUCT STRING, L_SHIPMODE STRING,\n         L_COMMENT STRING)\n    PARTITIONED BY(L_SHIPDATE STRING)\n    ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'\n    STORED AS TEXTFILE;\n\nOnce the partitioned table is created, you can either create static partitioning or dynamic partitioning.\n\n- **Static partitioning** means that you have already sharded data in the appropriate directories and you can ask Hive partitions manually based on the directory location. This is shown in the code snippet below.\n\n        INSERT OVERWRITE TABLE lineitem_part\n        PARTITION (L_SHIPDATE = ‘5/23/1996 12:00:00 AM’)\n        SELECT * FROM lineitem \n        WHERE lineitem.L_SHIPDATE = ‘5/23/1996 12:00:00 AM’\n\n        ALTER TABLE lineitem_part ADD PARTITION (L_SHIPDATE = ‘5/23/1996 12:00:00 AM’))\n        LOCATION ‘wasb://sampledata@ignitedemo.blob.core.windows.net/partitions/5_23_1996/'\n\n- **Dynamic partitioning** means that you want Hive to create partitions automatically for you. Since we have already created the partitioning table from the staging table, all we need to do is insert data to the partitioned table as shown below:\n\n        SET hive.exec.dynamic.partition = true;\n        SET hive.exec.dynamic.partition.mode = nonstrict;\n        INSERT INTO TABLE lineitem_part\n        PARTITION (L_SHIPDATE)\n        SELECT L_ORDERKEY as L_ORDERKEY, L_PARTKEY as L_PARTKEY , \n             L_SUPPKEY as L_SUPPKEY, L_LINENUMBER as L_LINENUMBER,\n             L_QUANTITY as L_QUANTITY, L_EXTENDEDPRICE as L_EXTENDEDPRICE,\n             L_DISCOUNT as L_DISCOUNT, L_TAX as L_TAX, L_RETURNFLAG as       L_RETURNFLAG, L_LINESTATUS as L_LINESTATUS, L_SHIPDATE as       L_SHIPDATE_PS, L_COMMITDATE as L_COMMITDATE, L_RECEIPTDATE as   L_RECEIPTDATE, L_SHIPINSTRUCT as L_SHIPINSTRUCT, L_SHIPMODE as      L_SHIPMODE, L_COMMENT as L_COMMENT, L_SHIPDATE as L_SHIPDATE FROM lineitem;\n\nFor more details, see [Partitioned Tables](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-PartitionedTables).\n\n##Use the ORCFile format\n\nHive supports different file formats. For example:\n\n- **Text**: this is the default file format and works with most scenarios\n- **Avro**: works well for interoperability scenarios\n- **ORC/Parquet**: best suited for performance\n\nORC (Optimized Row Columnar) format is a highly efficient way to store Hive data. Compared to other formats, ORC has the following advantages:\n\n- support for complex types including DateTime and complex and semi-structured types\n- up to 70% compression\n- indexes every 10,000 rows which allow skipping rows\n- a significant drop in run-time execution\n\nTo enable ORC format, you first create a table with the clause *Stored as ORC*:\n\n    CREATE TABLE lineitem_orc_part\n        (L_ORDERKEY INT, L_PARTKEY INT,L_SUPPKEY INT, L_LINENUMBER INT,\n         L_QUANTITY DOUBLE, L_EXTENDEDPRICE DOUBLE, L_DISCOUNT DOUBLE,\n         L_TAX DOUBLE, L_RETURNFLAG STRING, L_LINESTATUS STRING,\n         L_SHIPDATE_PS STRING, L_COMMITDATE STRING, L_RECEIPTDATE STRING,\n         L_SHIPINSTRUCT STRING, L_SHIPMODE STRING, L_COMMENT     STRING)\n    PARTITIONED BY(L_SHIPDATE STRING)\n    STORED AS ORC;\n\nNext, you insert data to the ORC table from the staging table. For example:\n\n    INSERT INTO TABLE lineitem_orc\n    SELECT L_ORDERKEY as L_ORDERKEY, \n           L_PARTKEY as L_PARTKEY , \n           L_SUPPKEY as L_SUPPKEY,\n           L_LINENUMBER as L_LINENUMBER,\n           L_QUANTITY as L_QUANTITY, \n           L_EXTENDEDPRICE as L_EXTENDEDPRICE,\n           L_DISCOUNT as L_DISCOUNT,\n           L_TAX as L_TAX,\n           L_RETURNFLAG as L_RETURNFLAG,\n           L_LINESTATUS as L_LINESTATUS,\n           L_SHIPDATE as L_SHIPDATE,\n           L_COMMITDATE as L_COMMITDATE,\n           L_RECEIPTDATE as L_RECEIPTDATE, \n           L_SHIPINSTRUCT as L_SHIPINSTRUCT,\n           L_SHIPMODE as L_SHIPMODE,\n           L_COMMENT as L_COMMENT\n    FROM lineitem;\n\nYou can read more on the ORC format [here](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC).\n\n##Vectorization\n\nVectorization allows Hive to process a batch of 1024 rows together instead of processing one row at a time. This means that simple operations are done faster because less internal code needs to run.\n\nTo enable vectorization prefix your Hive query with the following setting:\n\n    set hive.vectorized.execution.enabled = true;\n\nFor more information, see [Vectorized query execution](https://cwiki.apache.org/confluence/display/Hive/Vectorized+Query+Execution).\n\n\n##Other optimization methods\n\nThere are more optimization methods that you can consider, for example:\n\n- **Hive bucketing:** a technique that allows to cluster or segment large sets of data to optimize query performance.\n- **Join optimization:** optimization of Hive's query execution planning to improve the efficiency of joins and reduce the need for user hints. For more information, see [Join optimization](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+JoinOptimization#LanguageManualJoinOptimization-JoinOptimization).\n- **increase Reducers**\n\n##<a id=\"nextsteps\"></a> Next steps\nIn this article, you have learned several common Hive query optimization methods. To learn more, see the following articles:\n\n- [Use Apache Hive in HDInsight](hdinsight-use-hive.md)\n- [Analyze flight delay data by using Hive in HDInsight](hdinsight-analyze-flight-delay-data.md)\n- [Analyze Twitter data using Hive in HDInsight](hdinsight-analyze-twitter-data.md)\n- [Analyze sensor data using the Hive Query Console on Hadoop in HDInsight](hdinsight-hive-analyze-sensor-data.md)\n- [Use Hive with HDInsight to analyze logs from websites](hdinsight-hive-analyze-website-log.md)\n\n\n[image-hdi-optimize-hive-scaleout_1]: ./media/hdinsight-hadoop-optimize-hive-query-v1/scaleout_1.png\n[image-hdi-optimize-hive-scaleout_2]: ./media/hdinsight-hadoop-optimize-hive-query-v1/scaleout_2.png\n[image-hdi-optimize-hive-tez_1]: ./media/hdinsight-hadoop-optimize-hive-query-v1/tez_1.png\n[image-hdi-optimize-hive-partitioning_1]: ./media/hdinsight-hadoop-optimize-hive-query-v1/partitioning_1.png\n"
}