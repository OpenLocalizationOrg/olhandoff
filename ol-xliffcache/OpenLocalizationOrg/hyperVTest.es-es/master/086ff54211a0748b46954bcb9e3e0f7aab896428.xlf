<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" source-language="en-US" target-language="es-es">
    <body>
      <group id="main" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Use Azure Event Hubs with Apache Spark in HDInsight to process streaming data | Microsoft Azure</source>
          <target state="new">Use Azure Event Hubs with Apache Spark in HDInsight to process streaming data | Microsoft Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Step-by-step instructions on how to send a data stream to Azure Event Hub and then receive those events in Spark using a scala application</source>
          <target state="new">Step-by-step instructions on how to send a data stream to Azure Event Hub and then receive those events in Spark using a scala application</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Spark Streaming: Process events from Azure Event Hubs with Apache Spark on HDInsight (Linux)</source>
          <target state="new">Spark Streaming: Process events from Azure Event Hubs with Apache Spark on HDInsight (Linux)</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>Spark Streaming extends the core Spark API to build scalable, high-throughput, fault-tolerant stream processing applications.</source>
          <target state="new">Spark Streaming extends the core Spark API to build scalable, high-throughput, fault-tolerant stream processing applications.</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>Data can be ingested from many sources.</source>
          <target state="new">Data can be ingested from many sources.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>In this article we use Azure Event Hubs to ingest data.</source>
          <target state="new">In this article we use Azure Event Hubs to ingest data.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>Event Hubs is a highly scalable ingestion system that can intake millions of events per second.</source>
          <target state="new">Event Hubs is a highly scalable ingestion system that can intake millions of events per second.</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>In this tutorial, you will learn how to create an Azure Event Hub, how to ingest messages into an Event Hub using a console application in Java, and to retrieve them in parallel using a Spark application written in Scala.</source>
          <target state="new">In this tutorial, you will learn how to create an Azure Event Hub, how to ingest messages into an Event Hub using a console application in Java, and to retrieve them in parallel using a Spark application written in Scala.</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>This application consumes the data streamed through Event Hubs and routes it to different outputs (Azure Storage Blob, Hive table, and SQL table).</source>
          <target state="new">This application consumes the data streamed through Event Hubs and routes it to different outputs (Azure Storage Blob, Hive table, and SQL table).</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source><ph id="ph2">[AZURE.NOTE]</ph><ph id="ph3" /> To follow the instructions in this article, you will have to use both versions of the Azure portal.</source>
          <target state="new"><ph id="ph2">[AZURE.NOTE]</ph><ph id="ph3" /> To follow the instructions in this article, you will have to use both versions of the Azure portal.</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>To create an Event Hub you will use the <bpt id="p1">[</bpt>Azure portal<ept id="p1">](https://manage.windowsazure.com)</ept>.</source>
          <target state="new">To create an Event Hub you will use the <bpt id="p1">[</bpt>Azure portal<ept id="p1">](https://manage.windowsazure.com)</ept>.</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>To work with the HDInsight Spark cluster, you will use the <bpt id="p2">[</bpt>Azure Preview Portal<ept id="p2">](https://ms.portal.azure.com/)</ept>.</source>
          <target state="new">To work with the HDInsight Spark cluster, you will use the <bpt id="p2">[</bpt>Azure Preview Portal<ept id="p2">](https://ms.portal.azure.com/)</ept>.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source><bpt id="p3">**</bpt>Prerequisites:<ept id="p3">**</ept></source>
          <target state="new"><bpt id="p3">**</bpt>Prerequisites:<ept id="p3">**</ept></target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>You must have the following:</source>
          <target state="new">You must have the following:</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>An Azure subscription.</source>
          <target state="new">An Azure subscription.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>See <bpt id="p4">[</bpt>Get Azure free trial<ept id="p4">](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/)</ept>.</source>
          <target state="new">See <bpt id="p4">[</bpt>Get Azure free trial<ept id="p4">](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/)</ept>.</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>An Apache Spark cluster.</source>
          <target state="new">An Apache Spark cluster.</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>For instructions, see <bpt id="p5">[</bpt>Create Apache Spark clusters in Azure HDInsight<ept id="p5">](hdinsight-apache-spark-jupyter-spark-sql.md)</ept>.</source>
          <target state="new">For instructions, see <bpt id="p5">[</bpt>Create Apache Spark clusters in Azure HDInsight<ept id="p5">](hdinsight-apache-spark-jupyter-spark-sql.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>Oracle Java Development kit.</source>
          <target state="new">Oracle Java Development kit.</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>You can install it from <bpt id="p6">[</bpt>here<ept id="p6">](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)</ept>.</source>
          <target state="new">You can install it from <bpt id="p6">[</bpt>here<ept id="p6">](http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html)</ept>.</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>A Java IDE.</source>
          <target state="new">A Java IDE.</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>This article uses IntelliJ IDEA 15.0.1.</source>
          <target state="new">This article uses IntelliJ IDEA 15.0.1.</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>You can install it from <bpt id="p7">[</bpt>here<ept id="p7">](https://www.jetbrains.com/idea/download/)</ept>.</source>
          <target state="new">You can install it from <bpt id="p7">[</bpt>here<ept id="p7">](https://www.jetbrains.com/idea/download/)</ept>.</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>Microsoft JDBC driver for SQL Server, v4.1 or later.</source>
          <target state="new">Microsoft JDBC driver for SQL Server, v4.1 or later.</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>This is required to write the event data into a SQL Server database.</source>
          <target state="new">This is required to write the event data into a SQL Server database.</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>You can install it from <bpt id="p8">[</bpt>here<ept id="p8">](https://msdn.microsoft.com/sqlserver/aa937724.aspx)</ept>.</source>
          <target state="new">You can install it from <bpt id="p8">[</bpt>here<ept id="p8">](https://msdn.microsoft.com/sqlserver/aa937724.aspx)</ept>.</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>An Azure SQL database.</source>
          <target state="new">An Azure SQL database.</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>For instructions, see <bpt id="p9">[</bpt>Create a SQL database in minutes<ept id="p9">](../sql-database/sql-database-get-started.md)</ept></source>
          <target state="new">For instructions, see <bpt id="p9">[</bpt>Create a SQL database in minutes<ept id="p9">](../sql-database/sql-database-get-started.md)</ept></target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>What does this solution do?</source>
          <target state="new">What does this solution do?</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>This is how the streaming solution flows:</source>
          <target state="new">This is how the streaming solution flows:</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>Create an Azure Event Hub that will receive a stream of events.</source>
          <target state="new">Create an Azure Event Hub that will receive a stream of events.</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>Run a local standalone application that generates events and pushes it the Azure Event Hub.</source>
          <target state="new">Run a local standalone application that generates events and pushes it the Azure Event Hub.</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>The sample application that does this is published at <bpt id="p10">[</bpt>https://github.com/hdinsight/spark-streaming-data-persistence-examples<ept id="p10">](https://github.com/hdinsight/spark-streaming-data-persistence-examples)</ept>.</source>
          <target state="new">The sample application that does this is published at <bpt id="p10">[</bpt>https://github.com/hdinsight/spark-streaming-data-persistence-examples<ept id="p10">](https://github.com/hdinsight/spark-streaming-data-persistence-examples)</ept>.</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>Run a streaming application remotely on a Spark cluster that reads streaming events from Azure Event Hub and pushes it out to different locations (Azure Blob, Hive table, and SQL database table).</source>
          <target state="new">Run a streaming application remotely on a Spark cluster that reads streaming events from Azure Event Hub and pushes it out to different locations (Azure Blob, Hive table, and SQL database table).</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>Create Azure Event Hub</source>
          <target state="new">Create Azure Event Hub</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>From the <bpt id="p11">[</bpt>Azure portal<ept id="p11">](https://manage.windowsazure.com)</ept>, select <bpt id="p12">**</bpt>NEW<ept id="p12">**</ept><ph id="ph4" /> &gt; <bpt id="p13">**</bpt>Service Bus<ept id="p13">**</ept><ph id="ph5" /> &gt; <bpt id="p14">**</bpt>Event Hub<ept id="p14">**</ept><ph id="ph6" /> &gt; <bpt id="p15">**</bpt>Custom Create<ept id="p15">**</ept>.</source>
          <target state="new">From the <bpt id="p11">[</bpt>Azure portal<ept id="p11">](https://manage.windowsazure.com)</ept>, select <bpt id="p12">**</bpt>NEW<ept id="p12">**</ept><ph id="ph4" /> &gt; <bpt id="p13">**</bpt>Service Bus<ept id="p13">**</ept><ph id="ph5" /> &gt; <bpt id="p14">**</bpt>Event Hub<ept id="p14">**</ept><ph id="ph6" /> &gt; <bpt id="p15">**</bpt>Custom Create<ept id="p15">**</ept>.</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>On the <bpt id="p16">**</bpt>Add a new Event Hub<ept id="p16">**</ept><ph id="ph7" /> screen, enter an <bpt id="p17">**</bpt>Event Hub Name<ept id="p17">**</ept>, select the <bpt id="p18">**</bpt>Region<ept id="p18">**</ept><ph id="ph8" /> to create the hub in, and create a new namespace or select an existing one.</source>
          <target state="new">On the <bpt id="p16">**</bpt>Add a new Event Hub<ept id="p16">**</ept><ph id="ph7" /> screen, enter an <bpt id="p17">**</bpt>Event Hub Name<ept id="p17">**</ept>, select the <bpt id="p18">**</bpt>Region<ept id="p18">**</ept><ph id="ph8" /> to create the hub in, and create a new namespace or select an existing one.</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>Click the <bpt id="p19">**</bpt>Arrow<ept id="p19">**</ept><ph id="ph9" /> to continue.</source>
          <target state="new">Click the <bpt id="p19">**</bpt>Arrow<ept id="p19">**</ept><ph id="ph9" /> to continue.</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source><ph id="ph10">![</ph>wizard page 1<ph id="ph11">](./media/hdinsight-apache-spark-eventhub-streaming/hdispark.streaming.create.event.hub.png "Create an Azure Event Hub")</ph></source>
          <target state="new"><ph id="ph10">![</ph>wizard page 1<ph id="ph11">](./media/hdinsight-apache-spark-eventhub-streaming/hdispark.streaming.create.event.hub.png "Create an Azure Event Hub")</ph></target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source><ph id="ph12">[AZURE.NOTE]</ph><ph id="ph13" /> You should select the same <bpt id="p20">**</bpt>Location<ept id="p20">**</ept><ph id="ph14" /> as your Apache Spark cluster in HDInsight to reduce latency and costs.</source>
          <target state="new"><ph id="ph12">[AZURE.NOTE]</ph><ph id="ph13" /> You should select the same <bpt id="p20">**</bpt>Location<ept id="p20">**</ept><ph id="ph14" /> as your Apache Spark cluster in HDInsight to reduce latency and costs.</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>On the <bpt id="p21">**</bpt>Configure Event Hub<ept id="p21">**</ept><ph id="ph15" /> screen, enter the <bpt id="p22">**</bpt>Partition count<ept id="p22">**</ept><ph id="ph16" /> and <bpt id="p23">**</bpt>Message Retention<ept id="p23">**</ept><ph id="ph17" /> values, and then click the check mark.</source>
          <target state="new">On the <bpt id="p21">**</bpt>Configure Event Hub<ept id="p21">**</ept><ph id="ph15" /> screen, enter the <bpt id="p22">**</bpt>Partition count<ept id="p22">**</ept><ph id="ph16" /> and <bpt id="p23">**</bpt>Message Retention<ept id="p23">**</ept><ph id="ph17" /> values, and then click the check mark.</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>For this example, use a partition count of 10 and a message retention of 1.</source>
          <target state="new">For this example, use a partition count of 10 and a message retention of 1.</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>Note the partition count because you will need this value later.</source>
          <target state="new">Note the partition count because you will need this value later.</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source><ph id="ph18">![</ph>wizard page 2<ph id="ph19">](./media/hdinsight-apache-spark-eventhub-streaming/hdispark.streaming.create.event.hub2.png "Specify partition size and retention days for Event Hub")</ph></source>
          <target state="new"><ph id="ph18">![</ph>wizard page 2<ph id="ph19">](./media/hdinsight-apache-spark-eventhub-streaming/hdispark.streaming.create.event.hub2.png "Specify partition size and retention days for Event Hub")</ph></target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>Click the Event Hub that you created, click <bpt id="p24">**</bpt>Configure<ept id="p24">**</ept>, and then create two access policies for the event hub.</source>
          <target state="new">Click the Event Hub that you created, click <bpt id="p24">**</bpt>Configure<ept id="p24">**</ept>, and then create two access policies for the event hub.</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source><ph id="ph20">&lt;table&gt;</ph><ph id="ph21">
 &lt;tr&gt;</ph><ph id="ph22">&lt;th&gt;</ph>Name<ph id="ph23">&lt;/th&gt;</ph><ph id="ph24">&lt;th&gt;</ph>Permissions<ph id="ph25">&lt;/th&gt;</ph><ph id="ph26">&lt;/tr&gt;</ph><ph id="ph27">
 &lt;tr&gt;</ph><ph id="ph28">&lt;td&gt;</ph>mysendpolicy<ph id="ph29">&lt;/td&gt;</ph><ph id="ph30">&lt;td&gt;</ph>Send<ph id="ph31">&lt;/td&gt;</ph><ph id="ph32">&lt;/tr&gt;</ph><ph id="ph33">
 &lt;tr&gt;</ph><ph id="ph34">&lt;td&gt;</ph>myreceivepolicy<ph id="ph35">&lt;/td&gt;</ph><ph id="ph36">&lt;td&gt;</ph>Listen<ph id="ph37">&lt;/td&gt;</ph><ph id="ph38">&lt;/tr&gt;</ph><ph id="ph39">
 &lt;/table&gt;</ph></source>
          <target state="new"><ph id="ph20">&lt;table&gt;</ph><ph id="ph21">
 &lt;tr&gt;</ph><ph id="ph22">&lt;th&gt;</ph>Name<ph id="ph23">&lt;/th&gt;</ph><ph id="ph24">&lt;th&gt;</ph>Permissions<ph id="ph25">&lt;/th&gt;</ph><ph id="ph26">&lt;/tr&gt;</ph><ph id="ph27">
 &lt;tr&gt;</ph><ph id="ph28">&lt;td&gt;</ph>mysendpolicy<ph id="ph29">&lt;/td&gt;</ph><ph id="ph30">&lt;td&gt;</ph>Send<ph id="ph31">&lt;/td&gt;</ph><ph id="ph32">&lt;/tr&gt;</ph><ph id="ph33">
 &lt;tr&gt;</ph><ph id="ph34">&lt;td&gt;</ph>myreceivepolicy<ph id="ph35">&lt;/td&gt;</ph><ph id="ph36">&lt;td&gt;</ph>Listen<ph id="ph37">&lt;/td&gt;</ph><ph id="ph38">&lt;/tr&gt;</ph><ph id="ph39">
 &lt;/table&gt;</ph></target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>After You create the permissions, select the <bpt id="p25">**</bpt>Save<ept id="p25">**</ept><ph id="ph40" /> icon at the bottom of the page.</source>
          <target state="new">After You create the permissions, select the <bpt id="p25">**</bpt>Save<ept id="p25">**</ept><ph id="ph40" /> icon at the bottom of the page.</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>This creates the shared access policies that will be used to send (<bpt id="p26">**</bpt>mysendpolicy<ept id="p26">**</ept>) and listen (<bpt id="p27">**</bpt>myreceivepolicy<ept id="p27">**</ept>) to this Event Hub.</source>
          <target state="new">This creates the shared access policies that will be used to send (<bpt id="p26">**</bpt>mysendpolicy<ept id="p26">**</ept>) and listen (<bpt id="p27">**</bpt>myreceivepolicy<ept id="p27">**</ept>) to this Event Hub.</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source><ph id="ph41">![</ph>policies<ph id="ph42">](./media/hdinsight-apache-spark-eventhub-streaming/hdispark.streaming.event.hub.policies.png "Create Event Hub policies")</ph></source>
          <target state="new"><ph id="ph41">![</ph>policies<ph id="ph42">](./media/hdinsight-apache-spark-eventhub-streaming/hdispark.streaming.event.hub.policies.png "Create Event Hub policies")</ph></target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>On the same page, take a note of the policy keys generated for the two policies.</source>
          <target state="new">On the same page, take a note of the policy keys generated for the two policies.</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>Save these keys because they will be used later.</source>
          <target state="new">Save these keys because they will be used later.</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source><ph id="ph43">![</ph>policy keys<ph id="ph44">](./media/hdinsight-apache-spark-eventhub-streaming/hdispark.streaming.event.hub.policy.keys.png "Save policy keys")</ph></source>
          <target state="new"><ph id="ph43">![</ph>policy keys<ph id="ph44">](./media/hdinsight-apache-spark-eventhub-streaming/hdispark.streaming.event.hub.policy.keys.png "Save policy keys")</ph></target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>On the <bpt id="p28">**</bpt>Dashboard<ept id="p28">**</ept><ph id="ph45" /> page, click <bpt id="p29">**</bpt>Connection Information<ept id="p29">**</ept><ph id="ph46" /> from the bottom to retrieve and save the connection strings for the Event Hub using the two policies.</source>
          <target state="new">On the <bpt id="p28">**</bpt>Dashboard<ept id="p28">**</ept><ph id="ph45" /> page, click <bpt id="p29">**</bpt>Connection Information<ept id="p29">**</ept><ph id="ph46" /> from the bottom to retrieve and save the connection strings for the Event Hub using the two policies.</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source><ph id="ph47">![</ph>policy keys<ph id="ph48">](./media/hdinsight-apache-spark-eventhub-streaming/hdispark.streaming.event.hub.policy.connection.strings.png "Save policy connection strings")</ph></source>
          <target state="new"><ph id="ph47">![</ph>policy keys<ph id="ph48">](./media/hdinsight-apache-spark-eventhub-streaming/hdispark.streaming.event.hub.policy.connection.strings.png "Save policy connection strings")</ph></target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source>Use a Scala application to send messages to Event Hub</source>
          <target state="new">Use a Scala application to send messages to Event Hub</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>In this section you use a standalone local Scala application to send a stream of events to Azure Event Hub that you created in the previous step.</source>
          <target state="new">In this section you use a standalone local Scala application to send a stream of events to Azure Event Hub that you created in the previous step.</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>This application is available on GitHub at <bpt id="p30">[</bpt>https://github.com/hdinsight/eventhubs-sample-event-producer<ept id="p30">](https://github.com/hdinsight/eventhubs-sample-event-producer)</ept>.</source>
          <target state="new">This application is available on GitHub at <bpt id="p30">[</bpt>https://github.com/hdinsight/eventhubs-sample-event-producer<ept id="p30">](https://github.com/hdinsight/eventhubs-sample-event-producer)</ept>.</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>The steps here assume that you have already forked this GitHub repository.</source>
          <target state="new">The steps here assume that you have already forked this GitHub repository.</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>Open the application, <bpt id="p31">**</bpt>EventhubsSampleEventProducer<ept id="p31">**</ept>, in IntelliJ IDEA.</source>
          <target state="new">Open the application, <bpt id="p31">**</bpt>EventhubsSampleEventProducer<ept id="p31">**</ept>, in IntelliJ IDEA.</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source>Build the project.</source>
          <target state="new">Build the project.</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>From the <bpt id="p32">**</bpt>Build<ept id="p32">**</ept><ph id="ph49" /> menu, click <bpt id="p33">**</bpt>Make Project<ept id="p33">**</ept>.</source>
          <target state="new">From the <bpt id="p32">**</bpt>Build<ept id="p32">**</ept><ph id="ph49" /> menu, click <bpt id="p33">**</bpt>Make Project<ept id="p33">**</ept>.</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>The output jar is created under <bpt id="p34">**</bpt>\out\artifacts<ept id="p34">**</ept>.</source>
          <target state="new">The output jar is created under <bpt id="p34">**</bpt>\out\artifacts<ept id="p34">**</ept>.</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source><ph id="ph50">[AZURE.TIP]</ph><ph id="ph51" /> You can also use an option available in IntelliJ IDEA to directly create the project from a GitHub repository.</source>
          <target state="new"><ph id="ph50">[AZURE.TIP]</ph><ph id="ph51" /> You can also use an option available in IntelliJ IDEA to directly create the project from a GitHub repository.</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>To understand how to use that approach, use the instructions in the next section for guidance.</source>
          <target state="new">To understand how to use that approach, use the instructions in the next section for guidance.</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>Note that a lot of steps that are described in the next section will not be applicable for the Scala application that you create in this step.</source>
          <target state="new">Note that a lot of steps that are described in the next section will not be applicable for the Scala application that you create in this step.</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>For example:</source>
          <target state="new">For example:</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source>You will not have to update the POM to include the Spark version.</source>
          <target state="new">You will not have to update the POM to include the Spark version.</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source>That's because there is no dependency on Spark for creating this application</source>
          <target state="new">That's because there is no dependency on Spark for creating this application</target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source>You will not have to add some dependency jars to the project library.</source>
          <target state="new">You will not have to add some dependency jars to the project library.</target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>That's because those jars are not required for this project.</source>
          <target state="new">That's because those jars are not required for this project.</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>Update the Scala streaming application for receiving the events</source>
          <target state="new">Update the Scala streaming application for receiving the events</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source>A sample Scala application to receive the event and route it to different destinations is available at <bpt id="p35">[</bpt>https://github.com/hdinsight/spark-streaming-data-persistence-examples<ept id="p35">](https://github.com/hdinsight/spark-streaming-data-persistence-examples)</ept>.</source>
          <target state="new">A sample Scala application to receive the event and route it to different destinations is available at <bpt id="p35">[</bpt>https://github.com/hdinsight/spark-streaming-data-persistence-examples<ept id="p35">](https://github.com/hdinsight/spark-streaming-data-persistence-examples)</ept>.</target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source>Follow the steps below to update the application and create the output jar.</source>
          <target state="new">Follow the steps below to update the application and create the output jar.</target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source>Launch IntelliJ IDEA and from the launch screen select <bpt id="p36">**</bpt>Check out from Version Control<ept id="p36">**</ept><ph id="ph52" /> and then click <bpt id="p37">**</bpt>Git<ept id="p37">**</ept>.</source>
          <target state="new">Launch IntelliJ IDEA and from the launch screen select <bpt id="p36">**</bpt>Check out from Version Control<ept id="p36">**</ept><ph id="ph52" /> and then click <bpt id="p37">**</bpt>Git<ept id="p37">**</ept>.</target>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source><ph id="ph53">![</ph>Get sources from Git<ph id="ph54">](./media/hdinsight-apache-spark-eventhub-streaming/get-source-from-git.png)</ph></source>
          <target state="new"><ph id="ph53">![</ph>Get sources from Git<ph id="ph54">](./media/hdinsight-apache-spark-eventhub-streaming/get-source-from-git.png)</ph></target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source>In the <bpt id="p38">**</bpt>Clone Repository<ept id="p38">**</ept><ph id="ph55" /> dialog box, provide the URL to the Git repository to clone from, specify the directory to clone to, and then click <bpt id="p39">**</bpt>Clone<ept id="p39">**</ept>.</source>
          <target state="new">In the <bpt id="p38">**</bpt>Clone Repository<ept id="p38">**</ept><ph id="ph55" /> dialog box, provide the URL to the Git repository to clone from, specify the directory to clone to, and then click <bpt id="p39">**</bpt>Clone<ept id="p39">**</ept>.</target>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source><ph id="ph56">![</ph>Clone from Git<ph id="ph57">](./media/hdinsight-apache-spark-eventhub-streaming/clone-from-git.png)</ph></source>
          <target state="new"><ph id="ph56">![</ph>Clone from Git<ph id="ph57">](./media/hdinsight-apache-spark-eventhub-streaming/clone-from-git.png)</ph></target>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source>Follow the prompts till the project is completely cloned.</source>
          <target state="new">Follow the prompts till the project is completely cloned.</target>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source>Press <bpt id="p40">**</bpt>Alt + 1<ept id="p40">**</ept><ph id="ph58" /> to open the <bpt id="p41">**</bpt>Project View<ept id="p41">**</ept>.</source>
          <target state="new">Press <bpt id="p40">**</bpt>Alt + 1<ept id="p40">**</ept><ph id="ph58" /> to open the <bpt id="p41">**</bpt>Project View<ept id="p41">**</ept>.</target>
        </trans-unit>
        <trans-unit id="180" translate="yes" xml:space="preserve">
          <source>It should resemble the following.</source>
          <target state="new">It should resemble the following.</target>
        </trans-unit>
        <trans-unit id="181" translate="yes" xml:space="preserve">
          <source><ph id="ph59">![</ph>Project View<ph id="ph60">](./media/hdinsight-apache-spark-eventhub-streaming/project-view.png)</ph></source>
          <target state="new"><ph id="ph59">![</ph>Project View<ph id="ph60">](./media/hdinsight-apache-spark-eventhub-streaming/project-view.png)</ph></target>
        </trans-unit>
        <trans-unit id="182" translate="yes" xml:space="preserve">
          <source>Open the pom.xml and make sure the Spark version is correct.</source>
          <target state="new">Open the pom.xml and make sure the Spark version is correct.</target>
        </trans-unit>
        <trans-unit id="183" translate="yes" xml:space="preserve">
          <source>Under <ph id="ph61">&lt;properties&gt;</ph><ph id="ph62" /> node, look for the following snippet and verify the Spark version.</source>
          <target state="new">Under <ph id="ph61">&lt;properties&gt;</ph><ph id="ph62" /> node, look for the following snippet and verify the Spark version.</target>
        </trans-unit>
        <trans-unit id="184" translate="yes" xml:space="preserve">
          <source>Make sure the value for <bpt id="p42">**</bpt>spark.version<ept id="p42">**</ept><ph id="ph63" /> is set to <bpt id="p43">**</bpt>1.5.1<ept id="p43">**</ept>.</source>
          <target state="new">Make sure the value for <bpt id="p42">**</bpt>spark.version<ept id="p42">**</ept><ph id="ph63" /> is set to <bpt id="p43">**</bpt>1.5.1<ept id="p43">**</ept>.</target>
        </trans-unit>
        <trans-unit id="185" translate="yes" xml:space="preserve">
          <source>The application requires two dependency jars:</source>
          <target state="new">The application requires two dependency jars:</target>
        </trans-unit>
        <trans-unit id="186" translate="yes" xml:space="preserve">
          <source><bpt id="p44">**</bpt>EventHub receiver jar<ept id="p44">**</ept>.</source>
          <target state="new"><bpt id="p44">**</bpt>EventHub receiver jar<ept id="p44">**</ept>.</target>
        </trans-unit>
        <trans-unit id="187" translate="yes" xml:space="preserve">
          <source>This is required for Spark to receive the messages from Event Hub.</source>
          <target state="new">This is required for Spark to receive the messages from Event Hub.</target>
        </trans-unit>
        <trans-unit id="188" translate="yes" xml:space="preserve">
          <source>This jar is available on your Spark Linux cluster at <ph id="ph64">`/usr/hdp/current/spark-client/lib/spark-streaming-eventhubs-example-1.5.2.2.3.3.1-7-jar-with-dependencies.jar`</ph>.</source>
          <target state="new">This jar is available on your Spark Linux cluster at <ph id="ph64">`/usr/hdp/current/spark-client/lib/spark-streaming-eventhubs-example-1.5.2.2.3.3.1-7-jar-with-dependencies.jar`</ph>.</target>
        </trans-unit>
        <trans-unit id="189" translate="yes" xml:space="preserve">
          <source>You can use pscp to copy the jar to your local computer.</source>
          <target state="new">You can use pscp to copy the jar to your local computer.</target>
        </trans-unit>
        <trans-unit id="190" translate="yes" xml:space="preserve">
          <source>This will copy the jar file from the Spark cluster on to your local computer.</source>
          <target state="new">This will copy the jar file from the Spark cluster on to your local computer.</target>
        </trans-unit>
        <trans-unit id="191" translate="yes" xml:space="preserve">
          <source><bpt id="p45">**</bpt>JDBC driver jar<ept id="p45">**</ept>.</source>
          <target state="new"><bpt id="p45">**</bpt>JDBC driver jar<ept id="p45">**</ept>.</target>
        </trans-unit>
        <trans-unit id="192" translate="yes" xml:space="preserve">
          <source>This is required to write the messages received from Event Hub into an Azure SQL database.</source>
          <target state="new">This is required to write the messages received from Event Hub into an Azure SQL database.</target>
        </trans-unit>
        <trans-unit id="193" translate="yes" xml:space="preserve">
          <source>You can download v4.1 or later of this jar file from <bpt id="p46">[</bpt>here<ept id="p46">](https://msdn.microsoft.com/en-us/sqlserver/aa937724.aspx)</ept>.</source>
          <target state="new">You can download v4.1 or later of this jar file from <bpt id="p46">[</bpt>here<ept id="p46">](https://msdn.microsoft.com/en-us/sqlserver/aa937724.aspx)</ept>.</target>
        </trans-unit>
        <trans-unit id="194" translate="yes" xml:space="preserve">
          <source>Create the output jar file.</source>
          <target state="new">Create the output jar file.</target>
        </trans-unit>
        <trans-unit id="195" translate="yes" xml:space="preserve">
          <source>Perform the following steps.</source>
          <target state="new">Perform the following steps.</target>
        </trans-unit>
        <trans-unit id="196" translate="yes" xml:space="preserve">
          <source>In the <bpt id="p47">**</bpt>Project Structure<ept id="p47">**</ept><ph id="ph65" /> dialog box, click <bpt id="p48">**</bpt>Artifacts<ept id="p48">**</ept><ph id="ph66" /> and then click the plus symbol.</source>
          <target state="new">In the <bpt id="p47">**</bpt>Project Structure<ept id="p47">**</ept><ph id="ph65" /> dialog box, click <bpt id="p48">**</bpt>Artifacts<ept id="p48">**</ept><ph id="ph66" /> and then click the plus symbol.</target>
        </trans-unit>
        <trans-unit id="197" translate="yes" xml:space="preserve">
          <source>From the pop-up dialog box, click <bpt id="p49">**</bpt>JAR<ept id="p49">**</ept>, and then click <bpt id="p50">**</bpt>From modules with dependencies<ept id="p50">**</ept>.</source>
          <target state="new">From the pop-up dialog box, click <bpt id="p49">**</bpt>JAR<ept id="p49">**</ept>, and then click <bpt id="p50">**</bpt>From modules with dependencies<ept id="p50">**</ept>.</target>
        </trans-unit>
        <trans-unit id="198" translate="yes" xml:space="preserve">
          <source><ph id="ph67">![</ph>Create JAR<ph id="ph68">](./media/hdinsight-apache-spark-eventhub-streaming/create-jar-1.png)</ph></source>
          <target state="new"><ph id="ph67">![</ph>Create JAR<ph id="ph68">](./media/hdinsight-apache-spark-eventhub-streaming/create-jar-1.png)</ph></target>
        </trans-unit>
        <trans-unit id="199" translate="yes" xml:space="preserve">
          <source>In the <bpt id="p51">**</bpt>Create JAR from Modules<ept id="p51">**</ept><ph id="ph69" /> dialog box, click the ellipsis (<ph id="ph70">![</ph>ellipsis<ph id="ph71">](./media/hdinsight-apache-spark-eventhub-streaming/ellipsis.png)</ph>) against the <bpt id="p52">**</bpt>Main Class<ept id="p52">**</ept>.</source>
          <target state="new">In the <bpt id="p51">**</bpt>Create JAR from Modules<ept id="p51">**</ept><ph id="ph69" /> dialog box, click the ellipsis (<ph id="ph70">![</ph>ellipsis<ph id="ph71">](./media/hdinsight-apache-spark-eventhub-streaming/ellipsis.png)</ph>) against the <bpt id="p52">**</bpt>Main Class<ept id="p52">**</ept>.</target>
        </trans-unit>
        <trans-unit id="200" translate="yes" xml:space="preserve">
          <source>In the <bpt id="p53">**</bpt>Select Main Class<ept id="p53">**</ept><ph id="ph72" /> dialog box, select any of the available classes and then click <bpt id="p54">**</bpt>OK<ept id="p54">**</ept>.</source>
          <target state="new">In the <bpt id="p53">**</bpt>Select Main Class<ept id="p53">**</ept><ph id="ph72" /> dialog box, select any of the available classes and then click <bpt id="p54">**</bpt>OK<ept id="p54">**</ept>.</target>
        </trans-unit>
        <trans-unit id="201" translate="yes" xml:space="preserve">
          <source><ph id="ph73">![</ph>Create JAR<ph id="ph74">](./media/hdinsight-apache-spark-eventhub-streaming/create-jar-2.png)</ph></source>
          <target state="new"><ph id="ph73">![</ph>Create JAR<ph id="ph74">](./media/hdinsight-apache-spark-eventhub-streaming/create-jar-2.png)</ph></target>
        </trans-unit>
        <trans-unit id="202" translate="yes" xml:space="preserve">
          <source>In the <bpt id="p55">**</bpt>Create JAR from Modules<ept id="p55">**</ept><ph id="ph75" /> dialog box, make sure that the option to <bpt id="p56">**</bpt>extract to the target JAR<ept id="p56">**</ept><ph id="ph76" /> is selected, and then click <bpt id="p57">**</bpt>OK<ept id="p57">**</ept>.</source>
          <target state="new">In the <bpt id="p55">**</bpt>Create JAR from Modules<ept id="p55">**</ept><ph id="ph75" /> dialog box, make sure that the option to <bpt id="p56">**</bpt>extract to the target JAR<ept id="p56">**</ept><ph id="ph76" /> is selected, and then click <bpt id="p57">**</bpt>OK<ept id="p57">**</ept>.</target>
        </trans-unit>
        <trans-unit id="203" translate="yes" xml:space="preserve">
          <source>This creates a single JAR with all dependencies.</source>
          <target state="new">This creates a single JAR with all dependencies.</target>
        </trans-unit>
        <trans-unit id="204" translate="yes" xml:space="preserve">
          <source><ph id="ph77">![</ph>Create JAR<ph id="ph78">](./media/hdinsight-apache-spark-eventhub-streaming/create-jar-3.png)</ph></source>
          <target state="new"><ph id="ph77">![</ph>Create JAR<ph id="ph78">](./media/hdinsight-apache-spark-eventhub-streaming/create-jar-3.png)</ph></target>
        </trans-unit>
        <trans-unit id="205" translate="yes" xml:space="preserve">
          <source>The <bpt id="p58">**</bpt>Output Layout<ept id="p58">**</ept><ph id="ph79" /> tab lists all the jars that are included as part of the Maven project.</source>
          <target state="new">The <bpt id="p58">**</bpt>Output Layout<ept id="p58">**</ept><ph id="ph79" /> tab lists all the jars that are included as part of the Maven project.</target>
        </trans-unit>
        <trans-unit id="206" translate="yes" xml:space="preserve">
          <source>You can select and delete the ones on which the Scala application has no direct dependency.</source>
          <target state="new">You can select and delete the ones on which the Scala application has no direct dependency.</target>
        </trans-unit>
        <trans-unit id="207" translate="yes" xml:space="preserve">
          <source>For the application we are creating here, you can remove all but the last one (<bpt id="p59">**</bpt>microsoft-spark-streaming-examples compile output<ept id="p59">**</ept>).</source>
          <target state="new">For the application we are creating here, you can remove all but the last one (<bpt id="p59">**</bpt>microsoft-spark-streaming-examples compile output<ept id="p59">**</ept>).</target>
        </trans-unit>
        <trans-unit id="208" translate="yes" xml:space="preserve">
          <source>Select the jars to delete and then click the <bpt id="p60">**</bpt>Delete<ept id="p60">**</ept><ph id="ph80" /> icon (<ph id="ph81">![</ph>delete icon<ph id="ph82">](./media/hdinsight-apache-spark-eventhub-streaming/delete-icon.png)</ph>).</source>
          <target state="new">Select the jars to delete and then click the <bpt id="p60">**</bpt>Delete<ept id="p60">**</ept><ph id="ph80" /> icon (<ph id="ph81">![</ph>delete icon<ph id="ph82">](./media/hdinsight-apache-spark-eventhub-streaming/delete-icon.png)</ph>).</target>
        </trans-unit>
        <trans-unit id="209" translate="yes" xml:space="preserve">
          <source><ph id="ph83">![</ph>Create JAR<ph id="ph84">](./media/hdinsight-apache-spark-eventhub-streaming/delete-output-jars.png)</ph></source>
          <target state="new"><ph id="ph83">![</ph>Create JAR<ph id="ph84">](./media/hdinsight-apache-spark-eventhub-streaming/delete-output-jars.png)</ph></target>
        </trans-unit>
        <trans-unit id="210" translate="yes" xml:space="preserve">
          <source>Make sure <bpt id="p61">**</bpt>Build on make<ept id="p61">**</ept><ph id="ph85" /> box is selected, which ensures that the jar is created every time the project is built or updated.</source>
          <target state="new">Make sure <bpt id="p61">**</bpt>Build on make<ept id="p61">**</ept><ph id="ph85" /> box is selected, which ensures that the jar is created every time the project is built or updated.</target>
        </trans-unit>
        <trans-unit id="211" translate="yes" xml:space="preserve">
          <source>Click <bpt id="p62">**</bpt>Apply<ept id="p62">**</ept><ph id="ph86" /> and then <bpt id="p63">**</bpt>OK<ept id="p63">**</ept>.</source>
          <target state="new">Click <bpt id="p62">**</bpt>Apply<ept id="p62">**</ept><ph id="ph86" /> and then <bpt id="p63">**</bpt>OK<ept id="p63">**</ept>.</target>
        </trans-unit>
        <trans-unit id="212" translate="yes" xml:space="preserve">
          <source>In the <bpt id="p64">**</bpt>Output Layout<ept id="p64">**</ept><ph id="ph87" /> tab, right at the bottom of the Available Elements box, you have the two dependency jars that you added earlier to the project library.</source>
          <target state="new">In the <bpt id="p64">**</bpt>Output Layout<ept id="p64">**</ept><ph id="ph87" /> tab, right at the bottom of the Available Elements box, you have the two dependency jars that you added earlier to the project library.</target>
        </trans-unit>
        <trans-unit id="213" translate="yes" xml:space="preserve">
          <source>You must add these to the Output Layout tab.</source>
          <target state="new">You must add these to the Output Layout tab.</target>
        </trans-unit>
        <trans-unit id="214" translate="yes" xml:space="preserve">
          <source>Right-click each jar file, and then click <bpt id="p65">**</bpt>Extract Into Output Root<ept id="p65">**</ept>.</source>
          <target state="new">Right-click each jar file, and then click <bpt id="p65">**</bpt>Extract Into Output Root<ept id="p65">**</ept>.</target>
        </trans-unit>
        <trans-unit id="215" translate="yes" xml:space="preserve">
          <source><ph id="ph88">![</ph>Extract dependency jar<ph id="ph89">](./media/hdinsight-apache-spark-eventhub-streaming/extract-dependency-jar.png)</ph></source>
          <target state="new"><ph id="ph88">![</ph>Extract dependency jar<ph id="ph89">](./media/hdinsight-apache-spark-eventhub-streaming/extract-dependency-jar.png)</ph></target>
        </trans-unit>
        <trans-unit id="216" translate="yes" xml:space="preserve">
          <source>Repeat this step for the other dependency jar as well.</source>
          <target state="new">Repeat this step for the other dependency jar as well.</target>
        </trans-unit>
        <trans-unit id="217" translate="yes" xml:space="preserve">
          <source>The <bpt id="p66">**</bpt>Output Layout<ept id="p66">**</ept><ph id="ph90" /> tab should now look like this.</source>
          <target state="new">The <bpt id="p66">**</bpt>Output Layout<ept id="p66">**</ept><ph id="ph90" /> tab should now look like this.</target>
        </trans-unit>
        <trans-unit id="218" translate="yes" xml:space="preserve">
          <source><ph id="ph91">![</ph>Final output tab<ph id="ph92">](./media/hdinsight-apache-spark-eventhub-streaming/final-output-tab.png)</ph></source>
          <target state="new"><ph id="ph91">![</ph>Final output tab<ph id="ph92">](./media/hdinsight-apache-spark-eventhub-streaming/final-output-tab.png)</ph></target>
        </trans-unit>
        <trans-unit id="219" translate="yes" xml:space="preserve">
          <source>In the <bpt id="p67">**</bpt>Project Structure<ept id="p67">**</ept><ph id="ph93" /> dialog box, click <bpt id="p68">**</bpt>Apply<ept id="p68">**</ept><ph id="ph94" /> and then click <bpt id="p69">**</bpt>OK<ept id="p69">**</ept>.</source>
          <target state="new">In the <bpt id="p67">**</bpt>Project Structure<ept id="p67">**</ept><ph id="ph93" /> dialog box, click <bpt id="p68">**</bpt>Apply<ept id="p68">**</ept><ph id="ph94" /> and then click <bpt id="p69">**</bpt>OK<ept id="p69">**</ept>.</target>
        </trans-unit>
        <trans-unit id="220" translate="yes" xml:space="preserve">
          <source>From the menu bar, click <bpt id="p70">**</bpt>Build<ept id="p70">**</ept>, and then click <bpt id="p71">**</bpt>Make Project<ept id="p71">**</ept>.</source>
          <target state="new">From the menu bar, click <bpt id="p70">**</bpt>Build<ept id="p70">**</ept>, and then click <bpt id="p71">**</bpt>Make Project<ept id="p71">**</ept>.</target>
        </trans-unit>
        <trans-unit id="221" translate="yes" xml:space="preserve">
          <source>You can also click <bpt id="p72">**</bpt>Build Artifacts<ept id="p72">**</ept><ph id="ph95" /> to create the jar.</source>
          <target state="new">You can also click <bpt id="p72">**</bpt>Build Artifacts<ept id="p72">**</ept><ph id="ph95" /> to create the jar.</target>
        </trans-unit>
        <trans-unit id="222" translate="yes" xml:space="preserve">
          <source>The output jar is created under <bpt id="p73">**</bpt>\out\artifacts<ept id="p73">**</ept>.</source>
          <target state="new">The output jar is created under <bpt id="p73">**</bpt>\out\artifacts<ept id="p73">**</ept>.</target>
        </trans-unit>
        <trans-unit id="223" translate="yes" xml:space="preserve">
          <source><ph id="ph96">![</ph>Create JAR<ph id="ph97">](./media/hdinsight-apache-spark-create-standalone-application/output.png)</ph></source>
          <target state="new"><ph id="ph96">![</ph>Create JAR<ph id="ph97">](./media/hdinsight-apache-spark-create-standalone-application/output.png)</ph></target>
        </trans-unit>
        <trans-unit id="224" translate="yes" xml:space="preserve">
          <source>Run the applications remotely on a Spark cluster using Livy</source>
          <target state="new">Run the applications remotely on a Spark cluster using Livy</target>
        </trans-unit>
        <trans-unit id="225" translate="yes" xml:space="preserve">
          <source>We will use Livy to run the streaming application remotely on a Spark cluster.</source>
          <target state="new">We will use Livy to run the streaming application remotely on a Spark cluster.</target>
        </trans-unit>
        <trans-unit id="226" translate="yes" xml:space="preserve">
          <source>For detailed discussion on how to use Livy with HDInsight Spark cluster, see <bpt id="p74">[</bpt>Submit jobs remotely to an Apache Spark cluster on Azure HDInsight<ept id="p74">](hdinsight-apache-spark-livy-rest-interface.md)</ept>.</source>
          <target state="new">For detailed discussion on how to use Livy with HDInsight Spark cluster, see <bpt id="p74">[</bpt>Submit jobs remotely to an Apache Spark cluster on Azure HDInsight<ept id="p74">](hdinsight-apache-spark-livy-rest-interface.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="227" translate="yes" xml:space="preserve">
          <source>Before you can start running the remote jobs to stream events using Spark there are a couple of things you should do:</source>
          <target state="new">Before you can start running the remote jobs to stream events using Spark there are a couple of things you should do:</target>
        </trans-unit>
        <trans-unit id="228" translate="yes" xml:space="preserve">
          <source>Start the local standalone application to generate events and sent to Event Hub.</source>
          <target state="new">Start the local standalone application to generate events and sent to Event Hub.</target>
        </trans-unit>
        <trans-unit id="229" translate="yes" xml:space="preserve">
          <source>Use the following command to do so:</source>
          <target state="new">Use the following command to do so:</target>
        </trans-unit>
        <trans-unit id="230" translate="yes" xml:space="preserve">
          <source>Copy the streaming jar (<bpt id="p75">**</bpt>microsoft-spark-streaming-examples.jar<ept id="p75">**</ept>) to the Azure Blob storage associated with the cluster.</source>
          <target state="new">Copy the streaming jar (<bpt id="p75">**</bpt>microsoft-spark-streaming-examples.jar<ept id="p75">**</ept>) to the Azure Blob storage associated with the cluster.</target>
        </trans-unit>
        <trans-unit id="231" translate="yes" xml:space="preserve">
          <source>This makes the jar accessible to Livy.</source>
          <target state="new">This makes the jar accessible to Livy.</target>
        </trans-unit>
        <trans-unit id="232" translate="yes" xml:space="preserve">
          <source>You can use <bpt id="p76">[</bpt><bpt id="p77">**</bpt>AzCopy<ept id="p77">**</ept><ept id="p76">](../storage/storage-use-azcopy.md)</ept>, a command line utility, to do so.</source>
          <target state="new">You can use <bpt id="p76">[</bpt><bpt id="p77">**</bpt>AzCopy<ept id="p77">**</ept><ept id="p76">](../storage/storage-use-azcopy.md)</ept>, a command line utility, to do so.</target>
        </trans-unit>
        <trans-unit id="233" translate="yes" xml:space="preserve">
          <source>There are a lot of other clients you can use to upload data.</source>
          <target state="new">There are a lot of other clients you can use to upload data.</target>
        </trans-unit>
        <trans-unit id="234" translate="yes" xml:space="preserve">
          <source>You can find more about them at <bpt id="p78">[</bpt>Upload data for Hadoop jobs in HDInsight<ept id="p78">](hdinsight-upload-data.md)</ept>.</source>
          <target state="new">You can find more about them at <bpt id="p78">[</bpt>Upload data for Hadoop jobs in HDInsight<ept id="p78">](hdinsight-upload-data.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="235" translate="yes" xml:space="preserve">
          <source>Install CURL on the computer where you are running these applications from.</source>
          <target state="new">Install CURL on the computer where you are running these applications from.</target>
        </trans-unit>
        <trans-unit id="236" translate="yes" xml:space="preserve">
          <source>We use CURL to invoke the Livy endpoints to run the jobs remotely.</source>
          <target state="new">We use CURL to invoke the Livy endpoints to run the jobs remotely.</target>
        </trans-unit>
        <trans-unit id="237" translate="yes" xml:space="preserve">
          <source>Run the applications to receive the events into an Azure Storage Blob as text</source>
          <target state="new">Run the applications to receive the events into an Azure Storage Blob as text</target>
        </trans-unit>
        <trans-unit id="238" translate="yes" xml:space="preserve">
          <source>Open a command prompt, navigate to the directory where you installed CURL, and run the following command (replace username/password and cluster name):</source>
          <target state="new">Open a command prompt, navigate to the directory where you installed CURL, and run the following command (replace username/password and cluster name):</target>
        </trans-unit>
        <trans-unit id="239" translate="yes" xml:space="preserve">
          <source>The parameters in the file <bpt id="p79">**</bpt>inputBlob.txt<ept id="p79">**</ept><ph id="ph98" /> are defined as follows:</source>
          <target state="new">The parameters in the file <bpt id="p79">**</bpt>inputBlob.txt<ept id="p79">**</ept><ph id="ph98" /> are defined as follows:</target>
        </trans-unit>
        <trans-unit id="240" translate="yes" xml:space="preserve">
          <source>Let us understand what the parameters in the input file are:</source>
          <target state="new">Let us understand what the parameters in the input file are:</target>
        </trans-unit>
        <trans-unit id="241" translate="yes" xml:space="preserve">
          <source><bpt id="p80">**</bpt>file<ept id="p80">**</ept><ph id="ph99" /> is the path to the application jar file on the Azure storage account associated with the cluster.</source>
          <target state="new"><bpt id="p80">**</bpt>file<ept id="p80">**</ept><ph id="ph99" /> is the path to the application jar file on the Azure storage account associated with the cluster.</target>
        </trans-unit>
        <trans-unit id="242" translate="yes" xml:space="preserve">
          <source><bpt id="p81">**</bpt>className<ept id="p81">**</ept><ph id="ph100" /> is the name of the class in the jar.</source>
          <target state="new"><bpt id="p81">**</bpt>className<ept id="p81">**</ept><ph id="ph100" /> is the name of the class in the jar.</target>
        </trans-unit>
        <trans-unit id="243" translate="yes" xml:space="preserve">
          <source><bpt id="p82">**</bpt>args<ept id="p82">**</ept><ph id="ph101" /> is the list of arguments required by the class</source>
          <target state="new"><bpt id="p82">**</bpt>args<ept id="p82">**</ept><ph id="ph101" /> is the list of arguments required by the class</target>
        </trans-unit>
        <trans-unit id="244" translate="yes" xml:space="preserve">
          <source><bpt id="p83">**</bpt>numExecutors<ept id="p83">**</ept><ph id="ph102" /> is the number of cores used by Spark to run the streaming application.</source>
          <target state="new"><bpt id="p83">**</bpt>numExecutors<ept id="p83">**</ept><ph id="ph102" /> is the number of cores used by Spark to run the streaming application.</target>
        </trans-unit>
        <trans-unit id="245" translate="yes" xml:space="preserve">
          <source>This should always be at least twice the number of Event Hub partitions.</source>
          <target state="new">This should always be at least twice the number of Event Hub partitions.</target>
        </trans-unit>
        <trans-unit id="246" translate="yes" xml:space="preserve">
          <source><bpt id="p84">**</bpt>executorMemory<ept id="p84">**</ept>, <bpt id="p85">**</bpt>executorCores<ept id="p85">**</ept>, <bpt id="p86">**</bpt>driverMemory<ept id="p86">**</ept><ph id="ph103" /> are parameters used to assign required resources to the streaming application.</source>
          <target state="new"><bpt id="p84">**</bpt>executorMemory<ept id="p84">**</ept>, <bpt id="p85">**</bpt>executorCores<ept id="p85">**</ept>, <bpt id="p86">**</bpt>driverMemory<ept id="p86">**</ept><ph id="ph103" /> are parameters used to assign required resources to the streaming application.</target>
        </trans-unit>
        <trans-unit id="247" translate="yes" xml:space="preserve">
          <source><ph id="ph104">[AZURE.NOTE]</ph><ph id="ph105" /> You do not need to create the output folders (EventCheckpoint, EventCount/EventCount10) that are used as parameters.</source>
          <target state="new"><ph id="ph104">[AZURE.NOTE]</ph><ph id="ph105" /> You do not need to create the output folders (EventCheckpoint, EventCount/EventCount10) that are used as parameters.</target>
        </trans-unit>
        <trans-unit id="248" translate="yes" xml:space="preserve">
          <source>The streaming application creates them for you.</source>
          <target state="new">The streaming application creates them for you.</target>
        </trans-unit>
        <trans-unit id="249" translate="yes" xml:space="preserve">
          <source>When you run the command, you should see an output like the following:</source>
          <target state="new">When you run the command, you should see an output like the following:</target>
        </trans-unit>
        <trans-unit id="250" translate="yes" xml:space="preserve">
          <source>Make a note of the batch ID in the last line of the output (in this example it is '1').</source>
          <target state="new">Make a note of the batch ID in the last line of the output (in this example it is '1').</target>
        </trans-unit>
        <trans-unit id="251" translate="yes" xml:space="preserve">
          <source>To verify that the application runs successfully, you can look at your Azure storage account associated with the cluster and you should see the <bpt id="p87">**</bpt>/EventCount/EventCount10<ept id="p87">**</ept><ph id="ph106" /> folder created there.</source>
          <target state="new">To verify that the application runs successfully, you can look at your Azure storage account associated with the cluster and you should see the <bpt id="p87">**</bpt>/EventCount/EventCount10<ept id="p87">**</ept><ph id="ph106" /> folder created there.</target>
        </trans-unit>
        <trans-unit id="252" translate="yes" xml:space="preserve">
          <source>This folder should contain blobs that captures the number of events processed within the time period specified for the parameter <bpt id="p88">**</bpt>batch-interval-in-seconds<ept id="p88">**</ept>.</source>
          <target state="new">This folder should contain blobs that captures the number of events processed within the time period specified for the parameter <bpt id="p88">**</bpt>batch-interval-in-seconds<ept id="p88">**</ept>.</target>
        </trans-unit>
        <trans-unit id="253" translate="yes" xml:space="preserve">
          <source>The application will continue to run until you kill it.</source>
          <target state="new">The application will continue to run until you kill it.</target>
        </trans-unit>
        <trans-unit id="254" translate="yes" xml:space="preserve">
          <source>To do so, use the following command:</source>
          <target state="new">To do so, use the following command:</target>
        </trans-unit>
        <trans-unit id="255" translate="yes" xml:space="preserve">
          <source>Run the applications to receive the events into an Azure Storage Blob as JSON</source>
          <target state="new">Run the applications to receive the events into an Azure Storage Blob as JSON</target>
        </trans-unit>
        <trans-unit id="256" translate="yes" xml:space="preserve">
          <source>Open a command prompt, navigate to the directory where you installed CURL, and run the following command (replace username/password and cluster name):</source>
          <target state="new">Open a command prompt, navigate to the directory where you installed CURL, and run the following command (replace username/password and cluster name):</target>
        </trans-unit>
        <trans-unit id="257" translate="yes" xml:space="preserve">
          <source>The parameters in the file <bpt id="p89">**</bpt>inputJSON.txt<ept id="p89">**</ept><ph id="ph107" /> are defined as follows:</source>
          <target state="new">The parameters in the file <bpt id="p89">**</bpt>inputJSON.txt<ept id="p89">**</ept><ph id="ph107" /> are defined as follows:</target>
        </trans-unit>
        <trans-unit id="258" translate="yes" xml:space="preserve">
          <source>The parameters are similar to what you specified for the text output, in the previous step.</source>
          <target state="new">The parameters are similar to what you specified for the text output, in the previous step.</target>
        </trans-unit>
        <trans-unit id="259" translate="yes" xml:space="preserve">
          <source>Again, you do not need to create the output folders (EventCheckpoint, EventCount/EventCount10) that are used as parameters.</source>
          <target state="new">Again, you do not need to create the output folders (EventCheckpoint, EventCount/EventCount10) that are used as parameters.</target>
        </trans-unit>
        <trans-unit id="260" translate="yes" xml:space="preserve">
          <source>The streaming application creates them for you.</source>
          <target state="new">The streaming application creates them for you.</target>
        </trans-unit>
        <trans-unit id="261" translate="yes" xml:space="preserve">
          <source>After you run the command, you can look at your Azure storage account associated with the cluster and you should see the <bpt id="p90">**</bpt>/EventStore10<ept id="p90">**</ept><ph id="ph108" /> folder created there.</source>
          <target state="new">After you run the command, you can look at your Azure storage account associated with the cluster and you should see the <bpt id="p90">**</bpt>/EventStore10<ept id="p90">**</ept><ph id="ph108" /> folder created there.</target>
        </trans-unit>
        <trans-unit id="262" translate="yes" xml:space="preserve">
          <source>Open any file prefixed with <bpt id="p91">**</bpt>part-<ept id="p91">**</ept><ph id="ph109" /> and you should see the events processed in a JSON format.</source>
          <target state="new">Open any file prefixed with <bpt id="p91">**</bpt>part-<ept id="p91">**</ept><ph id="ph109" /> and you should see the events processed in a JSON format.</target>
        </trans-unit>
        <trans-unit id="263" translate="yes" xml:space="preserve">
          <source>Run the applications to receive the events into a Hive table</source>
          <target state="new">Run the applications to receive the events into a Hive table</target>
        </trans-unit>
        <trans-unit id="264" translate="yes" xml:space="preserve">
          <source>To run the application that streams events into a Hive table you need some additional components.</source>
          <target state="new">To run the application that streams events into a Hive table you need some additional components.</target>
        </trans-unit>
        <trans-unit id="265" translate="yes" xml:space="preserve">
          <source>These are:</source>
          <target state="new">These are:</target>
        </trans-unit>
        <trans-unit id="266" translate="yes" xml:space="preserve">
          <source>datanucleus-api-jdo-3.2.6.jar</source>
          <target state="new">datanucleus-api-jdo-3.2.6.jar</target>
        </trans-unit>
        <trans-unit id="267" translate="yes" xml:space="preserve">
          <source>datanucleus-rdbms-3.2.9.jar</source>
          <target state="new">datanucleus-rdbms-3.2.9.jar</target>
        </trans-unit>
        <trans-unit id="268" translate="yes" xml:space="preserve">
          <source>datanucleus-core-3.2.10.jar</source>
          <target state="new">datanucleus-core-3.2.10.jar</target>
        </trans-unit>
        <trans-unit id="269" translate="yes" xml:space="preserve">
          <source>hive-site.xml</source>
          <target state="new">hive-site.xml</target>
        </trans-unit>
        <trans-unit id="270" translate="yes" xml:space="preserve">
          <source>The <bpt id="p92">**</bpt>.jar<ept id="p92">**</ept><ph id="ph110" /> files are available on your HDInsight Spark cluster at <ph id="ph111">`/usr/hdp/current/spark-client/lib`</ph>.</source>
          <target state="new">The <bpt id="p92">**</bpt>.jar<ept id="p92">**</ept><ph id="ph110" /> files are available on your HDInsight Spark cluster at <ph id="ph111">`/usr/hdp/current/spark-client/lib`</ph>.</target>
        </trans-unit>
        <trans-unit id="271" translate="yes" xml:space="preserve">
          <source>The <bpt id="p93">**</bpt>hive-site.xml<ept id="p93">**</ept><ph id="ph112" /> is available at <ph id="ph113">`/usr/hdp/current/spark-client/conf`</ph>.</source>
          <target state="new">The <bpt id="p93">**</bpt>hive-site.xml<ept id="p93">**</ept><ph id="ph112" /> is available at <ph id="ph113">`/usr/hdp/current/spark-client/conf`</ph>.</target>
        </trans-unit>
        <trans-unit id="272" translate="yes" xml:space="preserve">
          <source>You can use <bpt id="p94">[</bpt>WinScp<ept id="p94">](http://winscp.net/eng/download.php)</ept><ph id="ph114" /> to copy over these files from the cluster to your local computer.</source>
          <target state="new">You can use <bpt id="p94">[</bpt>WinScp<ept id="p94">](http://winscp.net/eng/download.php)</ept><ph id="ph114" /> to copy over these files from the cluster to your local computer.</target>
        </trans-unit>
        <trans-unit id="273" translate="yes" xml:space="preserve">
          <source>You can then use tools to copy these files over to your storage account associated with the cluster.</source>
          <target state="new">You can then use tools to copy these files over to your storage account associated with the cluster.</target>
        </trans-unit>
        <trans-unit id="274" translate="yes" xml:space="preserve">
          <source>For more information on how to upload files to the storage account, see <bpt id="p95">[</bpt>Upload data for Hadoop jobs in HDInsight<ept id="p95">](hdinsight-upload-data.md)</ept>.</source>
          <target state="new">For more information on how to upload files to the storage account, see <bpt id="p95">[</bpt>Upload data for Hadoop jobs in HDInsight<ept id="p95">](hdinsight-upload-data.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="275" translate="yes" xml:space="preserve">
          <source>Once you have copied over the files to your Azure storage account, open a command prompt, navigate to the directory where you installed CURL, and run the following command (replace username/password and cluster name):</source>
          <target state="new">Once you have copied over the files to your Azure storage account, open a command prompt, navigate to the directory where you installed CURL, and run the following command (replace username/password and cluster name):</target>
        </trans-unit>
        <trans-unit id="276" translate="yes" xml:space="preserve">
          <source>The parameters in the file <bpt id="p96">**</bpt>inputHive.txt<ept id="p96">**</ept><ph id="ph115" /> are defined as follows:</source>
          <target state="new">The parameters in the file <bpt id="p96">**</bpt>inputHive.txt<ept id="p96">**</ept><ph id="ph115" /> are defined as follows:</target>
        </trans-unit>
        <trans-unit id="277" translate="yes" xml:space="preserve">
          <source>The parameters are similar to what you specified for the text output, in the previous steps.</source>
          <target state="new">The parameters are similar to what you specified for the text output, in the previous steps.</target>
        </trans-unit>
        <trans-unit id="278" translate="yes" xml:space="preserve">
          <source>Again, you do not need to create the output folders (EventCheckpoint, EventCount/EventCount10) or the output Hive table (EventHiveTable10) that are used as parameters.</source>
          <target state="new">Again, you do not need to create the output folders (EventCheckpoint, EventCount/EventCount10) or the output Hive table (EventHiveTable10) that are used as parameters.</target>
        </trans-unit>
        <trans-unit id="279" translate="yes" xml:space="preserve">
          <source>The streaming application creates them for you.</source>
          <target state="new">The streaming application creates them for you.</target>
        </trans-unit>
        <trans-unit id="280" translate="yes" xml:space="preserve">
          <source>Note that the <bpt id="p97">**</bpt>jars<ept id="p97">**</ept><ph id="ph116" /> and <bpt id="p98">**</bpt>files<ept id="p98">**</ept><ph id="ph117" /> option includes paths to the .jar files and the hive-site.xml that you copied over to the storage account.</source>
          <target state="new">Note that the <bpt id="p97">**</bpt>jars<ept id="p97">**</ept><ph id="ph116" /> and <bpt id="p98">**</bpt>files<ept id="p98">**</ept><ph id="ph117" /> option includes paths to the .jar files and the hive-site.xml that you copied over to the storage account.</target>
        </trans-unit>
        <trans-unit id="281" translate="yes" xml:space="preserve">
          <source>To verify that the hive table was successfully created, you can SSH into the cluster and run Hive queries.</source>
          <target state="new">To verify that the hive table was successfully created, you can SSH into the cluster and run Hive queries.</target>
        </trans-unit>
        <trans-unit id="282" translate="yes" xml:space="preserve">
          <source>For instructions, see <bpt id="p99">[</bpt>Use Hive with Hadoop in HDInsight with SSH<ept id="p99">](hdinsight-hadoop-use-hive-ssh.md)</ept>.</source>
          <target state="new">For instructions, see <bpt id="p99">[</bpt>Use Hive with Hadoop in HDInsight with SSH<ept id="p99">](hdinsight-hadoop-use-hive-ssh.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="283" translate="yes" xml:space="preserve">
          <source>Once you are connected using SSH, you can run the following command to verify that the Hive table, <bpt id="p100">**</bpt>EventHiveTable10<ept id="p100">**</ept>, is created.</source>
          <target state="new">Once you are connected using SSH, you can run the following command to verify that the Hive table, <bpt id="p100">**</bpt>EventHiveTable10<ept id="p100">**</ept>, is created.</target>
        </trans-unit>
        <trans-unit id="284" translate="yes" xml:space="preserve">
          <source>You should see an output similar to the following:</source>
          <target state="new">You should see an output similar to the following:</target>
        </trans-unit>
        <trans-unit id="285" translate="yes" xml:space="preserve">
          <source>You can also run a SELECT query to view the contents of the table.</source>
          <target state="new">You can also run a SELECT query to view the contents of the table.</target>
        </trans-unit>
        <trans-unit id="286" translate="yes" xml:space="preserve">
          <source>You should see an output like the following:</source>
          <target state="new">You should see an output like the following:</target>
        </trans-unit>
        <trans-unit id="287" translate="yes" xml:space="preserve">
          <source>Run the applications to receive the events into an Azure SQL database table</source>
          <target state="new">Run the applications to receive the events into an Azure SQL database table</target>
        </trans-unit>
        <trans-unit id="288" translate="yes" xml:space="preserve">
          <source>Before running this step, make sure you have an Azure SQL database created.</source>
          <target state="new">Before running this step, make sure you have an Azure SQL database created.</target>
        </trans-unit>
        <trans-unit id="289" translate="yes" xml:space="preserve">
          <source>You will need values for database name, database server name, and the database administrator credentials as parameters.</source>
          <target state="new">You will need values for database name, database server name, and the database administrator credentials as parameters.</target>
        </trans-unit>
        <trans-unit id="290" translate="yes" xml:space="preserve">
          <source>You do not need to create the database table though.</source>
          <target state="new">You do not need to create the database table though.</target>
        </trans-unit>
        <trans-unit id="291" translate="yes" xml:space="preserve">
          <source>The streaming application creates that for you.</source>
          <target state="new">The streaming application creates that for you.</target>
        </trans-unit>
        <trans-unit id="292" translate="yes" xml:space="preserve">
          <source>Open a command prompt, navigate to the directory where you installed CURL, and run the following command:</source>
          <target state="new">Open a command prompt, navigate to the directory where you installed CURL, and run the following command:</target>
        </trans-unit>
        <trans-unit id="293" translate="yes" xml:space="preserve">
          <source>The parameters in the file <bpt id="p101">**</bpt>inputSQL.txt<ept id="p101">**</ept><ph id="ph118" /> are defined as follows:</source>
          <target state="new">The parameters in the file <bpt id="p101">**</bpt>inputSQL.txt<ept id="p101">**</ept><ph id="ph118" /> are defined as follows:</target>
        </trans-unit>
        <trans-unit id="294" translate="yes" xml:space="preserve">
          <source>To verify that the application runs successfully, you can connect to the Azure SQL database using SQL Server Management Studio.</source>
          <target state="new">To verify that the application runs successfully, you can connect to the Azure SQL database using SQL Server Management Studio.</target>
        </trans-unit>
        <trans-unit id="295" translate="yes" xml:space="preserve">
          <source>For instructions on how to do that, see <bpt id="p102">[</bpt>Connect to SQL Database with SQL Server Management Studio<ept id="p102">](sql-database/sql-database-connect-query-ssms)</ept>.</source>
          <target state="new">For instructions on how to do that, see <bpt id="p102">[</bpt>Connect to SQL Database with SQL Server Management Studio<ept id="p102">](sql-database/sql-database-connect-query-ssms)</ept>.</target>
        </trans-unit>
        <trans-unit id="296" translate="yes" xml:space="preserve">
          <source>Once you are connected to the database, you can navigate to the <bpt id="p103">**</bpt>EventContent<ept id="p103">**</ept><ph id="ph119" /> table that was created by the streaming application.</source>
          <target state="new">Once you are connected to the database, you can navigate to the <bpt id="p103">**</bpt>EventContent<ept id="p103">**</ept><ph id="ph119" /> table that was created by the streaming application.</target>
        </trans-unit>
        <trans-unit id="297" translate="yes" xml:space="preserve">
          <source>You can run a quick query to get the data from the table.</source>
          <target state="new">You can run a quick query to get the data from the table.</target>
        </trans-unit>
        <trans-unit id="298" translate="yes" xml:space="preserve">
          <source>Run the following query:</source>
          <target state="new">Run the following query:</target>
        </trans-unit>
        <trans-unit id="299" translate="yes" xml:space="preserve">
          <source>You should see output similar to the following:</source>
          <target state="new">You should see output similar to the following:</target>
        </trans-unit>
        <trans-unit id="300" translate="yes" xml:space="preserve">
          <source>See also</source>
          <target state="new">See also</target>
        </trans-unit>
        <trans-unit id="301" translate="yes" xml:space="preserve">
          <source><bpt id="p104">[</bpt>Overview: Apache Spark on Azure HDInsight<ept id="p104">](hdinsight-apache-spark-overview.md)</ept></source>
          <target state="new"><bpt id="p104">[</bpt>Overview: Apache Spark on Azure HDInsight<ept id="p104">](hdinsight-apache-spark-overview.md)</ept></target>
        </trans-unit>
        <trans-unit id="302" translate="yes" xml:space="preserve">
          <source>Scenarios</source>
          <target state="new">Scenarios</target>
        </trans-unit>
        <trans-unit id="303" translate="yes" xml:space="preserve">
          <source><bpt id="p105">[</bpt>Spark with BI: Perform interactive data analysis using Spark in HDInsight with BI tools<ept id="p105">](hdinsight-apache-spark-use-bi-tools.md)</ept></source>
          <target state="new"><bpt id="p105">[</bpt>Spark with BI: Perform interactive data analysis using Spark in HDInsight with BI tools<ept id="p105">](hdinsight-apache-spark-use-bi-tools.md)</ept></target>
        </trans-unit>
        <trans-unit id="304" translate="yes" xml:space="preserve">
          <source><bpt id="p106">[</bpt>Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data<ept id="p106">](hdinsight-apache-spark-ipython-notebook-machine-learning.md)</ept></source>
          <target state="new"><bpt id="p106">[</bpt>Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data<ept id="p106">](hdinsight-apache-spark-ipython-notebook-machine-learning.md)</ept></target>
        </trans-unit>
        <trans-unit id="305" translate="yes" xml:space="preserve">
          <source><bpt id="p107">[</bpt>Spark with Machine Learning: Use Spark in HDInsight to predict food inspection results<ept id="p107">](hdinsight-apache-spark-machine-learning-mllib-ipython.md)</ept></source>
          <target state="new"><bpt id="p107">[</bpt>Spark with Machine Learning: Use Spark in HDInsight to predict food inspection results<ept id="p107">](hdinsight-apache-spark-machine-learning-mllib-ipython.md)</ept></target>
        </trans-unit>
        <trans-unit id="306" translate="yes" xml:space="preserve">
          <source><bpt id="p108">[</bpt>Website log analysis using Spark in HDInsight<ept id="p108">](hdinsight-apache-spark-custom-library-website-log-analysis.md)</ept></source>
          <target state="new"><bpt id="p108">[</bpt>Website log analysis using Spark in HDInsight<ept id="p108">](hdinsight-apache-spark-custom-library-website-log-analysis.md)</ept></target>
        </trans-unit>
        <trans-unit id="307" translate="yes" xml:space="preserve">
          <source>Create and run applications</source>
          <target state="new">Create and run applications</target>
        </trans-unit>
        <trans-unit id="308" translate="yes" xml:space="preserve">
          <source><bpt id="p109">[</bpt>Create a standalone application using Scala<ept id="p109">](hdinsight-apache-spark-create-standalone-application.md)</ept></source>
          <target state="new"><bpt id="p109">[</bpt>Create a standalone application using Scala<ept id="p109">](hdinsight-apache-spark-create-standalone-application.md)</ept></target>
        </trans-unit>
        <trans-unit id="309" translate="yes" xml:space="preserve">
          <source><bpt id="p110">[</bpt>Run jobs remotely on a Spark cluster using Livy<ept id="p110">](hdinsight-apache-spark-livy-rest-interface.md)</ept></source>
          <target state="new"><bpt id="p110">[</bpt>Run jobs remotely on a Spark cluster using Livy<ept id="p110">](hdinsight-apache-spark-livy-rest-interface.md)</ept></target>
        </trans-unit>
        <trans-unit id="310" translate="yes" xml:space="preserve">
          <source>Tools and extensions</source>
          <target state="new">Tools and extensions</target>
        </trans-unit>
        <trans-unit id="311" translate="yes" xml:space="preserve">
          <source><bpt id="p111">[</bpt>Use HDInsight Tools Plugin for IntelliJ IDEA to create and submit Spark Scala applicatons<ept id="p111">](hdinsight-apache-spark-intellij-tool-plugin.md)</ept></source>
          <target state="new"><bpt id="p111">[</bpt>Use HDInsight Tools Plugin for IntelliJ IDEA to create and submit Spark Scala applicatons<ept id="p111">](hdinsight-apache-spark-intellij-tool-plugin.md)</ept></target>
        </trans-unit>
        <trans-unit id="312" translate="yes" xml:space="preserve">
          <source><bpt id="p112">[</bpt>Use Zeppelin notebooks with a Spark cluster on HDInsight<ept id="p112">](hdinsight-apache-spark-use-zeppelin-notebook.md)</ept></source>
          <target state="new"><bpt id="p112">[</bpt>Use Zeppelin notebooks with a Spark cluster on HDInsight<ept id="p112">](hdinsight-apache-spark-use-zeppelin-notebook.md)</ept></target>
        </trans-unit>
        <trans-unit id="313" translate="yes" xml:space="preserve">
          <source><bpt id="p113">[</bpt>Kernels available for Jupyter notebook in Spark cluster for HDInsight<ept id="p113">](hdinsight-apache-spark-jupyter-notebook-kernels.md)</ept></source>
          <target state="new"><bpt id="p113">[</bpt>Kernels available for Jupyter notebook in Spark cluster for HDInsight<ept id="p113">](hdinsight-apache-spark-jupyter-notebook-kernels.md)</ept></target>
        </trans-unit>
        <trans-unit id="314" translate="yes" xml:space="preserve">
          <source>Manage resources</source>
          <target state="new">Manage resources</target>
        </trans-unit>
        <trans-unit id="315" translate="yes" xml:space="preserve">
          <source><bpt id="p114">[</bpt>Manage resources for the Apache Spark cluster in Azure HDInsight<ept id="p114">](hdinsight-apache-spark-resource-manager.md)</ept></source>
          <target state="new"><bpt id="p114">[</bpt>Manage resources for the Apache Spark cluster in Azure HDInsight<ept id="p114">](hdinsight-apache-spark-resource-manager.md)</ept></target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">086ff54211a0748b46954bcb9e3e0f7aab896428</xliffext:olfilehash>
  </header>
</xliff>