{
  "nodes": [
    {
      "pos": [
        27,
        85
      ],
      "content": "Learn what is Hive and how to use HiveQL | Microsoft Azure"
    },
    {
      "pos": [
        104,
        258
      ],
      "content": "Learn about Apache Hive and how to use it with Hadoop in HDInsight. Choose how to run your Hive job, and use HiveQL to analyze a sample Apache log4j file.",
      "nodes": [
        {
          "content": "Learn about Apache Hive and how to use it with Hadoop in HDInsight.",
          "pos": [
            0,
            67
          ]
        },
        {
          "content": "Choose how to run your Hive job, and use HiveQL to analyze a sample Apache log4j file.",
          "pos": [
            68,
            154
          ]
        }
      ]
    },
    {
      "pos": [
        623,
        705
      ],
      "content": "Use Hive and HiveQL with Hadoop in HDInsight to analyze a sample Apache log4j file"
    },
    {
      "pos": [
        788,
        980
      ],
      "content": "In this tutorial, you'll learn how to use Apache Hive in Hadoop on HDInsight, and choose how to run your Hive job. You'll also learn about HiveQL and how to analyze a sample Apache log4j file.",
      "nodes": [
        {
          "content": "In this tutorial, you'll learn how to use Apache Hive in Hadoop on HDInsight, and choose how to run your Hive job.",
          "pos": [
            0,
            114
          ]
        },
        {
          "content": "You'll also learn about HiveQL and how to analyze a sample Apache log4j file.",
          "pos": [
            115,
            192
          ]
        }
      ]
    },
    {
      "pos": [
        982,
        1319
      ],
      "content": "##<ph id=\"ph3\">&lt;a id=\"why\"&gt;</ph><ph id=\"ph4\">&lt;/a&gt;</ph>What is Hive and why use it?\n<bpt id=\"p1\">[</bpt>Apache Hive<ept id=\"p1\">](http://hive.apache.org/)</ept><ph id=\"ph5\"/> is a data warehouse system for Hadoop, which enables data summarization, querying, and analysis of data by using HiveQL (a query language similar to SQL). Hive can be used to interactively explore your data or to create reusable batch processing jobs.",
      "nodes": [
        {
          "content": "<ph id=\"ph3\">&lt;a id=\"why\"&gt;</ph><ph id=\"ph4\">&lt;/a&gt;</ph>What is Hive and why use it?",
          "pos": [
            2,
            94
          ]
        },
        {
          "content": "<bpt id=\"p1\">[</bpt>Apache Hive<ept id=\"p1\">](http://hive.apache.org/)</ept><ph id=\"ph5\"/> is a data warehouse system for Hadoop, which enables data summarization, querying, and analysis of data by using HiveQL (a query language similar to SQL).",
          "pos": [
            95,
            340
          ]
        },
        {
          "content": "Hive can be used to interactively explore your data or to create reusable batch processing jobs.",
          "pos": [
            341,
            437
          ]
        }
      ]
    },
    {
      "pos": [
        1321,
        1603
      ],
      "content": "Hive allows you to project structure on largely unstructured data. After you define the structure, you can use Hive to query that data without knowledge of Java or MapReduce. <bpt id=\"p2\">**</bpt>HiveQL<ept id=\"p2\">**</ept><ph id=\"ph6\"/> (the Hive query language) allows you to write queries with statements that are similar to T-SQL.",
      "nodes": [
        {
          "content": "Hive allows you to project structure on largely unstructured data.",
          "pos": [
            0,
            66
          ]
        },
        {
          "content": "After you define the structure, you can use Hive to query that data without knowledge of Java or MapReduce.",
          "pos": [
            67,
            174
          ]
        },
        {
          "content": "<bpt id=\"p2\">**</bpt>HiveQL<ept id=\"p2\">**</ept><ph id=\"ph6\"/> (the Hive query language) allows you to write queries with statements that are similar to T-SQL.",
          "pos": [
            175,
            334
          ]
        }
      ]
    },
    {
      "pos": [
        1605,
        2057
      ],
      "content": "Hive understands how to work with structured and semi-structured data, such as text files where the fields are delimited by specific characters. Hive also supports custom <bpt id=\"p3\">**</bpt>serializer/deserializers (SerDe)<ept id=\"p3\">**</ept><ph id=\"ph7\"/> for complex or irregularly structured data. For more information, see <bpt id=\"p4\">[</bpt>How to use a custom JSON SerDe with HDInsight<ept id=\"p4\">](http://blogs.msdn.com/b/bigdatasupport/archive/2014/06/18/how-to-use-a-custom-json-serde-with-microsoft-azure-hdinsight.aspx)</ept>.",
      "nodes": [
        {
          "content": "Hive understands how to work with structured and semi-structured data, such as text files where the fields are delimited by specific characters.",
          "pos": [
            0,
            144
          ]
        },
        {
          "content": "Hive also supports custom <bpt id=\"p3\">**</bpt>serializer/deserializers (SerDe)<ept id=\"p3\">**</ept><ph id=\"ph7\"/> for complex or irregularly structured data.",
          "pos": [
            145,
            303
          ]
        },
        {
          "content": "For more information, see <bpt id=\"p4\">[</bpt>How to use a custom JSON SerDe with HDInsight<ept id=\"p4\">](http://blogs.msdn.com/b/bigdatasupport/archive/2014/06/18/how-to-use-a-custom-json-serde-with-microsoft-azure-hdinsight.aspx)</ept>.",
          "pos": [
            304,
            542
          ]
        }
      ]
    },
    {
      "pos": [
        2059,
        2275
      ],
      "content": "Hive can also be extended through <bpt id=\"p5\">**</bpt>user-defined functions (UDF)<ept id=\"p5\">**</ept>. A UDF allows you to implement functionality or logic that isn't easily modeled in HiveQL. For an example of using UDFs with Hive, see the following:",
      "nodes": [
        {
          "content": "Hive can also be extended through <bpt id=\"p5\">**</bpt>user-defined functions (UDF)<ept id=\"p5\">**</ept>.",
          "pos": [
            0,
            105
          ]
        },
        {
          "content": "A UDF allows you to implement functionality or logic that isn't easily modeled in HiveQL.",
          "pos": [
            106,
            195
          ]
        },
        {
          "content": "For an example of using UDFs with Hive, see the following:",
          "pos": [
            196,
            254
          ]
        }
      ]
    },
    {
      "pos": [
        2279,
        2345
      ],
      "content": "<bpt id=\"p6\">[</bpt>Using Python with Hive and Pig in HDInsight<ept id=\"p6\">](hdinsight-python.md)</ept>"
    },
    {
      "pos": [
        2349,
        2436
      ],
      "content": "<bpt id=\"p7\">[</bpt>Use C# with Hive and Pig in HDInsight<ept id=\"p7\">](hdinsight-hadoop-hive-pig-udf-dotnet-csharp.md)</ept>"
    },
    {
      "pos": [
        2440,
        2588
      ],
      "content": "<bpt id=\"p8\">[</bpt>How to add a custom Hive UDF to HDInsight<ept id=\"p8\">](http://blogs.msdn.com/b/bigdatasupport/archive/2014/01/14/how-to-add-custom-hive-udfs-to-hdinsight.aspx)</ept>"
    },
    {
      "pos": [
        2594,
        2633
      ],
      "content": "Hive internal tables vs external tables"
    },
    {
      "pos": [
        2635,
        2724
      ],
      "content": "There are a few things you need to know about the Hive internal table and external table:"
    },
    {
      "pos": [
        2728,
        2839
      ],
      "content": "The <bpt id=\"p9\">**</bpt>CREATE TABLE<ept id=\"p9\">**</ept><ph id=\"ph8\"/> command creates an internal table. The data file must be located in the default container.",
      "nodes": [
        {
          "content": "The <bpt id=\"p9\">**</bpt>CREATE TABLE<ept id=\"p9\">**</ept><ph id=\"ph8\"/> command creates an internal table.",
          "pos": [
            0,
            107
          ]
        },
        {
          "content": "The data file must be located in the default container.",
          "pos": [
            108,
            163
          ]
        }
      ]
    },
    {
      "pos": [
        2842,
        2933
      ],
      "content": "The <bpt id=\"p10\">**</bpt>CREATE TABLE<ept id=\"p10\">**</ept><ph id=\"ph9\"/> command moves the data file to the /hive/warehouse/<ph id=\"ph10\">&lt;TableName&gt;</ph><ph id=\"ph11\"/> folder."
    },
    {
      "pos": [
        2936,
        3060
      ],
      "content": "The <bpt id=\"p11\">**</bpt>CREATE EXTERNAL TABLE<ept id=\"p11\">**</ept><ph id=\"ph12\"/> command creates an external table. The data file can be located outside the default container.",
      "nodes": [
        {
          "content": "The <bpt id=\"p11\">**</bpt>CREATE EXTERNAL TABLE<ept id=\"p11\">**</ept><ph id=\"ph12\"/> command creates an external table.",
          "pos": [
            0,
            119
          ]
        },
        {
          "content": "The data file can be located outside the default container.",
          "pos": [
            120,
            179
          ]
        }
      ]
    },
    {
      "pos": [
        3063,
        3129
      ],
      "content": "The <bpt id=\"p12\">**</bpt>CREATE EXTERNAL TABLE<ept id=\"p12\">**</ept><ph id=\"ph13\"/> command does not move the data file."
    },
    {
      "pos": [
        3132,
        3285
      ],
      "content": "The <bpt id=\"p13\">**</bpt>CREATE EXTERNAL TABLE<ept id=\"p13\">**</ept><ph id=\"ph14\"/> command doesn't allow any folders in the LOCATION. This is the reason why the tutorial makes a copy of the sample.log file.",
      "nodes": [
        {
          "content": "The <bpt id=\"p13\">**</bpt>CREATE EXTERNAL TABLE<ept id=\"p13\">**</ept><ph id=\"ph14\"/> command doesn't allow any folders in the LOCATION.",
          "pos": [
            0,
            135
          ]
        },
        {
          "content": "This is the reason why the tutorial makes a copy of the sample.log file.",
          "pos": [
            136,
            208
          ]
        }
      ]
    },
    {
      "pos": [
        3287,
        3390
      ],
      "content": "For more information, see <bpt id=\"p14\">[</bpt>HDInsight: Hive Internal and External Tables Intro<ept id=\"p14\">][cindygross-hive-tables]</ept>."
    },
    {
      "pos": [
        3393,
        3395
      ],
      "content": "##"
    },
    {
      "pos": [
        3412,
        3455
      ],
      "content": "About the sample data, an Apache log4j file"
    },
    {
      "pos": [
        3457,
        3715
      ],
      "content": "This example uses a <bpt id=\"p15\">*</bpt>log4j<ept id=\"p15\">*</ept><ph id=\"ph15\"/> sample file, which is stored at <bpt id=\"p16\">**</bpt>/example/data/sample.log<ept id=\"p16\">**</ept><ph id=\"ph16\"/> in your blob storage container. Each log inside the file consists of a line of fields that contains a <ph id=\"ph17\">`[LOG LEVEL]`</ph><ph id=\"ph18\"/> field to show the type and the severity, for example:",
      "nodes": [
        {
          "content": "This example uses a <bpt id=\"p15\">*</bpt>log4j<ept id=\"p15\">*</ept><ph id=\"ph15\"/> sample file, which is stored at <bpt id=\"p16\">**</bpt>/example/data/sample.log<ept id=\"p16\">**</ept><ph id=\"ph16\"/> in your blob storage container.",
          "pos": [
            0,
            230
          ]
        },
        {
          "content": "Each log inside the file consists of a line of fields that contains a <ph id=\"ph17\">`[LOG LEVEL]`</ph><ph id=\"ph18\"/> field to show the type and the severity, for example:",
          "pos": [
            231,
            402
          ]
        }
      ]
    },
    {
      "pos": [
        3796,
        3844
      ],
      "content": "In the previous example, the log level is ERROR."
    },
    {
      "pos": [
        3848,
        4253
      ],
      "content": "<ph id=\"ph19\">[AZURE.NOTE]</ph><ph id=\"ph20\"/> You can also generate a log4j file by using the <bpt id=\"p17\">[</bpt>Apache Log4j<ept id=\"p17\">](http://en.wikipedia.org/wiki/Log4j)</ept><ph id=\"ph21\"/> logging tool and then upload that file to the blob container. See <bpt id=\"p18\">[</bpt>Upload Data to HDInsight<ept id=\"p18\">](hdinsight-upload-data.md)</ept><ph id=\"ph22\"/> for instructions. For more information about how Azure Blob storage is used with HDInsight, see <bpt id=\"p19\">[</bpt>Use Azure Blob Storage with HDInsight<ept id=\"p19\">](hdinsight-hadoop-use-blob-storage.md)</ept>.",
      "nodes": [
        {
          "content": "<ph id=\"ph19\">[AZURE.NOTE]</ph><ph id=\"ph20\"/> You can also generate a log4j file by using the <bpt id=\"p17\">[</bpt>Apache Log4j<ept id=\"p17\">](http://en.wikipedia.org/wiki/Log4j)</ept><ph id=\"ph21\"/> logging tool and then upload that file to the blob container.",
          "pos": [
            0,
            262
          ]
        },
        {
          "content": "See <bpt id=\"p18\">[</bpt>Upload Data to HDInsight<ept id=\"p18\">](hdinsight-upload-data.md)</ept><ph id=\"ph22\"/> for instructions.",
          "pos": [
            263,
            392
          ]
        },
        {
          "content": "For more information about how Azure Blob storage is used with HDInsight, see <bpt id=\"p19\">[</bpt>Use Azure Blob Storage with HDInsight<ept id=\"p19\">](hdinsight-hadoop-use-blob-storage.md)</ept>.",
          "pos": [
            393,
            589
          ]
        }
      ]
    },
    {
      "pos": [
        4255,
        4505
      ],
      "content": "The sample data is stored in Azure Blob storage, which HDInsight uses as the default file system. HDInsight can access files stored in blobs by using the <bpt id=\"p20\">**</bpt>wasb<ept id=\"p20\">**</ept><ph id=\"ph23\"/> prefix. For example, to access the sample.log file, you would use the following syntax:",
      "nodes": [
        {
          "content": "The sample data is stored in Azure Blob storage, which HDInsight uses as the default file system.",
          "pos": [
            0,
            97
          ]
        },
        {
          "content": "HDInsight can access files stored in blobs by using the <bpt id=\"p20\">**</bpt>wasb<ept id=\"p20\">**</ept><ph id=\"ph23\"/> prefix.",
          "pos": [
            98,
            225
          ]
        },
        {
          "content": "For example, to access the sample.log file, you would use the following syntax:",
          "pos": [
            226,
            305
          ]
        }
      ]
    },
    {
      "pos": [
        4544,
        4688
      ],
      "content": "Because Azure Blob storage is the default storage for HDInsight, you can also access the file by using <bpt id=\"p21\">**</bpt>/example/data/sample.log<ept id=\"p21\">**</ept><ph id=\"ph24\"/> from HiveQL."
    },
    {
      "pos": [
        4692,
        5136
      ],
      "content": "<ph id=\"ph25\">[AZURE.NOTE]</ph><ph id=\"ph26\"/> The syntax, <bpt id=\"p22\">**</bpt>wasb:///<ept id=\"p22\">**</ept>, is used to access files stored in the default storage container for your HDInsight cluster. If you specified additional storage accounts when you provisioned your cluster, and you want to access files stored in these accounts, you can access the data by specifying the container name and storage account address, for example, <bpt id=\"p23\">**</bpt>wasb://mycontainer@mystorage.blob.core.windows.net/example/data/sample.log<ept id=\"p23\">**</ept>.",
      "nodes": [
        {
          "content": "<ph id=\"ph25\">[AZURE.NOTE]</ph><ph id=\"ph26\"/> The syntax, <bpt id=\"p22\">**</bpt>wasb:///<ept id=\"p22\">**</ept>, is used to access files stored in the default storage container for your HDInsight cluster.",
          "pos": [
            0,
            204
          ]
        },
        {
          "content": "If you specified additional storage accounts when you provisioned your cluster, and you want to access files stored in these accounts, you can access the data by specifying the container name and storage account address, for example, <bpt id=\"p23\">**</bpt>wasb://mycontainer@mystorage.blob.core.windows.net/example/data/sample.log<ept id=\"p23\">**</ept>.",
          "pos": [
            205,
            558
          ]
        }
      ]
    },
    {
      "pos": [
        5138,
        5140
      ],
      "content": "##"
    },
    {
      "pos": [
        5156,
        5203
      ],
      "content": "Sample job: Project columns onto delimited data"
    },
    {
      "pos": [
        5205,
        5335
      ],
      "content": "The following HiveQL statements will project columns onto delimited data that is stored in the <bpt id=\"p24\">**</bpt>wasb:///example/data<ept id=\"p24\">**</ept><ph id=\"ph27\"/> directory:"
    },
    {
      "pos": [
        5709,
        5786
      ],
      "content": "In the previous example, the HiveQL statements perform the following actions:"
    },
    {
      "pos": [
        5790,
        5870
      ],
      "content": "<bpt id=\"p25\">**</bpt>DROP TABLE<ept id=\"p25\">**</ept>: Deletes the table and the data file if the table already exists."
    },
    {
      "pos": [
        5873,
        6068
      ],
      "content": "<bpt id=\"p26\">**</bpt>CREATE EXTERNAL TABLE<ept id=\"p26\">**</ept>: Creates a new <bpt id=\"p27\">**</bpt>external<ept id=\"p27\">**</ept><ph id=\"ph28\"/> table in Hive. External tables only store the table definition in Hive; the data is left in the original location and in the original format.",
      "nodes": [
        {
          "content": "<bpt id=\"p26\">**</bpt>CREATE EXTERNAL TABLE<ept id=\"p26\">**</ept>: Creates a new <bpt id=\"p27\">**</bpt>external<ept id=\"p27\">**</ept><ph id=\"ph28\"/> table in Hive.",
          "pos": [
            0,
            163
          ]
        },
        {
          "content": "External tables only store the table definition in Hive; the data is left in the original location and in the original format.",
          "pos": [
            164,
            290
          ]
        }
      ]
    },
    {
      "pos": [
        6071,
        6187
      ],
      "content": "<bpt id=\"p28\">**</bpt>ROW FORMAT<ept id=\"p28\">**</ept>: Tells Hive how the data is formatted. In this case, the fields in each log are separated by a space.",
      "nodes": [
        {
          "content": "<bpt id=\"p28\">**</bpt>ROW FORMAT<ept id=\"p28\">**</ept>: Tells Hive how the data is formatted.",
          "pos": [
            0,
            93
          ]
        },
        {
          "content": "In this case, the fields in each log are separated by a space.",
          "pos": [
            94,
            156
          ]
        }
      ]
    },
    {
      "pos": [
        6190,
        6400
      ],
      "content": "<bpt id=\"p29\">**</bpt>STORED AS TEXTFILE LOCATION<ept id=\"p29\">**</ept>: Tells Hive where the data is stored (the example/data directory) and that it is stored as text. The data can be in one file or spread across multiple files within the directory.",
      "nodes": [
        {
          "content": "<bpt id=\"p29\">**</bpt>STORED AS TEXTFILE LOCATION<ept id=\"p29\">**</ept>: Tells Hive where the data is stored (the example/data directory) and that it is stored as text.",
          "pos": [
            0,
            168
          ]
        },
        {
          "content": "The data can be in one file or spread across multiple files within the directory.",
          "pos": [
            169,
            250
          ]
        }
      ]
    },
    {
      "pos": [
        6403,
        6588
      ],
      "content": "<bpt id=\"p30\">**</bpt>SELECT<ept id=\"p30\">**</ept>: Selects a count of all rows where the column <bpt id=\"p31\">**</bpt>t4<ept id=\"p31\">**</ept><ph id=\"ph29\"/> contains the value <bpt id=\"p32\">**</bpt>[ERROR]<ept id=\"p32\">**</ept>. This should return a value of <bpt id=\"p33\">**</bpt>3<ept id=\"p33\">**</ept><ph id=\"ph30\"/> because there are three rows that contain this value.",
      "nodes": [
        {
          "content": "<bpt id=\"p30\">**</bpt>SELECT<ept id=\"p30\">**</ept>: Selects a count of all rows where the column <bpt id=\"p31\">**</bpt>t4<ept id=\"p31\">**</ept><ph id=\"ph29\"/> contains the value <bpt id=\"p32\">**</bpt>[ERROR]<ept id=\"p32\">**</ept>.",
          "pos": [
            0,
            230
          ]
        },
        {
          "content": "This should return a value of <bpt id=\"p33\">**</bpt>3<ept id=\"p33\">**</ept><ph id=\"ph30\"/> because there are three rows that contain this value.",
          "pos": [
            231,
            375
          ]
        }
      ]
    },
    {
      "pos": [
        6591,
        6874
      ],
      "content": "<bpt id=\"p34\">**</bpt>INPUT__FILE__NAME LIKE '%.log'<ept id=\"p34\">**</ept><ph id=\"ph31\"/> - Tells Hive that we should only return data from files ending in .log. This restricts the search to the sample.log file that contains the data, and keeps it from returning data from other example data files that do not match the schema we defined.",
      "nodes": [
        {
          "content": "<bpt id=\"p34\">**</bpt>INPUT__FILE__NAME LIKE '%.log'<ept id=\"p34\">**</ept><ph id=\"ph31\"/> - Tells Hive that we should only return data from files ending in .log.",
          "pos": [
            0,
            161
          ]
        },
        {
          "content": "This restricts the search to the sample.log file that contains the data, and keeps it from returning data from other example data files that do not match the schema we defined.",
          "pos": [
            162,
            338
          ]
        }
      ]
    },
    {
      "pos": [
        6878,
        7128
      ],
      "content": "<ph id=\"ph32\">[AZURE.NOTE]</ph><ph id=\"ph33\"/> External tables should be used when you expect the underlying data to be updated by an external source, such as an automated data upload process, or by another MapReduce operation, and you always want Hive queries to use the latest data."
    },
    {
      "pos": [
        7133,
        7227
      ],
      "content": "Dropping an external table does <bpt id=\"p35\">**</bpt>not<ept id=\"p35\">**</ept><ph id=\"ph34\"/> delete the data, it only deletes the table definition."
    },
    {
      "pos": [
        7229,
        7330
      ],
      "content": "After creating the external table, the following statements are used to create an <bpt id=\"p36\">**</bpt>internal<ept id=\"p36\">**</ept><ph id=\"ph35\"/> table."
    },
    {
      "pos": [
        7618,
        7665
      ],
      "content": "These statements perform the following actions:"
    },
    {
      "pos": [
        7669,
        7898
      ],
      "content": "<bpt id=\"p37\">**</bpt>CREATE TABLE IF NOT EXISTS<ept id=\"p37\">**</ept>: Creates a table, if it does not already exist. Because the <bpt id=\"p38\">**</bpt>EXTERNAL<ept id=\"p38\">**</ept><ph id=\"ph36\"/> keyword is not used, this is an internal table, which is stored in the Hive data warehouse and is managed completely by Hive.",
      "nodes": [
        {
          "content": "<bpt id=\"p37\">**</bpt>CREATE TABLE IF NOT EXISTS<ept id=\"p37\">**</ept>: Creates a table, if it does not already exist.",
          "pos": [
            0,
            118
          ]
        },
        {
          "content": "Because the <bpt id=\"p38\">**</bpt>EXTERNAL<ept id=\"p38\">**</ept><ph id=\"ph36\"/> keyword is not used, this is an internal table, which is stored in the Hive data warehouse and is managed completely by Hive.",
          "pos": [
            119,
            324
          ]
        }
      ]
    },
    {
      "pos": [
        7901,
        8046
      ],
      "content": "<bpt id=\"p39\">**</bpt>STORED AS ORC<ept id=\"p39\">**</ept>: Stores the data in Optimized Row Columnar (ORC) format. This is a highly optimized and efficient format for storing Hive data.",
      "nodes": [
        {
          "content": "<bpt id=\"p39\">**</bpt>STORED AS ORC<ept id=\"p39\">**</ept>: Stores the data in Optimized Row Columnar (ORC) format.",
          "pos": [
            0,
            114
          ]
        },
        {
          "content": "This is a highly optimized and efficient format for storing Hive data.",
          "pos": [
            115,
            185
          ]
        }
      ]
    },
    {
      "pos": [
        8049,
        8206
      ],
      "content": "<bpt id=\"p40\">**</bpt>INSERT OVERWRITE ... SELECT<ept id=\"p40\">**</ept>: Selects rows from the <bpt id=\"p41\">**</bpt>log4jLogs<ept id=\"p41\">**</ept><ph id=\"ph37\"/> table that contains <bpt id=\"p42\">**</bpt>[ERROR]<ept id=\"p42\">**</ept>, and then inserts the data into the <bpt id=\"p43\">**</bpt>errorLogs<ept id=\"p43\">**</ept><ph id=\"ph38\"/> table."
    },
    {
      "pos": [
        8210,
        8307
      ],
      "content": "<ph id=\"ph39\">[AZURE.NOTE]</ph><ph id=\"ph40\"/> Unlike external tables, dropping an internal table also deletes the underlying data."
    },
    {
      "pos": [
        8309,
        8311
      ],
      "content": "##"
    },
    {
      "pos": [
        8330,
        8369
      ],
      "content": "Use Apache Tez for improved performance"
    },
    {
      "pos": [
        8371,
        8643
      ],
      "content": "<bpt id=\"p44\">[</bpt>Apache Tez<ept id=\"p44\">](http://tez.apache.org)</ept><ph id=\"ph41\"/> is a framework that allows data intensive applications, such as Hive, to run much more efficiently at scale. In the latest release of HDInsight, Hive supports running on Tez. Tez is enabled by default for Linux-based HDInsight clusters.",
      "nodes": [
        {
          "content": "<bpt id=\"p44\">[</bpt>Apache Tez<ept id=\"p44\">](http://tez.apache.org)</ept><ph id=\"ph41\"/> is a framework that allows data intensive applications, such as Hive, to run much more efficiently at scale.",
          "pos": [
            0,
            199
          ]
        },
        {
          "content": "In the latest release of HDInsight, Hive supports running on Tez.",
          "pos": [
            200,
            265
          ]
        },
        {
          "content": "Tez is enabled by default for Linux-based HDInsight clusters.",
          "pos": [
            266,
            327
          ]
        }
      ]
    },
    {
      "pos": [
        8647,
        8828
      ],
      "content": "<ph id=\"ph42\">[AZURE.NOTE]</ph><ph id=\"ph43\"/> Tez is currently off by default for Windows-based HDInsight clusters and it must be enabled. To take advantage of Tez, the following value must be set for a Hive query:",
      "nodes": [
        {
          "content": "<ph id=\"ph42\">[AZURE.NOTE]</ph><ph id=\"ph43\"/> Tez is currently off by default for Windows-based HDInsight clusters and it must be enabled.",
          "pos": [
            0,
            139
          ]
        },
        {
          "content": "To take advantage of Tez, the following value must be set for a Hive query:",
          "pos": [
            140,
            215
          ]
        }
      ]
    },
    {
      "pos": [
        8873,
        9177
      ],
      "content": "This can be submitted on a per-query basis by placing it at the beginning of your query. You can also set this to be on by default on a cluster by setting the configuration value when you create the cluster. You can find more details in <bpt id=\"p45\">[</bpt>Provisioning HDInsight Clusters<ept id=\"p45\">](hdinsight-provision-clusters.md)</ept>.",
      "nodes": [
        {
          "content": "This can be submitted on a per-query basis by placing it at the beginning of your query.",
          "pos": [
            0,
            88
          ]
        },
        {
          "content": "You can also set this to be on by default on a cluster by setting the configuration value when you create the cluster.",
          "pos": [
            89,
            207
          ]
        },
        {
          "content": "You can find more details in <bpt id=\"p45\">[</bpt>Provisioning HDInsight Clusters<ept id=\"p45\">](hdinsight-provision-clusters.md)</ept>.",
          "pos": [
            208,
            344
          ]
        }
      ]
    },
    {
      "pos": [
        9179,
        9363
      ],
      "content": "The <bpt id=\"p46\">[</bpt>Hive on Tez design documents<ept id=\"p46\">](https://cwiki.apache.org/confluence/display/Hive/Hive+on+Tez)</ept><ph id=\"ph45\"/> contain a number of details about the implementation choices and tuning configurations."
    },
    {
      "pos": [
        9365,
        9489
      ],
      "content": "To aid in debugging jobs ran using Tez, HDInsight provides the following web UIs that allow you to view details of Tez jobs:"
    },
    {
      "pos": [
        9493,
        9563
      ],
      "content": "<bpt id=\"p47\">[</bpt>Use the Tez UI on Windows-based HDInsight<ept id=\"p47\">](hdinsight-debug-tez-ui.md)</ept>"
    },
    {
      "pos": [
        9567,
        9653
      ],
      "content": "<bpt id=\"p48\">[</bpt>Use the Ambari Tez view on Linux-based HDInsight<ept id=\"p48\">](hdinsight-debug-ambari-tez-view.md)</ept>"
    },
    {
      "pos": [
        9655,
        9657
      ],
      "content": "##"
    },
    {
      "pos": [
        9673,
        9705
      ],
      "content": "Choose how to run the HiveQL job"
    },
    {
      "pos": [
        9707,
        9869
      ],
      "content": "HDInsight can run HiveQL jobs using a variety of methods. Use the following table to decide which method is right for you, then follow the link for a walkthrough.",
      "nodes": [
        {
          "content": "HDInsight can run HiveQL jobs using a variety of methods.",
          "pos": [
            0,
            57
          ]
        },
        {
          "content": "Use the following table to decide which method is right for you, then follow the link for a walkthrough.",
          "pos": [
            58,
            162
          ]
        }
      ]
    },
    {
      "pos": [
        9873,
        9900
      ],
      "content": "<bpt id=\"p49\">**</bpt>Use this<ept id=\"p49\">**</ept><ph id=\"ph46\"/> if you want..."
    },
    {
      "pos": [
        9955,
        9982
      ],
      "content": "...an <bpt id=\"p50\">**</bpt>interactive<ept id=\"p50\">**</ept><ph id=\"ph47\"/> shell"
    },
    {
      "pos": [
        9985,
        10008
      ],
      "content": "...<bpt id=\"p51\">**</bpt>batch<ept id=\"p51\">**</ept><ph id=\"ph48\"/> processing"
    },
    {
      "pos": [
        10011,
        10052
      ],
      "content": "...with this <bpt id=\"p52\">**</bpt>cluster operating system<ept id=\"p52\">**</ept>"
    },
    {
      "pos": [
        10055,
        10095
      ],
      "content": "...from this <bpt id=\"p53\">**</bpt>client operating system<ept id=\"p53\">**</ept>"
    },
    {
      "pos": [
        10327,
        10380
      ],
      "content": "<bpt id=\"p54\">[</bpt>Hive View<ept id=\"p54\">](hdinsight-hadoop-use-hive-ambari-view.md)</ept>"
    },
    {
      "pos": [
        10383,
        10384
      ],
      "content": "✔"
    },
    {
      "pos": [
        10387,
        10388
      ],
      "content": "✔"
    },
    {
      "pos": [
        10391,
        10396
      ],
      "content": "Linux"
    },
    {
      "pos": [
        10399,
        10418
      ],
      "content": "Any (browser based)"
    },
    {
      "pos": [
        10423,
        10500
      ],
      "content": "<bpt id=\"p55\">[</bpt>Beeline command (from an SSH session)<ept id=\"p55\">](hdinsight-hadoop-use-hive-beeline.md)</ept>"
    },
    {
      "pos": [
        10556,
        10557
      ],
      "content": "✔"
    },
    {
      "pos": [
        10584,
        10585
      ],
      "content": "✔"
    },
    {
      "pos": [
        10599,
        10604
      ],
      "content": "Linux"
    },
    {
      "pos": [
        10643,
        10676
      ],
      "content": "Linux, Unix, Mac OS X, or Windows"
    },
    {
      "pos": [
        10688,
        10758
      ],
      "content": "<bpt id=\"p56\">[</bpt>Hive command (from an SSH session)<ept id=\"p56\">](hdinsight-hadoop-use-hive-ssh.md)</ept>"
    },
    {
      "pos": [
        10814,
        10815
      ],
      "content": "✔"
    },
    {
      "pos": [
        10842,
        10843
      ],
      "content": "✔"
    },
    {
      "pos": [
        10857,
        10862
      ],
      "content": "Linux"
    },
    {
      "pos": [
        10901,
        10934
      ],
      "content": "Linux, Unix, Mac OS X, or Windows"
    },
    {
      "pos": [
        10946,
        10987
      ],
      "content": "<bpt id=\"p57\">[</bpt>Curl<ept id=\"p57\">](hdinsight-hadoop-use-hive-curl.md)</ept>"
    },
    {
      "pos": [
        11038,
        11044
      ],
      "content": "&amp;nbsp;"
    },
    {
      "pos": [
        11069,
        11070
      ],
      "content": "✔"
    },
    {
      "pos": [
        11084,
        11100
      ],
      "content": "Linux or Windows"
    },
    {
      "pos": [
        11128,
        11161
      ],
      "content": "Linux, Unix, Mac OS X, or Windows"
    },
    {
      "pos": [
        11173,
        11232
      ],
      "content": "<bpt id=\"p58\">[</bpt>Query console<ept id=\"p58\">](hdinsight-hadoop-use-hive-query-console.md)</ept>"
    },
    {
      "pos": [
        11265,
        11271
      ],
      "content": "&amp;nbsp;"
    },
    {
      "pos": [
        11296,
        11297
      ],
      "content": "✔"
    },
    {
      "pos": [
        11311,
        11318
      ],
      "content": "Windows"
    },
    {
      "pos": [
        11355,
        11374
      ],
      "content": "Any (browser based)"
    },
    {
      "pos": [
        11406,
        11485
      ],
      "content": "<bpt id=\"p59\">[</bpt>HDInsight tools for Visual Studio<ept id=\"p59\">](hdinsight-hadoop-use-hive-visual-studio.md)</ept>"
    },
    {
      "pos": [
        11498,
        11504
      ],
      "content": "&amp;nbsp;"
    },
    {
      "pos": [
        11529,
        11530
      ],
      "content": "✔"
    },
    {
      "pos": [
        11544,
        11560
      ],
      "content": "Linux or Windows"
    },
    {
      "pos": [
        11588,
        11595
      ],
      "content": "Windows"
    },
    {
      "pos": [
        11633,
        11694
      ],
      "content": "<bpt id=\"p60\">[</bpt>Windows PowerShell<ept id=\"p60\">](hdinsight-hadoop-use-hive-powershell.md)</ept>"
    },
    {
      "pos": [
        11725,
        11731
      ],
      "content": "&amp;nbsp;"
    },
    {
      "pos": [
        11756,
        11757
      ],
      "content": "✔"
    },
    {
      "pos": [
        11771,
        11787
      ],
      "content": "Linux or Windows"
    },
    {
      "pos": [
        11815,
        11822
      ],
      "content": "Windows"
    },
    {
      "pos": [
        11860,
        11921
      ],
      "content": "<bpt id=\"p61\">[</bpt>Remote Desktop<ept id=\"p61\">](hdinsight-hadoop-use-hive-remote-desktop.md)</ept>"
    },
    {
      "pos": [
        11955,
        11956
      ],
      "content": "✔"
    },
    {
      "pos": [
        11983,
        11984
      ],
      "content": "✔"
    },
    {
      "pos": [
        11998,
        12005
      ],
      "content": "Windows"
    },
    {
      "pos": [
        12042,
        12049
      ],
      "content": "Windows"
    },
    {
      "pos": [
        12089,
        12175
      ],
      "content": "Running Hive jobs on Azure HDInsight using on-premises SQL Server Integration Services"
    },
    {
      "pos": [
        12177,
        12356
      ],
      "content": "You can also use SQL Server Integration Services (SSIS) to run a Hive job. The Azure Feature Pack for SSIS provides the following components that work with Hive jobs on HDInsight.",
      "nodes": [
        {
          "content": "You can also use SQL Server Integration Services (SSIS) to run a Hive job.",
          "pos": [
            0,
            74
          ]
        },
        {
          "content": "The Azure Feature Pack for SSIS provides the following components that work with Hive jobs on HDInsight.",
          "pos": [
            75,
            179
          ]
        }
      ]
    },
    {
      "pos": [
        12361,
        12398
      ],
      "content": "<bpt id=\"p62\">[</bpt>Azure HDInsight Hive Task<ept id=\"p62\">][hivetask]</ept>"
    },
    {
      "pos": [
        12401,
        12459
      ],
      "content": "<bpt id=\"p63\">[</bpt>Azure Subscription Connection Manager<ept id=\"p63\">][connectionmanager]</ept>"
    },
    {
      "pos": [
        12462,
        12528
      ],
      "content": "Learn more about the Azure Feature Pack for SSIS <bpt id=\"p64\">[</bpt>here<ept id=\"p64\">][ssispack]</ept>."
    },
    {
      "pos": [
        12531,
        12533
      ],
      "content": "##"
    },
    {
      "pos": [
        12555,
        12565
      ],
      "content": "Next steps"
    },
    {
      "pos": [
        12567,
        12723
      ],
      "content": "Now that you've learned what Hive is and how to use it with Hadoop in HDInsight, use the following links to explore other ways to work with Azure HDInsight."
    },
    {
      "pos": [
        12728,
        12777
      ],
      "content": "<bpt id=\"p65\">[</bpt>Upload data to HDInsight<ept id=\"p65\">][hdinsight-upload-data]</ept>"
    },
    {
      "pos": [
        12780,
        12823
      ],
      "content": "<bpt id=\"p66\">[</bpt>Use Pig with HDInsight<ept id=\"p66\">][hdinsight-use-pig]</ept>"
    },
    {
      "pos": [
        12826,
        12886
      ],
      "content": "<bpt id=\"p67\">[</bpt>Use MapReduce jobs with HDInsight<ept id=\"p67\">][hdinsight-use-mapreduce]</ept>"
    }
  ],
  "content": "<properties\n    pageTitle=\"Learn what is Hive and how to use HiveQL | Microsoft Azure\"\n    description=\"Learn about Apache Hive and how to use it with Hadoop in HDInsight. Choose how to run your Hive job, and use HiveQL to analyze a sample Apache log4j file.\"\n    keywords=\"hiveql,what is hive\"\n    services=\"hdinsight\"\n    documentationCenter=\"\"\n    authors=\"Blackmist\"\n    manager=\"paulettm\"\n    editor=\"cgronlun\"\n    tags=\"azure-portal\"/>\n\n<tags\n    ms.service=\"hdinsight\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\"\n    ms.tgt_pltfrm=\"na\"\n    ms.workload=\"big-data\"\n    ms.date=\"02/16/2016\"\n    ms.author=\"larryfr\"/>\n\n# Use Hive and HiveQL with Hadoop in HDInsight to analyze a sample Apache log4j file\n\n[AZURE.INCLUDE [hive-selector](../../includes/hdinsight-selector-use-hive.md)]\n\n\nIn this tutorial, you'll learn how to use Apache Hive in Hadoop on HDInsight, and choose how to run your Hive job. You'll also learn about HiveQL and how to analyze a sample Apache log4j file.\n\n##<a id=\"why\"></a>What is Hive and why use it?\n[Apache Hive](http://hive.apache.org/) is a data warehouse system for Hadoop, which enables data summarization, querying, and analysis of data by using HiveQL (a query language similar to SQL). Hive can be used to interactively explore your data or to create reusable batch processing jobs.\n\nHive allows you to project structure on largely unstructured data. After you define the structure, you can use Hive to query that data without knowledge of Java or MapReduce. **HiveQL** (the Hive query language) allows you to write queries with statements that are similar to T-SQL.\n\nHive understands how to work with structured and semi-structured data, such as text files where the fields are delimited by specific characters. Hive also supports custom **serializer/deserializers (SerDe)** for complex or irregularly structured data. For more information, see [How to use a custom JSON SerDe with HDInsight](http://blogs.msdn.com/b/bigdatasupport/archive/2014/06/18/how-to-use-a-custom-json-serde-with-microsoft-azure-hdinsight.aspx).\n\nHive can also be extended through **user-defined functions (UDF)**. A UDF allows you to implement functionality or logic that isn't easily modeled in HiveQL. For an example of using UDFs with Hive, see the following:\n\n* [Using Python with Hive and Pig in HDInsight](hdinsight-python.md)\n\n* [Use C# with Hive and Pig in HDInsight](hdinsight-hadoop-hive-pig-udf-dotnet-csharp.md)\n\n* [How to add a custom Hive UDF to HDInsight](http://blogs.msdn.com/b/bigdatasupport/archive/2014/01/14/how-to-add-custom-hive-udfs-to-hdinsight.aspx)\n\n\n## Hive internal tables vs external tables\n\nThere are a few things you need to know about the Hive internal table and external table:\n\n- The **CREATE TABLE** command creates an internal table. The data file must be located in the default container.\n- The **CREATE TABLE** command moves the data file to the /hive/warehouse/<TableName> folder.\n- The **CREATE EXTERNAL TABLE** command creates an external table. The data file can be located outside the default container.\n- The **CREATE EXTERNAL TABLE** command does not move the data file.\n- The **CREATE EXTERNAL TABLE** command doesn't allow any folders in the LOCATION. This is the reason why the tutorial makes a copy of the sample.log file.\n\nFor more information, see [HDInsight: Hive Internal and External Tables Intro][cindygross-hive-tables].\n\n\n##<a id=\"data\"></a>About the sample data, an Apache log4j file\n\nThis example uses a *log4j* sample file, which is stored at **/example/data/sample.log** in your blob storage container. Each log inside the file consists of a line of fields that contains a `[LOG LEVEL]` field to show the type and the severity, for example:\n\n    2012-02-03 20:26:41 SampleClass3 [ERROR] verbose detail for id 1527353937\n\nIn the previous example, the log level is ERROR.\n\n> [AZURE.NOTE] You can also generate a log4j file by using the [Apache Log4j](http://en.wikipedia.org/wiki/Log4j) logging tool and then upload that file to the blob container. See [Upload Data to HDInsight](hdinsight-upload-data.md) for instructions. For more information about how Azure Blob storage is used with HDInsight, see [Use Azure Blob Storage with HDInsight](hdinsight-hadoop-use-blob-storage.md).\n\nThe sample data is stored in Azure Blob storage, which HDInsight uses as the default file system. HDInsight can access files stored in blobs by using the **wasb** prefix. For example, to access the sample.log file, you would use the following syntax:\n\n    wasb:///example/data/sample.log\n\nBecause Azure Blob storage is the default storage for HDInsight, you can also access the file by using **/example/data/sample.log** from HiveQL.\n\n> [AZURE.NOTE] The syntax, **wasb:///**, is used to access files stored in the default storage container for your HDInsight cluster. If you specified additional storage accounts when you provisioned your cluster, and you want to access files stored in these accounts, you can access the data by specifying the container name and storage account address, for example, **wasb://mycontainer@mystorage.blob.core.windows.net/example/data/sample.log**.\n\n##<a id=\"job\"></a>Sample job: Project columns onto delimited data\n\nThe following HiveQL statements will project columns onto delimited data that is stored in the **wasb:///example/data** directory:\n\n    DROP TABLE log4jLogs;\n    CREATE EXTERNAL TABLE log4jLogs (t1 string, t2 string, t3 string, t4 string, t5 string, t6 string, t7 string)\n    ROW FORMAT DELIMITED FIELDS TERMINATED BY ' '\n    STORED AS TEXTFILE LOCATION 'wasb:///example/data/';\n    SELECT t4 AS sev, COUNT(*) AS count FROM log4jLogs WHERE t4 = '[ERROR]' AND INPUT__FILE__NAME LIKE '%.log' GROUP BY t4;\n\nIn the previous example, the HiveQL statements perform the following actions:\n\n* **DROP TABLE**: Deletes the table and the data file if the table already exists.\n* **CREATE EXTERNAL TABLE**: Creates a new **external** table in Hive. External tables only store the table definition in Hive; the data is left in the original location and in the original format.\n* **ROW FORMAT**: Tells Hive how the data is formatted. In this case, the fields in each log are separated by a space.\n* **STORED AS TEXTFILE LOCATION**: Tells Hive where the data is stored (the example/data directory) and that it is stored as text. The data can be in one file or spread across multiple files within the directory.\n* **SELECT**: Selects a count of all rows where the column **t4** contains the value **[ERROR]**. This should return a value of **3** because there are three rows that contain this value.\n* **INPUT__FILE__NAME LIKE '%.log'** - Tells Hive that we should only return data from files ending in .log. This restricts the search to the sample.log file that contains the data, and keeps it from returning data from other example data files that do not match the schema we defined.\n\n> [AZURE.NOTE] External tables should be used when you expect the underlying data to be updated by an external source, such as an automated data upload process, or by another MapReduce operation, and you always want Hive queries to use the latest data.\n>\n> Dropping an external table does **not** delete the data, it only deletes the table definition.\n\nAfter creating the external table, the following statements are used to create an **internal** table.\n\n    CREATE TABLE IF NOT EXISTS errorLogs (t1 string, t2 string, t3 string, t4 string, t5 string, t6 string, t7 string)\n    STORED AS ORC;\n    INSERT OVERWRITE TABLE errorLogs\n    SELECT t1, t2, t3, t4, t5, t6, t7 FROM log4jLogs WHERE t4 = '[ERROR]' AND INPUT__FILE__NAME LIKE '%.log';\n\nThese statements perform the following actions:\n\n* **CREATE TABLE IF NOT EXISTS**: Creates a table, if it does not already exist. Because the **EXTERNAL** keyword is not used, this is an internal table, which is stored in the Hive data warehouse and is managed completely by Hive.\n* **STORED AS ORC**: Stores the data in Optimized Row Columnar (ORC) format. This is a highly optimized and efficient format for storing Hive data.\n* **INSERT OVERWRITE ... SELECT**: Selects rows from the **log4jLogs** table that contains **[ERROR]**, and then inserts the data into the **errorLogs** table.\n\n> [AZURE.NOTE] Unlike external tables, dropping an internal table also deletes the underlying data.\n\n##<a id=\"usetez\"></a>Use Apache Tez for improved performance\n\n[Apache Tez](http://tez.apache.org) is a framework that allows data intensive applications, such as Hive, to run much more efficiently at scale. In the latest release of HDInsight, Hive supports running on Tez. Tez is enabled by default for Linux-based HDInsight clusters.\n\n> [AZURE.NOTE] Tez is currently off by default for Windows-based HDInsight clusters and it must be enabled. To take advantage of Tez, the following value must be set for a Hive query:\n>\n> ```set hive.execution.engine=tez;```\n>\n>This can be submitted on a per-query basis by placing it at the beginning of your query. You can also set this to be on by default on a cluster by setting the configuration value when you create the cluster. You can find more details in [Provisioning HDInsight Clusters](hdinsight-provision-clusters.md).\n\nThe [Hive on Tez design documents](https://cwiki.apache.org/confluence/display/Hive/Hive+on+Tez) contain a number of details about the implementation choices and tuning configurations.\n\nTo aid in debugging jobs ran using Tez, HDInsight provides the following web UIs that allow you to view details of Tez jobs:\n\n* [Use the Tez UI on Windows-based HDInsight](hdinsight-debug-tez-ui.md)\n\n* [Use the Ambari Tez view on Linux-based HDInsight](hdinsight-debug-ambari-tez-view.md)\n\n##<a id=\"run\"></a>Choose how to run the HiveQL job\n\nHDInsight can run HiveQL jobs using a variety of methods. Use the following table to decide which method is right for you, then follow the link for a walkthrough.\n\n| **Use this** if you want...                                                     | ...an **interactive** shell | ...**batch** processing | ...with this **cluster operating system** | ...from this **client operating system** |\n|:--------------------------------------------------------------------------------|:---------------------------:|:-----------------------:|:------------------------------------------|:-----------------------------------------|\n| [Hive View](hdinsight-hadoop-use-hive-ambari-view.md) | ✔ | ✔ | Linux | Any (browser based) |\n| [Beeline command (from an SSH session)](hdinsight-hadoop-use-hive-beeline.md)                                         |              ✔              |            ✔            | Linux                                     | Linux, Unix, Mac OS X, or Windows        |\n| [Hive command (from an SSH session)](hdinsight-hadoop-use-hive-ssh.md)                                         |              ✔              |            ✔            | Linux                                     | Linux, Unix, Mac OS X, or Windows        |\n| [Curl](hdinsight-hadoop-use-hive-curl.md)                                       |           &nbsp;            |            ✔            | Linux or Windows                          | Linux, Unix, Mac OS X, or Windows        |\n| [Query console](hdinsight-hadoop-use-hive-query-console.md)                     |           &nbsp;            |            ✔            | Windows                                   | Any (browser based)                            |\n| [HDInsight tools for Visual Studio](hdinsight-hadoop-use-hive-visual-studio.md) |           &nbsp;            |            ✔            | Linux or Windows                          | Windows                                  |\n| [Windows PowerShell](hdinsight-hadoop-use-hive-powershell.md)                   |           &nbsp;            |            ✔            | Linux or Windows                          | Windows                                  |\n| [Remote Desktop](hdinsight-hadoop-use-hive-remote-desktop.md)                   |              ✔              |            ✔            | Windows                                   | Windows                                  |\n\n## Running Hive jobs on Azure HDInsight using on-premises SQL Server Integration Services\n\nYou can also use SQL Server Integration Services (SSIS) to run a Hive job. The Azure Feature Pack for SSIS provides the following components that work with Hive jobs on HDInsight.\n\n\n- [Azure HDInsight Hive Task][hivetask]\n- [Azure Subscription Connection Manager][connectionmanager]\n\n\nLearn more about the Azure Feature Pack for SSIS [here][ssispack].\n\n\n##<a id=\"nextsteps\"></a>Next steps\n\nNow that you've learned what Hive is and how to use it with Hadoop in HDInsight, use the following links to explore other ways to work with Azure HDInsight.\n\n\n- [Upload data to HDInsight][hdinsight-upload-data]\n- [Use Pig with HDInsight][hdinsight-use-pig]\n- [Use MapReduce jobs with HDInsight][hdinsight-use-mapreduce]\n\n[check]: ./media/hdinsight-use-hive/hdi.checkmark.png\n\n[hdinsight-sdk-documentation]: http://msdnstage.redmond.corp.microsoft.com/library/dn479185.aspx\n\n[azure-purchase-options]: http://azure.microsoft.com/pricing/purchase-options/\n[azure-member-offers]: http://azure.microsoft.com/pricing/member-offers/\n[azure-free-trial]: http://azure.microsoft.com/pricing/free-trial/\n\n[apache-tez]: http://tez.apache.org\n[apache-hive]: http://hive.apache.org/\n[apache-log4j]: http://en.wikipedia.org/wiki/Log4j\n[hive-on-tez-wiki]: https://cwiki.apache.org/confluence/display/Hive/Hive+on+Tez\n[import-to-excel]: http://azure.microsoft.com/documentation/articles/hdinsight-connect-excel-power-query/\n[hivetask]: http://msdn.microsoft.com/library/mt146771(v=sql.120).aspx\n[connectionmanager]: http://msdn.microsoft.com/library/mt146773(v=sql.120).aspx\n[ssispack]: http://msdn.microsoft.com/library/mt146770(v=sql.120).aspx\n\n[hdinsight-use-pig]: hdinsight-use-pig.md\n[hdinsight-use-oozie]: hdinsight-use-oozie.md\n[hdinsight-analyze-flight-data]: hdinsight-analyze-flight-delay-data.md\n[hdinsight-use-mapreduce]: hdinsight-use-mapreduce.md\n\n\n[hdinsight-storage]: hdinsight-hadoop-use-blob-storage.md\n\n[hdinsight-provision]: hdinsight-provision-clusters.md\n[hdinsight-submit-jobs]: hdinsight-submit-hadoop-jobs-programmatically.md\n[hdinsight-upload-data]: hdinsight-upload-data.md\n[hdinsight-get-started]: hdinsight-get-started.md\n\n[Powershell-install-configure]: ../install-configure-powershell.md\n[powershell-here-strings]: http://technet.microsoft.com/library/ee692792.aspx\n\n[image-hdi-hive-powershell]: ./media/hdinsight-use-hive/HDI.HIVE.PowerShell.png\n[img-hdi-hive-powershell-output]: ./media/hdinsight-use-hive/HDI.Hive.PowerShell.Output.png\n[image-hdi-hive-architecture]: ./media/hdinsight-use-hive/HDI.Hive.Architecture.png\n\n\n[cindygross-hive-tables]: http://blogs.msdn.com/b/cindygross/archive/2013/02/06/hdinsight-hive-internal-and-external-tables-intro.aspx"
}