{
  "nodes": [
    {
      "pos": [
        27,
        86
      ],
      "content": "Out of memory error (OOM) - Hive settings | Microsoft Azure"
    },
    {
      "pos": [
        105,
        238
      ],
      "content": "Fix an out of memory error (OOM) from a Hive query in Hadoop in HDInsight. The customer scenario is a query across many large tables.",
      "nodes": [
        {
          "content": "Fix an out of memory error (OOM) from a Hive query in Hadoop in HDInsight.",
          "pos": [
            0,
            74
          ]
        },
        {
          "content": "The customer scenario is a query across many large tables.",
          "pos": [
            75,
            133
          ]
        }
      ]
    },
    {
      "pos": [
        606,
        693
      ],
      "content": "Fix an Out of Memory (OOM) error with Hive memory settings in Hadoop in Azure HDInsight"
    },
    {
      "pos": [
        695,
        895
      ],
      "content": "One of the common problems our customers face is getting an Out of Memory (OOM) error when using Hive. This article describes a customer scenario and the Hive settings we recommended to fix the issue.",
      "nodes": [
        {
          "content": "One of the common problems our customers face is getting an Out of Memory (OOM) error when using Hive.",
          "pos": [
            0,
            102
          ]
        },
        {
          "content": "This article describes a customer scenario and the Hive settings we recommended to fix the issue.",
          "pos": [
            103,
            200
          ]
        }
      ]
    },
    {
      "pos": [
        900,
        940
      ],
      "content": "Scenario: Hive query across large tables"
    },
    {
      "pos": [
        942,
        984
      ],
      "content": "A customer ran the query below using Hive."
    },
    {
      "pos": [
        1248,
        1275
      ],
      "content": "Some nuances of this query:"
    },
    {
      "pos": [
        1279,
        1356
      ],
      "content": "T1 is an alias to a big table, TABLE1, which has lots of STRING column types."
    },
    {
      "pos": [
        1359,
        1427
      ],
      "content": "Other tables are not that big but do have a large number of columns."
    },
    {
      "pos": [
        1430,
        1522
      ],
      "content": "All tables are joining each other, in some cases with multiple columns in TABLE1 and others."
    },
    {
      "pos": [
        1524,
        1739
      ],
      "content": "When the customer ran the query using Hive on MapReduce on a 24 node A3 cluster, the query ran in about 26 minutes. The customer noticed the following warning messages when the query was run using Hive on MapReduce:",
      "nodes": [
        {
          "content": "When the customer ran the query using Hive on MapReduce on a 24 node A3 cluster, the query ran in about 26 minutes.",
          "pos": [
            0,
            115
          ]
        },
        {
          "content": "The customer noticed the following warning messages when the query was run using Hive on MapReduce:",
          "pos": [
            116,
            215
          ]
        }
      ]
    },
    {
      "pos": [
        1944,
        2122
      ],
      "content": "Because the query finished executing in about 26 minutes, the customer ignored these warnings and instead started to focus on how to improve the this queryâ€™s performance further."
    },
    {
      "pos": [
        2124,
        2400
      ],
      "content": "The customer consulted <bpt id=\"p1\">[</bpt>Optimize Hive queries for Hadoop in HDInsight<ept id=\"p1\">](hdinsight-hadoop-optimize-hive-query.md)</ept>, and decided to use Tez execution engine. Once the same query was run with the Tez setting enabled the query ran for 15 minutes, and then threw the following error:",
      "nodes": [
        {
          "content": "The customer consulted <bpt id=\"p1\">[</bpt>Optimize Hive queries for Hadoop in HDInsight<ept id=\"p1\">](hdinsight-hadoop-optimize-hive-query.md)</ept>, and decided to use Tez execution engine.",
          "pos": [
            0,
            191
          ]
        },
        {
          "content": "Once the same query was run with the Tez setting enabled the query ran for 15 minutes, and then threw the following error:",
          "pos": [
            192,
            314
          ]
        }
      ]
    },
    {
      "pos": [
        4035,
        4272
      ],
      "content": "The customer then decided to use a bigger VM (i.e. D12) thinking a bigger VM would have more heap space. Even then, the customer continued to see the error. The customer reached out to the HDInsight team for help in debugging this issue.",
      "nodes": [
        {
          "content": "The customer then decided to use a bigger VM (i.e. D12) thinking a bigger VM would have more heap space.",
          "pos": [
            0,
            104
          ]
        },
        {
          "content": "Even then, the customer continued to see the error.",
          "pos": [
            105,
            156
          ]
        },
        {
          "content": "The customer reached out to the HDInsight team for help in debugging this issue.",
          "pos": [
            157,
            237
          ]
        }
      ]
    },
    {
      "pos": [
        4277,
        4312
      ],
      "content": "Debug the Out of Memory (OOM) error"
    },
    {
      "pos": [
        4314,
        4551
      ],
      "content": "Our support and engineering teams together found one of the issues causing the Out of Memory (OOM) error was a <bpt id=\"p2\">[</bpt>known issue described in the Apache JIRA<ept id=\"p2\">](https://issues.apache.org/jira/browse/HIVE-8306)</ept>. From the description in the JIRA:",
      "nodes": [
        {
          "content": "Our support and engineering teams together found one of the issues causing the Out of Memory (OOM) error was a <bpt id=\"p2\">[</bpt>known issue described in the Apache JIRA<ept id=\"p2\">](https://issues.apache.org/jira/browse/HIVE-8306)</ept>.",
          "pos": [
            0,
            241
          ]
        },
        {
          "content": "From the description in the JIRA:",
          "pos": [
            242,
            275
          ]
        }
      ]
    },
    {
      "pos": [
        5014,
        5140
      ],
      "content": "We confirmed that <bpt id=\"p3\">**</bpt>hive.auto.convert.join.noconditionaltask<ept id=\"p3\">**</ept><ph id=\"ph2\"/> was indeed set to <bpt id=\"p4\">**</bpt>true<ept id=\"p4\">**</ept><ph id=\"ph3\"/> by looking under hive-site.xml file:"
    },
    {
      "pos": [
        5662,
        5806
      ],
      "content": "Based on the warning and the JIRA, our hypothesis was Map Join was the cause of the Java Heap Space OOM error. So we dug deeper into this issue.",
      "nodes": [
        {
          "content": "Based on the warning and the JIRA, our hypothesis was Map Join was the cause of the Java Heap Space OOM error.",
          "pos": [
            0,
            110
          ]
        },
        {
          "content": "So we dug deeper into this issue.",
          "pos": [
            111,
            144
          ]
        }
      ]
    },
    {
      "pos": [
        5808,
        6127
      ],
      "content": "As explained in the blog post <bpt id=\"p5\">[</bpt>Hadoop Yarn memory settings in HDInsight<ept id=\"p5\">](http://blogs.msdn.com/b/shanyu/archive/2014/07/31/hadoop-yarn-memory-settings-in-hdinsigh.aspx)</ept>, when Tez execution engine is used the heap space used actually belongs to the Tez container. See the image below describing the Tez container memory.",
      "nodes": [
        {
          "content": "As explained in the blog post <bpt id=\"p5\">[</bpt>Hadoop Yarn memory settings in HDInsight<ept id=\"p5\">](http://blogs.msdn.com/b/shanyu/archive/2014/07/31/hadoop-yarn-memory-settings-in-hdinsigh.aspx)</ept>, when Tez execution engine is used the heap space used actually belongs to the Tez container.",
          "pos": [
            0,
            300
          ]
        },
        {
          "content": "See the image below describing the Tez container memory.",
          "pos": [
            301,
            357
          ]
        }
      ]
    },
    {
      "pos": [
        6129,
        6300
      ],
      "content": "<ph id=\"ph4\">![</ph>Tez container memory diagram: Hive out of memory error  OOM<ph id=\"ph5\">](./media/hdinsight-hadoop-hive-out-of-memory-error-oom/hive-out-of-memory-error-oom-tez-container-memory.png)</ph>"
    },
    {
      "pos": [
        6303,
        6819
      ],
      "content": "As the blog post suggests, the following two memory settings define the container memory for the heap: <bpt id=\"p6\">**</bpt>hive.tez.container.size<ept id=\"p6\">**</ept><ph id=\"ph6\"/> and <bpt id=\"p7\">**</bpt>hive.tez.java.opts<ept id=\"p7\">**</ept>. From our experience, the OOM exception does not mean the container size is too small. It means the Java heap size (hive.tez.java.opts) is too small. So whenever you see OOM, you can try to increase <bpt id=\"p8\">**</bpt>hive.tez.java.opts<ept id=\"p8\">**</ept>. If needed you might have to increase <bpt id=\"p9\">**</bpt>hive.tez.container.size<ept id=\"p9\">**</ept>. The <bpt id=\"p10\">**</bpt>java.opts<ept id=\"p10\">**</ept><ph id=\"ph7\"/> setting should be around 80% of <bpt id=\"p11\">**</bpt>container.size<ept id=\"p11\">**</ept>.",
      "nodes": [
        {
          "content": "As the blog post suggests, the following two memory settings define the container memory for the heap: <bpt id=\"p6\">**</bpt>hive.tez.container.size<ept id=\"p6\">**</ept><ph id=\"ph6\"/> and <bpt id=\"p7\">**</bpt>hive.tez.java.opts<ept id=\"p7\">**</ept>.",
          "pos": [
            0,
            248
          ]
        },
        {
          "content": "From our experience, the OOM exception does not mean the container size is too small.",
          "pos": [
            249,
            334
          ]
        },
        {
          "content": "It means the Java heap size (hive.tez.java.opts) is too small.",
          "pos": [
            335,
            397
          ]
        },
        {
          "content": "So whenever you see OOM, you can try to increase <bpt id=\"p8\">**</bpt>hive.tez.java.opts<ept id=\"p8\">**</ept>.",
          "pos": [
            398,
            508
          ]
        },
        {
          "content": "If needed you might have to increase <bpt id=\"p9\">**</bpt>hive.tez.container.size<ept id=\"p9\">**</ept>.",
          "pos": [
            509,
            612
          ]
        },
        {
          "content": "The <bpt id=\"p10\">**</bpt>java.opts<ept id=\"p10\">**</ept><ph id=\"ph7\"/> setting should be around 80% of <bpt id=\"p11\">**</bpt>container.size<ept id=\"p11\">**</ept>.",
          "pos": [
            613,
            776
          ]
        }
      ]
    },
    {
      "pos": [
        6823,
        6928
      ],
      "content": "<ph id=\"ph8\">[AZURE.NOTE]</ph><ph id=\"ph9\"/>  The setting <bpt id=\"p12\">**</bpt>hive.tez.java.opts<ept id=\"p12\">**</ept><ph id=\"ph10\"/> must always be smaller than <bpt id=\"p13\">**</bpt>hive.tez.container.size<ept id=\"p13\">**</ept>."
    },
    {
      "pos": [
        6930,
        7107
      ],
      "content": "Since a D12 machine has 28GB memory, we decided to use a container size of 10GB (10240MB) and assign 80% to java.opts. This was done on the Hive console using the setting below:",
      "nodes": [
        {
          "content": "Since a D12 machine has 28GB memory, we decided to use a container size of 10GB (10240MB) and assign 80% to java.opts.",
          "pos": [
            0,
            118
          ]
        },
        {
          "content": "This was done on the Hive console using the setting below:",
          "pos": [
            119,
            177
          ]
        }
      ]
    },
    {
      "pos": [
        7185,
        7258
      ],
      "content": "Based on these settings, the query successfully ran in under ten minutes."
    },
    {
      "pos": [
        7263,
        7304
      ],
      "content": "Conclusion: OOM errors and container size"
    },
    {
      "pos": [
        7306,
        7519
      ],
      "content": "Getting an OOM error doesn't necessarily mean the container size is too small. Instead, you should configure the memory settings so that the heap size is increased and is at least 80% of the container memory size.",
      "nodes": [
        {
          "content": "Getting an OOM error doesn't necessarily mean the container size is too small.",
          "pos": [
            0,
            78
          ]
        },
        {
          "content": "Instead, you should configure the memory settings so that the heap size is increased and is at least 80% of the container memory size.",
          "pos": [
            79,
            213
          ]
        }
      ]
    }
  ],
  "content": "<properties\n    pageTitle=\"Out of memory error (OOM) - Hive settings | Microsoft Azure\"\n    description=\"Fix an out of memory error (OOM) from a Hive query in Hadoop in HDInsight. The customer scenario is a query across many large tables.\"\n    keywords=\"out of memory error, OOM, Hive settings\"\n    services=\"hdinsight\"\n    documentationCenter=\"\"\n    authors=\"rashimg\"\n    manager=\"paulettm\"\n    editor=\"cgronlun\"/>\n\n<tags\n    ms.service=\"hdinsight\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\"\n    ms.tgt_pltfrm=\"na\"\n    ms.workload=\"big-data\"\n    ms.date=\"12/10/2015\"\n    ms.author=\"rashimg;cgronlun\"/>\n\n# Fix an Out of Memory (OOM) error with Hive memory settings in Hadoop in Azure HDInsight\n\nOne of the common problems our customers face is getting an Out of Memory (OOM) error when using Hive. This article describes a customer scenario and the Hive settings we recommended to fix the issue.\n\n## Scenario: Hive query across large tables\n\nA customer ran the query below using Hive.\n\n    SELECT\n        COUNT (T1.COLUMN1) as DisplayColumn1,\n        â€¦\n        â€¦\n        â€¦.\n    FROM\n        TABLE1 T1,\n        TABLE2 T2,\n        TABLE3 T3,\n        TABLE5 T4,\n        TABLE6 T5,\n        TABLE7 T6\n    where (T1.KEY1 = T2.KEY1â€¦.\n        â€¦\n        â€¦\n\nSome nuances of this query:\n\n* T1 is an alias to a big table, TABLE1, which has lots of STRING column types.\n* Other tables are not that big but do have a large number of columns.\n* All tables are joining each other, in some cases with multiple columns in TABLE1 and others.\n\nWhen the customer ran the query using Hive on MapReduce on a 24 node A3 cluster, the query ran in about 26 minutes. The customer noticed the following warning messages when the query was run using Hive on MapReduce:\n\n    Warning: Map Join MAPJOIN[428][bigTable=?] in task 'Stage-21:MAPRED' is a cross product\n    Warning: Shuffle Join JOIN[8][tables = [t1933775, t1932766]] in Stage 'Stage-4:MAPRED' is a cross product\n\nBecause the query finished executing in about 26 minutes, the customer ignored these warnings and instead started to focus on how to improve the this queryâ€™s performance further.\n\nThe customer consulted [Optimize Hive queries for Hadoop in HDInsight](hdinsight-hadoop-optimize-hive-query.md), and decided to use Tez execution engine. Once the same query was run with the Tez setting enabled the query ran for 15 minutes, and then threw the following error:\n\n    Status: Failed\n    Vertex failed, vertexName=Map 5, vertexId=vertex_1443634917922_0008_1_05, diagnostics=[Task failed, taskId=task_1443634917922_0008_1_05_000006, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.OutOfMemoryError: Java heap space\n        at\n    org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:172)\n        at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:138)\n        at\n    org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)\n        at\n    org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)\n        at\n    org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n        at\n    org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)\n        at\n    org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:262)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:745)\n    Caused by: java.lang.OutOfMemoryError: Java heap space\n\nThe customer then decided to use a bigger VM (i.e. D12) thinking a bigger VM would have more heap space. Even then, the customer continued to see the error. The customer reached out to the HDInsight team for help in debugging this issue.\n\n## Debug the Out of Memory (OOM) error\n\nOur support and engineering teams together found one of the issues causing the Out of Memory (OOM) error was a [known issue described in the Apache JIRA](https://issues.apache.org/jira/browse/HIVE-8306). From the description in the JIRA:\n\n    When hive.auto.convert.join.noconditionaltask = true we check noconditionaltask.size and if the sum  of tables sizes in the map join is less than noconditionaltask.size the plan would generate a Map join, the issue with this is that the calculation doesnt take into account the overhead introduced by different HashTable implementation as results if the sum of input sizes is smaller than the noconditionaltask size by a small margin queries will hit OOM.\n\nWe confirmed that **hive.auto.convert.join.noconditionaltask** was indeed set to **true** by looking under hive-site.xml file:\n\n    <property>\n        <name>hive.auto.convert.join.noconditionaltask</name>\n        <value>true</value>\n        <description>\n            Whether Hive enables the optimization about converting common join into mapjoin based on the input file size.\n            If this parameter is on, and the sum of size for n-1 of the tables/partitions for a n-way join is smaller than the\n            specified size, the join is directly converted to a mapjoin (there is no conditional task).\n        </description>\n    </property>\n\nBased on the warning and the JIRA, our hypothesis was Map Join was the cause of the Java Heap Space OOM error. So we dug deeper into this issue.\n\nAs explained in the blog post [Hadoop Yarn memory settings in HDInsight](http://blogs.msdn.com/b/shanyu/archive/2014/07/31/hadoop-yarn-memory-settings-in-hdinsigh.aspx), when Tez execution engine is used the heap space used actually belongs to the Tez container. See the image below describing the Tez container memory.\n\n![Tez container memory diagram: Hive out of memory error  OOM](./media/hdinsight-hadoop-hive-out-of-memory-error-oom/hive-out-of-memory-error-oom-tez-container-memory.png)\n\n\nAs the blog post suggests, the following two memory settings define the container memory for the heap: **hive.tez.container.size** and **hive.tez.java.opts**. From our experience, the OOM exception does not mean the container size is too small. It means the Java heap size (hive.tez.java.opts) is too small. So whenever you see OOM, you can try to increase **hive.tez.java.opts**. If needed you might have to increase **hive.tez.container.size**. The **java.opts** setting should be around 80% of **container.size**.\n\n> [AZURE.NOTE]  The setting **hive.tez.java.opts** must always be smaller than **hive.tez.container.size**.\n\nSince a D12 machine has 28GB memory, we decided to use a container size of 10GB (10240MB) and assign 80% to java.opts. This was done on the Hive console using the setting below:\n\n    SET hive.tez.container.size=10240\n    SET hive.tez.java.opts=-Xmx8192m\n\nBased on these settings, the query successfully ran in under ten minutes.\n\n## Conclusion: OOM errors and container size\n\nGetting an OOM error doesn't necessarily mean the container size is too small. Instead, you should configure the memory settings so that the heap size is increased and is at least 80% of the container memory size.\n"
}