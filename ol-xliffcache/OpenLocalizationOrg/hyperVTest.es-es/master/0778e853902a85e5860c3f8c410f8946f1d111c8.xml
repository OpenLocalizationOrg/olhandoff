{
  "nodes": [
    {
      "pos": [
        28,
        122
      ],
      "content": "Kernels available with Jupyter notebooks on HDInsight Spark clusters on Linux| Microsoft Azure"
    },
    {
      "pos": [
        142,
        242
      ],
      "content": "Learn about the additional Jupyter notebook kernels available with Spark cluster on HDInsight Linux."
    },
    {
      "pos": [
        583,
        663
      ],
      "content": "Kernels available for Jupyter notebooks with Spark clusters on HDInsight (Linux)"
    },
    {
      "pos": [
        665,
        949
      ],
      "content": "Apache Spark cluster on HDInsight (Linux) includes Jupyter notebooks that you can use to test your applications. By default Jupyter notebook comes with a <bpt id=\"p1\">**</bpt>Python2<ept id=\"p1\">**</ept><ph id=\"ph2\"/> kernel. HDInsight Spark clusters provide two additional kernels that you can use with the Jupyter notebook. These are:",
      "nodes": [
        {
          "content": "Apache Spark cluster on HDInsight (Linux) includes Jupyter notebooks that you can use to test your applications.",
          "pos": [
            0,
            112
          ]
        },
        {
          "content": "By default Jupyter notebook comes with a <bpt id=\"p1\">**</bpt>Python2<ept id=\"p1\">**</ept><ph id=\"ph2\"/> kernel.",
          "pos": [
            113,
            225
          ]
        },
        {
          "content": "HDInsight Spark clusters provide two additional kernels that you can use with the Jupyter notebook.",
          "pos": [
            226,
            325
          ]
        },
        {
          "content": "These are:",
          "pos": [
            326,
            336
          ]
        }
      ]
    },
    {
      "pos": [
        954,
        999
      ],
      "content": "<bpt id=\"p2\">**</bpt>Spark<ept id=\"p2\">**</ept><ph id=\"ph3\"/> (for applications written in Scala)"
    },
    {
      "pos": [
        1003,
        1051
      ],
      "content": "<bpt id=\"p3\">**</bpt>PySpark<ept id=\"p3\">**</ept><ph id=\"ph4\"/> (for applications written in Python)"
    },
    {
      "pos": [
        1053,
        1166
      ],
      "content": "In this article, you will learn about how to use these kernels and what are the benefits you get from using them."
    },
    {
      "pos": [
        1168,
        1186
      ],
      "content": "<bpt id=\"p4\">**</bpt>Prerequisites:<ept id=\"p4\">**</ept>"
    },
    {
      "pos": [
        1188,
        1216
      ],
      "content": "You must have the following:"
    },
    {
      "pos": [
        1220,
        1374
      ],
      "content": "An Azure subscription. See <bpt id=\"p5\">[</bpt>Get Azure free trial<ept id=\"p5\">](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/)</ept>.",
      "nodes": [
        {
          "content": "An Azure subscription.",
          "pos": [
            0,
            22
          ]
        },
        {
          "content": "See <bpt id=\"p5\">[</bpt>Get Azure free trial<ept id=\"p5\">](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/)</ept>.",
          "pos": [
            23,
            192
          ]
        }
      ]
    },
    {
      "pos": [
        1377,
        1538
      ],
      "content": "An Apache Spark cluster on HDInsight Linux. For instructions, see <bpt id=\"p6\">[</bpt>Create Apache Spark clusters in Azure HDInsight<ept id=\"p6\">](hdinsight-apache-spark-jupyter-spark-sql.md)</ept>.",
      "nodes": [
        {
          "content": "An Apache Spark cluster on HDInsight Linux.",
          "pos": [
            0,
            43
          ]
        },
        {
          "content": "For instructions, see <bpt id=\"p6\">[</bpt>Create Apache Spark clusters in Azure HDInsight<ept id=\"p6\">](hdinsight-apache-spark-jupyter-spark-sql.md)</ept>.",
          "pos": [
            44,
            199
          ]
        }
      ]
    },
    {
      "pos": [
        1543,
        1568
      ],
      "content": "How do I use the kernels?"
    },
    {
      "pos": [
        1574,
        1815
      ],
      "content": "From the <bpt id=\"p7\">[</bpt>Azure Preview Portal<ept id=\"p7\">](https://portal.azure.com/)</ept>, from the startboard, click the tile for your Spark cluster (if you pinned it to the startboard). You can also navigate to your cluster under <bpt id=\"p8\">**</bpt>Browse All<ept id=\"p8\">**</ept><ph id=\"ph5\"/> &gt; <bpt id=\"p9\">**</bpt>HDInsight Clusters<ept id=\"p9\">**</ept>.",
      "nodes": [
        {
          "content": "From the <bpt id=\"p7\">[</bpt>Azure Preview Portal<ept id=\"p7\">](https://portal.azure.com/)</ept>, from the startboard, click the tile for your Spark cluster (if you pinned it to the startboard).",
          "pos": [
            0,
            194
          ]
        },
        {
          "content": "You can also navigate to your cluster under <bpt id=\"p8\">**</bpt>Browse All<ept id=\"p8\">**</ept><ph id=\"ph5\"/> &gt; <bpt id=\"p9\">**</bpt>HDInsight Clusters<ept id=\"p9\">**</ept>.",
          "pos": [
            195,
            372
          ]
        }
      ]
    },
    {
      "pos": [
        1823,
        2008
      ],
      "content": "From the Spark cluster blade, click <bpt id=\"p10\">**</bpt>Quick Links<ept id=\"p10\">**</ept>, and then from the <bpt id=\"p11\">**</bpt>Cluster Dashboard<ept id=\"p11\">**</ept><ph id=\"ph6\"/> blade, click <bpt id=\"p12\">**</bpt>Jupyter Notebook<ept id=\"p12\">**</ept>. If prompted, enter the admin credentials for the cluster.",
      "nodes": [
        {
          "content": "From the Spark cluster blade, click <bpt id=\"p10\">**</bpt>Quick Links<ept id=\"p10\">**</ept>, and then from the <bpt id=\"p11\">**</bpt>Cluster Dashboard<ept id=\"p11\">**</ept><ph id=\"ph6\"/> blade, click <bpt id=\"p12\">**</bpt>Jupyter Notebook<ept id=\"p12\">**</ept>.",
          "pos": [
            0,
            261
          ]
        },
        {
          "content": "If prompted, enter the admin credentials for the cluster.",
          "pos": [
            262,
            319
          ]
        }
      ]
    },
    {
      "pos": [
        2016,
        2186
      ],
      "content": "<ph id=\"ph7\">[AZURE.NOTE]</ph><ph id=\"ph8\"/> You may also reach the Jupyter Notebook for your cluster by opening the following URL in your browser. Replace <bpt id=\"p13\">__</bpt>CLUSTERNAME<ept id=\"p13\">__</ept><ph id=\"ph9\"/> with the name of your cluster:",
      "nodes": [
        {
          "content": "<ph id=\"ph7\">[AZURE.NOTE]</ph><ph id=\"ph8\"/> You may also reach the Jupyter Notebook for your cluster by opening the following URL in your browser.",
          "pos": [
            0,
            147
          ]
        },
        {
          "content": "Replace <bpt id=\"p13\">__</bpt>CLUSTERNAME<ept id=\"p13\">__</ept><ph id=\"ph9\"/> with the name of your cluster:",
          "pos": [
            148,
            256
          ]
        }
      ]
    },
    {
      "pos": [
        2252,
        2450
      ],
      "content": "Create a new notebook with the new kernels. Click <bpt id=\"p14\">**</bpt>New<ept id=\"p14\">**</ept>, and then click <bpt id=\"p15\">**</bpt>Pyspark<ept id=\"p15\">**</ept><ph id=\"ph11\"/> or <bpt id=\"p16\">**</bpt>Spark<ept id=\"p16\">**</ept>. You should use the Spark kernel for Scala applications and PySpark kernel for Python applications.",
      "nodes": [
        {
          "content": "Create a new notebook with the new kernels.",
          "pos": [
            0,
            43
          ]
        },
        {
          "content": "Click <bpt id=\"p14\">**</bpt>New<ept id=\"p14\">**</ept>, and then click <bpt id=\"p15\">**</bpt>Pyspark<ept id=\"p15\">**</ept><ph id=\"ph11\"/> or <bpt id=\"p16\">**</bpt>Spark<ept id=\"p16\">**</ept>.",
          "pos": [
            44,
            234
          ]
        },
        {
          "content": "You should use the Spark kernel for Scala applications and PySpark kernel for Python applications.",
          "pos": [
            235,
            333
          ]
        }
      ]
    },
    {
      "pos": [
        2456,
        2597
      ],
      "content": "<ph id=\"ph12\">![</ph>Create a new Jupyter notebook<ph id=\"ph13\">](./media/hdinsight-apache-spark-jupyter-notebook-kernels/jupyter-kernels.png \"Create a new Jupyter notebook\")</ph>"
    },
    {
      "pos": [
        2603,
        2664
      ],
      "content": "This should open a new notebook with the kernel you selected."
    },
    {
      "pos": [
        2669,
        2702
      ],
      "content": "Why should I use the new kernels?"
    },
    {
      "pos": [
        2704,
        2760
      ],
      "content": "There are a couple of benefits of using the new kernels."
    },
    {
      "pos": [
        2765,
        3044
      ],
      "content": "With the default <bpt id=\"p17\">**</bpt>Python2<ept id=\"p17\">**</ept><ph id=\"ph14\"/> kernel, you need to set the Spark, SQL, or Hive contexts before you can start working with the application you are developing. If you use the new kernels (<bpt id=\"p18\">**</bpt>Spark<ept id=\"p18\">**</ept><ph id=\"ph15\"/> or <bpt id=\"p19\">**</bpt>PySpark<ept id=\"p19\">**</ept>), these contexts are available for you by default. These contexts are:",
      "nodes": [
        {
          "content": "With the default <bpt id=\"p17\">**</bpt>Python2<ept id=\"p17\">**</ept><ph id=\"ph14\"/> kernel, you need to set the Spark, SQL, or Hive contexts before you can start working with the application you are developing.",
          "pos": [
            0,
            210
          ]
        },
        {
          "content": "If you use the new kernels (<bpt id=\"p18\">**</bpt>Spark<ept id=\"p18\">**</ept><ph id=\"ph15\"/> or <bpt id=\"p19\">**</bpt>PySpark<ept id=\"p19\">**</ept>), these contexts are available for you by default.",
          "pos": [
            211,
            409
          ]
        },
        {
          "content": "These contexts are:",
          "pos": [
            410,
            429
          ]
        }
      ]
    },
    {
      "pos": [
        3052,
        3078
      ],
      "content": "<bpt id=\"p20\">**</bpt>sc<ept id=\"p20\">**</ept><ph id=\"ph16\"/> - for Spark context"
    },
    {
      "pos": [
        3085,
        3117
      ],
      "content": "<bpt id=\"p21\">**</bpt>sqlContext<ept id=\"p21\">**</ept><ph id=\"ph17\"/> - for SQL context"
    },
    {
      "pos": [
        3124,
        3158
      ],
      "content": "<bpt id=\"p22\">**</bpt>hiveContext<ept id=\"p22\">**</ept><ph id=\"ph18\"/> - for Hive context"
    },
    {
      "pos": [
        3621,
        3814
      ],
      "content": "You can directly use the <bpt id=\"p23\">**</bpt>%sql<ept id=\"p23\">**</ept><ph id=\"ph19\"/> and <bpt id=\"p24\">**</bpt>%hive<ept id=\"p24\">**</ept><ph id=\"ph20\"/> magics to use SQL or Hive queries, respectively. So, something like this would directly work out-of-the-box, without any leading code statements.",
      "nodes": [
        {
          "content": "You can directly use the <bpt id=\"p23\">**</bpt>%sql<ept id=\"p23\">**</ept><ph id=\"ph19\"/> and <bpt id=\"p24\">**</bpt>%hive<ept id=\"p24\">**</ept><ph id=\"ph20\"/> magics to use SQL or Hive queries, respectively.",
          "pos": [
            0,
            206
          ]
        },
        {
          "content": "So, something like this would directly work out-of-the-box, without any leading code statements.",
          "pos": [
            207,
            303
          ]
        }
      ]
    },
    {
      "pos": [
        3881,
        3923
      ],
      "content": "Considerations while using the new kernels"
    },
    {
      "pos": [
        3925,
        4181
      ],
      "content": "Whichever kernel you use (Python2, PySpark, or Spark), leaving the notebooks running will consume your cluster resources. With the Python2 notebook, because you create the contexts explicitly, you can also kill those contexts when you exit the application.",
      "nodes": [
        {
          "content": "Whichever kernel you use (Python2, PySpark, or Spark), leaving the notebooks running will consume your cluster resources.",
          "pos": [
            0,
            121
          ]
        },
        {
          "content": "With the Python2 notebook, because you create the contexts explicitly, you can also kill those contexts when you exit the application.",
          "pos": [
            122,
            256
          ]
        }
      ]
    },
    {
      "pos": [
        4183,
        4589
      ],
      "content": "However, with PySpark and Spark kernels, because the contexts are preset, you cannot explicitly kill the context as well. So, if you just exit the notebook, the context might still be running, using your cluster resources. A good practice with the PySpark and Spark kernels would be to use the <bpt id=\"p25\">**</bpt>Close and Halt<ept id=\"p25\">**</ept><ph id=\"ph21\"/> option from the notebook's <bpt id=\"p26\">**</bpt>File<ept id=\"p26\">**</ept><ph id=\"ph22\"/> menu. This kills the context and then exits the notebook.",
      "nodes": [
        {
          "content": "However, with PySpark and Spark kernels, because the contexts are preset, you cannot explicitly kill the context as well.",
          "pos": [
            0,
            121
          ]
        },
        {
          "content": "So, if you just exit the notebook, the context might still be running, using your cluster resources.",
          "pos": [
            122,
            222
          ]
        },
        {
          "content": "A good practice with the PySpark and Spark kernels would be to use the <bpt id=\"p25\">**</bpt>Close and Halt<ept id=\"p25\">**</ept><ph id=\"ph21\"/> option from the notebook's <bpt id=\"p26\">**</bpt>File<ept id=\"p26\">**</ept><ph id=\"ph22\"/> menu.",
          "pos": [
            223,
            464
          ]
        },
        {
          "content": "This kills the context and then exits the notebook.",
          "pos": [
            465,
            516
          ]
        }
      ]
    },
    {
      "pos": [
        4597,
        4618
      ],
      "content": "Show me some examples"
    },
    {
      "pos": [
        4620,
        4707
      ],
      "content": "When you open a Jupyter notebook, you will see two folders available at the root level."
    },
    {
      "pos": [
        4711,
        4794
      ],
      "content": "The <bpt id=\"p27\">**</bpt>Python<ept id=\"p27\">**</ept><ph id=\"ph23\"/> folder has sample notebooks that use the default <bpt id=\"p28\">**</bpt>Python2<ept id=\"p28\">**</ept><ph id=\"ph24\"/> kernel."
    },
    {
      "pos": [
        4797,
        4873
      ],
      "content": "The <bpt id=\"p29\">**</bpt>Scala<ept id=\"p29\">**</ept><ph id=\"ph25\"/> folder has sample notebooks that use the new <bpt id=\"p30\">**</bpt>Spark<ept id=\"p30\">**</ept><ph id=\"ph26\"/> kernel."
    },
    {
      "pos": [
        4875,
        5125
      ],
      "content": "You can open the same (e.g. <bpt id=\"p31\">**</bpt>READ ME FIRST - Learn the Basics of Spark on HDInsight<ept id=\"p31\">**</ept>) notebook from the two folders to see how Python2 notebook always start with setting the required contexts, while the Spark notebook just uses the preset contexts."
    },
    {
      "pos": [
        5130,
        5138
      ],
      "content": "Feedback"
    },
    {
      "pos": [
        5140,
        5530
      ],
      "content": "The new kernels are in a pretty nascent stage and will evolve over time. This could also mean that APIs could change as these kernels mature. We would appreciate any feedback that you have while using these new kernels. This will be very useful in shaping the final release of these kernels. You can leave your comments/feedback under the <bpt id=\"p32\">**</bpt>Comments<ept id=\"p32\">**</ept><ph id=\"ph27\"/> section at the bottom of this article.",
      "nodes": [
        {
          "content": "The new kernels are in a pretty nascent stage and will evolve over time.",
          "pos": [
            0,
            72
          ]
        },
        {
          "content": "This could also mean that APIs could change as these kernels mature.",
          "pos": [
            73,
            141
          ]
        },
        {
          "content": "We would appreciate any feedback that you have while using these new kernels.",
          "pos": [
            142,
            219
          ]
        },
        {
          "content": "This will be very useful in shaping the final release of these kernels.",
          "pos": [
            220,
            291
          ]
        },
        {
          "content": "You can leave your comments/feedback under the <bpt id=\"p32\">**</bpt>Comments<ept id=\"p32\">**</ept><ph id=\"ph27\"/> section at the bottom of this article.",
          "pos": [
            292,
            445
          ]
        }
      ]
    },
    {
      "pos": [
        5558,
        5566
      ],
      "content": "See also"
    },
    {
      "pos": [
        5571,
        5650
      ],
      "content": "<bpt id=\"p33\">[</bpt>Overview: Apache Spark on Azure HDInsight<ept id=\"p33\">](hdinsight-apache-spark-overview.md)</ept>"
    },
    {
      "pos": [
        5656,
        5665
      ],
      "content": "Scenarios"
    },
    {
      "pos": [
        5669,
        5798
      ],
      "content": "<bpt id=\"p34\">[</bpt>Spark with BI: Perform interactive data analysis using Spark in HDInsight with BI tools<ept id=\"p34\">](hdinsight-apache-spark-use-bi-tools.md)</ept>"
    },
    {
      "pos": [
        5802,
        5967
      ],
      "content": "<bpt id=\"p35\">[</bpt>Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data<ept id=\"p35\">](hdinsight-apache-spark-ipython-notebook-machine-learning.md)</ept>"
    },
    {
      "pos": [
        5971,
        6117
      ],
      "content": "<bpt id=\"p36\">[</bpt>Spark with Machine Learning: Use Spark in HDInsight to predict food inspection results<ept id=\"p36\">](hdinsight-apache-spark-machine-learning-mllib-ipython.md)</ept>"
    },
    {
      "pos": [
        6121,
        6254
      ],
      "content": "<bpt id=\"p37\">[</bpt>Spark Streaming: Use Spark in HDInsight for building real-time streaming applications<ept id=\"p37\">](hdinsight-apache-spark-eventhub-streaming.md)</ept>"
    },
    {
      "pos": [
        6258,
        6368
      ],
      "content": "<bpt id=\"p38\">[</bpt>Website log analysis using Spark in HDInsight<ept id=\"p38\">](hdinsight-apache-spark-custom-library-website-log-analysis.md)</ept>"
    },
    {
      "pos": [
        6374,
        6401
      ],
      "content": "Create and run applications"
    },
    {
      "pos": [
        6405,
        6507
      ],
      "content": "<bpt id=\"p39\">[</bpt>Create a standalone application using Scala<ept id=\"p39\">](hdinsight-apache-spark-create-standalone-application.md)</ept>"
    },
    {
      "pos": [
        6511,
        6607
      ],
      "content": "<bpt id=\"p40\">[</bpt>Run jobs remotely on a Spark cluster using Livy<ept id=\"p40\">](hdinsight-apache-spark-livy-rest-interface.md)</ept>"
    },
    {
      "pos": [
        6613,
        6633
      ],
      "content": "Tools and extensions"
    },
    {
      "pos": [
        6637,
        6776
      ],
      "content": "<bpt id=\"p41\">[</bpt>Use HDInsight Tools Plugin for IntelliJ IDEA to create and submit Spark Scala applicatons<ept id=\"p41\">](hdinsight-apache-spark-intellij-tool-plugin.md)</ept>"
    },
    {
      "pos": [
        6780,
        6887
      ],
      "content": "<bpt id=\"p42\">[</bpt>Use Zeppelin notebooks with a Spark cluster on HDInsight<ept id=\"p42\">](hdinsight-apache-spark-use-zeppelin-notebook.md)</ept>"
    },
    {
      "pos": [
        6893,
        6909
      ],
      "content": "Manage resources"
    },
    {
      "pos": [
        6913,
        7023
      ],
      "content": "<bpt id=\"p43\">[</bpt>Manage resources for the Apache Spark cluster in Azure HDInsight<ept id=\"p43\">](hdinsight-apache-spark-resource-manager.md)</ept>"
    }
  ],
  "content": "<properties \n    pageTitle=\"Kernels available with Jupyter notebooks on HDInsight Spark clusters on Linux| Microsoft Azure\" \n    description=\"Learn about the additional Jupyter notebook kernels available with Spark cluster on HDInsight Linux.\" \n    services=\"hdinsight\" \n    documentationCenter=\"\" \n    authors=\"nitinme\" \n    manager=\"paulettm\" \n    editor=\"cgronlun\"\n    tags=\"azure-portal\"/>\n\n<tags \n    ms.service=\"hdinsight\" \n    ms.workload=\"big-data\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"02/05/2016\" \n    ms.author=\"nitinme\"/>\n\n\n# Kernels available for Jupyter notebooks with Spark clusters on HDInsight (Linux)\n\nApache Spark cluster on HDInsight (Linux) includes Jupyter notebooks that you can use to test your applications. By default Jupyter notebook comes with a **Python2** kernel. HDInsight Spark clusters provide two additional kernels that you can use with the Jupyter notebook. These are:\n\n1. **Spark** (for applications written in Scala)\n2. **PySpark** (for applications written in Python)\n\nIn this article, you will learn about how to use these kernels and what are the benefits you get from using them.\n\n**Prerequisites:**\n\nYou must have the following:\n\n- An Azure subscription. See [Get Azure free trial](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/).\n- An Apache Spark cluster on HDInsight Linux. For instructions, see [Create Apache Spark clusters in Azure HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).\n\n## How do I use the kernels? \n\n1. From the [Azure Preview Portal](https://portal.azure.com/), from the startboard, click the tile for your Spark cluster (if you pinned it to the startboard). You can also navigate to your cluster under **Browse All** > **HDInsight Clusters**.   \n\n2. From the Spark cluster blade, click **Quick Links**, and then from the **Cluster Dashboard** blade, click **Jupyter Notebook**. If prompted, enter the admin credentials for the cluster.\n\n    > [AZURE.NOTE] You may also reach the Jupyter Notebook for your cluster by opening the following URL in your browser. Replace __CLUSTERNAME__ with the name of your cluster:\n    >\n    > `https://CLUSTERNAME.azurehdinsight.net/jupyter`\n\n2. Create a new notebook with the new kernels. Click **New**, and then click **Pyspark** or **Spark**. You should use the Spark kernel for Scala applications and PySpark kernel for Python applications.\n\n    ![Create a new Jupyter notebook](./media/hdinsight-apache-spark-jupyter-notebook-kernels/jupyter-kernels.png \"Create a new Jupyter notebook\") \n\n3. This should open a new notebook with the kernel you selected.\n\n## Why should I use the new kernels?\n\nThere are a couple of benefits of using the new kernels.\n\n1. With the default **Python2** kernel, you need to set the Spark, SQL, or Hive contexts before you can start working with the application you are developing. If you use the new kernels (**Spark** or **PySpark**), these contexts are available for you by default. These contexts are:\n\n    * **sc** - for Spark context\n    * **sqlContext** - for SQL context\n    * **hiveContext** - for Hive context\n\n\n    So, you don't have to run statements like the following to set the contexts:\n\n        ###################################################\n        # YOU DO NOT NEED TO RUN THIS WITH THE NEW KERNELS\n        ###################################################\n        sc = SparkContext('yarn-client')\n        sqlContext = SQLContext(sc)\n        hiveContext = HiveContext(sc)\n\n    Instead, you can directly use the preset contexts in your application.\n    \n2. You can directly use the **%sql** and **%hive** magics to use SQL or Hive queries, respectively. So, something like this would directly work out-of-the-box, without any leading code statements.\n\n        %hive\n        SELECT * FROM hivesampletable LIMIT 10\n\n## Considerations while using the new kernels\n\nWhichever kernel you use (Python2, PySpark, or Spark), leaving the notebooks running will consume your cluster resources. With the Python2 notebook, because you create the contexts explicitly, you can also kill those contexts when you exit the application.\n\nHowever, with PySpark and Spark kernels, because the contexts are preset, you cannot explicitly kill the context as well. So, if you just exit the notebook, the context might still be running, using your cluster resources. A good practice with the PySpark and Spark kernels would be to use the **Close and Halt** option from the notebook's **File** menu. This kills the context and then exits the notebook.  \n\n\n## Show me some examples\n\nWhen you open a Jupyter notebook, you will see two folders available at the root level.\n\n* The **Python** folder has sample notebooks that use the default **Python2** kernel.\n* The **Scala** folder has sample notebooks that use the new **Spark** kernel.\n\nYou can open the same (e.g. **READ ME FIRST - Learn the Basics of Spark on HDInsight**) notebook from the two folders to see how Python2 notebook always start with setting the required contexts, while the Spark notebook just uses the preset contexts.\n\n## Feedback\n\nThe new kernels are in a pretty nascent stage and will evolve over time. This could also mean that APIs could change as these kernels mature. We would appreciate any feedback that you have while using these new kernels. This will be very useful in shaping the final release of these kernels. You can leave your comments/feedback under the **Comments** section at the bottom of this article.\n\n\n## <a name=\"seealso\"></a>See also\n\n\n* [Overview: Apache Spark on Azure HDInsight](hdinsight-apache-spark-overview.md)\n\n### Scenarios\n\n* [Spark with BI: Perform interactive data analysis using Spark in HDInsight with BI tools](hdinsight-apache-spark-use-bi-tools.md)\n\n* [Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data](hdinsight-apache-spark-ipython-notebook-machine-learning.md)\n\n* [Spark with Machine Learning: Use Spark in HDInsight to predict food inspection results](hdinsight-apache-spark-machine-learning-mllib-ipython.md)\n\n* [Spark Streaming: Use Spark in HDInsight for building real-time streaming applications](hdinsight-apache-spark-eventhub-streaming.md)\n\n* [Website log analysis using Spark in HDInsight](hdinsight-apache-spark-custom-library-website-log-analysis.md)\n\n### Create and run applications\n\n* [Create a standalone application using Scala](hdinsight-apache-spark-create-standalone-application.md)\n\n* [Run jobs remotely on a Spark cluster using Livy](hdinsight-apache-spark-livy-rest-interface.md)\n\n### Tools and extensions\n\n* [Use HDInsight Tools Plugin for IntelliJ IDEA to create and submit Spark Scala applicatons](hdinsight-apache-spark-intellij-tool-plugin.md)\n\n* [Use Zeppelin notebooks with a Spark cluster on HDInsight](hdinsight-apache-spark-use-zeppelin-notebook.md)\n\n### Manage resources\n\n* [Manage resources for the Apache Spark cluster in Azure HDInsight](hdinsight-apache-spark-resource-manager.md)\n"
}