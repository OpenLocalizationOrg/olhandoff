{
  "nodes": [
    {
      "pos": [
        27,
        110
      ],
      "content": "The Cortana Analytics Process in action: using SQL Data Warehouse | Microsoft Azure"
    },
    {
      "pos": [
        129,
        180
      ],
      "content": "Advanced Analytics Process and Technology in Action"
    },
    {
      "pos": [
        540,
        605
      ],
      "content": "The Cortana Analytics Process in action: using SQL Data Warehouse"
    },
    {
      "pos": [
        607,
        1056
      ],
      "content": "In this tutorial, we walk you through building and deploying a machine learning model using SQL Data Warehouse (SQL DW) for a publicly available dataset -- the <bpt id=\"p1\">[</bpt>NYC Taxi Trips<ept id=\"p1\">](http://www.andresmh.com/nyctaxitrips/)</ept><ph id=\"ph2\"/> dataset. The binary classification model constructed predicts whether or not a tip is paid for a trip, and models for multiclass classification and regression are also discussed that predict the distribution for the tip amounts paid.",
      "nodes": [
        {
          "content": "In this tutorial, we walk you through building and deploying a machine learning model using SQL Data Warehouse (SQL DW) for a publicly available dataset -- the <bpt id=\"p1\">[</bpt>NYC Taxi Trips<ept id=\"p1\">](http://www.andresmh.com/nyctaxitrips/)</ept><ph id=\"ph2\"/> dataset.",
          "pos": [
            0,
            276
          ]
        },
        {
          "content": "The binary classification model constructed predicts whether or not a tip is paid for a trip, and models for multiclass classification and regression are also discussed that predict the distribution for the tip amounts paid.",
          "pos": [
            277,
            501
          ]
        }
      ]
    },
    {
      "pos": [
        1058,
        1470
      ],
      "content": "The procedure follows the <bpt id=\"p2\">[</bpt>Cortana Analytics Process (CAP)<ept id=\"p2\">](https://azure.microsoft.com/documentation/learning-paths/cortana-analytics-process/)</ept><ph id=\"ph3\"/> workflow. We show how to setup a data science environment, how to load the data into SQL DW, and how use either SQL DW or an IPython Notebook to explore the data and engineer features to model. We then show how to build and deploy a model with Azure Machine Learning.",
      "nodes": [
        {
          "content": "The procedure follows the <bpt id=\"p2\">[</bpt>Cortana Analytics Process (CAP)<ept id=\"p2\">](https://azure.microsoft.com/documentation/learning-paths/cortana-analytics-process/)</ept><ph id=\"ph3\"/> workflow.",
          "pos": [
            0,
            206
          ]
        },
        {
          "content": "We show how to setup a data science environment, how to load the data into SQL DW, and how use either SQL DW or an IPython Notebook to explore the data and engineer features to model.",
          "pos": [
            207,
            390
          ]
        },
        {
          "content": "We then show how to build and deploy a model with Azure Machine Learning.",
          "pos": [
            391,
            464
          ]
        }
      ]
    },
    {
      "pos": [
        1498,
        1524
      ],
      "content": "The NYC Taxi Trips dataset"
    },
    {
      "pos": [
        1526,
        1964
      ],
      "content": "The NYC Taxi Trip data consists of about 20GB of compressed CSV files (~48GB uncompressed), recording more than 173 million individual trips and the fares paid for each trip. Each trip record includes the pickup and drop-off locations and times, anonymized hack (driver's) license number, and the medallion (taxi’s unique id) number. The data covers all trips in the year 2013 and is provided in the following two datasets for each month:",
      "nodes": [
        {
          "content": "The NYC Taxi Trip data consists of about 20GB of compressed CSV files (~48GB uncompressed), recording more than 173 million individual trips and the fares paid for each trip.",
          "pos": [
            0,
            174
          ]
        },
        {
          "content": "Each trip record includes the pickup and drop-off locations and times, anonymized hack (driver's) license number, and the medallion (taxi’s unique id) number.",
          "pos": [
            175,
            333
          ]
        },
        {
          "content": "The data covers all trips in the year 2013 and is provided in the following two datasets for each month:",
          "pos": [
            334,
            438
          ]
        }
      ]
    },
    {
      "pos": [
        1969,
        2138
      ],
      "content": "The <bpt id=\"p3\">**</bpt>trip_data.csv<ept id=\"p3\">**</ept><ph id=\"ph4\"/> file contains trip details, such as number of passengers, pickup and dropoff points, trip duration, and trip length. Here are a few sample records:",
      "nodes": [
        {
          "content": "The <bpt id=\"p3\">**</bpt>trip_data.csv<ept id=\"p3\">**</ept><ph id=\"ph4\"/> file contains trip details, such as number of passengers, pickup and dropoff points, trip duration, and trip length.",
          "pos": [
            0,
            190
          ]
        },
        {
          "content": "Here are a few sample records:",
          "pos": [
            191,
            221
          ]
        }
      ]
    },
    {
      "pos": [
        3234,
        3439
      ],
      "content": "The <bpt id=\"p4\">**</bpt>trip_fare.csv<ept id=\"p4\">**</ept><ph id=\"ph5\"/> file contains details of the fare paid for each trip, such as payment type, fare amount, surcharge and taxes, tips and tolls, and the total amount paid. Here are a few sample records:",
      "nodes": [
        {
          "content": "The <bpt id=\"p4\">**</bpt>trip_fare.csv<ept id=\"p4\">**</ept><ph id=\"ph5\"/> file contains details of the fare paid for each trip, such as payment type, fare amount, surcharge and taxes, tips and tolls, and the total amount paid.",
          "pos": [
            0,
            226
          ]
        },
        {
          "content": "Here are a few sample records:",
          "pos": [
            227,
            257
          ]
        }
      ]
    },
    {
      "pos": [
        4184,
        4284
      ],
      "content": "The <bpt id=\"p5\">**</bpt>unique key<ept id=\"p5\">**</ept><ph id=\"ph6\"/> used to join trip\\_data and trip\\_fare is composed of the following three fields:"
    },
    {
      "pos": [
        4289,
        4299
      ],
      "content": "medallion,"
    },
    {
      "pos": [
        4303,
        4320
      ],
      "content": "hack\\_license and"
    },
    {
      "pos": [
        4324,
        4341
      ],
      "content": "pickup\\_datetime."
    },
    {
      "pos": [
        4368,
        4407
      ],
      "content": "Address three types of prediction tasks"
    },
    {
      "pos": [
        4410,
        4520
      ],
      "content": "We formulate three prediction problems based on the <bpt id=\"p6\">*</bpt>tip\\_amount<ept id=\"p6\">*</ept><ph id=\"ph7\"/> to illustrate three kinds of modeling tasks:"
    },
    {
      "pos": [
        4525,
        4723
      ],
      "content": "<bpt id=\"p7\">**</bpt>Binary classification<ept id=\"p7\">**</ept>: To predict whether or not a tip was paid for a trip, i.e. a <bpt id=\"p8\">*</bpt>tip\\_amount<ept id=\"p8\">*</ept><ph id=\"ph8\"/> that is greater than $0 is a positive example, while a <bpt id=\"p9\">*</bpt>tip\\_amount<ept id=\"p9\">*</ept><ph id=\"ph9\"/> of $0 is a negative example."
    },
    {
      "pos": [
        4728,
        4860
      ],
      "content": "<bpt id=\"p10\">**</bpt>Multiclass classification<ept id=\"p10\">**</ept>: To predict the range of tip paid for the trip. We divide the <bpt id=\"p11\">*</bpt>tip\\_amount<ept id=\"p11\">*</ept><ph id=\"ph10\"/> into five bins or classes:",
      "nodes": [
        {
          "content": "<bpt id=\"p10\">**</bpt>Multiclass classification<ept id=\"p10\">**</ept>: To predict the range of tip paid for the trip.",
          "pos": [
            0,
            117
          ]
        },
        {
          "content": "We divide the <bpt id=\"p11\">*</bpt>tip\\_amount<ept id=\"p11\">*</ept><ph id=\"ph10\"/> into five bins or classes:",
          "pos": [
            118,
            227
          ]
        }
      ]
    },
    {
      "pos": [
        5103,
        5169
      ],
      "content": "<bpt id=\"p12\">**</bpt>Regression task<ept id=\"p12\">**</ept>: To predict the amount of tip paid for a trip."
    },
    {
      "pos": [
        5197,
        5261
      ],
      "content": "Set up the Azure data science environment for advanced analytics"
    },
    {
      "pos": [
        5263,
        5329
      ],
      "content": "To set up your Azure Data Science environment, follow these steps."
    },
    {
      "pos": [
        5331,
        5377
      ],
      "content": "<bpt id=\"p13\">**</bpt>Create your own Azure blob storage account<ept id=\"p13\">**</ept>"
    },
    {
      "pos": [
        5381,
        5802
      ],
      "content": "When you provision your own Azure blob storage, choose a geo-location for your Azure blob storage in or as close as possible to <bpt id=\"p14\">**</bpt>South Central US<ept id=\"p14\">**</ept>, which is where the NYC Taxi data is stored. The data will be copied using AzCopy from the public blob storage container to a container in your own storage account. The closer your Azure blob storage is to South Central US, the faster this task (Step 4) will be completed.",
      "nodes": [
        {
          "content": "When you provision your own Azure blob storage, choose a geo-location for your Azure blob storage in or as close as possible to <bpt id=\"p14\">**</bpt>South Central US<ept id=\"p14\">**</ept>, which is where the NYC Taxi data is stored.",
          "pos": [
            0,
            233
          ]
        },
        {
          "content": "The data will be copied using AzCopy from the public blob storage container to a container in your own storage account.",
          "pos": [
            234,
            353
          ]
        },
        {
          "content": "The closer your Azure blob storage is to South Central US, the faster this task (Step 4) will be completed.",
          "pos": [
            354,
            461
          ]
        }
      ]
    },
    {
      "pos": [
        5806,
        6071
      ],
      "content": "To create your own Azure storage account, follow the steps outlined at <bpt id=\"p15\">[</bpt>About Azure storage accounts<ept id=\"p15\">](storage-create-storage-account.md)</ept>. Be sure to make notes on the values for following storage account credentials as they will be needed later in this walkthrough.",
      "nodes": [
        {
          "content": "To create your own Azure storage account, follow the steps outlined at <bpt id=\"p15\">[</bpt>About Azure storage accounts<ept id=\"p15\">](storage-create-storage-account.md)</ept>.",
          "pos": [
            0,
            177
          ]
        },
        {
          "content": "Be sure to make notes on the values for following storage account credentials as they will be needed later in this walkthrough.",
          "pos": [
            178,
            305
          ]
        }
      ]
    },
    {
      "pos": [
        6078,
        6102
      ],
      "content": "<bpt id=\"p16\">**</bpt>Storage Account Name<ept id=\"p16\">**</ept>"
    },
    {
      "pos": [
        6107,
        6130
      ],
      "content": "<bpt id=\"p17\">**</bpt>Storage Account Key<ept id=\"p17\">**</ept>"
    },
    {
      "pos": [
        6135,
        6218
      ],
      "content": "<bpt id=\"p18\">**</bpt>Container Name<ept id=\"p18\">**</ept><ph id=\"ph11\"/> (which you want the data to be stored in the Azure blob storage)"
    },
    {
      "pos": [
        6220,
        6526
      ],
      "content": "<bpt id=\"p19\">**</bpt>Provision your Azure SQL DW instance.<ept id=\"p19\">**</ept><ph id=\"ph12\"/> \nFollow the documentation at <bpt id=\"p20\">[</bpt>Create a SQL Data Warehouse<ept id=\"p20\">](sql-data-warehouse-get-started-provision.md)</ept><ph id=\"ph13\"/> to provision a SQL Data Warehouse instance. Make sure that you make notations on the following SQL Data Warehouse credentials which will be used in later steps.",
      "nodes": [
        {
          "content": "<bpt id=\"p19\">**</bpt>Provision your Azure SQL DW instance.<ept id=\"p19\">**</ept><ph id=\"ph12\"/> \nFollow the documentation at <bpt id=\"p20\">[</bpt>Create a SQL Data Warehouse<ept id=\"p20\">](sql-data-warehouse-get-started-provision.md)</ept><ph id=\"ph13\"/> to provision a SQL Data Warehouse instance.",
          "pos": [
            0,
            299
          ]
        },
        {
          "content": "Make sure that you make notations on the following SQL Data Warehouse credentials which will be used in later steps.",
          "pos": [
            300,
            416
          ]
        }
      ]
    },
    {
      "pos": [
        6533,
        6584
      ],
      "content": "<bpt id=\"p21\">**</bpt>Server Name<ept id=\"p21\">**</ept>: <ph id=\"ph14\">&lt;server Name&gt;</ph>.database.windows.net"
    },
    {
      "pos": [
        6589,
        6614
      ],
      "content": "<bpt id=\"p22\">**</bpt>SQLDW (Database) Name<ept id=\"p22\">**</ept>"
    },
    {
      "pos": [
        6620,
        6632
      ],
      "content": "<bpt id=\"p23\">**</bpt>Username<ept id=\"p23\">**</ept>"
    },
    {
      "pos": [
        6637,
        6649
      ],
      "content": "<bpt id=\"p24\">**</bpt>Password<ept id=\"p24\">**</ept>"
    },
    {
      "pos": [
        6651,
        6864
      ],
      "content": "<bpt id=\"p25\">**</bpt>Install Visual Studio 2015 and SQL Server Data Tools.<ept id=\"p25\">**</ept><ph id=\"ph15\"/> For instructions, see <bpt id=\"p26\">[</bpt>Install Visual Studio 2015 and/or SSDT (SQL Server Data Tools) for SQL Data Warehouse<ept id=\"p26\">](sql-data-warehouse-install-visual-studio.md)</ept>."
    },
    {
      "pos": [
        6867,
        7057
      ],
      "content": "<bpt id=\"p27\">**</bpt>Connect to your Azure SQL DW with Visual Studio.<ept id=\"p27\">**</ept><ph id=\"ph16\"/> For instructions, see steps 1 &amp; 2 in <bpt id=\"p28\">[</bpt>Connect to Azure SQL Data Warehouse with Visual Studio<ept id=\"p28\">](sql-data-warehouse-get-started-connect.md)</ept>."
    },
    {
      "pos": [
        7061,
        7248
      ],
      "content": "<ph id=\"ph17\">[AZURE.NOTE]</ph><ph id=\"ph18\"/> Run the following SQL query on the database you created in your SQL Data Warehouse (instead of the query provided in step 3 of the connect topic,) to <bpt id=\"p29\">**</bpt>create a master key<ept id=\"p29\">**</ept>."
    },
    {
      "pos": [
        7426,
        7609
      ],
      "content": "<bpt id=\"p30\">**</bpt>Create an Azure Machine Learning workspace under your Azure subscription.<ept id=\"p30\">**</ept><ph id=\"ph19\"/> For instructions, see <bpt id=\"p31\">[</bpt>Create an Azure Machine Learning workspace<ept id=\"p31\">](machine-learning-create-workspace.md)</ept>."
    },
    {
      "pos": [
        7636,
        7673
      ],
      "content": "Load the data into SQL Data Warehouse"
    },
    {
      "pos": [
        7675,
        8046
      ],
      "content": "Open a Windows PowerShell command console. Run the following PowerShell commands to download the example SQL script files that we share with you on Github to a local directory that you specify with the parameter <bpt id=\"p32\">*</bpt>-DestDir<ept id=\"p32\">*</ept>. You can change the value of parameter <bpt id=\"p33\">*</bpt>-DestDir<ept id=\"p33\">*</ept><ph id=\"ph20\"/> to any local directory. If <bpt id=\"p34\">*</bpt>-DestDir<ept id=\"p34\">*</ept><ph id=\"ph21\"/> does not exist, it will be created by the PowerShell script.",
      "nodes": [
        {
          "content": "Open a Windows PowerShell command console.",
          "pos": [
            0,
            42
          ]
        },
        {
          "content": "Run the following PowerShell commands to download the example SQL script files that we share with you on Github to a local directory that you specify with the parameter <bpt id=\"p32\">*</bpt>-DestDir<ept id=\"p32\">*</ept>.",
          "pos": [
            43,
            263
          ]
        },
        {
          "content": "You can change the value of parameter <bpt id=\"p33\">*</bpt>-DestDir<ept id=\"p33\">*</ept><ph id=\"ph20\"/> to any local directory.",
          "pos": [
            264,
            391
          ]
        },
        {
          "content": "If <bpt id=\"p34\">*</bpt>-DestDir<ept id=\"p34\">*</ept><ph id=\"ph21\"/> does not exist, it will be created by the PowerShell script.",
          "pos": [
            392,
            521
          ]
        }
      ]
    },
    {
      "pos": [
        8050,
        8239
      ],
      "content": "<ph id=\"ph22\">[AZURE.NOTE]</ph><ph id=\"ph23\"/> You might need to <bpt id=\"p35\">**</bpt>Run as Administrator<ept id=\"p35\">**</ept><ph id=\"ph24\"/> when executing the following PowerShell script if your <bpt id=\"p36\">*</bpt>DestDir<ept id=\"p36\">*</ept><ph id=\"ph25\"/> directory needs Administrator privilege to create or to write to it."
    },
    {
      "pos": [
        8605,
        8731
      ],
      "content": "After successful execution, your current working directory changes to <bpt id=\"p37\">*</bpt>-DestDir<ept id=\"p37\">*</ept>. You should be able to see screen like below:",
      "nodes": [
        {
          "content": "After successful execution, your current working directory changes to <bpt id=\"p37\">*</bpt>-DestDir<ept id=\"p37\">*</ept>.",
          "pos": [
            0,
            121
          ]
        },
        {
          "content": "You should be able to see screen like below:",
          "pos": [
            122,
            166
          ]
        }
      ]
    },
    {
      "pos": [
        8742,
        8824
      ],
      "content": "In your <bpt id=\"p38\">*</bpt>-DestDir<ept id=\"p38\">*</ept>, execute the following PowerShell script in administrator mode:"
    },
    {
      "pos": [
        8855,
        9597
      ],
      "content": "When the PowerShell script runs for the first time, you will be asked to input the information from your Azure SQL DW and your Azure blob storage account. When this PowerShell script completes running for the first time, the credentials you input will have been written to a configuration file SQLDW.conf in the present working directory. The future run of this PowerShell script file has the option to read all needed parameters from this configuration file. If you need to change some parameters, you can choose to input the parameters on the screen upon prompt by deleting this configuration file and inputting the parameters values as prompted or to change the parameter values by editing the SQLDW.conf file in your <bpt id=\"p39\">*</bpt>-DestDir<ept id=\"p39\">*</ept><ph id=\"ph27\"/> directory.",
      "nodes": [
        {
          "content": "When the PowerShell script runs for the first time, you will be asked to input the information from your Azure SQL DW and your Azure blob storage account.",
          "pos": [
            0,
            154
          ]
        },
        {
          "content": "When this PowerShell script completes running for the first time, the credentials you input will have been written to a configuration file SQLDW.conf in the present working directory.",
          "pos": [
            155,
            338
          ]
        },
        {
          "content": "The future run of this PowerShell script file has the option to read all needed parameters from this configuration file.",
          "pos": [
            339,
            459
          ]
        },
        {
          "content": "If you need to change some parameters, you can choose to input the parameters on the screen upon prompt by deleting this configuration file and inputting the parameters values as prompted or to change the parameter values by editing the SQLDW.conf file in your <bpt id=\"p39\">*</bpt>-DestDir<ept id=\"p39\">*</ept><ph id=\"ph27\"/> directory.",
          "pos": [
            460,
            797
          ]
        }
      ]
    },
    {
      "pos": [
        9601,
        9984
      ],
      "content": "<ph id=\"ph28\">[AZURE.NOTE]</ph><ph id=\"ph29\"/> In order to avoid schema name conflicts with those that already exist in your Azure SQL DW, when reading parameters directly from the SQLDW.conf file, a 3-digit random number is added to the schema name from the SQLDW.conf file as the default schema name for each run. The PowerShell script may prompt you for a schema name: the name may be specified at user discretion.",
      "nodes": [
        {
          "content": "<ph id=\"ph28\">[AZURE.NOTE]</ph><ph id=\"ph29\"/> In order to avoid schema name conflicts with those that already exist in your Azure SQL DW, when reading parameters directly from the SQLDW.conf file, a 3-digit random number is added to the schema name from the SQLDW.conf file as the default schema name for each run.",
          "pos": [
            0,
            315
          ]
        },
        {
          "content": "The PowerShell script may prompt you for a schema name: the name may be specified at user discretion.",
          "pos": [
            316,
            417
          ]
        }
      ]
    },
    {
      "pos": [
        9986,
        10048
      ],
      "content": "This <bpt id=\"p40\">**</bpt>PowerShell script<ept id=\"p40\">**</ept><ph id=\"ph30\"/> file completes the following tasks:"
    },
    {
      "pos": [
        10052,
        10121
      ],
      "content": "<bpt id=\"p41\">**</bpt>Downloads and installs AzCopy<ept id=\"p41\">**</ept>, if AzCopy is not already installed"
    },
    {
      "pos": [
        11096,
        11181
      ],
      "content": "<bpt id=\"p42\">**</bpt>Copies data to your private blob storage account<ept id=\"p42\">**</ept><ph id=\"ph31\"/> from the public blob with AzCopy"
    },
    {
      "pos": [
        11840,
        11993
      ],
      "content": "<bpt id=\"p43\">**</bpt>Loads data using Polybase (by executing LoadDataToSQLDW.sql) to your Azure SQL DW<ept id=\"p43\">**</ept><ph id=\"ph32\"/> from your private blob storage account with the following commands."
    },
    {
      "pos": [
        12005,
        12020
      ],
      "content": "Create a schema"
    },
    {
      "pos": [
        12081,
        12116
      ],
      "content": "Create a database scoped credential"
    },
    {
      "pos": [
        12282,
        12338
      ],
      "content": "Create an external data source for an Azure storage blob"
    },
    {
      "pos": [
        12932,
        13049
      ],
      "content": "Create an external file format for a csv file. Data is uncompressed and fields are separated with the pipe character.",
      "nodes": [
        {
          "content": "Create an external file format for a csv file.",
          "pos": [
            0,
            46
          ]
        },
        {
          "content": "Data is uncompressed and fields are separated with the pipe character.",
          "pos": [
            47,
            117
          ]
        }
      ]
    },
    {
      "pos": [
        13392,
        13472
      ],
      "content": "Create external fare and trip tables for NYC taxi dataset in Azure blob storage."
    },
    {
      "pos": [
        15219,
        15293
      ],
      "content": "Load data from external tables in Azure blob storage to SQL Data Warehouse"
    },
    {
      "pos": [
        15882,
        16068
      ],
      "content": "Create a sample data table (NYCTaxi_Sample) and insert data to it from selecting SQL queries on the trip and fare tables. (Some steps of this walkthrough needs to use this sample table.)",
      "nodes": [
        {
          "content": "Create a sample data table (NYCTaxi_Sample) and insert data to it from selecting SQL queries on the trip and fare tables.",
          "pos": [
            0,
            121
          ]
        },
        {
          "content": "(Some steps of this walkthrough needs to use this sample table.)",
          "pos": [
            122,
            186
          ]
        }
      ]
    },
    {
      "pos": [
        17288,
        17611
      ],
      "content": "<ph id=\"ph33\">[AZURE.NOTE]</ph><ph id=\"ph34\"/> Depending on the geographical location of your private blob storage account, the process of copying data from a public blob to your private storage account can take about 15 minutes, or  even longer,and the process of loading data from your storage account to your Azure SQL DW could take 20 minutes or longer."
    },
    {
      "pos": [
        17616,
        18030
      ],
      "content": "<ph id=\"ph35\">[AZURE.NOTE]</ph><ph id=\"ph36\"/> If the .csv files to be copied from the public blob storage to your private blob storage account already exist in your private blob storage account, AzCopy will ask you whether you want to overwrite them. If you do not want to overwrite them, input <bpt id=\"p44\">**</bpt>n<ept id=\"p44\">**</ept><ph id=\"ph37\"/> when prompted. If you want to overwrite <bpt id=\"p45\">**</bpt>all<ept id=\"p45\">**</ept><ph id=\"ph38\"/> of them, input <bpt id=\"p46\">**</bpt>a<ept id=\"p46\">**</ept><ph id=\"ph39\"/> when prompted. You can also input <bpt id=\"p47\">**</bpt>y<ept id=\"p47\">**</ept><ph id=\"ph40\"/> to overwrite .csv files individually.",
      "nodes": [
        {
          "content": "<ph id=\"ph35\">[AZURE.NOTE]</ph><ph id=\"ph36\"/> If the .csv files to be copied from the public blob storage to your private blob storage account already exist in your private blob storage account, AzCopy will ask you whether you want to overwrite them.",
          "pos": [
            0,
            251
          ]
        },
        {
          "content": "If you do not want to overwrite them, input <bpt id=\"p44\">**</bpt>n<ept id=\"p44\">**</ept><ph id=\"ph37\"/> when prompted.",
          "pos": [
            252,
            371
          ]
        },
        {
          "content": "If you want to overwrite <bpt id=\"p45\">**</bpt>all<ept id=\"p45\">**</ept><ph id=\"ph38\"/> of them, input <bpt id=\"p46\">**</bpt>a<ept id=\"p46\">**</ept><ph id=\"ph39\"/> when prompted.",
          "pos": [
            372,
            550
          ]
        },
        {
          "content": "You can also input <bpt id=\"p47\">**</bpt>y<ept id=\"p47\">**</ept><ph id=\"ph40\"/> to overwrite .csv files individually.",
          "pos": [
            551,
            668
          ]
        }
      ]
    },
    {
      "pos": [
        18032,
        18047
      ],
      "content": "<ph id=\"ph41\">![</ph>Plot #21<ph id=\"ph42\">][21]</ph>"
    },
    {
      "pos": [
        18051,
        18470
      ],
      "content": "<ph id=\"ph43\">[AZURE.TIP]</ph> <bpt id=\"p48\">**</bpt>Use your own data:<ept id=\"p48\">**</ept><ph id=\"ph44\"/> If your data is in your on-premise machine in your real life application, you can still use AzCopy to upload on-premise data to your private Azure blob storage. You only need to change the <bpt id=\"p49\">**</bpt>Source<ept id=\"p49\">**</ept><ph id=\"ph45\"/> location, <ph id=\"ph46\">`$Source = \"http://getgoing.blob.core.windows.net/public/nyctaxidataset\"`</ph>, in the AzCopy command of the PowerShell script file to the local directory that contains your data.",
      "nodes": [
        {
          "content": "<ph id=\"ph43\">[AZURE.TIP]</ph> <bpt id=\"p48\">**</bpt>Use your own data:<ept id=\"p48\">**</ept><ph id=\"ph44\"/> If your data is in your on-premise machine in your real life application, you can still use AzCopy to upload on-premise data to your private Azure blob storage.",
          "pos": [
            0,
            269
          ]
        },
        {
          "content": "You only need to change the <bpt id=\"p49\">**</bpt>Source<ept id=\"p49\">**</ept><ph id=\"ph45\"/> location, <ph id=\"ph46\">`$Source = \"http://getgoing.blob.core.windows.net/public/nyctaxidataset\"`</ph>, in the AzCopy command of the PowerShell script file to the local directory that contains your data.",
          "pos": [
            270,
            567
          ]
        }
      ]
    },
    {
      "pos": [
        18477,
        18768
      ],
      "content": "<ph id=\"ph47\">[AZURE.TIP]</ph><ph id=\"ph48\"/> If your data is already in your private Azure blob storage in your real life application, you can skip the AzCopy step in the PowerShell script and directly upload the data to Azure SQL DW. This will require additional edits of the script to tailor it to the format of your data.",
      "nodes": [
        {
          "content": "<ph id=\"ph47\">[AZURE.TIP]</ph><ph id=\"ph48\"/> If your data is already in your private Azure blob storage in your real life application, you can skip the AzCopy step in the PowerShell script and directly upload the data to Azure SQL DW.",
          "pos": [
            0,
            235
          ]
        },
        {
          "content": "This will require additional edits of the script to tailor it to the format of your data.",
          "pos": [
            236,
            325
          ]
        }
      ]
    },
    {
      "pos": [
        18771,
        19061
      ],
      "content": "This Powershell script also plugs in the Azure SQL DW information into the data exploration example files SQLDW_Explorations.sql, SQLDW_Explorations.ipynb, and SQLDW_Explorations_Scripts.py so that these three files are ready to be tried out instantly after the PowerShell script completes."
    },
    {
      "pos": [
        19064,
        19125
      ],
      "content": "After a successful execution, you will see screen like below:"
    },
    {
      "pos": [
        19163,
        19231
      ],
      "content": "Data exploration and feature engineering in Azure SQL Data Warehouse"
    },
    {
      "pos": [
        19233,
        19814
      ],
      "content": "In this section, we perform data exploration and feature generation by running SQL queries against Azure SQL DW directly using <bpt id=\"p50\">**</bpt>Visual Studio Data Tools<ept id=\"p50\">**</ept>. All SQL queries used in this section can be found in the sample script named <bpt id=\"p51\">*</bpt>SQLDW_Explorations.sql<ept id=\"p51\">*</ept>. This file has already been downloaded to your local directory by the PowerShell script. You can also retrieve it from <bpt id=\"p52\">[</bpt>Github<ept id=\"p52\">](https://raw.githubusercontent.com/Azure/Azure-MachineLearning-DataScience/master/Misc/SQLDW/SQLDW_Explorations.sql)</ept>. But the file in Github does not have the Azure SQL DW information plugged in.",
      "nodes": [
        {
          "content": "In this section, we perform data exploration and feature generation by running SQL queries against Azure SQL DW directly using <bpt id=\"p50\">**</bpt>Visual Studio Data Tools<ept id=\"p50\">**</ept>.",
          "pos": [
            0,
            196
          ]
        },
        {
          "content": "All SQL queries used in this section can be found in the sample script named <bpt id=\"p51\">*</bpt>SQLDW_Explorations.sql<ept id=\"p51\">*</ept>.",
          "pos": [
            197,
            339
          ]
        },
        {
          "content": "This file has already been downloaded to your local directory by the PowerShell script.",
          "pos": [
            340,
            427
          ]
        },
        {
          "content": "You can also retrieve it from <bpt id=\"p52\">[</bpt>Github<ept id=\"p52\">](https://raw.githubusercontent.com/Azure/Azure-MachineLearning-DataScience/master/Misc/SQLDW/SQLDW_Explorations.sql)</ept>.",
          "pos": [
            428,
            623
          ]
        },
        {
          "content": "But the file in Github does not have the Azure SQL DW information plugged in.",
          "pos": [
            624,
            701
          ]
        }
      ]
    },
    {
      "pos": [
        19817,
        20044
      ],
      "content": "Connect to your Azure SQL DW using Visual Studio with the SQL DW login name and password and open up the <bpt id=\"p53\">**</bpt>SQL Object Explorer<ept id=\"p53\">**</ept><ph id=\"ph50\"/> to confirm the database and tables have been imported. Retrieve the <bpt id=\"p54\">*</bpt>SQLDW_Explorations.sql<ept id=\"p54\">*</ept><ph id=\"ph51\"/> file.",
      "nodes": [
        {
          "content": "Connect to your Azure SQL DW using Visual Studio with the SQL DW login name and password and open up the <bpt id=\"p53\">**</bpt>SQL Object Explorer<ept id=\"p53\">**</ept><ph id=\"ph50\"/> to confirm the database and tables have been imported.",
          "pos": [
            0,
            238
          ]
        },
        {
          "content": "Retrieve the <bpt id=\"p54\">*</bpt>SQLDW_Explorations.sql<ept id=\"p54\">*</ept><ph id=\"ph51\"/> file.",
          "pos": [
            239,
            337
          ]
        }
      ]
    },
    {
      "pos": [
        20047,
        20257
      ],
      "content": "<ph id=\"ph52\">[AZURE.NOTE]</ph><ph id=\"ph53\"/> To open a Parallel Data Warehouse (PDW) query editor, use the <bpt id=\"p55\">**</bpt>New Query<ept id=\"p55\">**</ept><ph id=\"ph54\"/> command while your PDW is selected in the <bpt id=\"p56\">**</bpt>SQL Object Explorer<ept id=\"p56\">**</ept>. The standard SQL query editor is not supported by PDW.",
      "nodes": [
        {
          "content": "<ph id=\"ph52\">[AZURE.NOTE]</ph><ph id=\"ph53\"/> To open a Parallel Data Warehouse (PDW) query editor, use the <bpt id=\"p55\">**</bpt>New Query<ept id=\"p55\">**</ept><ph id=\"ph54\"/> command while your PDW is selected in the <bpt id=\"p56\">**</bpt>SQL Object Explorer<ept id=\"p56\">**</ept>.",
          "pos": [
            0,
            284
          ]
        },
        {
          "content": "The standard SQL query editor is not supported by PDW.",
          "pos": [
            285,
            339
          ]
        }
      ]
    },
    {
      "pos": [
        20259,
        20352
      ],
      "content": "Here are the type of data exploration and feature generation tasks performed in this section:"
    },
    {
      "pos": [
        20356,
        20423
      ],
      "content": "Explore data distributions of a few fields in varying time windows."
    },
    {
      "pos": [
        20426,
        20488
      ],
      "content": "Investigate data quality of the longitude and latitude fields."
    },
    {
      "pos": [
        20491,
        20573
      ],
      "content": "Generate binary and multiclass classification labels based on the <bpt id=\"p57\">**</bpt>tip\\_amount<ept id=\"p57\">**</ept>."
    },
    {
      "pos": [
        20576,
        20629
      ],
      "content": "Generate features and compute/compare trip distances."
    },
    {
      "pos": [
        20632,
        20714
      ],
      "content": "Join the two tables and extract a random sample that will be used to build models."
    },
    {
      "pos": [
        20720,
        20744
      ],
      "content": "Data import verification"
    },
    {
      "pos": [
        20746,
        20893
      ],
      "content": "These queries provide a quick verification of the number of rows and columns in the tables populated earlier using Polybase's parallel bulk import,"
    },
    {
      "pos": [
        21247,
        21306
      ],
      "content": "<bpt id=\"p58\">**</bpt>Output:<ept id=\"p58\">**</ept><ph id=\"ph55\"/> You should get 173,179,759 rows and 14 columns."
    },
    {
      "pos": [
        21312,
        21355
      ],
      "content": "Exploration: Trip distribution by medallion"
    },
    {
      "pos": [
        21357,
        21703
      ],
      "content": "This example query identifies the medallions (taxi numbers) that completed more than 100 trips within a specified time period. The query would benefit from the partitioned table access since it is conditioned by the partition scheme of <bpt id=\"p59\">**</bpt>pickup\\_datetime<ept id=\"p59\">**</ept>. Querying the full dataset will also make use of the partitioned table and/or index scan.",
      "nodes": [
        {
          "content": "This example query identifies the medallions (taxi numbers) that completed more than 100 trips within a specified time period.",
          "pos": [
            0,
            126
          ]
        },
        {
          "content": "The query would benefit from the partitioned table access since it is conditioned by the partition scheme of <bpt id=\"p59\">**</bpt>pickup\\_datetime<ept id=\"p59\">**</ept>.",
          "pos": [
            127,
            297
          ]
        },
        {
          "content": "Querying the full dataset will also make use of the partitioned table and/or index scan.",
          "pos": [
            298,
            386
          ]
        }
      ]
    },
    {
      "pos": [
        21883,
        22096
      ],
      "content": "<bpt id=\"p60\">**</bpt>Output:<ept id=\"p60\">**</ept><ph id=\"ph56\"/> The query should return a table with rows specifying the 13,369 medallions (taxis) and the number of trip completed by them in 2013. The last column contains the count of the number of trips completed.",
      "nodes": [
        {
          "content": "<bpt id=\"p60\">**</bpt>Output:<ept id=\"p60\">**</ept><ph id=\"ph56\"/> The query should return a table with rows specifying the 13,369 medallions (taxis) and the number of trip completed by them in 2013.",
          "pos": [
            0,
            199
          ]
        },
        {
          "content": "The last column contains the count of the number of trips completed.",
          "pos": [
            200,
            268
          ]
        }
      ]
    },
    {
      "pos": [
        22102,
        22162
      ],
      "content": "Exploration: Trip distribution by medallion and hack_license"
    },
    {
      "pos": [
        22164,
        22319
      ],
      "content": "This example identifies the medallions (taxi numbers) and hack_license numbers (drivers) that completed more than 100 trips within a specified time period."
    },
    {
      "pos": [
        22527,
        22742
      ],
      "content": "<bpt id=\"p61\">**</bpt>Output:<ept id=\"p61\">**</ept><ph id=\"ph57\"/> The query should return a table with 13,369 rows specifying the 13,369 car/driver IDs that have completed more that 100 trips in 2013. The last column contains the count of the number of trips completed.",
      "nodes": [
        {
          "content": "<bpt id=\"p61\">**</bpt>Output:<ept id=\"p61\">**</ept><ph id=\"ph57\"/> The query should return a table with 13,369 rows specifying the 13,369 car/driver IDs that have completed more that 100 trips in 2013.",
          "pos": [
            0,
            201
          ]
        },
        {
          "content": "The last column contains the count of the number of trips completed.",
          "pos": [
            202,
            270
          ]
        }
      ]
    },
    {
      "pos": [
        22748,
        22828
      ],
      "content": "Data quality assessment: Verify records with incorrect longitude and/or latitude"
    },
    {
      "pos": [
        22830,
        23009
      ],
      "content": "This example investigates if any of the longitude and/or latitude fields either contain an invalid value (radian degrees should be between -90 and 90), or have (0, 0) coordinates."
    },
    {
      "pos": [
        23510,
        23605
      ],
      "content": "<bpt id=\"p62\">**</bpt>Output:<ept id=\"p62\">**</ept><ph id=\"ph58\"/> The query returns 837,467 trips that have invalid longitude and/or latitude fields."
    },
    {
      "pos": [
        23611,
        23664
      ],
      "content": "Exploration: Tipped vs. not tipped trips distribution"
    },
    {
      "pos": [
        23666,
        23967
      ],
      "content": "This example finds the number of trips that were tipped vs. the number that were not tipped in a specified time period (or in the full dataset if covering the full year as it is set up here). This distribution reflects the binary label distribution to be later used for binary classification modeling.",
      "nodes": [
        {
          "content": "This example finds the number of trips that were tipped vs. the number that were not tipped in a specified time period (or in the full dataset if covering the full year as it is set up here).",
          "pos": [
            0,
            191
          ]
        },
        {
          "content": "This distribution reflects the binary label distribution to be later used for binary classification modeling.",
          "pos": [
            192,
            301
          ]
        }
      ]
    },
    {
      "pos": [
        24222,
        24351
      ],
      "content": "<bpt id=\"p63\">**</bpt>Output:<ept id=\"p63\">**</ept><ph id=\"ph59\"/> The query should return the following tip frequencies for the year 2013: 90,447,622 tipped and 82,264,709 not-tipped."
    },
    {
      "pos": [
        24357,
        24398
      ],
      "content": "Exploration: Tip class/range distribution"
    },
    {
      "pos": [
        24400,
        24637
      ],
      "content": "This example computes the distribution of tip ranges in a given time period (or in the full dataset if covering the full year). This is the distribution of the label classes that will be used later for multiclass classification modeling.",
      "nodes": [
        {
          "content": "This example computes the distribution of tip ranges in a given time period (or in the full dataset if covering the full year).",
          "pos": [
            0,
            127
          ]
        },
        {
          "content": "This is the distribution of the label classes that will be used later for multiclass classification modeling.",
          "pos": [
            128,
            237
          ]
        }
      ]
    },
    {
      "pos": [
        25105,
        25116
      ],
      "content": "<bpt id=\"p64\">**</bpt>Output:<ept id=\"p64\">**</ept>"
    },
    {
      "pos": [
        25119,
        25128
      ],
      "content": "tip_class"
    },
    {
      "pos": [
        25132,
        25140
      ],
      "content": "tip_freq"
    },
    {
      "pos": [
        25167,
        25168
      ],
      "content": "1"
    },
    {
      "pos": [
        25172,
        25180
      ],
      "content": "82230915"
    },
    {
      "pos": [
        25184,
        25185
      ],
      "content": "2"
    },
    {
      "pos": [
        25189,
        25196
      ],
      "content": "6198803"
    },
    {
      "pos": [
        25200,
        25201
      ],
      "content": "3"
    },
    {
      "pos": [
        25205,
        25212
      ],
      "content": "1932223"
    },
    {
      "pos": [
        25216,
        25217
      ],
      "content": "0"
    },
    {
      "pos": [
        25221,
        25229
      ],
      "content": "82264625"
    },
    {
      "pos": [
        25233,
        25234
      ],
      "content": "4"
    },
    {
      "pos": [
        25238,
        25243
      ],
      "content": "85765"
    },
    {
      "pos": [
        25251,
        25297
      ],
      "content": "Exploration: Compute and compare trip distance"
    },
    {
      "pos": [
        25299,
        25631
      ],
      "content": "This example converts the pickup and drop-off longitude and latitude to SQL geography points, computes the trip distance using SQL geography points difference, and returns a random sample of the results for comparison. The example limits the results to valid coordinates only using the data quality assessment query covered earlier.",
      "nodes": [
        {
          "content": "This example converts the pickup and drop-off longitude and latitude to SQL geography points, computes the trip distance using SQL geography points difference, and returns a random sample of the results for comparison.",
          "pos": [
            0,
            218
          ]
        },
        {
          "content": "The example limits the results to valid coordinates only using the data quality assessment query covered earlier.",
          "pos": [
            219,
            332
          ]
        }
      ]
    },
    {
      "pos": [
        27202,
        27241
      ],
      "content": "Feature engineering using SQL functions"
    },
    {
      "pos": [
        27243,
        27511
      ],
      "content": "Sometimes SQL functions can be an efficient option for feature engineering. In this walkthrough, we defined a SQL function to calculate the direct distance between the pickup and dropoff locations. You can run the following SQL scripts in <bpt id=\"p65\">**</bpt>Visual Studio Data Tools<ept id=\"p65\">**</ept>.",
      "nodes": [
        {
          "content": "Sometimes SQL functions can be an efficient option for feature engineering.",
          "pos": [
            0,
            75
          ]
        },
        {
          "content": "In this walkthrough, we defined a SQL function to calculate the direct distance between the pickup and dropoff locations.",
          "pos": [
            76,
            197
          ]
        },
        {
          "content": "You can run the following SQL scripts in <bpt id=\"p65\">**</bpt>Visual Studio Data Tools<ept id=\"p65\">**</ept>.",
          "pos": [
            198,
            308
          ]
        }
      ]
    },
    {
      "pos": [
        27514,
        27572
      ],
      "content": "Here is the SQL script that defines the distance function."
    },
    {
      "pos": [
        28590,
        28670
      ],
      "content": "Here is an example to call this function to generate features in your SQL query:"
    },
    {
      "pos": [
        29193,
        29392
      ],
      "content": "<bpt id=\"p66\">**</bpt>Output:<ept id=\"p66\">**</ept><ph id=\"ph60\"/> This query generates a table (with 2,803,538 rows) with pickup and dropoff latitudes and longitudes and the corresponding direct distances in miles. Here are the results for first 3 rows:",
      "nodes": [
        {
          "content": "<bpt id=\"p66\">**</bpt>Output:<ept id=\"p66\">**</ept><ph id=\"ph60\"/> This query generates a table (with 2,803,538 rows) with pickup and dropoff latitudes and longitudes and the corresponding direct distances in miles.",
          "pos": [
            0,
            215
          ]
        },
        {
          "content": "Here are the results for first 3 rows:",
          "pos": [
            216,
            254
          ]
        }
      ]
    },
    {
      "pos": [
        29396,
        29411
      ],
      "content": "pickup_latitude"
    },
    {
      "pos": [
        29414,
        29430
      ],
      "content": "pickup_longitude"
    },
    {
      "pos": [
        29436,
        29452
      ],
      "content": "dropoff_latitude"
    },
    {
      "pos": [
        29454,
        29471
      ],
      "content": "dropoff_longitude"
    },
    {
      "pos": [
        29474,
        29488
      ],
      "content": "DirectDistance"
    },
    {
      "pos": [
        29548,
        29549
      ],
      "content": "1"
    },
    {
      "pos": [
        29553,
        29562
      ],
      "content": "40.731804"
    },
    {
      "pos": [
        29565,
        29575
      ],
      "content": "-74.001083"
    },
    {
      "pos": [
        29578,
        29587
      ],
      "content": "40.736622"
    },
    {
      "pos": [
        29590,
        29600
      ],
      "content": "-73.988953"
    },
    {
      "pos": [
        29603,
        29614
      ],
      "content": ".7169601222"
    },
    {
      "pos": [
        29618,
        29619
      ],
      "content": "2"
    },
    {
      "pos": [
        29623,
        29632
      ],
      "content": "40.715794"
    },
    {
      "pos": [
        29635,
        29645
      ],
      "content": "-74,010635"
    },
    {
      "pos": [
        29648,
        29657
      ],
      "content": "40.725338"
    },
    {
      "pos": [
        29660,
        29669
      ],
      "content": "-74.00399"
    },
    {
      "pos": [
        29672,
        29683
      ],
      "content": ".7448343721"
    },
    {
      "pos": [
        29687,
        29688
      ],
      "content": "3"
    },
    {
      "pos": [
        29692,
        29701
      ],
      "content": "40.761456"
    },
    {
      "pos": [
        29704,
        29714
      ],
      "content": "-73.999886"
    },
    {
      "pos": [
        29717,
        29726
      ],
      "content": "40.766544"
    },
    {
      "pos": [
        29729,
        29739
      ],
      "content": "-73.988228"
    },
    {
      "pos": [
        29742,
        29754
      ],
      "content": "0.7037227967"
    },
    {
      "pos": [
        29764,
        29795
      ],
      "content": "Prepare data for model building"
    },
    {
      "pos": [
        29797,
        30375
      ],
      "content": "The following query joins the <bpt id=\"p67\">**</bpt>nyctaxi\\_trip<ept id=\"p67\">**</ept><ph id=\"ph61\"/> and <bpt id=\"p68\">**</bpt>nyctaxi\\_fare<ept id=\"p68\">**</ept><ph id=\"ph62\"/> tables, generates a binary classification label <bpt id=\"p69\">**</bpt>tipped<ept id=\"p69\">**</ept>, a multi-class classification label <bpt id=\"p70\">**</bpt>tip\\_class<ept id=\"p70\">**</ept>, and extracts a sample from the full joined dataset. The sampling is done by retrieving a subset of the trips based on pickup time.  This query can be copied then pasted directly in the <bpt id=\"p71\">[</bpt>Azure Machine Learning Studio<ept id=\"p71\">](https://studio.azureml.net)</ept><ph id=\"ph63\"/> [Reader][reader] module for direct data ingestion from the SQL database instance in Azure. The query excludes records with incorrect (0, 0) coordinates.",
      "nodes": [
        {
          "content": "The following query joins the <bpt id=\"p67\">**</bpt>nyctaxi\\_trip<ept id=\"p67\">**</ept><ph id=\"ph61\"/> and <bpt id=\"p68\">**</bpt>nyctaxi\\_fare<ept id=\"p68\">**</ept><ph id=\"ph62\"/> tables, generates a binary classification label <bpt id=\"p69\">**</bpt>tipped<ept id=\"p69\">**</ept>, a multi-class classification label <bpt id=\"p70\">**</bpt>tip\\_class<ept id=\"p70\">**</ept>, and extracts a sample from the full joined dataset.",
          "pos": [
            0,
            422
          ]
        },
        {
          "content": "The sampling is done by retrieving a subset of the trips based on pickup time.",
          "pos": [
            423,
            501
          ]
        },
        {
          "content": "This query can be copied then pasted directly in the <bpt id=\"p71\">[</bpt>Azure Machine Learning Studio<ept id=\"p71\">](https://studio.azureml.net)</ept><ph id=\"ph63\"/> [Reader][reader] module for direct data ingestion from the SQL database instance in Azure.",
          "pos": [
            503,
            761
          ]
        },
        {
          "content": "The query excludes records with incorrect (0, 0) coordinates.",
          "pos": [
            762,
            823
          ]
        }
      ]
    },
    {
      "pos": [
        31138,
        31210
      ],
      "content": "When you are ready to proceed to Azure Machine Learning, you may either:"
    },
    {
      "pos": [
        31217,
        31367
      ],
      "content": "Save the final SQL query to extract and sample the data and copy-paste the query directly into a [Reader][reader] module in Azure Machine Learning, or"
    },
    {
      "pos": [
        31371,
        31668
      ],
      "content": "Persist the sampled and engineered data you plan to use for model building in a new SQL DW table and use the new table in the [Reader][reader] module in Azure Machine Learning. The PowerShell script in earlier step has done this for you. You can read directly from this table in the Reader module.",
      "nodes": [
        {
          "content": "Persist the sampled and engineered data you plan to use for model building in a new SQL DW table and use the new table in the [Reader][reader] module in Azure Machine Learning.",
          "pos": [
            0,
            176
          ]
        },
        {
          "content": "The PowerShell script in earlier step has done this for you.",
          "pos": [
            177,
            237
          ]
        },
        {
          "content": "You can read directly from this table in the Reader module.",
          "pos": [
            238,
            297
          ]
        }
      ]
    },
    {
      "pos": [
        31694,
        31754
      ],
      "content": "Data exploration and feature engineering in IPython notebook"
    },
    {
      "pos": [
        31756,
        32392
      ],
      "content": "In this section, we will perform data exploration and feature generation\nusing both Python and SQL queries against the SQL DW created earlier. A sample IPython notebook named <bpt id=\"p72\">**</bpt>SQLDW_Explorations.ipynb<ept id=\"p72\">**</ept><ph id=\"ph64\"/> and a Python script file <bpt id=\"p73\">**</bpt>SQLDW_Explorations_Scripts.py<ept id=\"p73\">**</ept><ph id=\"ph65\"/> have been downloaded to your local directory. They are also available on <bpt id=\"p74\">[</bpt>GitHub<ept id=\"p74\">](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/SQLDW)</ept>. These two files are identical in Python scripts. The Python script file is provided to you in case you do not have an IPython Notebook server. These two sample Python files are designed under <bpt id=\"p75\">**</bpt>Python 2.7<ept id=\"p75\">**</ept>.",
      "nodes": [
        {
          "content": "In this section, we will perform data exploration and feature generation\nusing both Python and SQL queries against the SQL DW created earlier.",
          "pos": [
            0,
            142
          ]
        },
        {
          "content": "A sample IPython notebook named <bpt id=\"p72\">**</bpt>SQLDW_Explorations.ipynb<ept id=\"p72\">**</ept><ph id=\"ph64\"/> and a Python script file <bpt id=\"p73\">**</bpt>SQLDW_Explorations_Scripts.py<ept id=\"p73\">**</ept><ph id=\"ph65\"/> have been downloaded to your local directory.",
          "pos": [
            143,
            418
          ]
        },
        {
          "content": "They are also available on <bpt id=\"p74\">[</bpt>GitHub<ept id=\"p74\">](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/SQLDW)</ept>.",
          "pos": [
            419,
            578
          ]
        },
        {
          "content": "These two files are identical in Python scripts.",
          "pos": [
            579,
            627
          ]
        },
        {
          "content": "The Python script file is provided to you in case you do not have an IPython Notebook server.",
          "pos": [
            628,
            721
          ]
        },
        {
          "content": "These two sample Python files are designed under <bpt id=\"p75\">**</bpt>Python 2.7<ept id=\"p75\">**</ept>.",
          "pos": [
            722,
            826
          ]
        }
      ]
    },
    {
      "pos": [
        32394,
        32623
      ],
      "content": "The needed Azure SQL DW information in the sample IPython Notebook and the Python script file downloaded to your local machine has been plugged in by the PowerShell script previously. They are executable without any modification.",
      "nodes": [
        {
          "content": "The needed Azure SQL DW information in the sample IPython Notebook and the Python script file downloaded to your local machine has been plugged in by the PowerShell script previously.",
          "pos": [
            0,
            183
          ]
        },
        {
          "content": "They are executable without any modification.",
          "pos": [
            184,
            229
          ]
        }
      ]
    },
    {
      "pos": [
        32625,
        32853
      ],
      "content": "If you have already set up an AzureML workspace, you can directly upload the sample IPython Notebook to the AzureML IPython Notebook service and start running it. Here are the steps to upload to AzureML IPython Notebook service:",
      "nodes": [
        {
          "content": "If you have already set up an AzureML workspace, you can directly upload the sample IPython Notebook to the AzureML IPython Notebook service and start running it.",
          "pos": [
            0,
            162
          ]
        },
        {
          "content": "Here are the steps to upload to AzureML IPython Notebook service:",
          "pos": [
            163,
            228
          ]
        }
      ]
    },
    {
      "pos": [
        32858,
        32974
      ],
      "content": "Log in to your AzureML workspace, click \"Studio\" at the top, and click \"NOTEBOOKS\" on the left side of the web page."
    },
    {
      "pos": [
        32981,
        32996
      ],
      "content": "<ph id=\"ph66\">![</ph>Plot #22<ph id=\"ph67\">][22]</ph>"
    },
    {
      "pos": [
        33001,
        33182
      ],
      "content": "Click \"NEW\" on the left bottom corner of the web page, and select \"Python 2\". Then, provide a name to the notebook and click the check mark to create the new blank IPython Notebook.",
      "nodes": [
        {
          "content": "Click \"NEW\" on the left bottom corner of the web page, and select \"Python 2\".",
          "pos": [
            0,
            77
          ]
        },
        {
          "content": "Then, provide a name to the notebook and click the check mark to create the new blank IPython Notebook.",
          "pos": [
            78,
            181
          ]
        }
      ]
    },
    {
      "pos": [
        33189,
        33204
      ],
      "content": "<ph id=\"ph68\">![</ph>Plot #23<ph id=\"ph69\">][23]</ph>"
    },
    {
      "pos": [
        33209,
        33287
      ],
      "content": "Click the \"Jupyter\" symbol on the left top corner of the new IPython Notebook."
    },
    {
      "pos": [
        33294,
        33309
      ],
      "content": "<ph id=\"ph70\">![</ph>Plot #24<ph id=\"ph71\">][24]</ph>"
    },
    {
      "pos": [
        33314,
        33532
      ],
      "content": "Drag and drop the sample IPython Notebook to the <bpt id=\"p76\">**</bpt>tree<ept id=\"p76\">**</ept><ph id=\"ph72\"/> page of your AzureML IPython Notebook service, and click <bpt id=\"p77\">**</bpt>Upload<ept id=\"p77\">**</ept>. Then, the sample IPython Notebook will be uploaded to the AzureML IPython Notebook service.",
      "nodes": [
        {
          "content": "Drag and drop the sample IPython Notebook to the <bpt id=\"p76\">**</bpt>tree<ept id=\"p76\">**</ept><ph id=\"ph72\"/> page of your AzureML IPython Notebook service, and click <bpt id=\"p77\">**</bpt>Upload<ept id=\"p77\">**</ept>.",
          "pos": [
            0,
            221
          ]
        },
        {
          "content": "Then, the sample IPython Notebook will be uploaded to the AzureML IPython Notebook service.",
          "pos": [
            222,
            313
          ]
        }
      ]
    },
    {
      "pos": [
        33539,
        33554
      ],
      "content": "<ph id=\"ph73\">![</ph>Plot #25<ph id=\"ph74\">][25]</ph>"
    },
    {
      "pos": [
        33556,
        33763
      ],
      "content": "In order to run the sample IPython Notebook or the Python script file, the following Python packages are needed. If you are using the AzureML IPython Notebook service, these packages have been pre-installed.",
      "nodes": [
        {
          "content": "In order to run the sample IPython Notebook or the Python script file, the following Python packages are needed.",
          "pos": [
            0,
            112
          ]
        },
        {
          "content": "If you are using the AzureML IPython Notebook service, these packages have been pre-installed.",
          "pos": [
            113,
            207
          ]
        }
      ]
    },
    {
      "pos": [
        33837,
        33950
      ],
      "content": "The recommended sequence when building advanced analytical solutions on AzureML with large data is the following:"
    },
    {
      "pos": [
        33954,
        34018
      ],
      "content": "Read in a small sample of the data into an in-memory data frame."
    },
    {
      "pos": [
        34021,
        34089
      ],
      "content": "Perform some visualizations and explorations using the sampled data."
    },
    {
      "pos": [
        34092,
        34151
      ],
      "content": "Experiment with feature engineering using the sampled data."
    },
    {
      "pos": [
        34154,
        34286
      ],
      "content": "For larger data exploration, data manipulation and feature engineering, use Python to issue SQL Queries directly against the SQL DW."
    },
    {
      "pos": [
        34289,
        34369
      ],
      "content": "Decide the sample size to be suitable for Azure Machine Learning model building."
    },
    {
      "pos": [
        34371,
        34569
      ],
      "content": "The followings are a few data exploration, data visualization, and feature engineering examples. More data explorations can be found in the sample IPython Notebook and the sample Python script file.",
      "nodes": [
        {
          "content": "The followings are a few data exploration, data visualization, and feature engineering examples.",
          "pos": [
            0,
            96
          ]
        },
        {
          "content": "More data explorations can be found in the sample IPython Notebook and the sample Python script file.",
          "pos": [
            97,
            198
          ]
        }
      ]
    },
    {
      "pos": [
        34575,
        34606
      ],
      "content": "Initialize database credentials"
    },
    {
      "pos": [
        34608,
        34680
      ],
      "content": "Initialize your database connection settings in the following variables:"
    },
    {
      "pos": [
        34832,
        34858
      ],
      "content": "Create database connection"
    },
    {
      "pos": [
        34860,
        34934
      ],
      "content": "Here is the connection string that creates the connection to the database."
    },
    {
      "pos": [
        35111,
        35168
      ],
      "content": "Report number of rows and columns in table &lt;nyctaxi_trip&gt;"
    },
    {
      "pos": [
        35627,
        35659
      ],
      "content": "Total number of rows = 173179759"
    },
    {
      "pos": [
        35664,
        35692
      ],
      "content": "Total number of columns = 14"
    },
    {
      "pos": [
        35698,
        35755
      ],
      "content": "Report number of rows and columns in table &lt;nyctaxi_fare&gt;"
    },
    {
      "pos": [
        36214,
        36246
      ],
      "content": "Total number of rows = 173179759"
    },
    {
      "pos": [
        36251,
        36279
      ],
      "content": "Total number of columns = 11"
    },
    {
      "pos": [
        36285,
        36349
      ],
      "content": "Read-in a small data sample from the SQL Data Warehouse Database"
    },
    {
      "pos": [
        37019,
        37123
      ],
      "content": "Time to read the sample table is 14.096495 seconds.  \n<ph id=\"ph75\"/>Number of rows and columns retrieved = (1000, 21).",
      "nodes": [
        {
          "content": "Time to read the sample table is 14.096495 seconds.",
          "pos": [
            0,
            51
          ]
        },
        {
          "content": "<ph id=\"ph75\"/>Number of rows and columns retrieved = (1000, 21).",
          "pos": [
            54,
            119
          ]
        }
      ]
    },
    {
      "pos": [
        37129,
        37151
      ],
      "content": "Descriptive statistics"
    },
    {
      "pos": [
        37153,
        37324
      ],
      "content": "Now you are ready to explore the sampled data. We start with\nlooking at some descriptive statistics for the <bpt id=\"p78\">**</bpt>trip\\_distance<ept id=\"p78\">**</ept><ph id=\"ph76\"/> (or any other fields you choose to specify).",
      "nodes": [
        {
          "content": "Now you are ready to explore the sampled data.",
          "pos": [
            0,
            46
          ]
        },
        {
          "content": "We start with\nlooking at some descriptive statistics for the <bpt id=\"p78\">**</bpt>trip\\_distance<ept id=\"p78\">**</ept><ph id=\"ph76\"/> (or any other fields you choose to specify).",
          "pos": [
            47,
            226
          ]
        }
      ]
    },
    {
      "pos": [
        37367,
        37398
      ],
      "content": "Visualization: Box plot example"
    },
    {
      "pos": [
        37400,
        37478
      ],
      "content": "Next we look at the box plot for the trip distance to visualize the quantiles."
    },
    {
      "pos": [
        37540,
        37553
      ],
      "content": "<ph id=\"ph77\">![</ph>Plot #1<ph id=\"ph78\">][1]</ph>"
    },
    {
      "pos": [
        37559,
        37599
      ],
      "content": "Visualization: Distribution plot example"
    },
    {
      "pos": [
        37601,
        37686
      ],
      "content": "Plots that visualize the distribution and a histogram for the sampled trip distances."
    },
    {
      "pos": [
        37898,
        37911
      ],
      "content": "<ph id=\"ph79\">![</ph>Plot #2<ph id=\"ph80\">][2]</ph>"
    },
    {
      "pos": [
        37917,
        37950
      ],
      "content": "Visualization: Bar and line plots"
    },
    {
      "pos": [
        37952,
        38043
      ],
      "content": "In this example, we bin the trip distance into five bins and visualize the binning results."
    },
    {
      "pos": [
        38204,
        38270
      ],
      "content": "We can plot the above bin distribution in a bar or line plot with:"
    },
    {
      "pos": [
        38337,
        38350
      ],
      "content": "<ph id=\"ph81\">![</ph>Plot #3<ph id=\"ph82\">][3]</ph>"
    },
    {
      "pos": [
        38352,
        38355
      ],
      "content": "and"
    },
    {
      "pos": [
        38423,
        38436
      ],
      "content": "<ph id=\"ph83\">![</ph>Plot #4<ph id=\"ph84\">][4]</ph>"
    },
    {
      "pos": [
        38442,
        38477
      ],
      "content": "Visualization: Scatterplot examples"
    },
    {
      "pos": [
        38479,
        38590
      ],
      "content": "We show scatter plot between <bpt id=\"p79\">**</bpt>trip\\_time\\_in\\_secs<ept id=\"p79\">**</ept><ph id=\"ph85\"/> and <bpt id=\"p80\">**</bpt>trip\\_distance<ept id=\"p80\">**</ept><ph id=\"ph86\"/> to see if there\nis any correlation"
    },
    {
      "pos": [
        38657,
        38670
      ],
      "content": "<ph id=\"ph87\">![</ph>Plot #6<ph id=\"ph88\">][6]</ph>"
    },
    {
      "pos": [
        38672,
        38758
      ],
      "content": "Similarly we can check the relationship between <bpt id=\"p81\">**</bpt>rate\\_code<ept id=\"p81\">**</ept><ph id=\"ph89\"/> and <bpt id=\"p82\">**</bpt>trip\\_distance<ept id=\"p82\">**</ept>."
    },
    {
      "pos": [
        38823,
        38836
      ],
      "content": "<ph id=\"ph90\">![</ph>Plot #8<ph id=\"ph91\">][8]</ph>"
    },
    {
      "pos": [
        38843,
        38913
      ],
      "content": "Data exploration on sampled data using SQL queries in IPython notebook"
    },
    {
      "pos": [
        38915,
        39113
      ],
      "content": "In this section, we explore data distributions using the sampled data which is persisted in the new table we created above. Note that similar explorations can be performed using the original tables.",
      "nodes": [
        {
          "content": "In this section, we explore data distributions using the sampled data which is persisted in the new table we created above.",
          "pos": [
            0,
            123
          ]
        },
        {
          "content": "Note that similar explorations can be performed using the original tables.",
          "pos": [
            124,
            198
          ]
        }
      ]
    },
    {
      "pos": [
        39120,
        39187
      ],
      "content": "Exploration: Report number of rows and columns in the sampled table"
    },
    {
      "pos": [
        39613,
        39657
      ],
      "content": "Exploration: Tipped/not tripped Distribution"
    },
    {
      "pos": [
        39834,
        39869
      ],
      "content": "Exploration: Tip class distribution"
    },
    {
      "pos": [
        40065,
        40112
      ],
      "content": "Exploration: Plot the tip distribution by class"
    },
    {
      "pos": [
        40163,
        40178
      ],
      "content": "<ph id=\"ph92\">![</ph>Plot #26<ph id=\"ph93\">][26]</ph>"
    },
    {
      "pos": [
        40187,
        40227
      ],
      "content": "Exploration: Daily distribution of trips"
    },
    {
      "pos": [
        40450,
        40494
      ],
      "content": "Exploration: Trip distribution per medallion"
    },
    {
      "pos": [
        40664,
        40724
      ],
      "content": "Exploration: Trip distribution by medallion and hack license"
    },
    {
      "pos": [
        40887,
        40922
      ],
      "content": "Exploration: Trip time distribution"
    },
    {
      "pos": [
        41096,
        41135
      ],
      "content": "Exploration: Trip distance distribution"
    },
    {
      "pos": [
        41334,
        41372
      ],
      "content": "Exploration: Payment type distribution"
    },
    {
      "pos": [
        41512,
        41557
      ],
      "content": "Verify the final form of the featurized table"
    },
    {
      "pos": [
        41683,
        41721
      ],
      "content": "Build models in Azure Machine Learning"
    },
    {
      "pos": [
        41723,
        41937
      ],
      "content": "We are now ready to proceed to model building and model deployment in <bpt id=\"p83\">[</bpt>Azure Machine Learning<ept id=\"p83\">](https://studio.azureml.net)</ept>. The data is ready to be used in any of the prediction problems identified earlier, namely:",
      "nodes": [
        {
          "content": "We are now ready to proceed to model building and model deployment in <bpt id=\"p83\">[</bpt>Azure Machine Learning<ept id=\"p83\">](https://studio.azureml.net)</ept>.",
          "pos": [
            0,
            163
          ]
        },
        {
          "content": "The data is ready to be used in any of the prediction problems identified earlier, namely:",
          "pos": [
            164,
            254
          ]
        }
      ]
    },
    {
      "pos": [
        41942,
        42021
      ],
      "content": "<bpt id=\"p84\">**</bpt>Binary classification<ept id=\"p84\">**</ept>: To predict whether or not a tip was paid for a trip."
    },
    {
      "pos": [
        42026,
        42135
      ],
      "content": "<bpt id=\"p85\">**</bpt>Multiclass classification<ept id=\"p85\">**</ept>: To predict the range of tip paid, according to the previously defined classes."
    },
    {
      "pos": [
        42140,
        42206
      ],
      "content": "<bpt id=\"p86\">**</bpt>Regression task<ept id=\"p86\">**</ept>: To predict the amount of tip paid for a trip."
    },
    {
      "pos": [
        42212,
        42428
      ],
      "content": "To begin the modeling exercise, log in to your <bpt id=\"p87\">**</bpt>Azure Machine Learning<ept id=\"p87\">**</ept><ph id=\"ph94\"/> workspace. If you have not yet created a machine learning workspace, see <bpt id=\"p88\">[</bpt>Create an Azure ML workspace<ept id=\"p88\">](machine-learning-create-workspace.md)</ept>.",
      "nodes": [
        {
          "content": "To begin the modeling exercise, log in to your <bpt id=\"p87\">**</bpt>Azure Machine Learning<ept id=\"p87\">**</ept><ph id=\"ph94\"/> workspace.",
          "pos": [
            0,
            139
          ]
        },
        {
          "content": "If you have not yet created a machine learning workspace, see <bpt id=\"p88\">[</bpt>Create an Azure ML workspace<ept id=\"p88\">](machine-learning-create-workspace.md)</ept>.",
          "pos": [
            140,
            311
          ]
        }
      ]
    },
    {
      "pos": [
        42433,
        42560
      ],
      "content": "To get started with Azure Machine Learning, see <bpt id=\"p89\">[</bpt>What is Azure Machine Learning Studio?<ept id=\"p89\">](machine-learning-what-is-ml-studio.md)</ept>"
    },
    {
      "pos": [
        42565,
        42635
      ],
      "content": "Log in to <bpt id=\"p90\">[</bpt>Azure Machine Learning Studio<ept id=\"p90\">](https://studio.azureml.net)</ept>."
    },
    {
      "pos": [
        42640,
        42946
      ],
      "content": "The Studio Home page provides a wealth of information, videos, tutorials, links to the Modules Reference, and other resources. For more information about Azure Machine Learning, consult the <bpt id=\"p91\">[</bpt>Azure Machine Learning Documentation Center<ept id=\"p91\">](https://azure.microsoft.com/documentation/services/machine-learning/)</ept>.",
      "nodes": [
        {
          "content": "The Studio Home page provides a wealth of information, videos, tutorials, links to the Modules Reference, and other resources.",
          "pos": [
            0,
            126
          ]
        },
        {
          "content": "For more information about Azure Machine Learning, consult the <bpt id=\"p91\">[</bpt>Azure Machine Learning Documentation Center<ept id=\"p91\">](https://azure.microsoft.com/documentation/services/machine-learning/)</ept>.",
          "pos": [
            127,
            346
          ]
        }
      ]
    },
    {
      "pos": [
        42948,
        43010
      ],
      "content": "A typical training experiment consists of the following steps:"
    },
    {
      "pos": [
        43015,
        43044
      ],
      "content": "Create a <bpt id=\"p92\">**</bpt>+NEW<ept id=\"p92\">**</ept><ph id=\"ph95\"/> experiment."
    },
    {
      "pos": [
        43048,
        43075
      ],
      "content": "Get the data into Azure ML."
    },
    {
      "pos": [
        43079,
        43136
      ],
      "content": "Pre-process, transform and manipulate the data as needed."
    },
    {
      "pos": [
        43140,
        43168
      ],
      "content": "Generate features as needed."
    },
    {
      "pos": [
        43172,
        43265
      ],
      "content": "Split the data into training/validation/testing datasets(or have separate datasets for each)."
    },
    {
      "pos": [
        43269,
        43427
      ],
      "content": "Select one or more machine learning algorithms depending on the learning problem to solve. E.g., binary classification, multiclass classification, regression.",
      "nodes": [
        {
          "content": "Select one or more machine learning algorithms depending on the learning problem to solve.",
          "pos": [
            0,
            90
          ]
        },
        {
          "content": "E.g., binary classification, multiclass classification, regression.",
          "pos": [
            91,
            158
          ]
        }
      ]
    },
    {
      "pos": [
        43431,
        43483
      ],
      "content": "Train one or more models using the training dataset."
    },
    {
      "pos": [
        43487,
        43543
      ],
      "content": "Score the validation dataset using the trained model(s)."
    },
    {
      "pos": [
        43547,
        43626
      ],
      "content": "Evaluate the model(s) to compute the relevant metrics for the learning problem."
    },
    {
      "pos": [
        43631,
        43690
      ],
      "content": "Fine tune the model(s) and select the best model to deploy."
    },
    {
      "pos": [
        43692,
        43904
      ],
      "content": "In this exercise, we have already explored and engineered the data in SQL Data Warehouse, and decided on the sample size to ingest in Azure ML. Here is the procedure to build one or more of the prediction models:",
      "nodes": [
        {
          "content": "In this exercise, we have already explored and engineered the data in SQL Data Warehouse, and decided on the sample size to ingest in Azure ML.",
          "pos": [
            0,
            143
          ]
        },
        {
          "content": "Here is the procedure to build one or more of the prediction models:",
          "pos": [
            144,
            212
          ]
        }
      ]
    },
    {
      "pos": [
        43909,
        44092
      ],
      "content": "Get the data into Azure ML using the [Reader][reader] module, available in the <bpt id=\"p93\">**</bpt>Data Input and Output<ept id=\"p93\">**</ept><ph id=\"ph96\"/> section. For more information, see the [Reader][reader] module reference page.",
      "nodes": [
        {
          "content": "Get the data into Azure ML using the [Reader][reader] module, available in the <bpt id=\"p93\">**</bpt>Data Input and Output<ept id=\"p93\">**</ept><ph id=\"ph96\"/> section.",
          "pos": [
            0,
            168
          ]
        },
        {
          "content": "For more information, see the [Reader][reader] module reference page.",
          "pos": [
            169,
            238
          ]
        }
      ]
    },
    {
      "pos": [
        44098,
        44120
      ],
      "content": "<ph id=\"ph97\">![</ph>Azure ML Reader<ph id=\"ph98\">][17]</ph>"
    },
    {
      "pos": [
        44125,
        44206
      ],
      "content": "Select <bpt id=\"p94\">**</bpt>Azure SQL Database<ept id=\"p94\">**</ept><ph id=\"ph99\"/> as the <bpt id=\"p95\">**</bpt>Data source<ept id=\"p95\">**</ept><ph id=\"ph100\"/> in the <bpt id=\"p96\">**</bpt>Properties<ept id=\"p96\">**</ept><ph id=\"ph101\"/> panel."
    },
    {
      "pos": [
        44211,
        44328
      ],
      "content": "Enter the database DNS name in the <bpt id=\"p97\">**</bpt>Database server name<ept id=\"p97\">**</ept><ph id=\"ph102\"/> field. Format: <ph id=\"ph103\">`tcp:&lt;your_virtual_machine_DNS_name&gt;,1433`</ph>",
      "nodes": [
        {
          "content": "Enter the database DNS name in the <bpt id=\"p97\">**</bpt>Database server name<ept id=\"p97\">**</ept><ph id=\"ph102\"/> field.",
          "pos": [
            0,
            122
          ]
        },
        {
          "content": "Format: <ph id=\"ph103\">`tcp:&lt;your_virtual_machine_DNS_name&gt;,1433`</ph>",
          "pos": [
            123,
            199
          ]
        }
      ]
    },
    {
      "pos": [
        44333,
        44388
      ],
      "content": "Enter the <bpt id=\"p98\">**</bpt>Database name<ept id=\"p98\">**</ept><ph id=\"ph104\"/> in the corresponding field."
    },
    {
      "pos": [
        44393,
        44515
      ],
      "content": "Enter the <bpt id=\"p99\">*</bpt>SQL user name<ept id=\"p99\">*</ept><ph id=\"ph105\"/> in the <bpt id=\"p100\">**</bpt>Server user account name<ept id=\"p100\">**</ept>, and the <bpt id=\"p101\">*</bpt>password<ept id=\"p101\">*</ept><ph id=\"ph106\"/> in the <bpt id=\"p102\">**</bpt>Server user account password<ept id=\"p102\">**</ept>."
    },
    {
      "pos": [
        44520,
        44571
      ],
      "content": "Check the <bpt id=\"p103\">**</bpt>Accept any server certificate<ept id=\"p103\">**</ept><ph id=\"ph107\"/> option."
    },
    {
      "pos": [
        44576,
        44783
      ],
      "content": "In the <bpt id=\"p104\">**</bpt>Database query<ept id=\"p104\">**</ept><ph id=\"ph108\"/> edit text area, paste the query which extracts the necessary database fields (including any computed fields such as the labels) and down samples the data to the desired sample size."
    },
    {
      "pos": [
        44785,
        45146
      ],
      "content": "An example of a binary classification experiment reading data directly from the SQL Data Warehouse database is in the figure below (remember to replace the table names nyctaxi_trip and nyctaxi_fare by the schema name and the table names you used in your walkthrough). Similar experiments can be constructed for multiclass classification and regression problems.",
      "nodes": [
        {
          "content": "An example of a binary classification experiment reading data directly from the SQL Data Warehouse database is in the figure below (remember to replace the table names nyctaxi_trip and nyctaxi_fare by the schema name and the table names you used in your walkthrough).",
          "pos": [
            0,
            267
          ]
        },
        {
          "content": "Similar experiments can be constructed for multiclass classification and regression problems.",
          "pos": [
            268,
            361
          ]
        }
      ]
    },
    {
      "pos": [
        45148,
        45169
      ],
      "content": "<ph id=\"ph109\">![</ph>Azure ML Train<ph id=\"ph110\">][10]</ph>"
    },
    {
      "pos": [
        45173,
        45730
      ],
      "content": "<ph id=\"ph111\">[AZURE.IMPORTANT]</ph><ph id=\"ph112\"/> In the modeling data extraction and sampling query examples provided in previous sections, <bpt id=\"p105\">**</bpt>all labels for the three modeling exercises are included in the query<ept id=\"p105\">**</ept>. An important (required) step in each of the modeling exercises is to <bpt id=\"p106\">**</bpt>exclude<ept id=\"p106\">**</ept><ph id=\"ph113\"/> the unnecessary labels for the other two problems, and any other <bpt id=\"p107\">**</bpt>target leaks<ept id=\"p107\">**</ept>. For example, when using binary classification, use the label <bpt id=\"p108\">**</bpt>tipped<ept id=\"p108\">**</ept><ph id=\"ph114\"/> and exclude the fields <bpt id=\"p109\">**</bpt>tip\\_class<ept id=\"p109\">**</ept>, <bpt id=\"p110\">**</bpt>tip\\_amount<ept id=\"p110\">**</ept>, and <bpt id=\"p111\">**</bpt>total\\_amount<ept id=\"p111\">**</ept>. The latter are target leaks since they imply the tip paid.",
      "nodes": [
        {
          "content": "<ph id=\"ph111\">[AZURE.IMPORTANT]</ph><ph id=\"ph112\"/> In the modeling data extraction and sampling query examples provided in previous sections, <bpt id=\"p105\">**</bpt>all labels for the three modeling exercises are included in the query<ept id=\"p105\">**</ept>.",
          "pos": [
            0,
            261
          ]
        },
        {
          "content": "An important (required) step in each of the modeling exercises is to <bpt id=\"p106\">**</bpt>exclude<ept id=\"p106\">**</ept><ph id=\"ph113\"/> the unnecessary labels for the other two problems, and any other <bpt id=\"p107\">**</bpt>target leaks<ept id=\"p107\">**</ept>.",
          "pos": [
            262,
            525
          ]
        },
        {
          "content": "For example, when using binary classification, use the label <bpt id=\"p108\">**</bpt>tipped<ept id=\"p108\">**</ept><ph id=\"ph114\"/> and exclude the fields <bpt id=\"p109\">**</bpt>tip\\_class<ept id=\"p109\">**</ept>, <bpt id=\"p110\">**</bpt>tip\\_amount<ept id=\"p110\">**</ept>, and <bpt id=\"p111\">**</bpt>total\\_amount<ept id=\"p111\">**</ept>.",
          "pos": [
            526,
            860
          ]
        },
        {
          "content": "The latter are target leaks since they imply the tip paid.",
          "pos": [
            861,
            919
          ]
        }
      ]
    },
    {
      "pos": [
        45735,
        46004
      ],
      "content": "To exclude any unnecessary columns or target leaks, you may use the [Project Columns][project-columns] module or the [Metadata Editor][metadata-editor]. For more information, see [Project Columns][project-columns] and [Metadata Editor][metadata-editor] reference pages.",
      "nodes": [
        {
          "content": "To exclude any unnecessary columns or target leaks, you may use the [Project Columns][project-columns] module or the [Metadata Editor][metadata-editor].",
          "pos": [
            0,
            152
          ]
        },
        {
          "content": "For more information, see [Project Columns][project-columns] and [Metadata Editor][metadata-editor] reference pages.",
          "pos": [
            153,
            269
          ]
        }
      ]
    },
    {
      "pos": [
        46032,
        46071
      ],
      "content": "Deploy models in Azure Machine Learning"
    },
    {
      "pos": [
        46073,
        46342
      ],
      "content": "When your model is ready, you can easily deploy it as a web service directly from the experiment. For more information about deploying Azure ML web services, see <bpt id=\"p112\">[</bpt>Deploy an Azure Machine Learning web service<ept id=\"p112\">](machine-learning-publish-a-machine-learning-web-service.md)</ept>.",
      "nodes": [
        {
          "content": "When your model is ready, you can easily deploy it as a web service directly from the experiment.",
          "pos": [
            0,
            97
          ]
        },
        {
          "content": "For more information about deploying Azure ML web services, see <bpt id=\"p112\">[</bpt>Deploy an Azure Machine Learning web service<ept id=\"p112\">](machine-learning-publish-a-machine-learning-web-service.md)</ept>.",
          "pos": [
            98,
            311
          ]
        }
      ]
    },
    {
      "pos": [
        46344,
        46385
      ],
      "content": "To deploy a new web service, you need to:"
    },
    {
      "pos": [
        46390,
        46418
      ],
      "content": "Create a scoring experiment."
    },
    {
      "pos": [
        46422,
        46445
      ],
      "content": "Deploy the web service."
    },
    {
      "pos": [
        46447,
        46579
      ],
      "content": "To create a scoring experiment from a <bpt id=\"p113\">**</bpt>Finished<ept id=\"p113\">**</ept><ph id=\"ph115\"/> training experiment, click <bpt id=\"p114\">**</bpt>CREATE SCORING EXPERIMENT<ept id=\"p114\">**</ept><ph id=\"ph116\"/> in the lower action bar."
    },
    {
      "pos": [
        46581,
        46601
      ],
      "content": "<ph id=\"ph117\">![</ph>Azure Scoring<ph id=\"ph118\">][18]</ph>"
    },
    {
      "pos": [
        46603,
        46745
      ],
      "content": "Azure Machine Learning will attempt to create a scoring experiment based on the components of the training experiment. In particular, it will:",
      "nodes": [
        {
          "content": "Azure Machine Learning will attempt to create a scoring experiment based on the components of the training experiment.",
          "pos": [
            0,
            118
          ]
        },
        {
          "content": "In particular, it will:",
          "pos": [
            119,
            142
          ]
        }
      ]
    },
    {
      "pos": [
        46750,
        46811
      ],
      "content": "Save the trained model and remove the model training modules."
    },
    {
      "pos": [
        46815,
        46893
      ],
      "content": "Identify a logical <bpt id=\"p115\">**</bpt>input port<ept id=\"p115\">**</ept><ph id=\"ph119\"/> to represent the expected input data schema."
    },
    {
      "pos": [
        46897,
        46984
      ],
      "content": "Identify a logical <bpt id=\"p116\">**</bpt>output port<ept id=\"p116\">**</ept><ph id=\"ph120\"/> to represent the expected web service output schema."
    },
    {
      "pos": [
        46986,
        47562
      ],
      "content": "When the scoring experiment is created, review it and make adjust as needed. A typical adjustment is to replace the input dataset and/or query with one which excludes label fields, as these will not be available when the service is called. It is also a good practice to reduce the size of the input dataset and/or query to a few records, just enough to indicate the input schema. For the output port, it is common to exclude all input fields and only include the <bpt id=\"p117\">**</bpt>Scored Labels<ept id=\"p117\">**</ept><ph id=\"ph121\"/> and <bpt id=\"p118\">**</bpt>Scored Probabilities<ept id=\"p118\">**</ept><ph id=\"ph122\"/> in the output using the [Project Columns][project-columns] module.",
      "nodes": [
        {
          "content": "When the scoring experiment is created, review it and make adjust as needed.",
          "pos": [
            0,
            76
          ]
        },
        {
          "content": "A typical adjustment is to replace the input dataset and/or query with one which excludes label fields, as these will not be available when the service is called.",
          "pos": [
            77,
            239
          ]
        },
        {
          "content": "It is also a good practice to reduce the size of the input dataset and/or query to a few records, just enough to indicate the input schema.",
          "pos": [
            240,
            379
          ]
        },
        {
          "content": "For the output port, it is common to exclude all input fields and only include the <bpt id=\"p117\">**</bpt>Scored Labels<ept id=\"p117\">**</ept><ph id=\"ph121\"/> and <bpt id=\"p118\">**</bpt>Scored Probabilities<ept id=\"p118\">**</ept><ph id=\"ph122\"/> in the output using the [Project Columns][project-columns] module.",
          "pos": [
            380,
            692
          ]
        }
      ]
    },
    {
      "pos": [
        47564,
        47712
      ],
      "content": "A sample scoring experiment is provided in the figure below. When ready to deploy, click the <bpt id=\"p119\">**</bpt>PUBLISH WEB SERVICE<ept id=\"p119\">**</ept><ph id=\"ph123\"/> button in the lower action bar.",
      "nodes": [
        {
          "content": "A sample scoring experiment is provided in the figure below.",
          "pos": [
            0,
            60
          ]
        },
        {
          "content": "When ready to deploy, click the <bpt id=\"p119\">**</bpt>PUBLISH WEB SERVICE<ept id=\"p119\">**</ept><ph id=\"ph123\"/> button in the lower action bar.",
          "pos": [
            61,
            206
          ]
        }
      ]
    },
    {
      "pos": [
        47714,
        47737
      ],
      "content": "<ph id=\"ph124\">![</ph>Azure ML Publish<ph id=\"ph125\">][11]</ph>"
    },
    {
      "pos": [
        47743,
        47750
      ],
      "content": "Summary"
    },
    {
      "pos": [
        47751,
        48066
      ],
      "content": "To recap what we have done in this walkthrough tutorial, you have created an Azure data science environment, worked with a large public dataset, taking it through the Cortana Analytics Process, all the way from data acquisition to model training, and then to the deployment of an Azure Machine Learning web service."
    },
    {
      "pos": [
        48072,
        48091
      ],
      "content": "License information"
    },
    {
      "pos": [
        48093,
        48317
      ],
      "content": "This sample walkthrough and its accompanying scripts and IPython notebook(s) are shared by Microsoft under the MIT license. Please check the LICENSE.txt file in in the directory of the sample code on GitHub for more details.",
      "nodes": [
        {
          "content": "This sample walkthrough and its accompanying scripts and IPython notebook(s) are shared by Microsoft under the MIT license.",
          "pos": [
            0,
            123
          ]
        },
        {
          "content": "Please check the LICENSE.txt file in in the directory of the sample code on GitHub for more details.",
          "pos": [
            124,
            224
          ]
        }
      ]
    },
    {
      "pos": [
        48322,
        48332
      ],
      "content": "References"
    },
    {
      "pos": [
        48334,
        48647
      ],
      "content": "•   <bpt id=\"p120\">[</bpt>Andrés Monroy NYC Taxi Trips Download Page<ept id=\"p120\">](http://www.andresmh.com/nyctaxitrips/)</ept><ph id=\"ph126\"/>  \n•   <bpt id=\"p121\">[</bpt>FOILing NYC’s Taxi Trip Data by Chris Whong<ept id=\"p121\">](http://chriswhong.com/open-data/foil_nyc_taxi/)</ept><ph id=\"ph127\"/>   \n•   <bpt id=\"p122\">[</bpt>NYC Taxi and Limousine Commission Research and Statistics<ept id=\"p122\">](https://www1.nyc.gov/html/tlc/html/about/statistics.shtml)</ept>"
    },
    {
      "pos": [
        51047,
        51331
      ],
      "content": "[metadata-editor]: https://msdn.microsoft.com/library/azure/370b6676-c11c-486f-bf73-35349f842a66/\n[project-columns]: https://msdn.microsoft.com/library/azure/1ec722fa-b623-4e26-a44e-a50c6d726223/\n[reader]: https://msdn.microsoft.com/library/azure/4e1b0fe6-aded-4b3f-a36f-39b8862b9004/"
    }
  ],
  "content": "<properties\n    pageTitle=\"The Cortana Analytics Process in action: using SQL Data Warehouse | Microsoft Azure\"\n    description=\"Advanced Analytics Process and Technology in Action\"  \n    services=\"machine-learning\"\n    documentationCenter=\"\"\n    authors=\"bradsev,hangzh-msft,wguo123\"\n    manager=\"paulettm\"\n    editor=\"cgronlun\" />\n\n<tags\n    ms.service=\"machine-learning\"\n    ms.workload=\"data-services\"\n    ms.tgt_pltfrm=\"na\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\"\n    ms.date=\"02/05/2016\" \n    ms.author=\"bradsev;hangzh;weig\"/>\n\n\n# The Cortana Analytics Process in action: using SQL Data Warehouse\n\nIn this tutorial, we walk you through building and deploying a machine learning model using SQL Data Warehouse (SQL DW) for a publicly available dataset -- the [NYC Taxi Trips](http://www.andresmh.com/nyctaxitrips/) dataset. The binary classification model constructed predicts whether or not a tip is paid for a trip, and models for multiclass classification and regression are also discussed that predict the distribution for the tip amounts paid.\n\nThe procedure follows the [Cortana Analytics Process (CAP)](https://azure.microsoft.com/documentation/learning-paths/cortana-analytics-process/) workflow. We show how to setup a data science environment, how to load the data into SQL DW, and how use either SQL DW or an IPython Notebook to explore the data and engineer features to model. We then show how to build and deploy a model with Azure Machine Learning.\n\n\n## <a name=\"dataset\"></a>The NYC Taxi Trips dataset\n\nThe NYC Taxi Trip data consists of about 20GB of compressed CSV files (~48GB uncompressed), recording more than 173 million individual trips and the fares paid for each trip. Each trip record includes the pickup and drop-off locations and times, anonymized hack (driver's) license number, and the medallion (taxi’s unique id) number. The data covers all trips in the year 2013 and is provided in the following two datasets for each month:\n\n1. The **trip_data.csv** file contains trip details, such as number of passengers, pickup and dropoff points, trip duration, and trip length. Here are a few sample records:\n\n        medallion,hack_license,vendor_id,rate_code,store_and_fwd_flag,pickup_datetime,dropoff_datetime,passenger_count,trip_time_in_secs,trip_distance,pickup_longitude,pickup_latitude,dropoff_longitude,dropoff_latitude\n        89D227B655E5C82AECF13C3F540D4CF4,BA96DE419E711691B9445D6A6307C170,CMT,1,N,2013-01-01 15:11:48,2013-01-01 15:18:10,4,382,1.00,-73.978165,40.757977,-73.989838,40.751171\n        0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,1,N,2013-01-06 00:18:35,2013-01-06 00:22:54,1,259,1.50,-74.006683,40.731781,-73.994499,40.75066\n        0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,1,N,2013-01-05 18:49:41,2013-01-05 18:54:23,1,282,1.10,-74.004707,40.73777,-74.009834,40.726002\n        DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,1,N,2013-01-07 23:54:15,2013-01-07 23:58:20,2,244,.70,-73.974602,40.759945,-73.984734,40.759388\n        DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,1,N,2013-01-07 23:25:03,2013-01-07 23:34:24,1,560,2.10,-73.97625,40.748528,-74.002586,40.747868\n\n2. The **trip_fare.csv** file contains details of the fare paid for each trip, such as payment type, fare amount, surcharge and taxes, tips and tolls, and the total amount paid. Here are a few sample records:\n\n        medallion, hack_license, vendor_id, pickup_datetime, payment_type, fare_amount, surcharge, mta_tax, tip_amount, tolls_amount, total_amount\n        89D227B655E5C82AECF13C3F540D4CF4,BA96DE419E711691B9445D6A6307C170,CMT,2013-01-01 15:11:48,CSH,6.5,0,0.5,0,0,7\n        0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,2013-01-06 00:18:35,CSH,6,0.5,0.5,0,0,7\n        0BD7C8F5BA12B88E0B67BED28BEA73D8,9FD8F69F0804BDB5549F40E9DA1BE472,CMT,2013-01-05 18:49:41,CSH,5.5,1,0.5,0,0,7\n        DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,2013-01-07 23:54:15,CSH,5,0.5,0.5,0,0,6\n        DFD2202EE08F7A8DC9A57B02ACB81FE2,51EE87E3205C985EF8431D850C786310,CMT,2013-01-07 23:25:03,CSH,9.5,0.5,0.5,0,0,10.5\n\nThe **unique key** used to join trip\\_data and trip\\_fare is composed of the following three fields: \n\n- medallion, \n- hack\\_license and \n- pickup\\_datetime.\n\n## <a name=\"mltasks\"></a>Address three types of prediction tasks \n\nWe formulate three prediction problems based on the *tip\\_amount* to illustrate three kinds of modeling tasks:\n\n1. **Binary classification**: To predict whether or not a tip was paid for a trip, i.e. a *tip\\_amount* that is greater than $0 is a positive example, while a *tip\\_amount* of $0 is a negative example.\n\n2. **Multiclass classification**: To predict the range of tip paid for the trip. We divide the *tip\\_amount* into five bins or classes:\n\n        Class 0 : tip_amount = $0\n        Class 1 : tip_amount > $0 and tip_amount <= $5\n        Class 2 : tip_amount > $5 and tip_amount <= $10\n        Class 3 : tip_amount > $10 and tip_amount <= $20\n        Class 4 : tip_amount > $20\n\n3. **Regression task**: To predict the amount of tip paid for a trip.  \n\n\n## <a name=\"setup\"></a>Set up the Azure data science environment for advanced analytics\n\nTo set up your Azure Data Science environment, follow these steps.\n\n**Create your own Azure blob storage account**\n\n- When you provision your own Azure blob storage, choose a geo-location for your Azure blob storage in or as close as possible to **South Central US**, which is where the NYC Taxi data is stored. The data will be copied using AzCopy from the public blob storage container to a container in your own storage account. The closer your Azure blob storage is to South Central US, the faster this task (Step 4) will be completed. \n- To create your own Azure storage account, follow the steps outlined at [About Azure storage accounts](storage-create-storage-account.md). Be sure to make notes on the values for following storage account credentials as they will be needed later in this walkthrough. \n\n  - **Storage Account Name**\n  - **Storage Account Key**\n  - **Container Name** (which you want the data to be stored in the Azure blob storage)\n\n**Provision your Azure SQL DW instance.** \nFollow the documentation at [Create a SQL Data Warehouse](sql-data-warehouse-get-started-provision.md) to provision a SQL Data Warehouse instance. Make sure that you make notations on the following SQL Data Warehouse credentials which will be used in later steps.\n \n  - **Server Name**: <server Name>.database.windows.net\n  - **SQLDW (Database) Name** \n  - **Username**\n  - **Password**\n\n**Install Visual Studio 2015 and SQL Server Data Tools.** For instructions, see [Install Visual Studio 2015 and/or SSDT (SQL Server Data Tools) for SQL Data Warehouse](sql-data-warehouse-install-visual-studio.md). \n\n**Connect to your Azure SQL DW with Visual Studio.** For instructions, see steps 1 & 2 in [Connect to Azure SQL Data Warehouse with Visual Studio](sql-data-warehouse-get-started-connect.md). \n\n>[AZURE.NOTE] Run the following SQL query on the database you created in your SQL Data Warehouse (instead of the query provided in step 3 of the connect topic,) to **create a master key**.\n\n    BEGIN TRY\n           --Try to create the master key\n        CREATE MASTER KEY\n    END TRY\n    BEGIN CATCH\n           --If the master key exists, do nothing\n    END CATCH;\n\n**Create an Azure Machine Learning workspace under your Azure subscription.** For instructions, see [Create an Azure Machine Learning workspace](machine-learning-create-workspace.md).\n\n## <a name=\"getdata\"></a>Load the data into SQL Data Warehouse\n\nOpen a Windows PowerShell command console. Run the following PowerShell commands to download the example SQL script files that we share with you on Github to a local directory that you specify with the parameter *-DestDir*. You can change the value of parameter *-DestDir* to any local directory. If *-DestDir* does not exist, it will be created by the PowerShell script. \n\n>[AZURE.NOTE] You might need to **Run as Administrator** when executing the following PowerShell script if your *DestDir* directory needs Administrator privilege to create or to write to it. \n\n    $source = \"https://raw.githubusercontent.com/Azure/Azure-MachineLearning-DataScience/master/Misc/SQLDW/Download_Scripts_SQLDW_Walkthrough.ps1\"\n    $ps1_dest = \"$pwd\\Download_Scripts_SQLDW_Walkthrough.ps1\"\n    $wc = New-Object System.Net.WebClient\n    $wc.DownloadFile($source, $ps1_dest) \n    .\\Download_Scripts_SQLDW_Walkthrough.ps1 –DestDir 'C:\\tempSQLDW'\n\nAfter successful execution, your current working directory changes to *-DestDir*. You should be able to see screen like below:\n\n![][19]\n\nIn your *-DestDir*, execute the following PowerShell script in administrator mode:\n\n    ./SQLDW_Data_Import.ps1\n\nWhen the PowerShell script runs for the first time, you will be asked to input the information from your Azure SQL DW and your Azure blob storage account. When this PowerShell script completes running for the first time, the credentials you input will have been written to a configuration file SQLDW.conf in the present working directory. The future run of this PowerShell script file has the option to read all needed parameters from this configuration file. If you need to change some parameters, you can choose to input the parameters on the screen upon prompt by deleting this configuration file and inputting the parameters values as prompted or to change the parameter values by editing the SQLDW.conf file in your *-DestDir* directory. \n\n>[AZURE.NOTE] In order to avoid schema name conflicts with those that already exist in your Azure SQL DW, when reading parameters directly from the SQLDW.conf file, a 3-digit random number is added to the schema name from the SQLDW.conf file as the default schema name for each run. The PowerShell script may prompt you for a schema name: the name may be specified at user discretion.\n\nThis **PowerShell script** file completes the following tasks:\n\n- **Downloads and installs AzCopy**, if AzCopy is not already installed\n\n        $AzCopy_path = SearchAzCopy\n        if ($AzCopy_path -eq $null){\n            Write-Host \"AzCopy.exe is not found in C:\\Program Files*. Now, start installing AzCopy...\" -ForegroundColor \"Yellow\"\n            InstallAzCopy\n            $AzCopy_path = SearchAzCopy\n        }\n            $env_path = $env:Path\n            for ($i=0; $i -lt $AzCopy_path.count; $i++){\n                if ($AzCopy_path.count -eq 1){\n                    $AzCopy_path_i = $AzCopy_path\n                } else {\n                    $AzCopy_path_i = $AzCopy_path[$i]\n                }\n                if ($env_path -notlike '*' +$AzCopy_path_i+'*'){\n                    Write-Host $AzCopy_path_i 'not in system path, add it...'\n                    [Environment]::SetEnvironmentVariable(\"Path\", \"$AzCopy_path_i;$env_path\", \"Machine\")\n                    $env:Path = [System.Environment]::GetEnvironmentVariable(\"Path\",\"Machine\") \n                    $env_path = $env:Path\n                }   \n\n- **Copies data to your private blob storage account** from the public blob with AzCopy\n\n        Write-Host \"AzCopy is copying data from public blob to yo storage account. It may take a while...\" -ForegroundColor \"Yellow\"    \n        $start_time = Get-Date\n        AzCopy.exe /Source:$Source /Dest:$DestURL /DestKey:$StorageAccountKey /S\n        $end_time = Get-Date\n        $time_span = $end_time - $start_time\n        $total_seconds = [math]::Round($time_span.TotalSeconds,2)\n        Write-Host \"AzCopy finished copying data. Please check your storage account to verify.\" -ForegroundColor \"Yellow\"\n        Write-Host \"This step (copying data from public blob to your storage account) takes $total_seconds seconds.\" -ForegroundColor \"Green\"\n\n\n- **Loads data using Polybase (by executing LoadDataToSQLDW.sql) to your Azure SQL DW** from your private blob storage account with the following commands.\n    \n    - Create a schema\n\n            EXEC (''CREATE SCHEMA {schemaname};'');\n\n    - Create a database scoped credential\n            \n            CREATE DATABASE SCOPED CREDENTIAL {KeyAlias} \n            WITH IDENTITY = ''asbkey'' , \n            Secret = ''{StorageAccountKey}''\n\n    - Create an external data source for an Azure storage blob\n\n            CREATE EXTERNAL DATA SOURCE {nyctaxi_trip_storage} \n            WITH\n            (\n                TYPE = HADOOP,\n                LOCATION =''wasbs://{ContainerName}@{StorageAccountName}.blob.core.windows.net'',\n                CREDENTIAL = {KeyAlias}\n            )\n            ;\n\n            CREATE EXTERNAL DATA SOURCE {nyctaxi_fare_storage} \n            WITH\n            (\n                TYPE = HADOOP,\n                LOCATION =''wasbs://{ContainerName}@{StorageAccountName}.blob.core.windows.net'',\n                CREDENTIAL = {KeyAlias}\n            )\n            ;\n\n    - Create an external file format for a csv file. Data is uncompressed and fields are separated with the pipe character.\n\n            CREATE EXTERNAL FILE FORMAT {csv_file_format} \n            WITH \n            (   \n                FORMAT_TYPE = DELIMITEDTEXT, \n                FORMAT_OPTIONS  \n                (\n                    FIELD_TERMINATOR ='','',\n                    USE_TYPE_DEFAULT = TRUE\n                )\n            )\n            ;\n        \n    - Create external fare and trip tables for NYC taxi dataset in Azure blob storage.\n\n            CREATE EXTERNAL TABLE {external_nyctaxi_fare}\n            (\n                medallion varchar(50) not null,\n                hack_license varchar(50) not null,\n                vendor_id char(3),\n                pickup_datetime datetime not null,\n                payment_type char(3),\n                fare_amount float,\n                surcharge float,\n                mta_tax float,\n                tip_amount float,\n                tolls_amount float,\n                total_amount float\n            )\n            with (\n                LOCATION    = ''/nyctaxifare/'',\n                DATA_SOURCE = {nyctaxi_fare_storage},\n                FILE_FORMAT = {csv_file_format},\n                REJECT_TYPE = VALUE,\n                REJECT_VALUE = 12     \n            )  \n\n\n            CREATE EXTERNAL TABLE {external_nyctaxi_trip}\n            (\n                medallion varchar(50) not null,\n                hack_license varchar(50)  not null,\n                vendor_id char(3),\n                rate_code char(3),\n                store_and_fwd_flag char(3),\n                pickup_datetime datetime  not null,\n                dropoff_datetime datetime, \n                passenger_count int,\n                trip_time_in_secs bigint,\n                trip_distance float,\n                pickup_longitude varchar(30),\n                pickup_latitude varchar(30),\n                dropoff_longitude varchar(30),\n                dropoff_latitude varchar(30)\n            )\n            with (\n                LOCATION    = ''/nyctaxitrip/'',\n                DATA_SOURCE = {nyctaxi_trip_storage},\n                FILE_FORMAT = {csv_file_format},\n                REJECT_TYPE = VALUE,\n                REJECT_VALUE = 12         \n            )\n\n    - Load data from external tables in Azure blob storage to SQL Data Warehouse \n\n            CREATE TABLE {schemaname}.{nyctaxi_fare}\n            WITH \n            (   \n                CLUSTERED COLUMNSTORE INDEX,\n                DISTRIBUTION = HASH(medallion)\n            )\n            AS \n            SELECT * \n            FROM   {external_nyctaxi_fare}\n            ;\n\n            CREATE TABLE {schemaname}.{nyctaxi_trip}\n            WITH \n            (   \n                CLUSTERED COLUMNSTORE INDEX,\n                DISTRIBUTION = HASH(medallion)\n            )\n            AS \n            SELECT * \n            FROM   {external_nyctaxi_trip}\n            ;\n\n    - Create a sample data table (NYCTaxi_Sample) and insert data to it from selecting SQL queries on the trip and fare tables. (Some steps of this walkthrough needs to use this sample table.)\n\n            CREATE TABLE {schemaname}.{nyctaxi_sample}\n            WITH \n            (   \n                CLUSTERED COLUMNSTORE INDEX,\n                DISTRIBUTION = HASH(medallion)\n            )\n            AS \n            (\n                SELECT t.*, f.payment_type, f.fare_amount, f.surcharge, f.mta_tax, f.tolls_amount, f.total_amount, f.tip_amount,\n                tipped = CASE WHEN (tip_amount > 0) THEN 1 ELSE 0 END,\n                tip_class = CASE \n                        WHEN (tip_amount = 0) THEN 0\n                        WHEN (tip_amount > 0 AND tip_amount <= 5) THEN 1\n                        WHEN (tip_amount > 5 AND tip_amount <= 10) THEN 2\n                        WHEN (tip_amount > 10 AND tip_amount <= 20) THEN 3\n                        ELSE 4\n                    END\n                FROM {schemaname}.{nyctaxi_trip} t, {schemaname}.{nyctaxi_fare} f\n                WHERE datepart(\"mi\",t.pickup_datetime) = 1\n                AND t.medallion = f.medallion\n                AND   t.hack_license = f.hack_license\n                AND   t.pickup_datetime = f.pickup_datetime\n                AND   pickup_longitude <> ''0''\n                AND   dropoff_longitude <> ''0''\n            )\n            ;\n\n>[AZURE.NOTE] Depending on the geographical location of your private blob storage account, the process of copying data from a public blob to your private storage account can take about 15 minutes, or  even longer,and the process of loading data from your storage account to your Azure SQL DW could take 20 minutes or longer.  \n\n>[AZURE.NOTE] If the .csv files to be copied from the public blob storage to your private blob storage account already exist in your private blob storage account, AzCopy will ask you whether you want to overwrite them. If you do not want to overwrite them, input **n** when prompted. If you want to overwrite **all** of them, input **a** when prompted. You can also input **y** to overwrite .csv files individually.\n\n![Plot #21][21] \n\n>[AZURE.TIP] **Use your own data:** If your data is in your on-premise machine in your real life application, you can still use AzCopy to upload on-premise data to your private Azure blob storage. You only need to change the **Source** location, `$Source = \"http://getgoing.blob.core.windows.net/public/nyctaxidataset\"`, in the AzCopy command of the PowerShell script file to the local directory that contains your data.\n    \n>[AZURE.TIP] If your data is already in your private Azure blob storage in your real life application, you can skip the AzCopy step in the PowerShell script and directly upload the data to Azure SQL DW. This will require additional edits of the script to tailor it to the format of your data.\n\n\nThis Powershell script also plugs in the Azure SQL DW information into the data exploration example files SQLDW_Explorations.sql, SQLDW_Explorations.ipynb, and SQLDW_Explorations_Scripts.py so that these three files are ready to be tried out instantly after the PowerShell script completes. \n\nAfter a successful execution, you will see screen like below:\n\n![][20]\n\n## <a name=\"dbexplore\"></a>Data exploration and feature engineering in Azure SQL Data Warehouse\n\nIn this section, we perform data exploration and feature generation by running SQL queries against Azure SQL DW directly using **Visual Studio Data Tools**. All SQL queries used in this section can be found in the sample script named *SQLDW_Explorations.sql*. This file has already been downloaded to your local directory by the PowerShell script. You can also retrieve it from [Github](https://raw.githubusercontent.com/Azure/Azure-MachineLearning-DataScience/master/Misc/SQLDW/SQLDW_Explorations.sql). But the file in Github does not have the Azure SQL DW information plugged in. \n\nConnect to your Azure SQL DW using Visual Studio with the SQL DW login name and password and open up the **SQL Object Explorer** to confirm the database and tables have been imported. Retrieve the *SQLDW_Explorations.sql* file.\n\n>[AZURE.NOTE] To open a Parallel Data Warehouse (PDW) query editor, use the **New Query** command while your PDW is selected in the **SQL Object Explorer**. The standard SQL query editor is not supported by PDW.\n\nHere are the type of data exploration and feature generation tasks performed in this section:\n\n- Explore data distributions of a few fields in varying time windows.\n- Investigate data quality of the longitude and latitude fields.\n- Generate binary and multiclass classification labels based on the **tip\\_amount**.\n- Generate features and compute/compare trip distances.\n- Join the two tables and extract a random sample that will be used to build models.\n\n### Data import verification\n\nThese queries provide a quick verification of the number of rows and columns in the tables populated earlier using Polybase's parallel bulk import,\n\n    -- Report number of rows in table <nyctaxi_trip> without table scan\n    SELECT SUM(rows) FROM sys.partitions WHERE object_id = OBJECT_ID('<schemaname>.<nyctaxi_trip>')\n\n    -- Report number of columns in table <nyctaxi_trip>\n    SELECT COUNT(*) FROM information_schema.columns WHERE table_name = '<nyctaxi_trip>' AND table_schema = '<schemaname>'\n\n**Output:** You should get 173,179,759 rows and 14 columns.\n\n### Exploration: Trip distribution by medallion\n\nThis example query identifies the medallions (taxi numbers) that completed more than 100 trips within a specified time period. The query would benefit from the partitioned table access since it is conditioned by the partition scheme of **pickup\\_datetime**. Querying the full dataset will also make use of the partitioned table and/or index scan.\n\n    SELECT medallion, COUNT(*)\n    FROM <schemaname>.<nyctaxi_fare>\n    WHERE pickup_datetime BETWEEN '20130101' AND '20130331'\n    GROUP BY medallion\n    HAVING COUNT(*) > 100\n\n**Output:** The query should return a table with rows specifying the 13,369 medallions (taxis) and the number of trip completed by them in 2013. The last column contains the count of the number of trips completed.\n\n### Exploration: Trip distribution by medallion and hack_license\n\nThis example identifies the medallions (taxi numbers) and hack_license numbers (drivers) that completed more than 100 trips within a specified time period.\n\n    SELECT medallion, hack_license, COUNT(*)\n    FROM <schemaname>.<nyctaxi_fare>\n    WHERE pickup_datetime BETWEEN '20130101' AND '20130131'\n    GROUP BY medallion, hack_license\n    HAVING COUNT(*) > 100\n\n**Output:** The query should return a table with 13,369 rows specifying the 13,369 car/driver IDs that have completed more that 100 trips in 2013. The last column contains the count of the number of trips completed.\n\n### Data quality assessment: Verify records with incorrect longitude and/or latitude\n\nThis example investigates if any of the longitude and/or latitude fields either contain an invalid value (radian degrees should be between -90 and 90), or have (0, 0) coordinates.\n\n    SELECT COUNT(*) FROM <schemaname>.<nyctaxi_trip>\n    WHERE pickup_datetime BETWEEN '20130101' AND '20130331'\n    AND  (CAST(pickup_longitude AS float) NOT BETWEEN -90 AND 90\n    OR    CAST(pickup_latitude AS float) NOT BETWEEN -90 AND 90\n    OR    CAST(dropoff_longitude AS float) NOT BETWEEN -90 AND 90\n    OR    CAST(dropoff_latitude AS float) NOT BETWEEN -90 AND 90\n    OR    (pickup_longitude = '0' AND pickup_latitude = '0')\n    OR    (dropoff_longitude = '0' AND dropoff_latitude = '0'))\n\n**Output:** The query returns 837,467 trips that have invalid longitude and/or latitude fields.\n\n### Exploration: Tipped vs. not tipped trips distribution\n\nThis example finds the number of trips that were tipped vs. the number that were not tipped in a specified time period (or in the full dataset if covering the full year as it is set up here). This distribution reflects the binary label distribution to be later used for binary classification modeling.\n\n    SELECT tipped, COUNT(*) AS tip_freq FROM (\n      SELECT CASE WHEN (tip_amount > 0) THEN 1 ELSE 0 END AS tipped, tip_amount\n      FROM <schemaname>.<nyctaxi_fare>\n      WHERE pickup_datetime BETWEEN '20130101' AND '20131231') tc\n    GROUP BY tipped\n\n**Output:** The query should return the following tip frequencies for the year 2013: 90,447,622 tipped and 82,264,709 not-tipped.\n\n### Exploration: Tip class/range distribution\n\nThis example computes the distribution of tip ranges in a given time period (or in the full dataset if covering the full year). This is the distribution of the label classes that will be used later for multiclass classification modeling.\n\n    SELECT tip_class, COUNT(*) AS tip_freq FROM (\n        SELECT CASE\n            WHEN (tip_amount = 0) THEN 0\n            WHEN (tip_amount > 0 AND tip_amount <= 5) THEN 1\n            WHEN (tip_amount > 5 AND tip_amount <= 10) THEN 2\n            WHEN (tip_amount > 10 AND tip_amount <= 20) THEN 3\n            ELSE 4\n        END AS tip_class\n    FROM <schemaname>.<nyctaxi_fare>\n    WHERE pickup_datetime BETWEEN '20130101' AND '20131231') tc\n    GROUP BY tip_class\n\n**Output:**\n\n|tip_class  | tip_freq |\n| --------- | -------|\n|1  | 82230915 |\n|2  | 6198803 |\n|3  | 1932223 |\n|0  | 82264625 |\n|4  | 85765 |\n\n### Exploration: Compute and compare trip distance\n\nThis example converts the pickup and drop-off longitude and latitude to SQL geography points, computes the trip distance using SQL geography points difference, and returns a random sample of the results for comparison. The example limits the results to valid coordinates only using the data quality assessment query covered earlier.\n\n    /****** Object:  UserDefinedFunction [dbo].[fnCalculateDistance] ******/\n    SET ANSI_NULLS ON\n    GO\n\n    SET QUOTED_IDENTIFIER ON\n    GO\n\n    IF EXISTS (SELECT * FROM sys.objects WHERE type IN ('FN', 'IF') AND name = 'fnCalculateDistance')\n      DROP FUNCTION fnCalculateDistance\n    GO\n\n    -- User-defined function to calculate the direct distance  in mile between two geographical coordinates.\n    CREATE FUNCTION [dbo].[fnCalculateDistance] (@Lat1 float, @Long1 float, @Lat2 float, @Long2 float)\n    \n    RETURNS float\n    AS\n    BEGIN\n        DECLARE @distance decimal(28, 10)\n        -- Convert to radians\n        SET @Lat1 = @Lat1 / 57.2958\n        SET @Long1 = @Long1 / 57.2958\n        SET @Lat2 = @Lat2 / 57.2958\n        SET @Long2 = @Long2 / 57.2958\n        -- Calculate distance\n        SET @distance = (SIN(@Lat1) * SIN(@Lat2)) + (COS(@Lat1) * COS(@Lat2) * COS(@Long2 - @Long1))\n        --Convert to miles\n        IF @distance <> 0\n        BEGIN\n            SET @distance = 3958.75 * ATAN(SQRT(1 - POWER(@distance, 2)) / @distance);\n        END\n        RETURN @distance\n    END\n    GO\n\n    SELECT pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude, \n    dbo.fnCalculateDistance(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude) AS DirectDistance\n    FROM <schemaname>.<nyctaxi_trip>\n    WHERE datepart(\"mi\",pickup_datetime)=1\n    AND CAST(pickup_latitude AS float) BETWEEN -90 AND 90\n    AND CAST(dropoff_latitude AS float) BETWEEN -90 AND 90\n    AND pickup_longitude != '0' AND dropoff_longitude != '0'\n\n### Feature engineering using SQL functions\n\nSometimes SQL functions can be an efficient option for feature engineering. In this walkthrough, we defined a SQL function to calculate the direct distance between the pickup and dropoff locations. You can run the following SQL scripts in **Visual Studio Data Tools**. \n\nHere is the SQL script that defines the distance function.\n\n    SET ANSI_NULLS ON\n    GO\n\n    SET QUOTED_IDENTIFIER ON\n    GO\n\n    IF EXISTS (SELECT * FROM sys.objects WHERE type IN ('FN', 'IF') AND name = 'fnCalculateDistance')\n      DROP FUNCTION fnCalculateDistance\n    GO\n\n    -- User-defined function calculate the direct distance between two geographical coordinates.\n    CREATE FUNCTION [dbo].[fnCalculateDistance] (@Lat1 float, @Long1 float, @Lat2 float, @Long2 float)\n    \n    RETURNS float\n    AS\n    BEGIN\n        DECLARE @distance decimal(28, 10)\n        -- Convert to radians\n        SET @Lat1 = @Lat1 / 57.2958\n        SET @Long1 = @Long1 / 57.2958\n        SET @Lat2 = @Lat2 / 57.2958\n        SET @Long2 = @Long2 / 57.2958\n        -- Calculate distance\n        SET @distance = (SIN(@Lat1) * SIN(@Lat2)) + (COS(@Lat1) * COS(@Lat2) * COS(@Long2 - @Long1))\n        --Convert to miles\n        IF @distance <> 0\n        BEGIN\n            SET @distance = 3958.75 * ATAN(SQRT(1 - POWER(@distance, 2)) / @distance);\n        END\n        RETURN @distance\n    END\n    GO \n\nHere is an example to call this function to generate features in your SQL query:\n\n    -- Sample query to call the function to create features\n    SELECT pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude, \n    dbo.fnCalculateDistance(pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude) AS DirectDistance\n    FROM <schemaname>.<nyctaxi_trip>\n    WHERE datepart(\"mi\",pickup_datetime)=1\n    AND CAST(pickup_latitude AS float) BETWEEN -90 AND 90\n    AND CAST(dropoff_latitude AS float) BETWEEN -90 AND 90\n    AND pickup_longitude != '0' AND dropoff_longitude != '0'\n\n**Output:** This query generates a table (with 2,803,538 rows) with pickup and dropoff latitudes and longitudes and the corresponding direct distances in miles. Here are the results for first 3 rows:\n\n||pickup_latitude | pickup_longitude    | dropoff_latitude |dropoff_longitude | DirectDistance |\n|---| --------- | -------|-------| --------- | -------|\n|1  | 40.731804 | -74.001083 | 40.736622 | -73.988953 | .7169601222 |\n|2  | 40.715794 | -74,010635 | 40.725338 | -74.00399 | .7448343721 |\n|3  | 40.761456 | -73.999886 | 40.766544 | -73.988228 | 0.7037227967 |\n\n\n\n### Prepare data for model building\n\nThe following query joins the **nyctaxi\\_trip** and **nyctaxi\\_fare** tables, generates a binary classification label **tipped**, a multi-class classification label **tip\\_class**, and extracts a sample from the full joined dataset. The sampling is done by retrieving a subset of the trips based on pickup time.  This query can be copied then pasted directly in the [Azure Machine Learning Studio](https://studio.azureml.net) [Reader][reader] module for direct data ingestion from the SQL database instance in Azure. The query excludes records with incorrect (0, 0) coordinates.\n\n    SELECT t.*, f.payment_type, f.fare_amount, f.surcharge, f.mta_tax, f.tolls_amount,  f.total_amount, f.tip_amount,\n        CASE WHEN (tip_amount > 0) THEN 1 ELSE 0 END AS tipped,\n        CASE WHEN (tip_amount = 0) THEN 0\n            WHEN (tip_amount > 0 AND tip_amount <= 5) THEN 1\n            WHEN (tip_amount > 5 AND tip_amount <= 10) THEN 2\n            WHEN (tip_amount > 10 AND tip_amount <= 20) THEN 3\n            ELSE 4\n        END AS tip_class\n    FROM <schemaname>.<nyctaxi_trip> t, <schemaname>.<nyctaxi_fare> f\n    WHERE datepart(\"mi\",t.pickup_datetime) = 1\n    AND   t.medallion = f.medallion\n    AND   t.hack_license = f.hack_license\n    AND   t.pickup_datetime = f.pickup_datetime\n    AND   pickup_longitude != '0' AND dropoff_longitude != '0'\n\nWhen you are ready to proceed to Azure Machine Learning, you may either:  \n\n1. Save the final SQL query to extract and sample the data and copy-paste the query directly into a [Reader][reader] module in Azure Machine Learning, or\n2. Persist the sampled and engineered data you plan to use for model building in a new SQL DW table and use the new table in the [Reader][reader] module in Azure Machine Learning. The PowerShell script in earlier step has done this for you. You can read directly from this table in the Reader module. \n\n\n## <a name=\"ipnb\"></a>Data exploration and feature engineering in IPython notebook\n\nIn this section, we will perform data exploration and feature generation\nusing both Python and SQL queries against the SQL DW created earlier. A sample IPython notebook named **SQLDW_Explorations.ipynb** and a Python script file **SQLDW_Explorations_Scripts.py** have been downloaded to your local directory. They are also available on [GitHub](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/SQLDW). These two files are identical in Python scripts. The Python script file is provided to you in case you do not have an IPython Notebook server. These two sample Python files are designed under **Python 2.7**.\n\nThe needed Azure SQL DW information in the sample IPython Notebook and the Python script file downloaded to your local machine has been plugged in by the PowerShell script previously. They are executable without any modification.\n\nIf you have already set up an AzureML workspace, you can directly upload the sample IPython Notebook to the AzureML IPython Notebook service and start running it. Here are the steps to upload to AzureML IPython Notebook service:\n\n1. Log in to your AzureML workspace, click \"Studio\" at the top, and click \"NOTEBOOKS\" on the left side of the web page. \n\n    ![Plot #22][22]\n\n2. Click \"NEW\" on the left bottom corner of the web page, and select \"Python 2\". Then, provide a name to the notebook and click the check mark to create the new blank IPython Notebook. \n\n    ![Plot #23][23]\n\n3. Click the \"Jupyter\" symbol on the left top corner of the new IPython Notebook. \n\n    ![Plot #24][24]\n\n4. Drag and drop the sample IPython Notebook to the **tree** page of your AzureML IPython Notebook service, and click **Upload**. Then, the sample IPython Notebook will be uploaded to the AzureML IPython Notebook service. \n\n    ![Plot #25][25]\n\nIn order to run the sample IPython Notebook or the Python script file, the following Python packages are needed. If you are using the AzureML IPython Notebook service, these packages have been pre-installed. \n\n    - pandas\n    - numpy\n    - matplotlib\n    - pyodbc\n    - PyTables\n\nThe recommended sequence when building advanced analytical solutions on AzureML with large data is the following:\n\n- Read in a small sample of the data into an in-memory data frame.\n- Perform some visualizations and explorations using the sampled data.\n- Experiment with feature engineering using the sampled data.\n- For larger data exploration, data manipulation and feature engineering, use Python to issue SQL Queries directly against the SQL DW.\n- Decide the sample size to be suitable for Azure Machine Learning model building.\n\nThe followings are a few data exploration, data visualization, and feature engineering examples. More data explorations can be found in the sample IPython Notebook and the sample Python script file.\n\n### Initialize database credentials\n\nInitialize your database connection settings in the following variables:\n\n    SERVER_NAME=<server name>\n    DATABASE_NAME=<database name>\n    USERID=<user name>\n    PASSWORD=<password>\n    DB_DRIVER = <database driver>\n\n### Create database connection\n\nHere is the connection string that creates the connection to the database.\n\n    CONNECTION_STRING = 'DRIVER={'+DRIVER+'};SERVER='+SERVER_NAME+';DATABASE='+DATABASE_NAME+';UID='+USERID+';PWD='+PASSWORD\n    conn = pyodbc.connect(CONNECTION_STRING)\n\n### Report number of rows and columns in table <nyctaxi_trip>\n\n    nrows = pd.read_sql('''\n        SELECT SUM(rows) FROM sys.partitions\n        WHERE object_id = OBJECT_ID('<schemaname>.<nyctaxi_trip>')\n    ''', conn)\n\n    print 'Total number of rows = %d' % nrows.iloc[0,0]\n\n    ncols = pd.read_sql('''\n        SELECT COUNT(*) FROM information_schema.columns\n        WHERE table_name = ('<nyctaxi_trip>') AND table_schema = ('<schemaname>')\n    ''', conn)\n\n    print 'Total number of columns = %d' % ncols.iloc[0,0]\n\n- Total number of rows = 173179759  \n- Total number of columns = 14\n\n### Report number of rows and columns in table <nyctaxi_fare>\n\n    nrows = pd.read_sql('''\n        SELECT SUM(rows) FROM sys.partitions\n        WHERE object_id = OBJECT_ID('<schemaname>.<nyctaxi_fare>')\n    ''', conn)\n\n    print 'Total number of rows = %d' % nrows.iloc[0,0]\n\n    ncols = pd.read_sql('''\n        SELECT COUNT(*) FROM information_schema.columns\n        WHERE table_name = ('<nyctaxi_fare>') AND table_schema = ('<schemaname>')\n    ''', conn)\n\n    print 'Total number of columns = %d' % ncols.iloc[0,0]\n\n- Total number of rows = 173179759  \n- Total number of columns = 11\n\n### Read-in a small data sample from the SQL Data Warehouse Database\n\n    t0 = time.time()\n\n    query = '''\n        SELECT TOP 10000 t.*, f.payment_type, f.fare_amount, f.surcharge, f.mta_tax,\n            f.tolls_amount, f.total_amount, f.tip_amount\n        FROM <schemaname>.<nyctaxi_trip> t, <schemaname>.<nyctaxi_fare> f\n        WHERE datepart(\"mi\",t.pickup_datetime) = 1\n        AND   t.medallion = f.medallion\n        AND   t.hack_license = f.hack_license\n        AND   t.pickup_datetime = f.pickup_datetime\n    '''\n\n    df1 = pd.read_sql(query, conn)\n\n    t1 = time.time()\n    print 'Time to read the sample table is %f seconds' % (t1-t0)\n\n    print 'Number of rows and columns retrieved = (%d, %d)' % (df1.shape[0], df1.shape[1])\n\nTime to read the sample table is 14.096495 seconds.  \nNumber of rows and columns retrieved = (1000, 21).\n\n### Descriptive statistics\n\nNow you are ready to explore the sampled data. We start with\nlooking at some descriptive statistics for the **trip\\_distance** (or any other fields you choose to specify).\n\n    df1['trip_distance'].describe()\n\n### Visualization: Box plot example\n\nNext we look at the box plot for the trip distance to visualize the quantiles.\n\n    df1.boxplot(column='trip_distance',return_type='dict')\n\n![Plot #1][1]\n\n### Visualization: Distribution plot example\n\nPlots that visualize the distribution and a histogram for the sampled trip distances.\n\n    fig = plt.figure()\n    ax1 = fig.add_subplot(1,2,1)\n    ax2 = fig.add_subplot(1,2,2)\n    df1['trip_distance'].plot(ax=ax1,kind='kde', style='b-')\n    df1['trip_distance'].hist(ax=ax2, bins=100, color='k')\n\n![Plot #2][2]\n\n### Visualization: Bar and line plots\n\nIn this example, we bin the trip distance into five bins and visualize the binning results.\n\n    trip_dist_bins = [0, 1, 2, 4, 10, 1000]\n    df1['trip_distance']\n    trip_dist_bin_id = pd.cut(df1['trip_distance'], trip_dist_bins)\n    trip_dist_bin_id\n\nWe can plot the above bin distribution in a bar or line plot with:\n\n    pd.Series(trip_dist_bin_id).value_counts().plot(kind='bar')\n\n![Plot #3][3]\n\nand\n\n    pd.Series(trip_dist_bin_id).value_counts().plot(kind='line')\n\n![Plot #4][4]\n\n### Visualization: Scatterplot examples\n\nWe show scatter plot between **trip\\_time\\_in\\_secs** and **trip\\_distance** to see if there\nis any correlation\n\n    plt.scatter(df1['trip_time_in_secs'], df1['trip_distance'])\n\n![Plot #6][6]\n\nSimilarly we can check the relationship between **rate\\_code** and **trip\\_distance**.\n\n    plt.scatter(df1['passenger_count'], df1['trip_distance'])\n\n![Plot #8][8]\n\n\n### Data exploration on sampled data using SQL queries in IPython notebook\n\nIn this section, we explore data distributions using the sampled data which is persisted in the new table we created above. Note that similar explorations can be performed using the original tables.\n\n#### Exploration: Report number of rows and columns in the sampled table\n\n    nrows = pd.read_sql('''SELECT SUM(rows) FROM sys.partitions WHERE object_id = OBJECT_ID('<schemaname>.<nyctaxi_sample>')''', conn)\n    print 'Number of rows in sample = %d' % nrows.iloc[0,0]\n\n    ncols = pd.read_sql('''SELECT count(*) FROM information_schema.columns WHERE table_name = ('<nyctaxi_sample>') AND table_schema = '<schemaname>'''', conn)\n    print 'Number of columns in sample = %d' % ncols.iloc[0,0]\n\n#### Exploration: Tipped/not tripped Distribution\n\n    query = '''\n        SELECT tipped, count(*) AS tip_freq\n        FROM <schemaname>.<nyctaxi_sample>\n        GROUP BY tipped\n        '''\n\n    pd.read_sql(query, conn)\n\n#### Exploration: Tip class distribution\n\n    query = '''\n        SELECT tip_class, count(*) AS tip_freq\n        FROM <schemaname>.<nyctaxi_sample>\n        GROUP BY tip_class\n    '''\n\n    tip_class_dist = pd.read_sql(query, conn)\n\n#### Exploration: Plot the tip distribution by class\n\n    tip_class_dist['tip_freq'].plot(kind='bar')\n\n![Plot #26][26] \n\n\n#### Exploration: Daily distribution of trips\n\n    query = '''\n        SELECT CONVERT(date, dropoff_datetime) AS date, COUNT(*) AS c\n        FROM <schemaname>.<nyctaxi_sample>\n        GROUP BY CONVERT(date, dropoff_datetime)\n    '''\n\n    pd.read_sql(query,conn)\n\n#### Exploration: Trip distribution per medallion\n\n    query = '''\n        SELECT medallion,count(*) AS c\n        FROM <schemaname>.<nyctaxi_sample>\n        GROUP BY medallion\n    '''\n\n    pd.read_sql(query,conn)\n\n#### Exploration: Trip distribution by medallion and hack license\n\n    query = '''select medallion, hack_license,count(*) from <schemaname>.<nyctaxi_sample> group by medallion, hack_license'''\n    pd.read_sql(query,conn)\n\n\n#### Exploration: Trip time distribution\n\n    query = '''select trip_time_in_secs, count(*) from <schemaname>.<nyctaxi_sample> group by trip_time_in_secs order by count(*) desc'''\n    pd.read_sql(query,conn)\n\n#### Exploration: Trip distance distribution\n\n    query = '''select floor(trip_distance/5)*5 as tripbin, count(*) from <schemaname>.<nyctaxi_sample> group by floor(trip_distance/5)*5 order by count(*) desc'''\n    pd.read_sql(query,conn)\n\n#### Exploration: Payment type distribution\n\n    query = '''select payment_type,count(*) from <schemaname>.<nyctaxi_sample> group by payment_type'''\n    pd.read_sql(query,conn)\n\n#### Verify the final form of the featurized table\n\n    query = '''SELECT TOP 100 * FROM <schemaname>.<nyctaxi_sample>'''\n    pd.read_sql(query,conn)\n\n## <a name=\"mlmodel\"></a>Build models in Azure Machine Learning\n\nWe are now ready to proceed to model building and model deployment in [Azure Machine Learning](https://studio.azureml.net). The data is ready to be used in any of the prediction problems identified earlier, namely:\n\n1. **Binary classification**: To predict whether or not a tip was paid for a trip.\n\n2. **Multiclass classification**: To predict the range of tip paid, according to the previously defined classes.\n\n3. **Regression task**: To predict the amount of tip paid for a trip.  \n\n\n\nTo begin the modeling exercise, log in to your **Azure Machine Learning** workspace. If you have not yet created a machine learning workspace, see [Create an Azure ML workspace](machine-learning-create-workspace.md).\n\n1. To get started with Azure Machine Learning, see [What is Azure Machine Learning Studio?](machine-learning-what-is-ml-studio.md)\n\n2. Log in to [Azure Machine Learning Studio](https://studio.azureml.net).\n\n3. The Studio Home page provides a wealth of information, videos, tutorials, links to the Modules Reference, and other resources. For more information about Azure Machine Learning, consult the [Azure Machine Learning Documentation Center](https://azure.microsoft.com/documentation/services/machine-learning/).\n\nA typical training experiment consists of the following steps:\n\n1. Create a **+NEW** experiment.\n2. Get the data into Azure ML.\n3. Pre-process, transform and manipulate the data as needed.\n4. Generate features as needed.\n5. Split the data into training/validation/testing datasets(or have separate datasets for each).\n6. Select one or more machine learning algorithms depending on the learning problem to solve. E.g., binary classification, multiclass classification, regression.\n7. Train one or more models using the training dataset.\n8. Score the validation dataset using the trained model(s).\n9. Evaluate the model(s) to compute the relevant metrics for the learning problem.\n10. Fine tune the model(s) and select the best model to deploy.\n\nIn this exercise, we have already explored and engineered the data in SQL Data Warehouse, and decided on the sample size to ingest in Azure ML. Here is the procedure to build one or more of the prediction models:\n\n1. Get the data into Azure ML using the [Reader][reader] module, available in the **Data Input and Output** section. For more information, see the [Reader][reader] module reference page.\n\n    ![Azure ML Reader][17]\n\n2. Select **Azure SQL Database** as the **Data source** in the **Properties** panel.\n\n3. Enter the database DNS name in the **Database server name** field. Format: `tcp:<your_virtual_machine_DNS_name>,1433`\n\n4. Enter the **Database name** in the corresponding field.\n\n5. Enter the *SQL user name* in the **Server user account name**, and the *password* in the **Server user account password**.\n\n6. Check the **Accept any server certificate** option.\n\n7. In the **Database query** edit text area, paste the query which extracts the necessary database fields (including any computed fields such as the labels) and down samples the data to the desired sample size.\n\nAn example of a binary classification experiment reading data directly from the SQL Data Warehouse database is in the figure below (remember to replace the table names nyctaxi_trip and nyctaxi_fare by the schema name and the table names you used in your walkthrough). Similar experiments can be constructed for multiclass classification and regression problems.\n\n![Azure ML Train][10]\n\n> [AZURE.IMPORTANT] In the modeling data extraction and sampling query examples provided in previous sections, **all labels for the three modeling exercises are included in the query**. An important (required) step in each of the modeling exercises is to **exclude** the unnecessary labels for the other two problems, and any other **target leaks**. For example, when using binary classification, use the label **tipped** and exclude the fields **tip\\_class**, **tip\\_amount**, and **total\\_amount**. The latter are target leaks since they imply the tip paid.\n>\n> To exclude any unnecessary columns or target leaks, you may use the [Project Columns][project-columns] module or the [Metadata Editor][metadata-editor]. For more information, see [Project Columns][project-columns] and [Metadata Editor][metadata-editor] reference pages.\n\n## <a name=\"mldeploy\"></a>Deploy models in Azure Machine Learning\n\nWhen your model is ready, you can easily deploy it as a web service directly from the experiment. For more information about deploying Azure ML web services, see [Deploy an Azure Machine Learning web service](machine-learning-publish-a-machine-learning-web-service.md).\n\nTo deploy a new web service, you need to:\n\n1. Create a scoring experiment.\n2. Deploy the web service.\n\nTo create a scoring experiment from a **Finished** training experiment, click **CREATE SCORING EXPERIMENT** in the lower action bar.\n\n![Azure Scoring][18]\n\nAzure Machine Learning will attempt to create a scoring experiment based on the components of the training experiment. In particular, it will:\n\n1. Save the trained model and remove the model training modules.\n2. Identify a logical **input port** to represent the expected input data schema.\n3. Identify a logical **output port** to represent the expected web service output schema.\n\nWhen the scoring experiment is created, review it and make adjust as needed. A typical adjustment is to replace the input dataset and/or query with one which excludes label fields, as these will not be available when the service is called. It is also a good practice to reduce the size of the input dataset and/or query to a few records, just enough to indicate the input schema. For the output port, it is common to exclude all input fields and only include the **Scored Labels** and **Scored Probabilities** in the output using the [Project Columns][project-columns] module.\n\nA sample scoring experiment is provided in the figure below. When ready to deploy, click the **PUBLISH WEB SERVICE** button in the lower action bar.\n\n![Azure ML Publish][11]\n\n\n## Summary\nTo recap what we have done in this walkthrough tutorial, you have created an Azure data science environment, worked with a large public dataset, taking it through the Cortana Analytics Process, all the way from data acquisition to model training, and then to the deployment of an Azure Machine Learning web service.\n\n### License information\n\nThis sample walkthrough and its accompanying scripts and IPython notebook(s) are shared by Microsoft under the MIT license. Please check the LICENSE.txt file in in the directory of the sample code on GitHub for more details.\n\n## References\n\n•   [Andrés Monroy NYC Taxi Trips Download Page](http://www.andresmh.com/nyctaxitrips/)  \n•   [FOILing NYC’s Taxi Trip Data by Chris Whong](http://chriswhong.com/open-data/foil_nyc_taxi/)   \n•   [NYC Taxi and Limousine Commission Research and Statistics](https://www1.nyc.gov/html/tlc/html/about/statistics.shtml)\n\n\n[1]: ./media/machine-learning-data-science-process-sqldw-walkthrough/sql-walkthrough_26_1.png\n[2]: ./media/machine-learning-data-science-process-sqldw-walkthrough/sql-walkthrough_28_1.png\n[3]: ./media/machine-learning-data-science-process-sqldw-walkthrough/sql-walkthrough_35_1.png\n[4]: ./media/machine-learning-data-science-process-sqldw-walkthrough/sql-walkthrough_36_1.png\n[5]: ./media/machine-learning-data-science-process-sqldw-walkthrough/sql-walkthrough_39_1.png\n[6]: ./media/machine-learning-data-science-process-sqldw-walkthrough/sql-walkthrough_42_1.png\n[7]: ./media/machine-learning-data-science-process-sqldw-walkthrough/sql-walkthrough_44_1.png\n[8]: ./media/machine-learning-data-science-process-sqldw-walkthrough/sql-walkthrough_46_1.png\n[9]: ./media/machine-learning-data-science-process-sqldw-walkthrough/sql-walkthrough_71_1.png\n[10]: ./media/machine-learning-data-science-process-sqldw-walkthrough/azuremltrain.png\n[11]: ./media/machine-learning-data-science-process-sqldw-walkthrough/azuremlpublish.png\n[12]: ./media/machine-learning-data-science-process-sqldw-walkthrough/ssmsconnect.png\n[13]: ./media/machine-learning-data-science-process-sqldw-walkthrough/executescript.png\n[14]: ./media/machine-learning-data-science-process-sqldw-walkthrough/sqlserverproperties.png\n[15]: ./media/machine-learning-data-science-process-sqldw-walkthrough/sqldefaultdirs.png\n[16]: ./media/machine-learning-data-science-process-sqldw-walkthrough/bulkimport.png\n[17]: ./media/machine-learning-data-science-process-sqldw-walkthrough/amlreader.png\n[18]: ./media/machine-learning-data-science-process-sqldw-walkthrough/amlscoring.png\n[19]: ./media/machine-learning-data-science-process-sqldw-walkthrough/ps_download_scripts.png\n[20]: ./media/machine-learning-data-science-process-sqldw-walkthrough/ps_load_data.png\n[21]: ./media/machine-learning-data-science-process-sqldw-walkthrough/azcopy-overwrite.png\n[22]: ./media/machine-learning-data-science-process-sqldw-walkthrough/ipnb-service-aml-1.png\n[23]: ./media/machine-learning-data-science-process-sqldw-walkthrough/ipnb-service-aml-2.png\n[24]: ./media/machine-learning-data-science-process-sqldw-walkthrough/ipnb-service-aml-3.png\n[25]: ./media/machine-learning-data-science-process-sqldw-walkthrough/ipnb-service-aml-4.png\n[26]: ./media/machine-learning-data-science-process-sqldw-walkthrough/tip_class_hist_1.png\n\n\n<!-- Module References -->\n[metadata-editor]: https://msdn.microsoft.com/library/azure/370b6676-c11c-486f-bf73-35349f842a66/\n[project-columns]: https://msdn.microsoft.com/library/azure/1ec722fa-b623-4e26-a44e-a50c6d726223/\n[reader]: https://msdn.microsoft.com/library/azure/4e1b0fe6-aded-4b3f-a36f-39b8862b9004/\n"
}