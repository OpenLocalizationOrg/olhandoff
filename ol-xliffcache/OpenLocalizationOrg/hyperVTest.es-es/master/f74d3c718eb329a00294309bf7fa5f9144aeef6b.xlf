<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" source-language="en-US" target-language="es-es">
    <body>
      <group id="main" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Ten things you can do on the Data science Virtual Machine  | Microsoft Azure</source>
          <target state="new">Ten things you can do on the Data science Virtual Machine  | Microsoft Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Perform various data exploration and modeling task on the Data science Virtual Machine.</source>
          <target state="new">Perform various data exploration and modeling task on the Data science Virtual Machine.</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Ten things you can do on the Data science Virtual Machine</source>
          <target state="new">Ten things you can do on the Data science Virtual Machine</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>The Microsoft Data Science Virtual Machine (DSVM) is a powerful data science development environment that enables you to perform various data exploration and modeling tasks.</source>
          <target state="new">The Microsoft Data Science Virtual Machine (DSVM) is a powerful data science development environment that enables you to perform various data exploration and modeling tasks.</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>The environment comes already built and bundled with several popular data analytics tools that make it easy to get started quickly with your analysis.</source>
          <target state="new">The environment comes already built and bundled with several popular data analytics tools that make it easy to get started quickly with your analysis.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>The DSVM works closely with many Azure services and is able to read and process data that is already stored on Azure in Azure SQL Data Warehouse, Azure Data Lake, Azure Storage, or DocumentDB.</source>
          <target state="new">The DSVM works closely with many Azure services and is able to read and process data that is already stored on Azure in Azure SQL Data Warehouse, Azure Data Lake, Azure Storage, or DocumentDB.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>It can also leverage analytics tools like Azure Machine Learning and Azure Data Factory.</source>
          <target state="new">It can also leverage analytics tools like Azure Machine Learning and Azure Data Factory.</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>In this article we walk you through how to use your DSVM to perform various data science tasks and interact with other Azure services.</source>
          <target state="new">In this article we walk you through how to use your DSVM to perform various data science tasks and interact with other Azure services.</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>Here is some  of the things you can do on the DSVM:</source>
          <target state="new">Here is some  of the things you can do on the DSVM:</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>Explore data and develop models locally on the DSVM using Microsoft R Server, Python</source>
          <target state="new">Explore data and develop models locally on the DSVM using Microsoft R Server, Python</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>Use a Jupyter notebook to experiment with your data on a browser using Python 2, Python 3, Microsoft R an enterprise ready version of R designed for scalability and performance</source>
          <target state="new">Use a Jupyter notebook to experiment with your data on a browser using Python 2, Python 3, Microsoft R an enterprise ready version of R designed for scalability and performance</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>Operationalize models built using R and Python on Azure Machine Learning so client applications can access your models using a simple web services interface</source>
          <target state="new">Operationalize models built using R and Python on Azure Machine Learning so client applications can access your models using a simple web services interface</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>Administer your Azure resources using  Azure Portal or Powershell</source>
          <target state="new">Administer your Azure resources using  Azure Portal or Powershell</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>Extend your storage space and share large scale datasets / code across your whole team by creating an Azure File Storage as a mountable drive on your DSVM</source>
          <target state="new">Extend your storage space and share large scale datasets / code across your whole team by creating an Azure File Storage as a mountable drive on your DSVM</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>Share code with your team using Github and access your repository using the pre-installed Git clients - Git Bash, Git GUI.</source>
          <target state="new">Share code with your team using Github and access your repository using the pre-installed Git clients - Git Bash, Git GUI.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>Access various Azure data and analytics services like Azure blob storage, Azure Data Lake, Azure HDInsight (Hadoop), Azure DocumentDB, Azure SQL Data Warehouse &amp; databases</source>
          <target state="new">Access various Azure data and analytics services like Azure blob storage, Azure Data Lake, Azure HDInsight (Hadoop), Azure DocumentDB, Azure SQL Data Warehouse &amp; databases</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>Build reports and dashboard using the Power BI Desktop pre-installed on the DSVM and deploy them on the cloud</source>
          <target state="new">Build reports and dashboard using the Power BI Desktop pre-installed on the DSVM and deploy them on the cloud</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>Dynamically scale your DSVM to meet your project needs</source>
          <target state="new">Dynamically scale your DSVM to meet your project needs</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>Install additional tools on your virtual machine</source>
          <target state="new">Install additional tools on your virtual machine</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Prerequisites<ept id="p1">**</ept>
You will need an Azure subscription.</source>
          <target state="new"><bpt id="p1">**</bpt>Prerequisites<ept id="p1">**</ept>
You will need an Azure subscription.</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>You can sign-up for a free trial <bpt id="p2">[</bpt>here<ept id="p2">](https://azure.microsoft.com/free/)</ept>
Instructions for provisioning a Data Science Virtual Machine on the Azure Portal are available at <bpt id="p3">[</bpt>Creating a virtual machine<ept id="p3">](https://ms.portal.azure.com/#create/microsoft-ads.standard-data-science-vmstandard-data-science-vm)</ept>.</source>
          <target state="new">You can sign-up for a free trial <bpt id="p2">[</bpt>here<ept id="p2">](https://azure.microsoft.com/free/)</ept>
Instructions for provisioning a Data Science Virtual Machine on the Azure Portal are available at <bpt id="p3">[</bpt>Creating a virtual machine<ept id="p3">](https://ms.portal.azure.com/#create/microsoft-ads.standard-data-science-vmstandard-data-science-vm)</ept>.</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>1. Explore data and develop models using Microsoft R Server or Python</source>
          <target state="new">1. Explore data and develop models using Microsoft R Server or Python</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>You can use languages like R and Python to do your data analytics right on the DSVM.</source>
          <target state="new">You can use languages like R and Python to do your data analytics right on the DSVM.</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>For R, you can use an IDE called "Revolution R Enterprise 8.0" that can be found on the start menu or the desktop.</source>
          <target state="new">For R, you can use an IDE called "Revolution R Enterprise 8.0" that can be found on the start menu or the desktop.</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>Microsoft has provided additional libraries on top of the Open source/CRAN-R to enable scalable analytics and the ability to analyze data larger than the memory size allowed by doing parallel chunked analysis.</source>
          <target state="new">Microsoft has provided additional libraries on top of the Open source/CRAN-R to enable scalable analytics and the ability to analyze data larger than the memory size allowed by doing parallel chunked analysis.</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>Here is a screen shot of  using the R IDE.</source>
          <target state="new">Here is a screen shot of  using the R IDE.</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source><ph id="ph2">![</ph>R IDE<ph id="ph3">](./media/machine-learning-data-science-vm-do-ten-things/RevoIDE.png)</ph></source>
          <target state="new"><ph id="ph2">![</ph>R IDE<ph id="ph3">](./media/machine-learning-data-science-vm-do-ten-things/RevoIDE.png)</ph></target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>For Python, you can use an IDE like Visual Studio Community Edition which has the Python Tools for Visual Studio (PTVS) extension pre-installed.</source>
          <target state="new">For Python, you can use an IDE like Visual Studio Community Edition which has the Python Tools for Visual Studio (PTVS) extension pre-installed.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>By default, only a basic Python 2.7 (without any analytics library like SciKit, Pandas) is configured on PTVS.</source>
          <target state="new">By default, only a basic Python 2.7 (without any analytics library like SciKit, Pandas) is configured on PTVS.</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>In order to enable Anaconda Python 2.7 and 3.5, you need to do the following:</source>
          <target state="new">In order to enable Anaconda Python 2.7 and 3.5, you need to do the following:</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>Create custom environments for each version by navigating to Tools -&gt; Python Tools -&gt; Python Environments and then clicking "+ Custom" in the Visual Studio 2015 Community Edition</source>
          <target state="new">Create custom environments for each version by navigating to Tools -&gt; Python Tools -&gt; Python Environments and then clicking "+ Custom" in the Visual Studio 2015 Community Edition</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>Give a description and set the environment prefix paths as c:\anaconda for Anaconda Python 2.7 OR c:\anaconda\envs\py35 for Anaconda Python 3.5</source>
          <target state="new">Give a description and set the environment prefix paths as c:\anaconda for Anaconda Python 2.7 OR c:\anaconda\envs\py35 for Anaconda Python 3.5</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>Click <bpt id="p4">**</bpt>Auto Detect<ept id="p4">**</ept><ph id="ph4" /> and then <bpt id="p5">**</bpt>Apply<ept id="p5">**</ept><ph id="ph5" /> to save the environment.</source>
          <target state="new">Click <bpt id="p4">**</bpt>Auto Detect<ept id="p4">**</ept><ph id="ph4" /> and then <bpt id="p5">**</bpt>Apply<ept id="p5">**</ept><ph id="ph5" /> to save the environment.</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>Here is what the custom environment setup looks like in Visual Studio.</source>
          <target state="new">Here is what the custom environment setup looks like in Visual Studio.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source><ph id="ph6">![</ph>PTVS Setup<ph id="ph7">](./media/machine-learning-data-science-vm-do-ten-things/PTVSSetup.png)</ph></source>
          <target state="new"><ph id="ph6">![</ph>PTVS Setup<ph id="ph7">](./media/machine-learning-data-science-vm-do-ten-things/PTVSSetup.png)</ph></target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>See <bpt id="p6">[</bpt>PTVS documentation<ept id="p6">](https://github.com/Microsoft/PTVS/wiki/Selecting-and-Installing-Python-Interpreters#hey-i-already-have-an-interpreter-on-my-machine-but-ptvs-doesnt-seem-to-know-about-it)</ept><ph id="ph8" /> for more details on how to create the Python Environments.</source>
          <target state="new">See <bpt id="p6">[</bpt>PTVS documentation<ept id="p6">](https://github.com/Microsoft/PTVS/wiki/Selecting-and-Installing-Python-Interpreters#hey-i-already-have-an-interpreter-on-my-machine-but-ptvs-doesnt-seem-to-know-about-it)</ept><ph id="ph8" /> for more details on how to create the Python Environments.</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>Now you are set up to open a project and begin working!</source>
          <target state="new">Now you are set up to open a project and begin working!</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>2. Using a Jupyter Notebook to explore and model your data with Python or R</source>
          <target state="new">2. Using a Jupyter Notebook to explore and model your data with Python or R</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>The Jupyter Notebook is a powerful environment that provides a browser-based "IDE" for data exploration and modeling.</source>
          <target state="new">The Jupyter Notebook is a powerful environment that provides a browser-based "IDE" for data exploration and modeling.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source><ph id="ph9" />You can use Python 2, Python 3 or R (both Open Source and Microsoft R Server).</source>
          <target state="new"><ph id="ph9" />You can use Python 2, Python 3 or R (both Open Source and Microsoft R Server).</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source><ph id="ph10" />To launch the Jupyter notebook click on the start menu icon / desktop icon titled <bpt id="p7">**</bpt>Jupyter Notebook<ept id="p7">**</ept>.</source>
          <target state="new"><ph id="ph10" />To launch the Jupyter notebook click on the start menu icon / desktop icon titled <bpt id="p7">**</bpt>Jupyter Notebook<ept id="p7">**</ept>.</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>On the DSVM you can also browse to "https://localhost:9999/" to access the Jupiter notebook.</source>
          <target state="new">On the DSVM you can also browse to "https://localhost:9999/" to access the Jupiter notebook.</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>If it prompts you for a password, please use instructions found on the 
<bpt id="p8">[</bpt>DSVM documentation page<ept id="p8">](machine-learning-data-science-provision-vm.md/#how-to-create-a-strong-password-on-the-jupyter-notebook-server)</ept><ph id="ph11" /> 
to create a strong password to access the Jupyter notebook.</source>
          <target state="new">If it prompts you for a password, please use instructions found on the 
<bpt id="p8">[</bpt>DSVM documentation page<ept id="p8">](machine-learning-data-science-provision-vm.md/#how-to-create-a-strong-password-on-the-jupyter-notebook-server)</ept><ph id="ph11" /> 
to create a strong password to access the Jupyter notebook.</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>TBD: SCREEN SHOT</source>
          <target state="new">TBD: SCREEN SHOT</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>Once you are on the notebook, you will see a directory that contains a few example notebooks that are pre-packaged into the DSVM.</source>
          <target state="new">Once you are on the notebook, you will see a directory that contains a few example notebooks that are pre-packaged into the DSVM.</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>You can click on the notebook and see the code.</source>
          <target state="new">You can click on the notebook and see the code.</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>You can execute each cell by pressing <bpt id="p9">**</bpt>SHIFT-ENTER<ept id="p9">**</ept>.</source>
          <target state="new">You can execute each cell by pressing <bpt id="p9">**</bpt>SHIFT-ENTER<ept id="p9">**</ept>.</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source><ph id="ph12" />You can run the entire notebook by clicking on <bpt id="p10">**</bpt>Cell<ept id="p10">**</ept><ph id="ph13" /> -&gt; <bpt id="p11">**</bpt>Run<ept id="p11">**</ept>.</source>
          <target state="new"><ph id="ph12" />You can run the entire notebook by clicking on <bpt id="p10">**</bpt>Cell<ept id="p10">**</ept><ph id="ph13" /> -&gt; <bpt id="p11">**</bpt>Run<ept id="p11">**</ept>.</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>You can create a new notebook by clicking on the Jupyter Icon (left top corner) and then clicking <bpt id="p12">**</bpt>New<ept id="p12">**</ept><ph id="ph14" /> button on the right and then choosing the notebook language (also known as kernels).</source>
          <target state="new">You can create a new notebook by clicking on the Jupyter Icon (left top corner) and then clicking <bpt id="p12">**</bpt>New<ept id="p12">**</ept><ph id="ph14" /> button on the right and then choosing the notebook language (also known as kernels).</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>Currently we support Python 2.7, Python 3.5 and R. The R kernel supports programming in both Open source R as well as the enterprise scalable Microsoft R Server.</source>
          <target state="new">Currently we support Python 2.7, Python 3.5 and R. The R kernel supports programming in both Open source R as well as the enterprise scalable Microsoft R Server.</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>Once you are in the notebook you can explore your data, build the model, test the model using your choice of libraries.</source>
          <target state="new">Once you are in the notebook you can explore your data, build the model, test the model using your choice of libraries.</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>3. Build models using R and Python and Operationalize it on Azure Machine Learning</source>
          <target state="new">3. Build models using R and Python and Operationalize it on Azure Machine Learning</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>Once you have built and validated your model the next step is usually to deploy it into production.</source>
          <target state="new">Once you have built and validated your model the next step is usually to deploy it into production.</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source><ph id="ph15" />This allows your client applications to invoke the model predictions on a real time or a batch basis.</source>
          <target state="new"><ph id="ph15" />This allows your client applications to invoke the model predictions on a real time or a batch basis.</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source>Azure Machine Learning provides a mechanism to operationalize the model built in either R or Python.</source>
          <target state="new">Azure Machine Learning provides a mechanism to operationalize the model built in either R or Python.</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source><ph id="ph16" />When you operationalize your model in Azure Machine Learning, a web service is exposed that allows clients to make REST calls that pass in input parameters and receive predictions from the model as outputs.</source>
          <target state="new"><ph id="ph16" />When you operationalize your model in Azure Machine Learning, a web service is exposed that allows clients to make REST calls that pass in input parameters and receive predictions from the model as outputs.</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source><ph id="ph17" />If you have not yet signed up for AzureML, you can obtain a free 8-hour guest access or a free workspace by visiting the <bpt id="p13">[</bpt>AzureML Studio<ept id="p13">](https://studio.azureml.net/)</ept><ph id="ph18" /> home page and clicking on "Get Started".</source>
          <target state="new"><ph id="ph17" />If you have not yet signed up for AzureML, you can obtain a free 8-hour guest access or a free workspace by visiting the <bpt id="p13">[</bpt>AzureML Studio<ept id="p13">](https://studio.azureml.net/)</ept><ph id="ph18" /> home page and clicking on "Get Started".</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>Build and Operationalizing models built using Python</source>
          <target state="new">Build and Operationalizing models built using Python</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>Upload the notebook entitled "IrisClassifierPyMLWebService" to your Jupyter.</source>
          <target state="new">Upload the notebook entitled "IrisClassifierPyMLWebService" to your Jupyter.</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source>Here is a simple model built in Python using SciKit-learn as found in the notebook.</source>
          <target state="new">Here is a simple model built in Python using SciKit-learn as found in the notebook.</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>The method used to deploy your python models to AzureML is to wrap the prediction of a model into a function</source>
          <target state="new">The method used to deploy your python models to AzureML is to wrap the prediction of a model into a function</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>and decorate it with attributes provided by the AzureML library denoting your AzureML workspace ID, API Key, the input parameters and return parameters.</source>
          <target state="new">and decorate it with attributes provided by the AzureML library denoting your AzureML workspace ID, API Key, the input parameters and return parameters.</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>A client can now make calls to the web service.</source>
          <target state="new">A client can now make calls to the web service.</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>There are convenience wrappers that construct the REST API requests.</source>
          <target state="new">There are convenience wrappers that construct the REST API requests.</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>Here is a sample code to consume the web service.</source>
          <target state="new">Here is a sample code to consume the web service.</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>NOTE: The AzureML library is support on Python 2.7 only currently.</source>
          <target state="new">NOTE: The AzureML library is support on Python 2.7 only currently.</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source>Build and Operationalize R models</source>
          <target state="new">Build and Operationalize R models</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source>You can deploy R models built on the Data Science Virtual Machine or elsewhere onto Azure ML in a manner that is similar to how it is done for Python.</source>
          <target state="new">You can deploy R models built on the Data Science Virtual Machine or elsewhere onto Azure ML in a manner that is similar to how it is done for Python.</target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source>First create a settings.json file as below to provide your workspace ID and auth token.</source>
          <target state="new">First create a settings.json file as below to provide your workspace ID and auth token.</target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>You then write a wrapper for the model's predict function.</source>
          <target state="new">You then write a wrapper for the model's predict function.</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>Then you call the <ph id="ph19">```publishWebService```</ph><ph id="ph20" /> call in the AzureML library passing in the function wrapper.</source>
          <target state="new">Then you call the <ph id="ph19">```publishWebService```</ph><ph id="ph20" /> call in the AzureML library passing in the function wrapper.</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source><ph id="ph21" />Here is a code snippet that can be used to publish a model as a web service in Azure ML.</source>
          <target state="new"><ph id="ph21" />Here is a code snippet that can be used to publish a model as a web service in Azure ML.</target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source>Settings.json File:</source>
          <target state="new">Settings.json File:</target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source>Build a model in R and publishing it in Azure ML</source>
          <target state="new">Build a model in R and publishing it in Azure ML</target>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source>Consume the model deployed in Azure ML</source>
          <target state="new">Consume the model deployed in Azure ML</target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source>To consume the model from a client application, we use the AzureML library to lookup the published web service by name using the <ph id="ph22">`services`</ph><ph id="ph23" /> API call to determine the endpoint.</source>
          <target state="new">To consume the model from a client application, we use the AzureML library to lookup the published web service by name using the <ph id="ph22">`services`</ph><ph id="ph23" /> API call to determine the endpoint.</target>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source>Then you just call the <ph id="ph24">`consume`</ph><ph id="ph25" /> function and pass in the data frame to be predicted.</source>
          <target state="new">Then you just call the <ph id="ph24">`consume`</ph><ph id="ph25" /> function and pass in the data frame to be predicted.</target>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source><ph id="ph26" />The following code is used to consume the model published as an AzureML web service.</source>
          <target state="new"><ph id="ph26" />The following code is used to consume the model published as an AzureML web service.</target>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source>4. Administer your Azure resources using Azure Portal or Powershell</source>
          <target state="new">4. Administer your Azure resources using Azure Portal or Powershell</target>
        </trans-unit>
        <trans-unit id="180" translate="yes" xml:space="preserve">
          <source>The DSVM not only allows you to build your analytics solution locally on the virtual machine, but also allows you to access services on Microsoft's Azure cloud.</source>
          <target state="new">The DSVM not only allows you to build your analytics solution locally on the virtual machine, but also allows you to access services on Microsoft's Azure cloud.</target>
        </trans-unit>
        <trans-unit id="181" translate="yes" xml:space="preserve">
          <source>Azure provides several compute, storage, data analytics services and other services that you can administer and access from your DSVM.</source>
          <target state="new">Azure provides several compute, storage, data analytics services and other services that you can administer and access from your DSVM.</target>
        </trans-unit>
        <trans-unit id="182" translate="yes" xml:space="preserve">
          <source>To administer your Azure subscription and cloud resources you can use your browser and point to the 
<bpt id="p14">[</bpt>Azure Portal<ept id="p14">](portal.azure.com)</ept>.</source>
          <target state="new">To administer your Azure subscription and cloud resources you can use your browser and point to the 
<bpt id="p14">[</bpt>Azure Portal<ept id="p14">](portal.azure.com)</ept>.</target>
        </trans-unit>
        <trans-unit id="183" translate="yes" xml:space="preserve">
          <source>You can also use Azure Powershell to adminster your Azure subscription and resources via a script.</source>
          <target state="new">You can also use Azure Powershell to adminster your Azure subscription and resources via a script.</target>
        </trans-unit>
        <trans-unit id="184" translate="yes" xml:space="preserve">
          <source><ph id="ph27" />You can run Azure Powershell from a shortcut on the desktop or from the start menu titled "Microsoft Azure Powershell".</source>
          <target state="new"><ph id="ph27" />You can run Azure Powershell from a shortcut on the desktop or from the start menu titled "Microsoft Azure Powershell".</target>
        </trans-unit>
        <trans-unit id="185" translate="yes" xml:space="preserve">
          <source>Refer to 
<bpt id="p15">[</bpt>Microsoft Azure Powershell documentation<ept id="p15">](../powershell-azure-resource-manager.md)</ept><ph id="ph28" /> for more information on how you can administer your Azure subscription and resources using Windows Powershell scripts.</source>
          <target state="new">Refer to 
<bpt id="p15">[</bpt>Microsoft Azure Powershell documentation<ept id="p15">](../powershell-azure-resource-manager.md)</ept><ph id="ph28" /> for more information on how you can administer your Azure subscription and resources using Windows Powershell scripts.</target>
        </trans-unit>
        <trans-unit id="186" translate="yes" xml:space="preserve">
          <source>5. Extend your storage space with a shared file system</source>
          <target state="new">5. Extend your storage space with a shared file system</target>
        </trans-unit>
        <trans-unit id="187" translate="yes" xml:space="preserve">
          <source>Data scientists can share large datasets, code etc within the team.</source>
          <target state="new">Data scientists can share large datasets, code etc within the team.</target>
        </trans-unit>
        <trans-unit id="188" translate="yes" xml:space="preserve">
          <source>The DSVM itself has about 70GB of space available.</source>
          <target state="new">The DSVM itself has about 70GB of space available.</target>
        </trans-unit>
        <trans-unit id="189" translate="yes" xml:space="preserve">
          <source>To extend your storage, you can use the Azure File Service and mount it on the DSVM or access via REST API.</source>
          <target state="new">To extend your storage, you can use the Azure File Service and mount it on the DSVM or access via REST API.</target>
        </trans-unit>
        <trans-unit id="191" translate="yes" xml:space="preserve">
          <source>The maximum space of the Azure File Service share is 5TB and individual file size limit is 1TB.</source>
          <target state="new">The maximum space of the Azure File Service share is 5TB and individual file size limit is 1TB.</target>
        </trans-unit>
        <trans-unit id="192" translate="yes" xml:space="preserve">
          <source>You can use Azure Powershell to create a  Azure File Service share.</source>
          <target state="new">You can use Azure Powershell to create a  Azure File Service share.</target>
        </trans-unit>
        <trans-unit id="193" translate="yes" xml:space="preserve">
          <source>Here is the script to run under Azure PowerShell to create a Azure File service share.</source>
          <target state="new">Here is the script to run under Azure PowerShell to create a Azure File service share.</target>
        </trans-unit>
        <trans-unit id="194" translate="yes" xml:space="preserve">
          <source>Now that you have creates an Azure file share, you can mount it in any virtual machine in Azure.</source>
          <target state="new">Now that you have creates an Azure file share, you can mount it in any virtual machine in Azure.</target>
        </trans-unit>
        <trans-unit id="195" translate="yes" xml:space="preserve">
          <source>It is highly recommended that the VM is in same Azure data center as the storage account to avoid latency and data transfer charges.</source>
          <target state="new">It is highly recommended that the VM is in same Azure data center as the storage account to avoid latency and data transfer charges.</target>
        </trans-unit>
        <trans-unit id="196" translate="yes" xml:space="preserve">
          <source>Here is the commands to mount the drive on the DSVM that you can run on Azure Powershell.</source>
          <target state="new">Here is the commands to mount the drive on the DSVM that you can run on Azure Powershell.</target>
        </trans-unit>
        <trans-unit id="197" translate="yes" xml:space="preserve">
          <source>Now you can access this drive as any normal drive on the VM.</source>
          <target state="new">Now you can access this drive as any normal drive on the VM.</target>
        </trans-unit>
        <trans-unit id="198" translate="yes" xml:space="preserve">
          <source>6. Share code with your team using Github</source>
          <target state="new">6. Share code with your team using Github</target>
        </trans-unit>
        <trans-unit id="199" translate="yes" xml:space="preserve">
          <source>Github is a code repository where you can find a lot of sample code and sources for different tools using various technologies shared by the developer community.</source>
          <target state="new">Github is a code repository where you can find a lot of sample code and sources for different tools using various technologies shared by the developer community.</target>
        </trans-unit>
        <trans-unit id="200" translate="yes" xml:space="preserve">
          <source>It uses Git as the technology to track and store versions of the code files.</source>
          <target state="new">It uses Git as the technology to track and store versions of the code files.</target>
        </trans-unit>
        <trans-unit id="201" translate="yes" xml:space="preserve">
          <source>Github is also a platform where you can create your own repository to store your team's shared code and documentation, implement version control and also control who have access to view and contribute code.</source>
          <target state="new">Github is also a platform where you can create your own repository to store your team's shared code and documentation, implement version control and also control who have access to view and contribute code.</target>
        </trans-unit>
        <trans-unit id="202" translate="yes" xml:space="preserve">
          <source>Please visit the <bpt id="p16">[</bpt>Github help pages<ept id="p16">](https://help.github.com/)</ept><ph id="ph29" /> for more information on using Git.</source>
          <target state="new">Please visit the <bpt id="p16">[</bpt>Github help pages<ept id="p16">](https://help.github.com/)</ept><ph id="ph29" /> for more information on using Git.</target>
        </trans-unit>
        <trans-unit id="203" translate="yes" xml:space="preserve">
          <source>You can use Github as one of the ways to collaborate with your team, use code developed by the community and contribute code back to the community.</source>
          <target state="new">You can use Github as one of the ways to collaborate with your team, use code developed by the community and contribute code back to the community.</target>
        </trans-unit>
        <trans-unit id="204" translate="yes" xml:space="preserve">
          <source>The DSVM already comes loaded with client tools on both command line as well GUI to access Github repository.</source>
          <target state="new">The DSVM already comes loaded with client tools on both command line as well GUI to access Github repository.</target>
        </trans-unit>
        <trans-unit id="205" translate="yes" xml:space="preserve">
          <source>The command line tool to work with Git and Github is called <ph id="ph30">```git-bash```</ph>.</source>
          <target state="new">The command line tool to work with Git and Github is called <ph id="ph30">```git-bash```</ph>.</target>
        </trans-unit>
        <trans-unit id="206" translate="yes" xml:space="preserve">
          <source>Visual Studio installed on the DSVM has the Git extensions.</source>
          <target state="new">Visual Studio installed on the DSVM has the Git extensions.</target>
        </trans-unit>
        <trans-unit id="207" translate="yes" xml:space="preserve">
          <source>You can find start-up icons for these tools on the start menu and the desktop.</source>
          <target state="new">You can find start-up icons for these tools on the start menu and the desktop.</target>
        </trans-unit>
        <trans-unit id="208" translate="yes" xml:space="preserve">
          <source>To download code from a Github repository you will use the <ph id="ph31">```git clone```</ph><ph id="ph32" /> command.</source>
          <target state="new">To download code from a Github repository you will use the <ph id="ph31">```git clone```</ph><ph id="ph32" /> command.</target>
        </trans-unit>
        <trans-unit id="209" translate="yes" xml:space="preserve">
          <source>For example to download data science repository published by Microsoft into the current directory you can run the following command once you are in <ph id="ph33">```git-bash```</ph>.</source>
          <target state="new">For example to download data science repository published by Microsoft into the current directory you can run the following command once you are in <ph id="ph33">```git-bash```</ph>.</target>
        </trans-unit>
        <trans-unit id="210" translate="yes" xml:space="preserve">
          <source>In Visual Studio, you can do the same clone operation.</source>
          <target state="new">In Visual Studio, you can do the same clone operation.</target>
        </trans-unit>
        <trans-unit id="211" translate="yes" xml:space="preserve">
          <source>See the screen-shot below for accessing Git and Github tools in Visual Studio.</source>
          <target state="new">See the screen-shot below for accessing Git and Github tools in Visual Studio.</target>
        </trans-unit>
        <trans-unit id="212" translate="yes" xml:space="preserve">
          <source><ph id="ph34">![</ph>Git in Visual Studio<ph id="ph35">](./media/machine-learning-data-science-vm-do-ten-things/VSGit.PNG)</ph></source>
          <target state="new"><ph id="ph34">![</ph>Git in Visual Studio<ph id="ph35">](./media/machine-learning-data-science-vm-do-ten-things/VSGit.PNG)</ph></target>
        </trans-unit>
        <trans-unit id="213" translate="yes" xml:space="preserve">
          <source>You can find more information on using Git to work with your Github repository by using several resources available on github.com.</source>
          <target state="new">You can find more information on using Git to work with your Github repository by using several resources available on github.com.</target>
        </trans-unit>
        <trans-unit id="214" translate="yes" xml:space="preserve">
          <source>You may find the <bpt id="p17">[</bpt>cheat sheet<ept id="p17">](https://training.github.com/kit/downloads/github-git-cheat-sheet.pdf)</ept><ph id="ph36" /> as a useful reference.</source>
          <target state="new">You may find the <bpt id="p17">[</bpt>cheat sheet<ept id="p17">](https://training.github.com/kit/downloads/github-git-cheat-sheet.pdf)</ept><ph id="ph36" /> as a useful reference.</target>
        </trans-unit>
        <trans-unit id="215" translate="yes" xml:space="preserve">
          <source>7. Access various Azure data and analytics services</source>
          <target state="new">7. Access various Azure data and analytics services</target>
        </trans-unit>
        <trans-unit id="216" translate="yes" xml:space="preserve">
          <source>Azure Blob</source>
          <target state="new">Azure Blob</target>
        </trans-unit>
        <trans-unit id="217" translate="yes" xml:space="preserve">
          <source><bpt id="p18">**</bpt>Prerequisite<ept id="p18">**</ept></source>
          <target state="new"><bpt id="p18">**</bpt>Prerequisite<ept id="p18">**</ept></target>
        </trans-unit>
        <trans-unit id="218" translate="yes" xml:space="preserve">
          <source><bpt id="p19">**</bpt>Create your Azure Blob storage account from <bpt id="p20">[</bpt>Azure Portal<ept id="p20">](http://portal.azure.com)</ept>.<ept id="p19">**</ept></source>
          <target state="new"><bpt id="p19">**</bpt>Create your Azure Blob storage account from <bpt id="p20">[</bpt>Azure Portal<ept id="p20">](http://portal.azure.com)</ept>.<ept id="p19">**</ept></target>
        </trans-unit>
        <trans-unit id="219" translate="yes" xml:space="preserve">
          <source><ph id="ph37">![</ph>Create_Azure_Blob<ph id="ph38">](./media/machine-learning-data-science-vm-do-ten-things/Create_Azure_Blob.PNG)</ph></source>
          <target state="new"><ph id="ph37">![</ph>Create_Azure_Blob<ph id="ph38">](./media/machine-learning-data-science-vm-do-ten-things/Create_Azure_Blob.PNG)</ph></target>
        </trans-unit>
        <trans-unit id="220" translate="yes" xml:space="preserve">
          <source>**Make sure the pre-installed AzCopy tool found at <ph id="ph39">```C:\Program Files (x86)\Microsoft SDKs\Azure\AzCopy\azcopy.exe```</ph><ph id="ph40" /> is added to your environment variable.</source>
          <target state="new">**Make sure the pre-installed AzCopy tool found at <ph id="ph39">```C:\Program Files (x86)\Microsoft SDKs\Azure\AzCopy\azcopy.exe```</ph><ph id="ph40" /> is added to your environment variable.</target>
        </trans-unit>
        <trans-unit id="221" translate="yes" xml:space="preserve">
          <source>For more info on AzCopy please refer to <bpt id="p21">[</bpt>AzCopy documentation<ept id="p21">](../storage/storage-use-azcopy.md)</ept></source>
          <target state="new">For more info on AzCopy please refer to <bpt id="p21">[</bpt>AzCopy documentation<ept id="p21">](../storage/storage-use-azcopy.md)</ept></target>
        </trans-unit>
        <trans-unit id="222" translate="yes" xml:space="preserve">
          <source><bpt id="p22">**</bpt>Start the Azure Storage Explorer from <bpt id="p23">[</bpt>here<ept id="p23">](https://azurestorageexplorer.codeplex.com/)</ept>.<ept id="p22">**</ept></source>
          <target state="new"><bpt id="p22">**</bpt>Start the Azure Storage Explorer from <bpt id="p23">[</bpt>here<ept id="p23">](https://azurestorageexplorer.codeplex.com/)</ept>.<ept id="p22">**</ept></target>
        </trans-unit>
        <trans-unit id="223" translate="yes" xml:space="preserve">
          <source><ph id="ph41">![</ph>AzureStorageExplorer_v4<ph id="ph42">](./media/machine-learning-data-science-vm-do-ten-things/AzureStorageExplorer_v4.png)</ph></source>
          <target state="new"><ph id="ph41">![</ph>AzureStorageExplorer_v4<ph id="ph42">](./media/machine-learning-data-science-vm-do-ten-things/AzureStorageExplorer_v4.png)</ph></target>
        </trans-unit>
        <trans-unit id="224" translate="yes" xml:space="preserve">
          <source><bpt id="p24">**</bpt>Move data from VM to Azure Blob: AzCopy<ept id="p24">**</ept></source>
          <target state="new"><bpt id="p24">**</bpt>Move data from VM to Azure Blob: AzCopy<ept id="p24">**</ept></target>
        </trans-unit>
        <trans-unit id="225" translate="yes" xml:space="preserve">
          <source>To move data between your local files and blob storage, you can use AzCopy in command line or PowerShell:
<ph id="ph43">`AzCopy /Source:C:\myfolder /Dest:https://&lt;mystorageaccount&gt;.blob.core.windows.net/&lt;mycontainer&gt; /DestKey:&lt;storage account key&gt; /Pattern:abc.txt`</ph></source>
          <target state="new">To move data between your local files and blob storage, you can use AzCopy in command line or PowerShell:
<ph id="ph43">`AzCopy /Source:C:\myfolder /Dest:https://&lt;mystorageaccount&gt;.blob.core.windows.net/&lt;mycontainer&gt; /DestKey:&lt;storage account key&gt; /Pattern:abc.txt`</ph></target>
        </trans-unit>
        <trans-unit id="226" translate="yes" xml:space="preserve">
          <source>Replace <bpt id="p25">**</bpt>C:\myfolder<ept id="p25">**</ept><ph id="ph44" /> to the path where your file is stored, <bpt id="p26">**</bpt>mystorageaccount<ept id="p26">**</ept><ph id="ph45" /> to your blob storage account name, <bpt id="p27">**</bpt>mycontainer<ept id="p27">**</ept><ph id="ph46" /> to the container name, <bpt id="p28">**</bpt>storage account key<ept id="p28">**</ept><ph id="ph47" /> to your blob storage access key.</source>
          <target state="new">Replace <bpt id="p25">**</bpt>C:\myfolder<ept id="p25">**</ept><ph id="ph44" /> to the path where your file is stored, <bpt id="p26">**</bpt>mystorageaccount<ept id="p26">**</ept><ph id="ph45" /> to your blob storage account name, <bpt id="p27">**</bpt>mycontainer<ept id="p27">**</ept><ph id="ph46" /> to the container name, <bpt id="p28">**</bpt>storage account key<ept id="p28">**</ept><ph id="ph47" /> to your blob storage access key.</target>
        </trans-unit>
        <trans-unit id="227" translate="yes" xml:space="preserve">
          <source>You can find your storage account credentials in <bpt id="p29">[</bpt>Azure Portal<ept id="p29">](http://portal.azure.com)</ept>.</source>
          <target state="new">You can find your storage account credentials in <bpt id="p29">[</bpt>Azure Portal<ept id="p29">](http://portal.azure.com)</ept>.</target>
        </trans-unit>
        <trans-unit id="228" translate="yes" xml:space="preserve">
          <source><ph id="ph48">![</ph>StorageAccountCredential_v2<ph id="ph49">](./media/machine-learning-data-science-vm-do-ten-things/StorageAccountCredential_v2.png)</ph></source>
          <target state="new"><ph id="ph48">![</ph>StorageAccountCredential_v2<ph id="ph49">](./media/machine-learning-data-science-vm-do-ten-things/StorageAccountCredential_v2.png)</ph></target>
        </trans-unit>
        <trans-unit id="229" translate="yes" xml:space="preserve">
          <source>Run AzCopy command in PowerShell or in command prompt.</source>
          <target state="new">Run AzCopy command in PowerShell or in command prompt.</target>
        </trans-unit>
        <trans-unit id="230" translate="yes" xml:space="preserve">
          <source>Here is some example usage of AzCopy command:</source>
          <target state="new">Here is some example usage of AzCopy command:</target>
        </trans-unit>
        <trans-unit id="231" translate="yes" xml:space="preserve">
          <source>Once you run your AzCopy command to copy to an Azure blob you see your file shows up in Azure Storage Explorer shortly.</source>
          <target state="new">Once you run your AzCopy command to copy to an Azure blob you see your file shows up in Azure Storage Explorer shortly.</target>
        </trans-unit>
        <trans-unit id="232" translate="yes" xml:space="preserve">
          <source><ph id="ph50">![</ph>AzCopy_run_finshed_Storage_Explorer_v3<ph id="ph51">](./media/machine-learning-data-science-vm-do-ten-things/AzCopy_run_finshed_Storage_Explorer_v3.png)</ph></source>
          <target state="new"><ph id="ph50">![</ph>AzCopy_run_finshed_Storage_Explorer_v3<ph id="ph51">](./media/machine-learning-data-science-vm-do-ten-things/AzCopy_run_finshed_Storage_Explorer_v3.png)</ph></target>
        </trans-unit>
        <trans-unit id="233" translate="yes" xml:space="preserve">
          <source><bpt id="p30">**</bpt>Move data from VM to Azure Blob: Azure Storage Explorer<ept id="p30">**</ept></source>
          <target state="new"><bpt id="p30">**</bpt>Move data from VM to Azure Blob: Azure Storage Explorer<ept id="p30">**</ept></target>
        </trans-unit>
        <trans-unit id="234" translate="yes" xml:space="preserve">
          <source>You can also upload data from the local file in your VM using Azure Storage Explorer:</source>
          <target state="new">You can also upload data from the local file in your VM using Azure Storage Explorer:</target>
        </trans-unit>
        <trans-unit id="235" translate="yes" xml:space="preserve">
          <source><bpt id="p31">**</bpt>Read data from Azure Blob: AML reader module<ept id="p31">**</ept></source>
          <target state="new"><bpt id="p31">**</bpt>Read data from Azure Blob: AML reader module<ept id="p31">**</ept></target>
        </trans-unit>
        <trans-unit id="236" translate="yes" xml:space="preserve">
          <source>In Azure Machine Learning Studio you can use a <bpt id="p32">**</bpt>Reader module<ept id="p32">**</ept><ph id="ph53" /> to read data from your blob.</source>
          <target state="new">In Azure Machine Learning Studio you can use a <bpt id="p32">**</bpt>Reader module<ept id="p32">**</ept><ph id="ph53" /> to read data from your blob.</target>
        </trans-unit>
        <trans-unit id="237" translate="yes" xml:space="preserve">
          <source><ph id="ph54">![</ph>AML_ReaderBlob_Module_v3<ph id="ph55">](./media/machine-learning-data-science-vm-do-ten-things/AML_ReaderBlob_Module_v3.png)</ph></source>
          <target state="new"><ph id="ph54">![</ph>AML_ReaderBlob_Module_v3<ph id="ph55">](./media/machine-learning-data-science-vm-do-ten-things/AML_ReaderBlob_Module_v3.png)</ph></target>
        </trans-unit>
        <trans-unit id="238" translate="yes" xml:space="preserve">
          <source><bpt id="p33">**</bpt>Read data from Azure Blob: Python ODBC<ept id="p33">**</ept></source>
          <target state="new"><bpt id="p33">**</bpt>Read data from Azure Blob: Python ODBC<ept id="p33">**</ept></target>
        </trans-unit>
        <trans-unit id="239" translate="yes" xml:space="preserve">
          <source>In the IPython Notebook <bpt id="p34">**</bpt>NYC Data wrangling using IPython Notebook and Azure Blob Storage<ept id="p34">**</ept>, you can use <bpt id="p35">**</bpt>pyodbc<ept id="p35">**</ept><ph id="ph56" /> package to read data directly from blob.</source>
          <target state="new">In the IPython Notebook <bpt id="p34">**</bpt>NYC Data wrangling using IPython Notebook and Azure Blob Storage<ept id="p34">**</ept>, you can use <bpt id="p35">**</bpt>pyodbc<ept id="p35">**</ept><ph id="ph56" /> package to read data directly from blob.</target>
        </trans-unit>
        <trans-unit id="240" translate="yes" xml:space="preserve">
          <source>First, import required packages:</source>
          <target state="new">First, import required packages:</target>
        </trans-unit>
        <trans-unit id="241" translate="yes" xml:space="preserve">
          <source>Then plug in your Azure Blob account credentials and read data from Blob:</source>
          <target state="new">Then plug in your Azure Blob account credentials and read data from Blob:</target>
        </trans-unit>
        <trans-unit id="242" translate="yes" xml:space="preserve">
          <source>You can see the data is read in as a data frame:</source>
          <target state="new">You can see the data is read in as a data frame:</target>
        </trans-unit>
        <trans-unit id="243" translate="yes" xml:space="preserve">
          <source><ph id="ph57">![</ph>IPNB_data_readin<ph id="ph58">](./media/machine-learning-data-science-vm-do-ten-things/IPNB_data_readin.PNG)</ph></source>
          <target state="new"><ph id="ph57">![</ph>IPNB_data_readin<ph id="ph58">](./media/machine-learning-data-science-vm-do-ten-things/IPNB_data_readin.PNG)</ph></target>
        </trans-unit>
        <trans-unit id="244" translate="yes" xml:space="preserve">
          <source>Azure Data Lake</source>
          <target state="new">Azure Data Lake</target>
        </trans-unit>
        <trans-unit id="245" translate="yes" xml:space="preserve">
          <source><bpt id="p36">**</bpt>Prerequisite<ept id="p36">**</ept></source>
          <target state="new"><bpt id="p36">**</bpt>Prerequisite<ept id="p36">**</ept></target>
        </trans-unit>
        <trans-unit id="246" translate="yes" xml:space="preserve">
          <source>Create your Azure Data Lake Analytics in <bpt id="p37">[</bpt>Azure Portal<ept id="p37">](http://portal.azure.com)</ept>.</source>
          <target state="new">Create your Azure Data Lake Analytics in <bpt id="p37">[</bpt>Azure Portal<ept id="p37">](http://portal.azure.com)</ept>.</target>
        </trans-unit>
        <trans-unit id="247" translate="yes" xml:space="preserve">
          <source><ph id="ph59">![</ph>Azure_Data_Lake_Create_v2<ph id="ph60">](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Lake_Create_v2.png)</ph></source>
          <target state="new"><ph id="ph59">![</ph>Azure_Data_Lake_Create_v2<ph id="ph60">](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Lake_Create_v2.png)</ph></target>
        </trans-unit>
        <trans-unit id="248" translate="yes" xml:space="preserve">
          <source>The  <bpt id="p38">**</bpt>Azure Data Lake Tools<ept id="p38">**</ept><ph id="ph61" /> in <bpt id="p39">**</bpt>Visual Studio<ept id="p39">**</ept><ph id="ph62" /> found at this  <bpt id="p40">[</bpt>link<ept id="p40">](https://www.microsoft.com/download/details.aspx?id=49504)</ept><ph id="ph63" /> is already installed on the Visual Studio Community Edition which is on the virtual machione.</source>
          <target state="new">The  <bpt id="p38">**</bpt>Azure Data Lake Tools<ept id="p38">**</ept><ph id="ph61" /> in <bpt id="p39">**</bpt>Visual Studio<ept id="p39">**</ept><ph id="ph62" /> found at this  <bpt id="p40">[</bpt>link<ept id="p40">](https://www.microsoft.com/download/details.aspx?id=49504)</ept><ph id="ph63" /> is already installed on the Visual Studio Community Edition which is on the virtual machione.</target>
        </trans-unit>
        <trans-unit id="249" translate="yes" xml:space="preserve">
          <source>After starting Visual Studio and logging in your Azure subscription, you will see your Azure Data Analytics account and storage in the left panel of Visual Studio.</source>
          <target state="new">After starting Visual Studio and logging in your Azure subscription, you will see your Azure Data Analytics account and storage in the left panel of Visual Studio.</target>
        </trans-unit>
        <trans-unit id="250" translate="yes" xml:space="preserve">
          <source><ph id="ph64">![</ph>Azure_Data_Lake_PlugIn_v2<ph id="ph65">](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Lake_PlugIn_v2.PNG)</ph></source>
          <target state="new"><ph id="ph64">![</ph>Azure_Data_Lake_PlugIn_v2<ph id="ph65">](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Lake_PlugIn_v2.PNG)</ph></target>
        </trans-unit>
        <trans-unit id="251" translate="yes" xml:space="preserve">
          <source>Install <bpt id="p41">**</bpt>Data Management Gateway<ept id="p41">**</ept><ph id="ph66" /> following this <bpt id="p42">[</bpt>document<ept id="p42">](../data-factory/data-factory-move-data-between-onprem-and-cloud.md)</ept>.</source>
          <target state="new">Install <bpt id="p41">**</bpt>Data Management Gateway<ept id="p41">**</ept><ph id="ph66" /> following this <bpt id="p42">[</bpt>document<ept id="p42">](../data-factory/data-factory-move-data-between-onprem-and-cloud.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="252" translate="yes" xml:space="preserve">
          <source><ph id="ph67">![</ph>Azure_Data_Gateway_v2<ph id="ph68">](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Gateway_v2.PNG)</ph></source>
          <target state="new"><ph id="ph67">![</ph>Azure_Data_Gateway_v2<ph id="ph68">](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Gateway_v2.PNG)</ph></target>
        </trans-unit>
        <trans-unit id="253" translate="yes" xml:space="preserve">
          <source>Share the local folder where your data is stored to everyone so that Azure Data Gateway can access it.</source>
          <target state="new">Share the local folder where your data is stored to everyone so that Azure Data Gateway can access it.</target>
        </trans-unit>
        <trans-unit id="254" translate="yes" xml:space="preserve">
          <source><ph id="ph69">![</ph>Share_Folder<ph id="ph70">](./media/machine-learning-data-science-vm-do-ten-things/Share_Folder.PNG)</ph></source>
          <target state="new"><ph id="ph69">![</ph>Share_Folder<ph id="ph70">](./media/machine-learning-data-science-vm-do-ten-things/Share_Folder.PNG)</ph></target>
        </trans-unit>
        <trans-unit id="255" translate="yes" xml:space="preserve">
          <source><bpt id="p43">**</bpt>Move data from VM to Data Lake: Azure Data Lake Explorer<ept id="p43">**</ept></source>
          <target state="new"><bpt id="p43">**</bpt>Move data from VM to Data Lake: Azure Data Lake Explorer<ept id="p43">**</ept></target>
        </trans-unit>
        <trans-unit id="256" translate="yes" xml:space="preserve">
          <source>You can use <bpt id="p44">**</bpt>Azure Data Lake Explorer<ept id="p44">**</ept><ph id="ph71" /> to upload data from the local files in your Virtual Machine to Data Lake storage.</source>
          <target state="new">You can use <bpt id="p44">**</bpt>Azure Data Lake Explorer<ept id="p44">**</ept><ph id="ph71" /> to upload data from the local files in your Virtual Machine to Data Lake storage.</target>
        </trans-unit>
        <trans-unit id="257" translate="yes" xml:space="preserve">
          <source><ph id="ph72">![</ph>Azure_Data_Lake_UploadData<ph id="ph73">](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Lake_UploadData.PNG)</ph></source>
          <target state="new"><ph id="ph72">![</ph>Azure_Data_Lake_UploadData<ph id="ph73">](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Lake_UploadData.PNG)</ph></target>
        </trans-unit>
        <trans-unit id="258" translate="yes" xml:space="preserve">
          <source><bpt id="p45">**</bpt>Move data from VM to Data Lake: Azure Data Factory<ept id="p45">**</ept></source>
          <target state="new"><bpt id="p45">**</bpt>Move data from VM to Data Lake: Azure Data Factory<ept id="p45">**</ept></target>
        </trans-unit>
        <trans-unit id="259" translate="yes" xml:space="preserve">
          <source>Azure Data Factory is service to create, schedule, and manage data pipelines.</source>
          <target state="new">Azure Data Factory is service to create, schedule, and manage data pipelines.</target>
        </trans-unit>
        <trans-unit id="260" translate="yes" xml:space="preserve">
          <source>We can use Azure Data Factory to move data between different storage.</source>
          <target state="new">We can use Azure Data Factory to move data between different storage.</target>
        </trans-unit>
        <trans-unit id="261" translate="yes" xml:space="preserve">
          <source>Create <bpt id="p46">[</bpt>Azure Data Factory<ept id="p46">](https://azure.microsoft.com/services/data-factory/)</ept><ph id="ph74" /> in <bpt id="p47">[</bpt>Azure Portal<ept id="p47">](http://portal.azure.com)</ept>:</source>
          <target state="new">Create <bpt id="p46">[</bpt>Azure Data Factory<ept id="p46">](https://azure.microsoft.com/services/data-factory/)</ept><ph id="ph74" /> in <bpt id="p47">[</bpt>Azure Portal<ept id="p47">](http://portal.azure.com)</ept>:</target>
        </trans-unit>
        <trans-unit id="262" translate="yes" xml:space="preserve">
          <source><ph id="ph75">![</ph>Azure_Data_Factory_Create<ph id="ph76">](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Factory_Create.PNG)</ph></source>
          <target state="new"><ph id="ph75">![</ph>Azure_Data_Factory_Create<ph id="ph76">](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Factory_Create.PNG)</ph></target>
        </trans-unit>
        <trans-unit id="263" translate="yes" xml:space="preserve">
          <source>Once it is created, you can build pipelines to move data between different storage.</source>
          <target state="new">Once it is created, you can build pipelines to move data between different storage.</target>
        </trans-unit>
        <trans-unit id="264" translate="yes" xml:space="preserve">
          <source>In <bpt id="p48">**</bpt>Author and deploy<ept id="p48">**</ept><ph id="ph77" /> you can specify datasets to transfer and setup your pipelines.</source>
          <target state="new">In <bpt id="p48">**</bpt>Author and deploy<ept id="p48">**</ept><ph id="ph77" /> you can specify datasets to transfer and setup your pipelines.</target>
        </trans-unit>
        <trans-unit id="265" translate="yes" xml:space="preserve">
          <source><ph id="ph78">![</ph>Azure_Data_Factory_Overview_v4<ph id="ph79">](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Factory_Overview_v4.PNG)</ph></source>
          <target state="new"><ph id="ph78">![</ph>Azure_Data_Factory_Overview_v4<ph id="ph79">](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Factory_Overview_v4.PNG)</ph></target>
        </trans-unit>
        <trans-unit id="266" translate="yes" xml:space="preserve">
          <source>The major steps to move data from Virtual Machine to Azure Data Lake are as follows.</source>
          <target state="new">The major steps to move data from Virtual Machine to Azure Data Lake are as follows.</target>
        </trans-unit>
        <trans-unit id="267" translate="yes" xml:space="preserve">
          <source>Details about moving data using Azure Data Factory can be found <bpt id="p49">[</bpt>here<ept id="p49">](../data-factory/data-factory-data-movement-activities.md)</ept>.</source>
          <target state="new">Details about moving data using Azure Data Factory can be found <bpt id="p49">[</bpt>here<ept id="p49">](../data-factory/data-factory-data-movement-activities.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="268" translate="yes" xml:space="preserve">
          <source>The JSON files below will be in your Data Science VM.</source>
          <target state="new">The JSON files below will be in your Data Science VM.</target>
        </trans-unit>
        <trans-unit id="269" translate="yes" xml:space="preserve">
          <source><bpt id="p50">**</bpt>Create Linked Services<ept id="p50">**</ept></source>
          <target state="new"><bpt id="p50">**</bpt>Create Linked Services<ept id="p50">**</ept></target>
        </trans-unit>
        <trans-unit id="270" translate="yes" xml:space="preserve">
          <source>Click <bpt id="p51">**</bpt>New Data Store<ept id="p51">**</ept><ph id="ph80" /> then choose <bpt id="p52">**</bpt>Azure Data Lake Storage<ept id="p52">**</ept>, plug in your credentials and parameters in the JSON file.</source>
          <target state="new">Click <bpt id="p51">**</bpt>New Data Store<ept id="p51">**</ept><ph id="ph80" /> then choose <bpt id="p52">**</bpt>Azure Data Lake Storage<ept id="p52">**</ept>, plug in your credentials and parameters in the JSON file.</target>
        </trans-unit>
        <trans-unit id="271" translate="yes" xml:space="preserve">
          <source>Click <bpt id="p53">**</bpt>New Data Store<ept id="p53">**</ept><ph id="ph81" /> then choose <bpt id="p54">**</bpt>File System<ept id="p54">**</ept>, plug in your credentials and parameters in the JSON file.</source>
          <target state="new">Click <bpt id="p53">**</bpt>New Data Store<ept id="p53">**</ept><ph id="ph81" /> then choose <bpt id="p54">**</bpt>File System<ept id="p54">**</ept>, plug in your credentials and parameters in the JSON file.</target>
        </trans-unit>
        <trans-unit id="272" translate="yes" xml:space="preserve">
          <source><bpt id="p55">**</bpt>Create Datasets<ept id="p55">**</ept></source>
          <target state="new"><bpt id="p55">**</bpt>Create Datasets<ept id="p55">**</ept></target>
        </trans-unit>
        <trans-unit id="273" translate="yes" xml:space="preserve">
          <source>Click <bpt id="p56">**</bpt>New dataset<ept id="p56">**</ept><ph id="ph82" /> then choose <bpt id="p57">**</bpt>Azure Data Lake<ept id="p57">**</ept>, plug in <bpt id="p58">**</bpt>Linked Service<ept id="p58">**</ept><ph id="ph83" /> name and <bpt id="p59">**</bpt>folder path<ept id="p59">**</ept><ph id="ph84" /> in the JSON file.</source>
          <target state="new">Click <bpt id="p56">**</bpt>New dataset<ept id="p56">**</ept><ph id="ph82" /> then choose <bpt id="p57">**</bpt>Azure Data Lake<ept id="p57">**</ept>, plug in <bpt id="p58">**</bpt>Linked Service<ept id="p58">**</ept><ph id="ph83" /> name and <bpt id="p59">**</bpt>folder path<ept id="p59">**</ept><ph id="ph84" /> in the JSON file.</target>
        </trans-unit>
        <trans-unit id="274" translate="yes" xml:space="preserve">
          <source>Click <bpt id="p60">**</bpt>New dataset<ept id="p60">**</ept><ph id="ph85" /> then choose <bpt id="p61">**</bpt>On-premises file<ept id="p61">**</ept>, plug in <bpt id="p62">**</bpt>dataset schema<ept id="p62">**</ept>, <bpt id="p63">**</bpt>Linked Service<ept id="p63">**</ept><ph id="ph86" /> name, <bpt id="p64">**</bpt>file name<ept id="p64">**</ept>, and <bpt id="p65">**</bpt>folder path<ept id="p65">**</ept><ph id="ph87" /> in the JSON file.</source>
          <target state="new">Click <bpt id="p60">**</bpt>New dataset<ept id="p60">**</ept><ph id="ph85" /> then choose <bpt id="p61">**</bpt>On-premises file<ept id="p61">**</ept>, plug in <bpt id="p62">**</bpt>dataset schema<ept id="p62">**</ept>, <bpt id="p63">**</bpt>Linked Service<ept id="p63">**</ept><ph id="ph86" /> name, <bpt id="p64">**</bpt>file name<ept id="p64">**</ept>, and <bpt id="p65">**</bpt>folder path<ept id="p65">**</ept><ph id="ph87" /> in the JSON file.</target>
        </trans-unit>
        <trans-unit id="275" translate="yes" xml:space="preserve">
          <source><bpt id="p66">**</bpt>Create Pipelines<ept id="p66">**</ept></source>
          <target state="new"><bpt id="p66">**</bpt>Create Pipelines<ept id="p66">**</ept></target>
        </trans-unit>
        <trans-unit id="276" translate="yes" xml:space="preserve">
          <source>Click <bpt id="p67">**</bpt>New pipeline<ept id="p67">**</ept>, specify the <bpt id="p68">**</bpt>input and output datasets<ept id="p68">**</ept>, <bpt id="p69">**</bpt>activity type<ept id="p69">**</ept>, and etc. in the JSON file.</source>
          <target state="new">Click <bpt id="p67">**</bpt>New pipeline<ept id="p67">**</ept>, specify the <bpt id="p68">**</bpt>input and output datasets<ept id="p68">**</ept>, <bpt id="p69">**</bpt>activity type<ept id="p69">**</ept>, and etc. in the JSON file.</target>
        </trans-unit>
        <trans-unit id="277" translate="yes" xml:space="preserve">
          <source><bpt id="p70">**</bpt>Create Data GateWays<ept id="p70">**</ept></source>
          <target state="new"><bpt id="p70">**</bpt>Create Data GateWays<ept id="p70">**</ept></target>
        </trans-unit>
        <trans-unit id="278" translate="yes" xml:space="preserve">
          <source>Register your gateway key and make sure the registration is <bpt id="p71">**</bpt>Registered<ept id="p71">**</ept><ph id="ph88" /> and status is <bpt id="p72">**</bpt>Started<ept id="p72">**</ept>.</source>
          <target state="new">Register your gateway key and make sure the registration is <bpt id="p71">**</bpt>Registered<ept id="p71">**</ept><ph id="ph88" /> and status is <bpt id="p72">**</bpt>Started<ept id="p72">**</ept>.</target>
        </trans-unit>
        <trans-unit id="279" translate="yes" xml:space="preserve">
          <source>Click <bpt id="p73">**</bpt>New data gateway<ept id="p73">**</ept>, fill in **data gateway name **, the <bpt id="p74">**</bpt>Configure<ept id="p74">**</ept><ph id="ph89" /> part will setup automatically if your data gateway is installed properly in the previous steps.</source>
          <target state="new">Click <bpt id="p73">**</bpt>New data gateway<ept id="p73">**</ept>, fill in **data gateway name **, the <bpt id="p74">**</bpt>Configure<ept id="p74">**</ept><ph id="ph89" /> part will setup automatically if your data gateway is installed properly in the previous steps.</target>
        </trans-unit>
        <trans-unit id="280" translate="yes" xml:space="preserve">
          <source><ph id="ph90">![</ph>Azure_Data_Gateway_part2_v2<ph id="ph91">](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Gateway_part2_v2.png)</ph></source>
          <target state="new"><ph id="ph90">![</ph>Azure_Data_Gateway_part2_v2<ph id="ph91">](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Gateway_part2_v2.png)</ph></target>
        </trans-unit>
        <trans-unit id="281" translate="yes" xml:space="preserve">
          <source><ph id="ph92">![</ph>Azure_Data_Factory_Template<ph id="ph93">](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Factory_Template.PNG)</ph></source>
          <target state="new"><ph id="ph92">![</ph>Azure_Data_Factory_Template<ph id="ph93">](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Factory_Template.PNG)</ph></target>
        </trans-unit>
        <trans-unit id="282" translate="yes" xml:space="preserve">
          <source><ph id="ph94">![</ph>Azure_Data_Factory_Template_Json_v2<ph id="ph95">](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Factory_Template_Json_v2.PNG)</ph></source>
          <target state="new"><ph id="ph94">![</ph>Azure_Data_Factory_Template_Json_v2<ph id="ph95">](./media/machine-learning-data-science-vm-do-ten-things/Azure_Data_Factory_Template_Json_v2.PNG)</ph></target>
        </trans-unit>
        <trans-unit id="283" translate="yes" xml:space="preserve">
          <source>The JSON files used in the above steps are:</source>
          <target state="new">The JSON files used in the above steps are:</target>
        </trans-unit>
        <trans-unit id="284" translate="yes" xml:space="preserve">
          <source><bpt id="p75">**</bpt>Linked services: AzureDataLakeStoreLinkedService<ept id="p75">**</ept></source>
          <target state="new"><bpt id="p75">**</bpt>Linked services: AzureDataLakeStoreLinkedService<ept id="p75">**</ept></target>
        </trans-unit>
        <trans-unit id="285" translate="yes" xml:space="preserve">
          <source><bpt id="p76">**</bpt>Linked services: OnPremisesFileServerLinkedService<ept id="p76">**</ept></source>
          <target state="new"><bpt id="p76">**</bpt>Linked services: OnPremisesFileServerLinkedService<ept id="p76">**</ept></target>
        </trans-unit>
        <trans-unit id="286" translate="yes" xml:space="preserve">
          <source><bpt id="p77">**</bpt>Datasets: OnPremisesFile<ept id="p77">**</ept></source>
          <target state="new"><bpt id="p77">**</bpt>Datasets: OnPremisesFile<ept id="p77">**</ept></target>
        </trans-unit>
        <trans-unit id="287" translate="yes" xml:space="preserve">
          <source><bpt id="p78">**</bpt>Datasets: weiglakenew1<ept id="p78">**</ept></source>
          <target state="new"><bpt id="p78">**</bpt>Datasets: weiglakenew1<ept id="p78">**</ept></target>
        </trans-unit>
        <trans-unit id="288" translate="yes" xml:space="preserve">
          <source><bpt id="p79">**</bpt>Datasets: vmtodatalake_tripdata<ept id="p79">**</ept></source>
          <target state="new"><bpt id="p79">**</bpt>Datasets: vmtodatalake_tripdata<ept id="p79">**</ept></target>
        </trans-unit>
        <trans-unit id="289" translate="yes" xml:space="preserve">
          <source><bpt id="p80">**</bpt>Data Gateways: weiggateway<ept id="p80">**</ept></source>
          <target state="new"><bpt id="p80">**</bpt>Data Gateways: weiggateway<ept id="p80">**</ept></target>
        </trans-unit>
        <trans-unit id="290" translate="yes" xml:space="preserve">
          <source>After the pipeline is built, you can look at the pipeline in the <bpt id="p81">**</bpt>Diagram<ept id="p81">**</ept><ph id="ph96" /> in the Azure Data Factory dashboard.</source>
          <target state="new">After the pipeline is built, you can look at the pipeline in the <bpt id="p81">**</bpt>Diagram<ept id="p81">**</ept><ph id="ph96" /> in the Azure Data Factory dashboard.</target>
        </trans-unit>
        <trans-unit id="291" translate="yes" xml:space="preserve">
          <source>Click the <bpt id="p82">**</bpt>Diagram<ept id="p82">**</ept><ph id="ph98" /> box to see the pipeline:</source>
          <target state="new">Click the <bpt id="p82">**</bpt>Diagram<ept id="p82">**</ept><ph id="ph98" /> box to see the pipeline:</target>
        </trans-unit>
        <trans-unit id="292" translate="yes" xml:space="preserve">
          <source>You can monitor the data pipeline in <bpt id="p83">**</bpt>Contents<ept id="p83">**</ept>-&gt;<bpt id="p84">**</bpt>Datasets<ept id="p84">**</ept>-&gt;<bpt id="p85">**</bpt>Monitoring<ept id="p85">**</ept></source>
          <target state="new">You can monitor the data pipeline in <bpt id="p83">**</bpt>Contents<ept id="p83">**</ept>-&gt;<bpt id="p84">**</bpt>Datasets<ept id="p84">**</ept>-&gt;<bpt id="p85">**</bpt>Monitoring<ept id="p85">**</ept></target>
        </trans-unit>
        <trans-unit id="293" translate="yes" xml:space="preserve">
          <source>You may also check if the data is moved to Azure Data Lake using <bpt id="p86">**</bpt>Azure Data Lake Explorer<ept id="p86">**</ept><ph id="ph101" /> in <bpt id="p87">**</bpt>Visual Studio<ept id="p87">**</ept>.</source>
          <target state="new">You may also check if the data is moved to Azure Data Lake using <bpt id="p86">**</bpt>Azure Data Lake Explorer<ept id="p86">**</ept><ph id="ph101" /> in <bpt id="p87">**</bpt>Visual Studio<ept id="p87">**</ept>.</target>
        </trans-unit>
        <trans-unit id="294" translate="yes" xml:space="preserve">
          <source><bpt id="p88">**</bpt>Read data from Azure Blob to Data Lake: U-SQL<ept id="p88">**</ept></source>
          <target state="new"><bpt id="p88">**</bpt>Read data from Azure Blob to Data Lake: U-SQL<ept id="p88">**</ept></target>
        </trans-unit>
        <trans-unit id="295" translate="yes" xml:space="preserve">
          <source>If your data resides in Azure Blob storage, you can directly read data from Azure storage blob in U-SQL query.</source>
          <target state="new">If your data resides in Azure Blob storage, you can directly read data from Azure storage blob in U-SQL query.</target>
        </trans-unit>
        <trans-unit id="296" translate="yes" xml:space="preserve">
          <source>Before composing your U-SQL query, make sure your blob storage account is linked to your Azure Data Lake.</source>
          <target state="new">Before composing your U-SQL query, make sure your blob storage account is linked to your Azure Data Lake.</target>
        </trans-unit>
        <trans-unit id="297" translate="yes" xml:space="preserve">
          <source>Go to <bpt id="p89">**</bpt>Azure Portal<ept id="p89">**</ept>, find your Azure Data Lake Analytics dashboard, click <bpt id="p90">**</bpt>Add Data Source<ept id="p90">**</ept>, select storage type to <bpt id="p91">**</bpt>Azure Storage<ept id="p91">**</ept><ph id="ph103" /> and plug in your Azure Storage Account Name and Key.</source>
          <target state="new">Go to <bpt id="p89">**</bpt>Azure Portal<ept id="p89">**</ept>, find your Azure Data Lake Analytics dashboard, click <bpt id="p90">**</bpt>Add Data Source<ept id="p90">**</ept>, select storage type to <bpt id="p91">**</bpt>Azure Storage<ept id="p91">**</ept><ph id="ph103" /> and plug in your Azure Storage Account Name and Key.</target>
        </trans-unit>
        <trans-unit id="298" translate="yes" xml:space="preserve">
          <source>Then you will be able to reference the data stored in the storage account.</source>
          <target state="new">Then you will be able to reference the data stored in the storage account.</target>
        </trans-unit>
        <trans-unit id="299" translate="yes" xml:space="preserve">
          <source>In Visual Studio, you can read data from blob, do some data manipulation, feature engineering, and output the resulting data to either Azure Data Lake or Azure Blob Storage.</source>
          <target state="new">In Visual Studio, you can read data from blob, do some data manipulation, feature engineering, and output the resulting data to either Azure Data Lake or Azure Blob Storage.</target>
        </trans-unit>
        <trans-unit id="300" translate="yes" xml:space="preserve">
          <source>When you reference the data in blob storage, use <bpt id="p92">**</bpt>wasb://<ept id="p92">**</ept>; when you reference the data in Azure Data Lake, use <bpt id="p93">**</bpt>swbhdfs://<ept id="p93">**</ept></source>
          <target state="new">When you reference the data in blob storage, use <bpt id="p92">**</bpt>wasb://<ept id="p92">**</ept>; when you reference the data in Azure Data Lake, use <bpt id="p93">**</bpt>swbhdfs://<ept id="p93">**</ept></target>
        </trans-unit>
        <trans-unit id="301" translate="yes" xml:space="preserve">
          <source>You may use the following U-SQL queries in Visual Studio:</source>
          <target state="new">You may use the following U-SQL queries in Visual Studio:</target>
        </trans-unit>
        <trans-unit id="302" translate="yes" xml:space="preserve">
          <source>After your query is submitted to the server, a diagram showing the status of your job will be displayed.</source>
          <target state="new">After your query is submitted to the server, a diagram showing the status of your job will be displayed.</target>
        </trans-unit>
        <trans-unit id="303" translate="yes" xml:space="preserve">
          <source><bpt id="p94">**</bpt>Query data in Data Lake: U-SQL<ept id="p94">**</ept></source>
          <target state="new"><bpt id="p94">**</bpt>Query data in Data Lake: U-SQL<ept id="p94">**</ept></target>
        </trans-unit>
        <trans-unit id="304" translate="yes" xml:space="preserve">
          <source>After the dataset is ingested into Azure Data Lake, you can use <bpt id="p95">[</bpt>U-SQL language<ept id="p95">](../data-lake-analytics/data-lake-analytics-u-sql-get-started.md)</ept><ph id="ph107" /> to query and explore the data.</source>
          <target state="new">After the dataset is ingested into Azure Data Lake, you can use <bpt id="p95">[</bpt>U-SQL language<ept id="p95">](../data-lake-analytics/data-lake-analytics-u-sql-get-started.md)</ept><ph id="ph107" /> to query and explore the data.</target>
        </trans-unit>
        <trans-unit id="305" translate="yes" xml:space="preserve">
          <source>U-SQL language is similar to T-SQL, but combines some features from C# so that users can write customized modules, User Defined Functions, and etc. You can use the scripts in the previous step.</source>
          <target state="new">U-SQL language is similar to T-SQL, but combines some features from C# so that users can write customized modules, User Defined Functions, and etc. You can use the scripts in the previous step.</target>
        </trans-unit>
        <trans-unit id="306" translate="yes" xml:space="preserve">
          <source>After the query is submitted to server, tripdata_summary.CSV can be found shortly in <bpt id="p96">**</bpt>Azure Data Lake Explorer<ept id="p96">**</ept>, you may preview the data by right click the file.</source>
          <target state="new">After the query is submitted to server, tripdata_summary.CSV can be found shortly in <bpt id="p96">**</bpt>Azure Data Lake Explorer<ept id="p96">**</ept>, you may preview the data by right click the file.</target>
        </trans-unit>
        <trans-unit id="307" translate="yes" xml:space="preserve">
          <source>To see the file information:</source>
          <target state="new">To see the file information:</target>
        </trans-unit>
        <trans-unit id="308" translate="yes" xml:space="preserve">
          <source>HDInsight Hadoop Clusters</source>
          <target state="new">HDInsight Hadoop Clusters</target>
        </trans-unit>
        <trans-unit id="309" translate="yes" xml:space="preserve">
          <source><bpt id="p97">**</bpt>Prerequisite<ept id="p97">**</ept></source>
          <target state="new"><bpt id="p97">**</bpt>Prerequisite<ept id="p97">**</ept></target>
        </trans-unit>
        <trans-unit id="310" translate="yes" xml:space="preserve">
          <source>Create your Azure Blob storage account from <bpt id="p98">[</bpt>Azure Portal<ept id="p98">](http://portal.azure.com)</ept>.</source>
          <target state="new">Create your Azure Blob storage account from <bpt id="p98">[</bpt>Azure Portal<ept id="p98">](http://portal.azure.com)</ept>.</target>
        </trans-unit>
        <trans-unit id="311" translate="yes" xml:space="preserve">
          <source>This storage account is used to store data for HDInsight clusters.</source>
          <target state="new">This storage account is used to store data for HDInsight clusters.</target>
        </trans-unit>
        <trans-unit id="312" translate="yes" xml:space="preserve">
          <source>Customize Azure HDInsight Hadoop Clusters from <bpt id="p99">[</bpt>Azure Portal<ept id="p99">](machine-learning-data-science-customize-hadoop-cluster.md)</ept></source>
          <target state="new">Customize Azure HDInsight Hadoop Clusters from <bpt id="p99">[</bpt>Azure Portal<ept id="p99">](machine-learning-data-science-customize-hadoop-cluster.md)</ept></target>
        </trans-unit>
        <trans-unit id="313" translate="yes" xml:space="preserve">
          <source>You must link the storage account created with your HDInsight cluster when it is created.</source>
          <target state="new">You must link the storage account created with your HDInsight cluster when it is created.</target>
        </trans-unit>
        <trans-unit id="314" translate="yes" xml:space="preserve">
          <source>This storage account is used for accessing data that can be processed within the cluster.</source>
          <target state="new">This storage account is used for accessing data that can be processed within the cluster.</target>
        </trans-unit>
        <trans-unit id="315" translate="yes" xml:space="preserve">
          <source>Or you can follow this <bpt id="p100">[</bpt>walkthrough<ept id="p100">](machine-learning-data-science-process-hive-walkthrough.md)</ept><ph id="ph115" /> to upload NYC Taxi data to HDI cluster.</source>
          <target state="new">Or you can follow this <bpt id="p100">[</bpt>walkthrough<ept id="p100">](machine-learning-data-science-process-hive-walkthrough.md)</ept><ph id="ph115" /> to upload NYC Taxi data to HDI cluster.</target>
        </trans-unit>
        <trans-unit id="316" translate="yes" xml:space="preserve">
          <source>Major steps include:</source>
          <target state="new">Major steps include:</target>
        </trans-unit>
        <trans-unit id="317" translate="yes" xml:space="preserve">
          <source>AzCopy: download zipped CSV's from public blob to your local folder</source>
          <target state="new">AzCopy: download zipped CSV's from public blob to your local folder</target>
        </trans-unit>
        <trans-unit id="318" translate="yes" xml:space="preserve">
          <source>AzCopy: upload unzipped CSV's from local folder to HDI cluster</source>
          <target state="new">AzCopy: upload unzipped CSV's from local folder to HDI cluster</target>
        </trans-unit>
        <trans-unit id="319" translate="yes" xml:space="preserve">
          <source>Log into the head node of Hadoop cluster and prepare for exploratory data analysis</source>
          <target state="new">Log into the head node of Hadoop cluster and prepare for exploratory data analysis</target>
        </trans-unit>
        <trans-unit id="320" translate="yes" xml:space="preserve">
          <source>After the data is loaded to HDI cluster, you can check your data in Azure Storage Explore.</source>
          <target state="new">After the data is loaded to HDI cluster, you can check your data in Azure Storage Explore.</target>
        </trans-unit>
        <trans-unit id="321" translate="yes" xml:space="preserve">
          <source>And you have a database nyctaxidb created in HDI cluster.</source>
          <target state="new">And you have a database nyctaxidb created in HDI cluster.</target>
        </trans-unit>
        <trans-unit id="322" translate="yes" xml:space="preserve">
          <source><bpt id="p101">**</bpt>Data exploration: Hive Queries in Python<ept id="p101">**</ept></source>
          <target state="new"><bpt id="p101">**</bpt>Data exploration: Hive Queries in Python<ept id="p101">**</ept></target>
        </trans-unit>
        <trans-unit id="323" translate="yes" xml:space="preserve">
          <source>Since the data is in Hadoop cluster, you can use pyodbc package to connect to Hadoop Clusters and query database using Hive to do exploration and feature engineering.</source>
          <target state="new">Since the data is in Hadoop cluster, you can use pyodbc package to connect to Hadoop Clusters and query database using Hive to do exploration and feature engineering.</target>
        </trans-unit>
        <trans-unit id="324" translate="yes" xml:space="preserve">
          <source>You can view the existing tables we created in the prerequisite step.</source>
          <target state="new">You can view the existing tables we created in the prerequisite step.</target>
        </trans-unit>
        <trans-unit id="325" translate="yes" xml:space="preserve">
          <source>Let's look at the number of records in each month and the frequencies of tipped or not in the trip table:</source>
          <target state="new">Let's look at the number of records in each month and the frequencies of tipped or not in the trip table:</target>
        </trans-unit>
        <trans-unit id="326" translate="yes" xml:space="preserve">
          <source>We can also compute the distance between pickup location and dropoff location and then compare it to the trip distance.</source>
          <target state="new">We can also compute the distance between pickup location and dropoff location and then compare it to the trip distance.</target>
        </trans-unit>
        <trans-unit id="327" translate="yes" xml:space="preserve">
          <source>Now let's prepare a downsampled (1%) data for modeling.</source>
          <target state="new">Now let's prepare a downsampled (1%) data for modeling.</target>
        </trans-unit>
        <trans-unit id="328" translate="yes" xml:space="preserve">
          <source>We can use this data in AML reader module.</source>
          <target state="new">We can use this data in AML reader module.</target>
        </trans-unit>
        <trans-unit id="329" translate="yes" xml:space="preserve">
          <source>After a while, you can see the data has been loaded in Hadoop clusters:</source>
          <target state="new">After a while, you can see the data has been loaded in Hadoop clusters:</target>
        </trans-unit>
        <trans-unit id="330" translate="yes" xml:space="preserve">
          <source><bpt id="p102">**</bpt>Read data from HDI using AML: reader module<ept id="p102">**</ept></source>
          <target state="new"><bpt id="p102">**</bpt>Read data from HDI using AML: reader module<ept id="p102">**</ept></target>
        </trans-unit>
        <trans-unit id="331" translate="yes" xml:space="preserve">
          <source>You may also use the <bpt id="p103">**</bpt>reader<ept id="p103">**</ept><ph id="ph123" /> module in AML studio to access the database in Hadoop cluster.</source>
          <target state="new">You may also use the <bpt id="p103">**</bpt>reader<ept id="p103">**</ept><ph id="ph123" /> module in AML studio to access the database in Hadoop cluster.</target>
        </trans-unit>
        <trans-unit id="332" translate="yes" xml:space="preserve">
          <source>Plug in the credentials of your HDI clusters and Azure Storage Account and you will be able to build machine learning models using database in HDI clusters.</source>
          <target state="new">Plug in the credentials of your HDI clusters and Azure Storage Account and you will be able to build machine learning models using database in HDI clusters.</target>
        </trans-unit>
        <trans-unit id="333" translate="yes" xml:space="preserve">
          <source>The scored dataset can then be viewed:</source>
          <target state="new">The scored dataset can then be viewed:</target>
        </trans-unit>
        <trans-unit id="334" translate="yes" xml:space="preserve">
          <source>Azure SQL Data Warehouse &amp; databases</source>
          <target state="new">Azure SQL Data Warehouse &amp; databases</target>
        </trans-unit>
        <trans-unit id="335" translate="yes" xml:space="preserve">
          <source>Azure DocumentDB</source>
          <target state="new">Azure DocumentDB</target>
        </trans-unit>
        <trans-unit id="336" translate="yes" xml:space="preserve">
          <source>Azure DocumentDB is a NoSQL database in the cloud.</source>
          <target state="new">Azure DocumentDB is a NoSQL database in the cloud.</target>
        </trans-unit>
        <trans-unit id="337" translate="yes" xml:space="preserve">
          <source>It allows you to work with documents like JSON and allow you to store and query the documents.</source>
          <target state="new">It allows you to work with documents like JSON and allow you to store and query the documents.</target>
        </trans-unit>
        <trans-unit id="338" translate="yes" xml:space="preserve">
          <source>You need to do the following per-requisites steps to access DocumentDB from the DSVM.</source>
          <target state="new">You need to do the following per-requisites steps to access DocumentDB from the DSVM.</target>
        </trans-unit>
        <trans-unit id="339" translate="yes" xml:space="preserve">
          <source>Install DocumentDB Python SDK (Run <ph id="ph126">```pip install pydocumentdb```</ph><ph id="ph127" /> from command prompt)</source>
          <target state="new">Install DocumentDB Python SDK (Run <ph id="ph126">```pip install pydocumentdb```</ph><ph id="ph127" /> from command prompt)</target>
        </trans-unit>
        <trans-unit id="340" translate="yes" xml:space="preserve">
          <source>Create DocumentDB account and Document DB database from <bpt id="p104">[</bpt>Azure portal<ept id="p104">](https://portal.azure.com)</ept></source>
          <target state="new">Create DocumentDB account and Document DB database from <bpt id="p104">[</bpt>Azure portal<ept id="p104">](https://portal.azure.com)</ept></target>
        </trans-unit>
        <trans-unit id="341" translate="yes" xml:space="preserve">
          <source>Download "DocumentDB Migration tool" from <bpt id="p105">[</bpt>here<ept id="p105">](http://www.microsoft.com/downloads/details.aspx?FamilyID=cda7703a-2774-4c07-adcc-ad02ddc1a44d)</ept><ph id="ph128" /> and extract to a directory of your choice</source>
          <target state="new">Download "DocumentDB Migration tool" from <bpt id="p105">[</bpt>here<ept id="p105">](http://www.microsoft.com/downloads/details.aspx?FamilyID=cda7703a-2774-4c07-adcc-ad02ddc1a44d)</ept><ph id="ph128" /> and extract to a directory of your choice</target>
        </trans-unit>
        <trans-unit id="342" translate="yes" xml:space="preserve">
          <source>Import JSON data (volcano data) stored on a <bpt id="p106">[</bpt>public blob<ept id="p106">](https://cahandson.blob.core.windows.net/samples/volcano.json)</ept><ph id="ph129" /> into DocumentDB with following command parameters to the migration tool (dtui.exe from the directory where you installed the DocumentDB migration tool).</source>
          <target state="new">Import JSON data (volcano data) stored on a <bpt id="p106">[</bpt>public blob<ept id="p106">](https://cahandson.blob.core.windows.net/samples/volcano.json)</ept><ph id="ph129" /> into DocumentDB with following command parameters to the migration tool (dtui.exe from the directory where you installed the DocumentDB migration tool).</target>
        </trans-unit>
        <trans-unit id="343" translate="yes" xml:space="preserve">
          <source>Enter the source and target location parameters from below.</source>
          <target state="new">Enter the source and target location parameters from below.</target>
        </trans-unit>
        <trans-unit id="344" translate="yes" xml:space="preserve">
          <source>/s:JsonFile /s.Files:https://cahandson.blob.core.windows.net/samples/volcano.json /t:DocumentDBBulk /t.ConnectionString:AccountEndpoint=https://[DocDBAccountName].documents.azure.com:443/;AccountKey=[[KEY];Database=volcano /t.Collection:volcano1</source>
          <target state="new">/s:JsonFile /s.Files:https://cahandson.blob.core.windows.net/samples/volcano.json /t:DocumentDBBulk /t.ConnectionString:AccountEndpoint=https://[DocDBAccountName].documents.azure.com:443/;AccountKey=[[KEY];Database=volcano /t.Collection:volcano1</target>
        </trans-unit>
        <trans-unit id="345" translate="yes" xml:space="preserve">
          <source>Once you import the data, you can go to Jupyter and open the notebook titled <ph id="ph130">```DocumentDBSample```</ph><ph id="ph131" /> which contains python code to access DocumentDB and do some basic querying.</source>
          <target state="new">Once you import the data, you can go to Jupyter and open the notebook titled <ph id="ph130">```DocumentDBSample```</ph><ph id="ph131" /> which contains python code to access DocumentDB and do some basic querying.</target>
        </trans-unit>
        <trans-unit id="346" translate="yes" xml:space="preserve">
          <source>You can learn more about DocumentDB by visiting the service <bpt id="p107">[</bpt>documentation page<ept id="p107">](https://azure.microsoft.com/documentation/learning-paths/documentdb/)</ept></source>
          <target state="new">You can learn more about DocumentDB by visiting the service <bpt id="p107">[</bpt>documentation page<ept id="p107">](https://azure.microsoft.com/documentation/learning-paths/documentdb/)</ept></target>
        </trans-unit>
        <trans-unit id="347" translate="yes" xml:space="preserve">
          <source>8. Build reports and dashboard using the Power BI Desktop</source>
          <target state="new">8. Build reports and dashboard using the Power BI Desktop</target>
        </trans-unit>
        <trans-unit id="348" translate="yes" xml:space="preserve">
          <source>Let us visualize the Volcano JSON file we saw in the DocumentDB example above in Power BI to gain visual insights into the data.</source>
          <target state="new">Let us visualize the Volcano JSON file we saw in the DocumentDB example above in Power BI to gain visual insights into the data.</target>
        </trans-unit>
        <trans-unit id="349" translate="yes" xml:space="preserve">
          <source>Detailed steps are found in the <bpt id="p108">[</bpt>Power BI article<ept id="p108">](../documentdb/documentdb-powerbi-visualize.md)</ept>.</source>
          <target state="new">Detailed steps are found in the <bpt id="p108">[</bpt>Power BI article<ept id="p108">](../documentdb/documentdb-powerbi-visualize.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="350" translate="yes" xml:space="preserve">
          <source>The high level steps are below :</source>
          <target state="new">The high level steps are below :</target>
        </trans-unit>
        <trans-unit id="351" translate="yes" xml:space="preserve">
          <source>Open Power BI Desktop and do "Get Data".</source>
          <target state="new">Open Power BI Desktop and do "Get Data".</target>
        </trans-unit>
        <trans-unit id="352" translate="yes" xml:space="preserve">
          <source>Specify the URL as: https://cahandson.blob.core.windows.net/samples/volcano.json</source>
          <target state="new">Specify the URL as: https://cahandson.blob.core.windows.net/samples/volcano.json</target>
        </trans-unit>
        <trans-unit id="353" translate="yes" xml:space="preserve">
          <source>You will see the JSON records imported as a list</source>
          <target state="new">You will see the JSON records imported as a list</target>
        </trans-unit>
        <trans-unit id="354" translate="yes" xml:space="preserve">
          <source>Next convert the list to a table so PowerBI can work with the same</source>
          <target state="new">Next convert the list to a table so PowerBI can work with the same</target>
        </trans-unit>
        <trans-unit id="355" translate="yes" xml:space="preserve">
          <source>Then you expand the columns by clicking on the expand icon (the one with the "left arrow and a right arrow" icon on the right of the column)</source>
          <target state="new">Then you expand the columns by clicking on the expand icon (the one with the "left arrow and a right arrow" icon on the right of the column)</target>
        </trans-unit>
        <trans-unit id="356" translate="yes" xml:space="preserve">
          <source>You will see that location is a "Record" field.</source>
          <target state="new">You will see that location is a "Record" field.</target>
        </trans-unit>
        <trans-unit id="357" translate="yes" xml:space="preserve">
          <source>Expand the record and select only the coordinates.</source>
          <target state="new">Expand the record and select only the coordinates.</target>
        </trans-unit>
        <trans-unit id="358" translate="yes" xml:space="preserve">
          <source>Corordinate is a list column</source>
          <target state="new">Corordinate is a list column</target>
        </trans-unit>
        <trans-unit id="359" translate="yes" xml:space="preserve">
          <source>Next you will add a new column to convert the list coordinate column into a comma separate LatLong column contatenating the two elements in the coordinate list field using the formula <ph id="ph132">```Text.From([coordinates]{1})&amp;","&amp;Text.From([coordinates]{0})```</ph>.</source>
          <target state="new">Next you will add a new column to convert the list coordinate column into a comma separate LatLong column contatenating the two elements in the coordinate list field using the formula <ph id="ph132">```Text.From([coordinates]{1})&amp;","&amp;Text.From([coordinates]{0})```</ph>.</target>
        </trans-unit>
        <trans-unit id="360" translate="yes" xml:space="preserve">
          <source>Finally convert the <ph id="ph133">```Elevation```</ph><ph id="ph134" /> column to Decimal and hit the Close and Apply.</source>
          <target state="new">Finally convert the <ph id="ph133">```Elevation```</ph><ph id="ph134" /> column to Decimal and hit the Close and Apply.</target>
        </trans-unit>
        <trans-unit id="361" translate="yes" xml:space="preserve">
          <source>Instead of steps above, you can paste the following code that scripts out the steps above in the Advanced Editor in PowerBI that allows you to write the data transformations in a query language.</source>
          <target state="new">Instead of steps above, you can paste the following code that scripts out the steps above in the Advanced Editor in PowerBI that allows you to write the data transformations in a query language.</target>
        </trans-unit>
        <trans-unit id="362" translate="yes" xml:space="preserve">
          <source>You now have the data in your Power BI data model.</source>
          <target state="new">You now have the data in your Power BI data model.</target>
        </trans-unit>
        <trans-unit id="363" translate="yes" xml:space="preserve">
          <source>Your Power BI desktop should look as shown below.</source>
          <target state="new">Your Power BI desktop should look as shown below.</target>
        </trans-unit>
        <trans-unit id="364" translate="yes" xml:space="preserve">
          <source>You can start building reports and visualizations using the data model.</source>
          <target state="new">You can start building reports and visualizations using the data model.</target>
        </trans-unit>
        <trans-unit id="365" translate="yes" xml:space="preserve">
          <source>You can follow the steps in this <bpt id="p109">[</bpt>Power BI article<ept id="p109">](../documentdb/documentdb-powerbi-visualize.md#build-the-reports)</ept><ph id="ph136" /> to build a report.</source>
          <target state="new">You can follow the steps in this <bpt id="p109">[</bpt>Power BI article<ept id="p109">](../documentdb/documentdb-powerbi-visualize.md#build-the-reports)</ept><ph id="ph136" /> to build a report.</target>
        </trans-unit>
        <trans-unit id="366" translate="yes" xml:space="preserve">
          <source>The end result will be a report that looks like the following.</source>
          <target state="new">The end result will be a report that looks like the following.</target>
        </trans-unit>
        <trans-unit id="367" translate="yes" xml:space="preserve">
          <source>TBD: Volcano Map Report image URL missing - not correct.</source>
          <target state="new">TBD: Volcano Map Report image URL missing - not correct.</target>
        </trans-unit>
        <trans-unit id="368" translate="yes" xml:space="preserve">
          <source>9. Dynamically scale your DSVM to meet your project needs</source>
          <target state="new">9. Dynamically scale your DSVM to meet your project needs</target>
        </trans-unit>
        <trans-unit id="369" translate="yes" xml:space="preserve">
          <source>You can scale up and down the DSVM to meet your project needs.</source>
          <target state="new">You can scale up and down the DSVM to meet your project needs.</target>
        </trans-unit>
        <trans-unit id="370" translate="yes" xml:space="preserve">
          <source>If you dont need to use the VM in the evening or weekends, you can just shutdown the VM from the <bpt id="p110">[</bpt>Azure Portal<ept id="p110">](https://portal.azure.com)</ept>.</source>
          <target state="new">If you dont need to use the VM in the evening or weekends, you can just shutdown the VM from the <bpt id="p110">[</bpt>Azure Portal<ept id="p110">](https://portal.azure.com)</ept>.</target>
        </trans-unit>
        <trans-unit id="371" translate="yes" xml:space="preserve">
          <source>NOTE:  that you will incur compute charges if you use just the Operating system shutdown button on the VM.</source>
          <target state="new">NOTE:  that you will incur compute charges if you use just the Operating system shutdown button on the VM.</target>
        </trans-unit>
        <trans-unit id="372" translate="yes" xml:space="preserve">
          <source>If you need to handle some large scale analysis and need more CPU and/or memory and/or disk capacity you can find a large choice of VM sizes in terms of CPU cores, memory capacity and disk types (including Solid state drives) that meet your compute  and budgetary needs.</source>
          <target state="new">If you need to handle some large scale analysis and need more CPU and/or memory and/or disk capacity you can find a large choice of VM sizes in terms of CPU cores, memory capacity and disk types (including Solid state drives) that meet your compute  and budgetary needs.</target>
        </trans-unit>
        <trans-unit id="373" translate="yes" xml:space="preserve">
          <source>The full list of VMs along with their hourly compute pricing is available on the <bpt id="p111">[</bpt>Azure Virtual Machines Pricing<ept id="p111">](https://azure.microsoft.com/pricing/details/virtual-machines/)</ept><ph id="ph137" /> page.</source>
          <target state="new">The full list of VMs along with their hourly compute pricing is available on the <bpt id="p111">[</bpt>Azure Virtual Machines Pricing<ept id="p111">](https://azure.microsoft.com/pricing/details/virtual-machines/)</ept><ph id="ph137" /> page.</target>
        </trans-unit>
        <trans-unit id="374" translate="yes" xml:space="preserve">
          <source>Similarly, if your needs for VM processing capacity reduces (for example: you moved a major workload to a Hadoop or a Spark cluster), you can scale down the cluster from the <bpt id="p112">[</bpt>Azure Portal<ept id="p112">](https://portal.azure.com)</ept><ph id="ph138" /> and going to the settings of your VM instance.</source>
          <target state="new">Similarly, if your needs for VM processing capacity reduces (for example: you moved a major workload to a Hadoop or a Spark cluster), you can scale down the cluster from the <bpt id="p112">[</bpt>Azure Portal<ept id="p112">](https://portal.azure.com)</ept><ph id="ph138" /> and going to the settings of your VM instance.</target>
        </trans-unit>
        <trans-unit id="375" translate="yes" xml:space="preserve">
          <source>Here is a screenshot.</source>
          <target state="new">Here is a screenshot.</target>
        </trans-unit>
        <trans-unit id="376" translate="yes" xml:space="preserve">
          <source>10. Install additional tools on your virtual machine</source>
          <target state="new">10. Install additional tools on your virtual machine</target>
        </trans-unit>
        <trans-unit id="377" translate="yes" xml:space="preserve">
          <source>We have packaged several tools that we believe will be able to address many of the common data analytics needs and save you time by avoiding  installing and configuring your environment one by one and paying for only what you use.</source>
          <target state="new">We have packaged several tools that we believe will be able to address many of the common data analytics needs and save you time by avoiding  installing and configuring your environment one by one and paying for only what you use.</target>
        </trans-unit>
        <trans-unit id="378" translate="yes" xml:space="preserve">
          <source>You can leverage other Azure data and analytics services as seen earlier in this article to enhance your analytics environment.</source>
          <target state="new">You can leverage other Azure data and analytics services as seen earlier in this article to enhance your analytics environment.</target>
        </trans-unit>
        <trans-unit id="379" translate="yes" xml:space="preserve">
          <source>We understand that in some cases your needs may require additional tools including some proprietary third party tools.</source>
          <target state="new">We understand that in some cases your needs may require additional tools including some proprietary third party tools.</target>
        </trans-unit>
        <trans-unit id="380" translate="yes" xml:space="preserve">
          <source>You have full administrative access on the virtual machine to install new tools you need.</source>
          <target state="new">You have full administrative access on the virtual machine to install new tools you need.</target>
        </trans-unit>
        <trans-unit id="381" translate="yes" xml:space="preserve">
          <source>You can also install additional packages in Python and R that are not pre-installed.</source>
          <target state="new">You can also install additional packages in Python and R that are not pre-installed.</target>
        </trans-unit>
        <trans-unit id="382" translate="yes" xml:space="preserve">
          <source>For Python  you can use either <ph id="ph140">```conda```</ph><ph id="ph141" /> or <ph id="ph142">```pip```</ph>.</source>
          <target state="new">For Python  you can use either <ph id="ph140">```conda```</ph><ph id="ph141" /> or <ph id="ph142">```pip```</ph>.</target>
        </trans-unit>
        <trans-unit id="383" translate="yes" xml:space="preserve">
          <source>For R you can use the <ph id="ph143">```install.packages()```</ph><ph id="ph144" /> in the R console or use the IDE and choose "Packages -&gt; Install Packages...".</source>
          <target state="new">For R you can use the <ph id="ph143">```install.packages()```</ph><ph id="ph144" /> in the R console or use the IDE and choose "Packages -&gt; Install Packages...".</target>
        </trans-unit>
        <trans-unit id="384" translate="yes" xml:space="preserve">
          <source>This is just a few things you can do on the Microsoft Data Science Virtual Machine.</source>
          <target state="new">This is just a few things you can do on the Microsoft Data Science Virtual Machine.</target>
        </trans-unit>
        <trans-unit id="385" translate="yes" xml:space="preserve">
          <source>There are many more things you can do to make it an effective analytics environment.</source>
          <target state="new">There are many more things you can do to make it an effective analytics environment.</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">f74d3c718eb329a00294309bf7fa5f9144aeef6b</xliffext:olfilehash>
  </header>
</xliff>