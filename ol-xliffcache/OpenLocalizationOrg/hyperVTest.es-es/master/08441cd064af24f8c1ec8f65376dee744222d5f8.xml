{
  "nodes": [
    {
      "pos": [
        28,
        119
      ],
      "content": "How to identify scenarios and plan for advanced analytics data processing | Microsoft Azure"
    },
    {
      "pos": [
        139,
        208
      ],
      "content": "Plan for advanced analytics by considering a series of key questions."
    },
    {
      "pos": [
        546,
        619
      ],
      "content": "How to identify scenarios and plan for advanced analytics data processing"
    },
    {
      "pos": [
        621,
        1339
      ],
      "content": "What resources should you plan to include when setting up an environment to do advanced analytics processing on a dataset? This article suggests a series of questions to ask that will help identify the tasks and resources relevant your scenario. The order of high-level steps for predictive analytics is outlined in the <bpt id=\"p1\">[</bpt>What is the Cortana Analytics Process (CAP)?<ept id=\"p1\">](machine-learning-data-science-the-cortana-analytics-process.md)</ept><ph id=\"ph2\"/> document. Each of these steps will require specific resources for the  tasks relevant to your particular scenario. The key questions to identify your scenario concern data logistics, characteristics, the quality of the datasets, and the tools and languages you prefer to do the analysis.",
      "nodes": [
        {
          "content": "What resources should you plan to include when setting up an environment to do advanced analytics processing on a dataset?",
          "pos": [
            0,
            122
          ]
        },
        {
          "content": "This article suggests a series of questions to ask that will help identify the tasks and resources relevant your scenario.",
          "pos": [
            123,
            245
          ]
        },
        {
          "content": "The order of high-level steps for predictive analytics is outlined in the <bpt id=\"p1\">[</bpt>What is the Cortana Analytics Process (CAP)?<ept id=\"p1\">](machine-learning-data-science-the-cortana-analytics-process.md)</ept><ph id=\"ph2\"/> document.",
          "pos": [
            246,
            492
          ]
        },
        {
          "content": "Each of these steps will require specific resources for the  tasks relevant to your particular scenario.",
          "pos": [
            493,
            597
          ]
        },
        {
          "content": "The key questions to identify your scenario concern data logistics, characteristics, the quality of the datasets, and the tools and languages you prefer to do the analysis.",
          "pos": [
            598,
            770
          ]
        }
      ]
    },
    {
      "pos": [
        1438,
        1485
      ],
      "content": "Logistic questions: data locations and movement"
    },
    {
      "pos": [
        1486,
        1871
      ],
      "content": "The logistic questions concern the location of the <bpt id=\"p2\">**</bpt>data source<ept id=\"p2\">**</ept>, the <bpt id=\"p3\">**</bpt>target destination<ept id=\"p3\">**</ept><ph id=\"ph4\"/> in Azure, and requirements for moving the data, including the schedule, amount and resources involved. The data may need to be moved several times during the analytics process. A common scenario is to move local data into some form of storage on Azure and then into Machine Learning Studio.",
      "nodes": [
        {
          "content": "The logistic questions concern the location of the <bpt id=\"p2\">**</bpt>data source<ept id=\"p2\">**</ept>, the <bpt id=\"p3\">**</bpt>target destination<ept id=\"p3\">**</ept><ph id=\"ph4\"/> in Azure, and requirements for moving the data, including the schedule, amount and resources involved.",
          "pos": [
            0,
            287
          ]
        },
        {
          "content": "The data may need to be moved several times during the analytics process.",
          "pos": [
            288,
            361
          ]
        },
        {
          "content": "A common scenario is to move local data into some form of storage on Azure and then into Machine Learning Studio.",
          "pos": [
            362,
            475
          ]
        }
      ]
    },
    {
      "pos": [
        1876,
        1947
      ],
      "content": "<bpt id=\"p4\">**</bpt>What is your data source?<ept id=\"p4\">**</ept><ph id=\"ph5\"/> Is it local or in the cloud? For example:",
      "nodes": [
        {
          "content": "<bpt id=\"p4\">**</bpt>What is your data source?<ept id=\"p4\">**</ept><ph id=\"ph5\"/> Is it local or in the cloud?",
          "pos": [
            0,
            110
          ]
        },
        {
          "content": "For example:",
          "pos": [
            111,
            123
          ]
        }
      ]
    },
    {
      "pos": [
        1954,
        2004
      ],
      "content": "The data is publicly available at an HTTP address."
    },
    {
      "pos": [
        2011,
        2061
      ],
      "content": "The data resides in a local/network file location."
    },
    {
      "pos": [
        2068,
        2105
      ],
      "content": "The data is in a SQL Server database."
    },
    {
      "pos": [
        2112,
        2160
      ],
      "content": "The data is stored in an Azure storage container"
    },
    {
      "pos": [
        2165,
        2265
      ],
      "content": "<bpt id=\"p5\">**</bpt>What is the Azure destination?<ept id=\"p5\">**</ept><ph id=\"ph6\"/> Where does it need to be for processing or modeling? For example:",
      "nodes": [
        {
          "content": "<bpt id=\"p5\">**</bpt>What is the Azure destination?<ept id=\"p5\">**</ept><ph id=\"ph6\"/> Where does it need to be for processing or modeling?",
          "pos": [
            0,
            139
          ]
        },
        {
          "content": "For example:",
          "pos": [
            140,
            152
          ]
        }
      ]
    },
    {
      "pos": [
        2272,
        2290
      ],
      "content": "Azure Blob Storage"
    },
    {
      "pos": [
        2297,
        2316
      ],
      "content": "SQL Azure databases"
    },
    {
      "pos": [
        2323,
        2345
      ],
      "content": "SQL Server on Azure VM"
    },
    {
      "pos": [
        2352,
        2394
      ],
      "content": "HDInsight (Hadoop on Azure) or Hive tables"
    },
    {
      "pos": [
        2401,
        2423
      ],
      "content": "Azure Machine Learning"
    },
    {
      "pos": [
        2430,
        2465
      ],
      "content": "Mountable Azure virtual hard disks."
    },
    {
      "pos": [
        2470,
        2673
      ],
      "content": "<bpt id=\"p6\">**</bpt>How are you going to move the data?<ept id=\"p6\">**</ept><ph id=\"ph7\"/> The procedures and resources available to ingest or load data into a variety of different storage and processing environments are outlined in the following topics."
    },
    {
      "pos": [
        2682,
        2779
      ],
      "content": "<bpt id=\"p7\">[</bpt>Load data into storage environments for analytics<ept id=\"p7\">](machine-learning-data-science-ingest-data.md)</ept>"
    },
    {
      "pos": [
        2788,
        2923
      ],
      "content": "<bpt id=\"p8\">[</bpt>Import your training data into Azure Machine Learning Studio from various data sources<ept id=\"p8\">](machine-learning-data-science-import-data,md)</ept>."
    },
    {
      "pos": [
        2928,
        3479
      ],
      "content": "<bpt id=\"p9\">**</bpt>Does the data need to be moved on a regular schedule or modified during migration?<ept id=\"p9\">**</ept><ph id=\"ph8\"/> Consider using Azure Data Factory (ADF) when data needs to be continually migrated, particularly if a hybrid scenario that accesses both on-premise and cloud resources is involved, or when the data is transacted or needs to be modified or have business logic added to it in the course of being migrated. For further information, see <bpt id=\"p10\">[</bpt>Move data from an on-premise SQL server to SQL Azure with Azure Data Factory<ept id=\"p10\">](machine-learning-data-science-move-sql-azure-adf.md)</ept>",
      "nodes": [
        {
          "content": "<bpt id=\"p9\">**</bpt>Does the data need to be moved on a regular schedule or modified during migration?<ept id=\"p9\">**</ept><ph id=\"ph8\"/> Consider using Azure Data Factory (ADF) when data needs to be continually migrated, particularly if a hybrid scenario that accesses both on-premise and cloud resources is involved, or when the data is transacted or needs to be modified or have business logic added to it in the course of being migrated.",
          "pos": [
            0,
            442
          ]
        },
        {
          "content": "For further information, see <bpt id=\"p10\">[</bpt>Move data from an on-premise SQL server to SQL Azure with Azure Data Factory<ept id=\"p10\">](machine-learning-data-science-move-sql-azure-adf.md)</ept>",
          "pos": [
            443,
            643
          ]
        }
      ]
    },
    {
      "pos": [
        3484,
        3959
      ],
      "content": "<bpt id=\"p11\">**</bpt>How much of the data is to be moved to Azure?<ept id=\"p11\">**</ept><ph id=\"ph9\"/> Datasets that are very large may exceed the storage capacity of certain environments. For an example, see the discussion of size limits for Machine Learning Studio in the next section. In such cases a sample of the data may be used during the analysis. For details of how to down-sample a dataset in various Azure environments, see <bpt id=\"p12\">[</bpt>Sample data in the Cortana Analytics Process<ept id=\"p12\">](machine-learning-data-science-sample-data.md)</ept>.",
      "nodes": [
        {
          "content": "<bpt id=\"p11\">**</bpt>How much of the data is to be moved to Azure?<ept id=\"p11\">**</ept><ph id=\"ph9\"/> Datasets that are very large may exceed the storage capacity of certain environments.",
          "pos": [
            0,
            189
          ]
        },
        {
          "content": "For an example, see the discussion of size limits for Machine Learning Studio in the next section.",
          "pos": [
            190,
            288
          ]
        },
        {
          "content": "In such cases a sample of the data may be used during the analysis.",
          "pos": [
            289,
            356
          ]
        },
        {
          "content": "For details of how to down-sample a dataset in various Azure environments, see <bpt id=\"p12\">[</bpt>Sample data in the Cortana Analytics Process<ept id=\"p12\">](machine-learning-data-science-sample-data.md)</ept>.",
          "pos": [
            357,
            569
          ]
        }
      ]
    },
    {
      "pos": [
        3965,
        4018
      ],
      "content": "Data characteristics questions: type, format and size"
    },
    {
      "pos": [
        4019,
        4196
      ],
      "content": "These questions are key to planning your storage and processing environments, each of which are appropriate to various types of data and each of which have certain restrictions."
    },
    {
      "pos": [
        4202,
        4243
      ],
      "content": "<bpt id=\"p13\">**</bpt>What are the data types?<ept id=\"p13\">**</ept><ph id=\"ph10\"/> For Example:"
    },
    {
      "pos": [
        4251,
        4260
      ],
      "content": "Numerical"
    },
    {
      "pos": [
        4267,
        4278
      ],
      "content": "Categorical"
    },
    {
      "pos": [
        4285,
        4292
      ],
      "content": "Strings"
    },
    {
      "pos": [
        4299,
        4305
      ],
      "content": "Binary"
    },
    {
      "pos": [
        4310,
        4354
      ],
      "content": "<bpt id=\"p14\">**</bpt>How is your data formatted?<ept id=\"p14\">**</ept><ph id=\"ph11\"/> For Example:"
    },
    {
      "pos": [
        4361,
        4416
      ],
      "content": "Comma-separated (CSV) or tab-separated (TSV) flat files"
    },
    {
      "pos": [
        4423,
        4449
      ],
      "content": "Compressed or uncompressed"
    },
    {
      "pos": [
        4456,
        4467
      ],
      "content": "Azure blobs"
    },
    {
      "pos": [
        4474,
        4492
      ],
      "content": "Hadoop Hive tables"
    },
    {
      "pos": [
        4499,
        4516
      ],
      "content": "SQL Server tables"
    },
    {
      "pos": [
        4521,
        4548
      ],
      "content": "<bpt id=\"p15\">**</bpt>How large is your data?<ept id=\"p15\">**</ept>"
    },
    {
      "pos": [
        4555,
        4575
      ],
      "content": "Small: Less than 2GB"
    },
    {
      "pos": [
        4582,
        4625
      ],
      "content": "Medium: Greater than 2GB and less than 10GB"
    },
    {
      "pos": [
        4632,
        4656
      ],
      "content": "Large: Greater than 10GB"
    },
    {
      "pos": [
        4658,
        4721
      ],
      "content": "Take the Azure Machine Learning Studio environment for example:"
    },
    {
      "pos": [
        4725,
        4946
      ],
      "content": "For a list of the data formats and types supported by Azure Machine Learning Studio, see\n<bpt id=\"p16\">[</bpt>Data formats and data types supported<ept id=\"p16\">](machine-learning-data-science-import-data.md#data-formats-and-data-types-supported)</ept><ph id=\"ph12\"/> section."
    },
    {
      "pos": [
        4949,
        5199
      ],
      "content": "For information on data limitations for Azure Machine Learning Studio, see the <bpt id=\"p17\">**</bpt>How large can the data set be for my modules?<ept id=\"p17\">**</ept><ph id=\"ph13\"/> section of <bpt id=\"p18\">[</bpt>Importing and exporting data for Machine Learning<ept id=\"p18\">](machine-learning-faq.md#machine-learning-studio-questions)</ept>"
    },
    {
      "pos": [
        5201,
        5398
      ],
      "content": "For information on the limitations of other Azure services used in the analytics process, see <bpt id=\"p19\">[</bpt>Azure Subscription and Service Limits, Quotas, and Constraints<ept id=\"p19\">](azure-subscription-service-limits.md)</ept>."
    },
    {
      "pos": [
        5403,
        5457
      ],
      "content": "Data quality questions: exploration and pre-processing"
    },
    {
      "pos": [
        5462,
        6173
      ],
      "content": "<bpt id=\"p20\">**</bpt>What do you know about your data?<ept id=\"p20\">**</ept><ph id=\"ph14\"/> Explore data when you need to gain an understand its basic characteristics. What patterns or trends it exhibits, what outliers is has or how many values are missing. This step is important for determining the extent of pre-processing needed, for formulating hypotheses that could suggest the most appropriate features or type of analysis, and for formulating plans for additional data collection. Calculating descriptive statistics and plotting visualizations are useful techniques for data inspection. For details of how to explore a dataset in various Azure environments, see <bpt id=\"p21\">[</bpt>Explore data in the Cortana Analytics Process<ept id=\"p21\">](machine-learning-data-science-explore-data.md)</ept>.",
      "nodes": [
        {
          "content": "<bpt id=\"p20\">**</bpt>What do you know about your data?<ept id=\"p20\">**</ept><ph id=\"ph14\"/> Explore data when you need to gain an understand its basic characteristics.",
          "pos": [
            0,
            168
          ]
        },
        {
          "content": "What patterns or trends it exhibits, what outliers is has or how many values are missing.",
          "pos": [
            169,
            258
          ]
        },
        {
          "content": "This step is important for determining the extent of pre-processing needed, for formulating hypotheses that could suggest the most appropriate features or type of analysis, and for formulating plans for additional data collection.",
          "pos": [
            259,
            489
          ]
        },
        {
          "content": "Calculating descriptive statistics and plotting visualizations are useful techniques for data inspection.",
          "pos": [
            490,
            595
          ]
        },
        {
          "content": "For details of how to explore a dataset in various Azure environments, see <bpt id=\"p21\">[</bpt>Explore data in the Cortana Analytics Process<ept id=\"p21\">](machine-learning-data-science-explore-data.md)</ept>.",
          "pos": [
            596,
            806
          ]
        }
      ]
    },
    {
      "pos": [
        6178,
        6631
      ],
      "content": "<bpt id=\"p22\">**</bpt>Does the data require pre-processing or cleaning?<ept id=\"p22\">**</ept>\nPre-processing and cleaning data are important tasks that typically must be conducted before dataset can be used effectively for machine learning. Raw data is often noisy and unreliable, and may be missing values. Using such data for modeling can produce misleading results. For a description, see <bpt id=\"p23\">[</bpt>Tasks to prepare data for enhanced machine learning<ept id=\"p23\">](machine-learning-data-science-prepare-data.md)</ept>.",
      "nodes": [
        {
          "content": "<bpt id=\"p22\">**</bpt>Does the data require pre-processing or cleaning?<ept id=\"p22\">**</ept>\nPre-processing and cleaning data are important tasks that typically must be conducted before dataset can be used effectively for machine learning.",
          "pos": [
            0,
            240
          ]
        },
        {
          "content": "Raw data is often noisy and unreliable, and may be missing values.",
          "pos": [
            241,
            307
          ]
        },
        {
          "content": "Using such data for modeling can produce misleading results.",
          "pos": [
            308,
            368
          ]
        },
        {
          "content": "For a description, see <bpt id=\"p23\">[</bpt>Tasks to prepare data for enhanced machine learning<ept id=\"p23\">](machine-learning-data-science-prepare-data.md)</ept>.",
          "pos": [
            369,
            533
          ]
        }
      ]
    },
    {
      "pos": [
        6636,
        6665
      ],
      "content": "Tools and languages questions"
    },
    {
      "pos": [
        6666,
        6802
      ],
      "content": "There are lots of options here depending on what languages and development environments or tools you need or are most conformable using."
    },
    {
      "pos": [
        6808,
        6861
      ],
      "content": "<bpt id=\"p24\">**</bpt>What languages do you prefer to use for analysis?<ept id=\"p24\">**</ept>"
    },
    {
      "pos": [
        6870,
        6871
      ],
      "content": "R"
    },
    {
      "pos": [
        6878,
        6884
      ],
      "content": "Python"
    },
    {
      "pos": [
        6891,
        6894
      ],
      "content": "SQL"
    },
    {
      "pos": [
        6899,
        6947
      ],
      "content": "<bpt id=\"p25\">**</bpt>What tools should you use for data analysis?<ept id=\"p25\">**</ept>"
    },
    {
      "pos": [
        6954,
        7097
      ],
      "content": "<bpt id=\"p26\">[</bpt>Microsoft Azure Powershell<ept id=\"p26\">](powershell-install-configure.md)</ept><ph id=\"ph15\"/> - a script language used to administer your Azure resources in a script language."
    },
    {
      "pos": [
        7104,
        7172
      ],
      "content": "<bpt id=\"p27\">[</bpt>Azure Machine Learning Studio<ept id=\"p27\">](machine-learning-what-is-ml-studio/)</ept>"
    },
    {
      "pos": [
        7179,
        7255
      ],
      "content": "<bpt id=\"p28\">[</bpt>Revolution Analytics<ept id=\"p28\">](http://www.revolutionanalytics.com/revolution-r-open)</ept>"
    },
    {
      "pos": [
        7262,
        7295
      ],
      "content": "<bpt id=\"p29\">[</bpt>RStudio<ept id=\"p29\">](http://www.rstudio.com)</ept>"
    },
    {
      "pos": [
        7302,
        7368
      ],
      "content": "<bpt id=\"p30\">[</bpt>Python Tools for Visual Studio<ept id=\"p30\">](http://microsoft.github.io/PTVS/)</ept>"
    },
    {
      "pos": [
        7375,
        7424
      ],
      "content": "<bpt id=\"p31\">[</bpt>Anaconda<ept id=\"p31\">](https://www.continuum.io/why-anaconda)</ept>"
    },
    {
      "pos": [
        7431,
        7471
      ],
      "content": "<bpt id=\"p32\">[</bpt>Jupiter notebooks<ept id=\"p32\">](http://jupyter.org/)</ept>"
    },
    {
      "pos": [
        7478,
        7528
      ],
      "content": "<bpt id=\"p33\">[</bpt>Microsoft Power BI<ept id=\"p33\">](http://powerbi.microsoft.com)</ept>"
    },
    {
      "pos": [
        7535,
        7576
      ],
      "content": "Identify your advanced analytics scenario"
    },
    {
      "pos": [
        7577,
        7859
      ],
      "content": "Once you have answered the questions in the previous section, you are ready to determine which scenario best fits your case. The sample scenarios are outlined in <bpt id=\"p34\">[</bpt>Scenarios for advanced analytics in Azure Machine Learning<ept id=\"p34\">](../machine-learning-data-science-plan-sample-scenarios.md)</ept>.",
      "nodes": [
        {
          "content": "Once you have answered the questions in the previous section, you are ready to determine which scenario best fits your case.",
          "pos": [
            0,
            124
          ]
        },
        {
          "content": "The sample scenarios are outlined in <bpt id=\"p34\">[</bpt>Scenarios for advanced analytics in Azure Machine Learning<ept id=\"p34\">](../machine-learning-data-science-plan-sample-scenarios.md)</ept>.",
          "pos": [
            125,
            322
          ]
        }
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"How to identify scenarios and plan for advanced analytics data processing | Microsoft Azure\" \n    description=\"Plan for advanced analytics by considering a series of key questions.\" \n    services=\"machine-learning\" \n    documentationCenter=\"\" \n    authors=\"bradsev\"\n    manager=\"paulettm\" \n    editor=\"cgronlun\" />\n\n<tags \n    ms.service=\"machine-learning\" \n    ms.workload=\"data-services\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"02/08/2016\" \n    ms.author=\"bradsev\" /> \n\n\n# How to identify scenarios and plan for advanced analytics data processing\n\nWhat resources should you plan to include when setting up an environment to do advanced analytics processing on a dataset? This article suggests a series of questions to ask that will help identify the tasks and resources relevant your scenario. The order of high-level steps for predictive analytics is outlined in the [What is the Cortana Analytics Process (CAP)?](machine-learning-data-science-the-cortana-analytics-process.md) document. Each of these steps will require specific resources for the  tasks relevant to your particular scenario. The key questions to identify your scenario concern data logistics, characteristics, the quality of the datasets, and the tools and languages you prefer to do the analysis.\n\n[AZURE.INCLUDE [machine-learning-free-trial](../../includes/machine-learning-free-trial.md)]\n\n## Logistic questions: data locations and movement\nThe logistic questions concern the location of the **data source**, the **target destination** in Azure, and requirements for moving the data, including the schedule, amount and resources involved. The data may need to be moved several times during the analytics process. A common scenario is to move local data into some form of storage on Azure and then into Machine Learning Studio.\n\n1. **What is your data source?** Is it local or in the cloud? For example:\n    - The data is publicly available at an HTTP address.\n    - The data resides in a local/network file location.\n    - The data is in a SQL Server database.\n    - The data is stored in an Azure storage container\n\n2. **What is the Azure destination?** Where does it need to be for processing or modeling? For example:\n    - Azure Blob Storage\n    - SQL Azure databases\n    - SQL Server on Azure VM\n    - HDInsight (Hadoop on Azure) or Hive tables\n    - Azure Machine Learning\n    - Mountable Azure virtual hard disks.\n\n3. **How are you going to move the data?** The procedures and resources available to ingest or load data into a variety of different storage and processing environments are outlined in the following topics.\n\n    -  [Load data into storage environments for analytics](machine-learning-data-science-ingest-data.md) \n    -  [Import your training data into Azure Machine Learning Studio from various data sources](machine-learning-data-science-import-data,md).\n\n4. **Does the data need to be moved on a regular schedule or modified during migration?** Consider using Azure Data Factory (ADF) when data needs to be continually migrated, particularly if a hybrid scenario that accesses both on-premise and cloud resources is involved, or when the data is transacted or needs to be modified or have business logic added to it in the course of being migrated. For further information, see [Move data from an on-premise SQL server to SQL Azure with Azure Data Factory](machine-learning-data-science-move-sql-azure-adf.md)\n\n5. **How much of the data is to be moved to Azure?** Datasets that are very large may exceed the storage capacity of certain environments. For an example, see the discussion of size limits for Machine Learning Studio in the next section. In such cases a sample of the data may be used during the analysis. For details of how to down-sample a dataset in various Azure environments, see [Sample data in the Cortana Analytics Process](machine-learning-data-science-sample-data.md).\n\n\n## Data characteristics questions: type, format and size\nThese questions are key to planning your storage and processing environments, each of which are appropriate to various types of data and each of which have certain restrictions. \n\n1. **What are the data types?** For Example: \n    - Numerical\n    - Categorical\n    - Strings\n    - Binary\n\n2. **How is your data formatted?** For Example:\n    - Comma-separated (CSV) or tab-separated (TSV) flat files\n    - Compressed or uncompressed\n    - Azure blobs\n    - Hadoop Hive tables\n    - SQL Server tables\n\n2. **How large is your data?**\n    - Small: Less than 2GB\n    - Medium: Greater than 2GB and less than 10GB\n    - Large: Greater than 10GB\n\nTake the Azure Machine Learning Studio environment for example:\n\n- For a list of the data formats and types supported by Azure Machine Learning Studio, see\n[Data formats and data types supported](machine-learning-data-science-import-data.md#data-formats-and-data-types-supported) section.\n- For information on data limitations for Azure Machine Learning Studio, see the **How large can the data set be for my modules?** section of [Importing and exporting data for Machine Learning](machine-learning-faq.md#machine-learning-studio-questions)\n\nFor information on the limitations of other Azure services used in the analytics process, see [Azure Subscription and Service Limits, Quotas, and Constraints](azure-subscription-service-limits.md).\n\n## Data quality questions: exploration and pre-processing\n\n1. **What do you know about your data?** Explore data when you need to gain an understand its basic characteristics. What patterns or trends it exhibits, what outliers is has or how many values are missing. This step is important for determining the extent of pre-processing needed, for formulating hypotheses that could suggest the most appropriate features or type of analysis, and for formulating plans for additional data collection. Calculating descriptive statistics and plotting visualizations are useful techniques for data inspection. For details of how to explore a dataset in various Azure environments, see [Explore data in the Cortana Analytics Process](machine-learning-data-science-explore-data.md).\n\n2. **Does the data require pre-processing or cleaning?**\nPre-processing and cleaning data are important tasks that typically must be conducted before dataset can be used effectively for machine learning. Raw data is often noisy and unreliable, and may be missing values. Using such data for modeling can produce misleading results. For a description, see [Tasks to prepare data for enhanced machine learning](machine-learning-data-science-prepare-data.md).\n\n## Tools and languages questions\nThere are lots of options here depending on what languages and development environments or tools you need or are most conformable using.\n \n1. **What languages do you prefer to use for analysis?**  \n    - R\n    - Python\n    - SQL\n\n2. **What tools should you use for data analysis?**\n    - [Microsoft Azure Powershell](powershell-install-configure.md) - a script language used to administer your Azure resources in a script language.\n    - [Azure Machine Learning Studio](machine-learning-what-is-ml-studio/)\n    - [Revolution Analytics](http://www.revolutionanalytics.com/revolution-r-open)\n    - [RStudio](http://www.rstudio.com)\n    - [Python Tools for Visual Studio](http://microsoft.github.io/PTVS/)\n    - [Anaconda](https://www.continuum.io/why-anaconda)\n    - [Jupiter notebooks](http://jupyter.org/)\n    - [Microsoft Power BI](http://powerbi.microsoft.com) \n\n\n## Identify your advanced analytics scenario\nOnce you have answered the questions in the previous section, you are ready to determine which scenario best fits your case. The sample scenarios are outlined in [Scenarios for advanced analytics in Azure Machine Learning](../machine-learning-data-science-plan-sample-scenarios.md).\n\n\n\n\n\n\n\n "
}