{
  "nodes": [
    {
      "pos": [
        27,
        91
      ],
      "content": "Serialize data with the Microsoft Avro Library | Microsoft Azure"
    },
    {
      "pos": [
        110,
        168
      ],
      "content": "Learn how Azure HDInsight uses Avro to serialize big data."
    },
    {
      "pos": [
        494,
        550
      ],
      "content": "Serialize data in Hadoop with the Microsoft Avro Library"
    },
    {
      "pos": [
        552,
        583
      ],
      "content": "This topic shows how to use the"
    },
    {
      "pos": [
        671,
        693
      ],
      "content": "Microsoft Avro Library"
    },
    {
      "pos": [
        698,
        882
      ],
      "content": "to serialize objects and other data structures into streams in order to persist them to memory, a database, or a file, and also how to deserialize them to recover the original objects."
    },
    {
      "pos": [
        960,
        971
      ],
      "content": "Apache Avro"
    },
    {
      "pos": [
        972,
        975
      ],
      "content": "The"
    },
    {
      "pos": [
        1063,
        1085
      ],
      "content": "Microsoft Avro Library"
    },
    {
      "pos": [
        1090,
        1266
      ],
      "content": "implements the Apache Avro data serialization system for the Microsoft.NET environment. Apache Avro provides a compact binary data interchange format for serialization. It uses",
      "nodes": [
        {
          "content": "implements the Apache Avro data serialization system for the Microsoft.NET environment.",
          "pos": [
            0,
            87
          ]
        },
        {
          "content": "Apache Avro provides a compact binary data interchange format for serialization.",
          "pos": [
            88,
            168
          ]
        },
        {
          "content": "It uses",
          "pos": [
            169,
            176
          ]
        }
      ]
    },
    {
      "pos": [
        1313,
        1317
      ],
      "content": "JSON"
    },
    {
      "pos": [
        1322,
        1578
      ],
      "content": "to define a language-agnostic schema that underwrites language interoperability. Data serialized in one language can be read in another. Currently C, C++, C#, Java, PHP, Python, and Ruby are supported. Detailed information on the format can be found in the",
      "nodes": [
        {
          "content": "to define a language-agnostic schema that underwrites language interoperability.",
          "pos": [
            0,
            80
          ]
        },
        {
          "content": "Data serialized in one language can be read in another.",
          "pos": [
            81,
            136
          ]
        },
        {
          "content": "Currently C, C++, C#, Java, PHP, Python, and Ruby are supported.",
          "pos": [
            137,
            201
          ]
        },
        {
          "content": "Detailed information on the format can be found in the",
          "pos": [
            202,
            256
          ]
        }
      ]
    },
    {
      "pos": [
        1651,
        1676
      ],
      "content": "Apache Avro Specification"
    },
    {
      "pos": [
        1680,
        1820
      ],
      "content": ". Note that the current version of the Microsoft Avro Library does not support the remote procedure calls (RPCs) part of this specification.",
      "nodes": [
        {
          "content": ".",
          "pos": [
            0,
            1
          ]
        },
        {
          "content": "Note that the current version of the Microsoft Avro Library does not support the remote procedure calls (RPCs) part of this specification.",
          "pos": [
            2,
            140
          ]
        }
      ]
    },
    {
      "pos": [
        1822,
        2269
      ],
      "content": "The serialized representation of an object in the Avro system consists of two parts: schema and actual value. The Avro schema describes the language-independent data model of the serialized data with JSON. It is present side-by-side with a binary representation of data. Having the schema separate from the binary representation permits each object to be written with no per-value overheads, making serialization fast and the representation small.",
      "nodes": [
        {
          "content": "The serialized representation of an object in the Avro system consists of two parts: schema and actual value.",
          "pos": [
            0,
            109
          ]
        },
        {
          "content": "The Avro schema describes the language-independent data model of the serialized data with JSON.",
          "pos": [
            110,
            205
          ]
        },
        {
          "content": "It is present side-by-side with a binary representation of data.",
          "pos": [
            206,
            270
          ]
        },
        {
          "content": "Having the schema separate from the binary representation permits each object to be written with no per-value overheads, making serialization fast and the representation small.",
          "pos": [
            271,
            447
          ]
        }
      ]
    },
    {
      "pos": [
        2273,
        2292
      ],
      "content": "The Hadoop scenario"
    },
    {
      "pos": [
        2293,
        2803
      ],
      "content": "The Apache Avro serialization format is widely used in Azure HDInsight and other Apache Hadoop environments. Avro provides a convenient way to represent complex data structures within a Hadoop MapReduce job. The format of Avro files (Avro object container file) has been designed to support the distributed MapReduce programming model. The key feature that enables the distribution is that the files are “splittable” in the sense that one can seek any point in a file and start reading from a particular block.",
      "nodes": [
        {
          "content": "The Apache Avro serialization format is widely used in Azure HDInsight and other Apache Hadoop environments.",
          "pos": [
            0,
            108
          ]
        },
        {
          "content": "Avro provides a convenient way to represent complex data structures within a Hadoop MapReduce job.",
          "pos": [
            109,
            207
          ]
        },
        {
          "content": "The format of Avro files (Avro object container file) has been designed to support the distributed MapReduce programming model.",
          "pos": [
            208,
            335
          ]
        },
        {
          "content": "The key feature that enables the distribution is that the files are “splittable” in the sense that one can seek any point in a file and start reading from a particular block.",
          "pos": [
            336,
            510
          ]
        }
      ]
    },
    {
      "pos": [
        2807,
        2836
      ],
      "content": "Serialization in Avro Library"
    },
    {
      "pos": [
        2837,
        2904
      ],
      "content": "The .NET Library for Avro supports two ways of serializing objects:"
    },
    {
      "pos": [
        2908,
        3047
      ],
      "content": "<bpt id=\"p1\">**</bpt>reflection<ept id=\"p1\">**</ept><ph id=\"ph3\"/> - The JSON schema for the types is automatically built from the data contract attributes of the .NET types to be serialized."
    },
    {
      "pos": [
        3050,
        3320
      ],
      "content": "<bpt id=\"p2\">**</bpt>generic record<ept id=\"p2\">**</ept><ph id=\"ph4\"/> - A JSON schema is explicitly specified in a record represented by the <bpt id=\"p3\">[</bpt><bpt id=\"p4\">**</bpt>AvroRecord<ept id=\"p4\">**</ept><ept id=\"p3\">](http://msdn.microsoft.com/library/microsoft.hadoop.avro.avrorecord.aspx)</ept><ph id=\"ph5\"/> class when no .NET types are present to describe the schema for the data to be serialized."
    },
    {
      "pos": [
        3322,
        3697
      ],
      "content": "When the data schema is known to both the writer and reader of the stream, the data can be sent without its schema. In cases when an Avro object container file is used, the schema is stored within the file. Other parameters, such as the codec used for data compression, can be specified. These scenarios are outlined in more detail and illustrated in the code examples below.",
      "nodes": [
        {
          "content": "When the data schema is known to both the writer and reader of the stream, the data can be sent without its schema.",
          "pos": [
            0,
            115
          ]
        },
        {
          "content": "In cases when an Avro object container file is used, the schema is stored within the file.",
          "pos": [
            116,
            206
          ]
        },
        {
          "content": "Other parameters, such as the codec used for data compression, can be specified.",
          "pos": [
            207,
            287
          ]
        },
        {
          "content": "These scenarios are outlined in more detail and illustrated in the code examples below.",
          "pos": [
            288,
            375
          ]
        }
      ]
    },
    {
      "pos": [
        3703,
        3723
      ],
      "content": "Install Avro Library"
    },
    {
      "pos": [
        3725,
        3782
      ],
      "content": "The following are required before you install the libary:"
    },
    {
      "pos": [
        3786,
        3898
      ],
      "content": "<ph id=\"ph6\">&lt;a href=\"http://www.microsoft.com/download/details.aspx?id=17851\" target=\"_blank\"&gt;</ph>Microsoft .NET Framework 4<ph id=\"ph7\">&lt;/a&gt;</ph>"
    },
    {
      "pos": [
        3901,
        4000
      ],
      "content": "<ph id=\"ph8\">&lt;a href=\"http://james.newtonking.com/json\" target=\"_blank\"&gt;</ph>Newtonsoft Json.NET<ph id=\"ph9\">&lt;/a&gt;</ph><ph id=\"ph10\"/> (6.0.4 or later)"
    },
    {
      "pos": [
        4002,
        4188
      ],
      "content": "Note that the Newtonsoft.Json.dll dependency is downloaded automatically with the installation of the Microsoft Avro Library. The procedure for this is provided in the following section.",
      "nodes": [
        {
          "content": "Note that the Newtonsoft.Json.dll dependency is downloaded automatically with the installation of the Microsoft Avro Library.",
          "pos": [
            0,
            125
          ]
        },
        {
          "content": "The procedure for this is provided in the following section.",
          "pos": [
            126,
            186
          ]
        }
      ]
    },
    {
      "pos": [
        4191,
        4321
      ],
      "content": "The Microsoft Avro Library is distributed as a NuGet package that can be installed from Visual Studio via the following procedure:"
    },
    {
      "pos": [
        4326,
        4384
      ],
      "content": "Select the <bpt id=\"p5\">**</bpt>Project<ept id=\"p5\">**</ept><ph id=\"ph11\"/> tab -&gt; <bpt id=\"p6\">**</bpt>Manage NuGet Packages...<ept id=\"p6\">**</ept>"
    },
    {
      "pos": [
        4388,
        4452
      ],
      "content": "Search for \"Microsoft.Hadoop.Avro\" in the <bpt id=\"p7\">**</bpt>Search Online<ept id=\"p7\">**</ept><ph id=\"ph12\"/> box."
    },
    {
      "pos": [
        4456,
        4536
      ],
      "content": "Click the <bpt id=\"p8\">**</bpt>Install<ept id=\"p8\">**</ept><ph id=\"ph13\"/> button next to <bpt id=\"p9\">**</bpt>Microsoft Azure HDInsight Avro Library<ept id=\"p9\">**</ept>."
    },
    {
      "pos": [
        4538,
        4658
      ],
      "content": "Note that the Newtonsoft.Json.dll (&gt;=6.0.4) dependency is also downloaded automatically with the Microsoft Avro Library."
    },
    {
      "pos": [
        4660,
        4685
      ],
      "content": "You may want to visit the"
    },
    {
      "pos": [
        4773,
        4805
      ],
      "content": "Microsoft Avro Library home page"
    },
    {
      "pos": [
        4810,
        4844
      ],
      "content": "to read the current release notes."
    },
    {
      "pos": [
        4847,
        4905
      ],
      "content": "The Microsoft Avro Library source code is available at the"
    },
    {
      "pos": [
        4993,
        5025
      ],
      "content": "Microsoft Avro Library home page"
    },
    {
      "pos": [
        5029,
        5030
      ],
      "content": "."
    },
    {
      "pos": [
        5034,
        5068
      ],
      "content": "Compile schemas using Avro Library"
    },
    {
      "pos": [
        5070,
        5344
      ],
      "content": "The Microsoft Avro Library contains a code generation utility that allows creating C# types automatically based on the previously defined JSON schema. The code generation utility is not distributed as a binary executable, but can be easily built via the following procedure:",
      "nodes": [
        {
          "content": "The Microsoft Avro Library contains a code generation utility that allows creating C# types automatically based on the previously defined JSON schema.",
          "pos": [
            0,
            150
          ]
        },
        {
          "content": "The code generation utility is not distributed as a binary executable, but can be easily built via the following procedure:",
          "pos": [
            151,
            274
          ]
        }
      ]
    },
    {
      "pos": [
        5349,
        5572
      ],
      "content": "Download the .zip file with the latest version of HDInsight SDK source code from <ph id=\"ph14\">&lt;a href=\"http://hadoopsdk.codeplex.com/SourceControl/latest\" target=\"_blank\"&gt;</ph>Microsoft .NET SDK For Hadoop<ph id=\"ph15\">&lt;/a&gt;</ph>. (Click the <bpt id=\"p10\">**</bpt>Download<ept id=\"p10\">**</ept><ph id=\"ph16\"/> icon.)",
      "nodes": [
        {
          "content": "Download the .zip file with the latest version of HDInsight SDK source code from <ph id=\"ph14\">&lt;a href=\"http://hadoopsdk.codeplex.com/SourceControl/latest\" target=\"_blank\"&gt;</ph>Microsoft .NET SDK For Hadoop<ph id=\"ph15\">&lt;/a&gt;</ph>.",
          "pos": [
            0,
            242
          ]
        },
        {
          "content": "(Click the <bpt id=\"p10\">**</bpt>Download<ept id=\"p10\">**</ept><ph id=\"ph16\"/> icon.)",
          "pos": [
            243,
            328
          ]
        }
      ]
    },
    {
      "pos": [
        5577,
        5813
      ],
      "content": "Extract the HDInsight SDK to a directory on the machine with .NET Framework 4 installed and connected to the Internet for downloading necessary dependency NuGet packages. Below we will assume that the source code is extracted to C:\\SDK.",
      "nodes": [
        {
          "content": "Extract the HDInsight SDK to a directory on the machine with .NET Framework 4 installed and connected to the Internet for downloading necessary dependency NuGet packages.",
          "pos": [
            0,
            170
          ]
        },
        {
          "content": "Below we will assume that the source code is extracted to C:\\SDK.",
          "pos": [
            171,
            236
          ]
        }
      ]
    },
    {
      "pos": [
        5818,
        6239
      ],
      "content": "Go to the folder C:\\SDK\\src\\Microsoft.Hadoop.Avro.Tools and run build.bat. (The file will call MSBuild from the 32-bit distribution of the .NET Framework. If you would like to use the 64-bit version, edit build.bat, following the comments inside the file.) Ensure that the build is successful. (On some systems, MSBuild may produce warnings. These warnings do not affect the utility as long as there are no build errors.)",
      "nodes": [
        {
          "content": "Go to the folder C:\\SDK\\src\\Microsoft.Hadoop.Avro.Tools and run build.bat.",
          "pos": [
            0,
            74
          ]
        },
        {
          "content": "(The file will call MSBuild from the 32-bit distribution of the .NET Framework.",
          "pos": [
            75,
            154
          ]
        },
        {
          "content": "If you would like to use the 64-bit version, edit build.bat, following the comments inside the file.) Ensure that the build is successful.",
          "pos": [
            155,
            293
          ]
        },
        {
          "content": "(On some systems, MSBuild may produce warnings.",
          "pos": [
            294,
            341
          ]
        },
        {
          "content": "These warnings do not affect the utility as long as there are no build errors.)",
          "pos": [
            342,
            421
          ]
        }
      ]
    },
    {
      "pos": [
        6244,
        6335
      ],
      "content": "The compiled utility is located in C:\\SDK\\Bin\\Unsigned\\Release\\Microsoft.Hadoop.Avro.Tools."
    },
    {
      "pos": [
        6338,
        6521
      ],
      "content": "To get familiar with the command-line syntax, execute the following command from the folder where the code generation utility is located: <ph id=\"ph17\">`Microsoft.Hadoop.Avro.Tools help /c:codegen`</ph>"
    },
    {
      "pos": [
        6523,
        6666
      ],
      "content": "To test the utility, you can generate C# classes from the sample JSON schema file provided with the source code. Execute the following command:",
      "nodes": [
        {
          "content": "To test the utility, you can generate C# classes from the sample JSON schema file provided with the source code.",
          "pos": [
            0,
            112
          ]
        },
        {
          "content": "Execute the following command:",
          "pos": [
            113,
            143
          ]
        }
      ]
    },
    {
      "pos": [
        6788,
        6885
      ],
      "content": "This is supposed to produce two C# files in the current directory: SensorData.cs and Location.cs."
    },
    {
      "pos": [
        6887,
        7097
      ],
      "content": "To understand the logic that the code generation utility is using while converting the JSON schema to C# types, see the file GenerationVerification.feature located in C:\\SDK\\src\\Microsoft.Hadoop.Avro.Tools\\Doc."
    },
    {
      "pos": [
        7099,
        7576
      ],
      "content": "Please note that namespaces are extracted from the JSON schema, using the logic described in the file mentioned in the previous paragraph. Namespaces extracted from the schema take precedence over whatever is provided with the /n parameter in the utility command line. If you want to override the namespaces contained within the schema, use the /nf parameter. For example, to change all namespaces from the SampleJSONSchema.avsc to my.own.nspace, execute the following command:",
      "nodes": [
        {
          "content": "Please note that namespaces are extracted from the JSON schema, using the logic described in the file mentioned in the previous paragraph.",
          "pos": [
            0,
            138
          ]
        },
        {
          "content": "Namespaces extracted from the schema take precedence over whatever is provided with the /n parameter in the utility command line.",
          "pos": [
            139,
            268
          ]
        },
        {
          "content": "If you want to override the namespaces contained within the schema, use the /nf parameter.",
          "pos": [
            269,
            359
          ]
        },
        {
          "content": "For example, to change all namespaces from the SampleJSONSchema.avsc to my.own.nspace, execute the following command:",
          "pos": [
            360,
            477
          ]
        }
      ]
    },
    {
      "pos": [
        7720,
        7727
      ],
      "content": "Samples"
    },
    {
      "pos": [
        7728,
        8190
      ],
      "content": "Six examples provided in this topic illustrate different scenarios supported by the Microsoft Avro Library. The Microsoft Avro Library is designed to work with any stream. In these examples, data is manipulated via memory streams rather than file streams or databases for simplicity and consistency. The approach taken in a production environment will depend on the exact scenario requirements, data source and volume, performance constraints, and other factors.",
      "nodes": [
        {
          "content": "Six examples provided in this topic illustrate different scenarios supported by the Microsoft Avro Library.",
          "pos": [
            0,
            107
          ]
        },
        {
          "content": "The Microsoft Avro Library is designed to work with any stream.",
          "pos": [
            108,
            171
          ]
        },
        {
          "content": "In these examples, data is manipulated via memory streams rather than file streams or databases for simplicity and consistency.",
          "pos": [
            172,
            299
          ]
        },
        {
          "content": "The approach taken in a production environment will depend on the exact scenario requirements, data source and volume, performance constraints, and other factors.",
          "pos": [
            300,
            462
          ]
        }
      ]
    },
    {
      "pos": [
        8192,
        8424
      ],
      "content": "The first two examples show how to serialize and deserialize data into memory stream buffers by using reflection and generic records. The schema in these two cases is assumed to be shared between the readers and writers out-of-band.",
      "nodes": [
        {
          "content": "The first two examples show how to serialize and deserialize data into memory stream buffers by using reflection and generic records.",
          "pos": [
            0,
            133
          ]
        },
        {
          "content": "The schema in these two cases is assumed to be shared between the readers and writers out-of-band.",
          "pos": [
            134,
            232
          ]
        }
      ]
    },
    {
      "pos": [
        8426,
        8678
      ],
      "content": "The third and fourth examples show how to serialize and deserialize data by using the Avro object container files. When data is stored in an Avro container file, its schema is always stored with it because the schema must be shared for deserialization.",
      "nodes": [
        {
          "content": "The third and fourth examples show how to serialize and deserialize data by using the Avro object container files.",
          "pos": [
            0,
            114
          ]
        },
        {
          "content": "When data is stored in an Avro container file, its schema is always stored with it because the schema must be shared for deserialization.",
          "pos": [
            115,
            252
          ]
        }
      ]
    },
    {
      "pos": [
        8680,
        8752
      ],
      "content": "The sample containing the first four examples can be downloaded from the"
    },
    {
      "pos": [
        8856,
        8874
      ],
      "content": "Azure code samples"
    },
    {
      "pos": [
        8879,
        8884
      ],
      "content": "site."
    },
    {
      "pos": [
        8886,
        9060
      ],
      "content": "The fifth example shows how to how to use a custom compression codec for Avro object container files. A sample containing the code for this example can be downloaded from the",
      "nodes": [
        {
          "content": "The fifth example shows how to how to use a custom compression codec for Avro object container files.",
          "pos": [
            0,
            101
          ]
        },
        {
          "content": "A sample containing the code for this example can be downloaded from the",
          "pos": [
            102,
            174
          ]
        }
      ]
    },
    {
      "pos": [
        9164,
        9182
      ],
      "content": "Azure code samples"
    },
    {
      "pos": [
        9187,
        9192
      ],
      "content": "site."
    },
    {
      "pos": [
        9194,
        9383
      ],
      "content": "The sixth sample shows how to use Avro serialization to upload data to Azure Blob storage and then analyze it by using Hive with an HDInsight (Hadoop) cluster. It can be downloaded from the",
      "nodes": [
        {
          "content": "The sixth sample shows how to use Avro serialization to upload data to Azure Blob storage and then analyze it by using Hive with an HDInsight (Hadoop) cluster.",
          "pos": [
            0,
            159
          ]
        },
        {
          "content": "It can be downloaded from the",
          "pos": [
            160,
            189
          ]
        }
      ]
    },
    {
      "pos": [
        9490,
        9508
      ],
      "content": "Azure code samples"
    },
    {
      "pos": [
        9513,
        9518
      ],
      "content": "site."
    },
    {
      "pos": [
        9520,
        9577
      ],
      "content": "Here are links to the six samples discussed in the topic:"
    },
    {
      "pos": [
        9582,
        9743
      ],
      "content": "<ph id=\"ph18\">&lt;a href=\"#Scenario1\"&gt;</ph><bpt id=\"p11\">**</bpt>Serialization with reflection<ept id=\"p11\">**</ept><ph id=\"ph19\">&lt;/a&gt;</ph><ph id=\"ph20\"/> - The JSON schema for types to be serialized is automatically built from the data contract attributes."
    },
    {
      "pos": [
        9747,
        9910
      ],
      "content": "<ph id=\"ph21\">&lt;a href=\"#Scenario2\"&gt;</ph><bpt id=\"p12\">**</bpt>Serialization with generic record<ept id=\"p12\">**</ept><ph id=\"ph22\">&lt;/a&gt;</ph><ph id=\"ph23\"/> - The JSON schema is explicitly specified in a record when no .NET type is available for reflection."
    },
    {
      "pos": [
        9914,
        10122
      ],
      "content": "<ph id=\"ph24\">&lt;a href=\"#Scenario3\"&gt;</ph><bpt id=\"p13\">**</bpt>Serialization using object container files with reflection<ept id=\"p13\">**</ept><ph id=\"ph25\">&lt;/a&gt;</ph><ph id=\"ph26\"/> - The JSON schema is automatically built and shared together with the serialized data via an Avro object container file."
    },
    {
      "pos": [
        10126,
        10353
      ],
      "content": "<ph id=\"ph27\">&lt;a href=\"#Scenario4\"&gt;</ph><bpt id=\"p14\">**</bpt>Serialization using object container files with generic record<ept id=\"p14\">**</ept><ph id=\"ph28\">&lt;/a&gt;</ph><ph id=\"ph29\"/> - The JSON schema is explicitly specified before the serialization and shared together with the data via an Avro object container file."
    },
    {
      "pos": [
        10357,
        10601
      ],
      "content": "<ph id=\"ph30\">&lt;a href=\"#Scenario5\"&gt;</ph><bpt id=\"p15\">**</bpt>Serialization using object container files with a custom compression codec<ept id=\"p15\">**</ept><ph id=\"ph31\">&lt;/a&gt;</ph><ph id=\"ph32\"/> - The example shows how to create an Avro object container file with a customized .NET implementation of the Deflate data compression codec."
    },
    {
      "pos": [
        10605,
        10892
      ],
      "content": "<ph id=\"ph33\">&lt;a href=\"#Scenario6\"&gt;</ph><bpt id=\"p16\">**</bpt>Using Avro to upload data for the Microsoft Azure HDInsight service<ept id=\"p16\">**</ept><ph id=\"ph34\">&lt;/a&gt;</ph><ph id=\"ph35\"/> - The example illustrates how Avro serialization interacts with the HDInsight service. An active Azure subscription and access to an Azure HDInsight cluster are required to run this example.",
      "nodes": [
        {
          "content": "<ph id=\"ph33\">&lt;a href=\"#Scenario6\"&gt;</ph><bpt id=\"p16\">**</bpt>Using Avro to upload data for the Microsoft Azure HDInsight service<ept id=\"p16\">**</ept><ph id=\"ph34\">&lt;/a&gt;</ph><ph id=\"ph35\"/> - The example illustrates how Avro serialization interacts with the HDInsight service.",
          "pos": [
            0,
            288
          ]
        },
        {
          "content": "An active Azure subscription and access to an Azure HDInsight cluster are required to run this example.",
          "pos": [
            289,
            392
          ]
        }
      ]
    },
    {
      "pos": [
        10894,
        10897
      ],
      "content": "###"
    },
    {
      "pos": [
        10921,
        10960
      ],
      "content": "Sample 1: Serialization with reflection"
    },
    {
      "pos": [
        10962,
        11285
      ],
      "content": "The JSON schema for the types can be automatically built by the Microsoft Avro Library via reflection from the data contract attributes of the C# objects to be serialized. The Microsoft Avro Library creates an <bpt id=\"p17\">[</bpt><bpt id=\"p18\">**</bpt>IAvroSeralizer<ph id=\"ph36\">&lt;T&gt;</ph><ept id=\"p18\">**</ept><ept id=\"p17\">](http://msdn.microsoft.com/library/dn627341.aspx)</ept><ph id=\"ph37\"/> to identify the fields to be serialized.",
      "nodes": [
        {
          "content": "The JSON schema for the types can be automatically built by the Microsoft Avro Library via reflection from the data contract attributes of the C# objects to be serialized.",
          "pos": [
            0,
            171
          ]
        },
        {
          "content": "The Microsoft Avro Library creates an <bpt id=\"p17\">[</bpt><bpt id=\"p18\">**</bpt>IAvroSeralizer<ph id=\"ph36\">&lt;T&gt;</ph><ept id=\"p18\">**</ept><ept id=\"p17\">](http://msdn.microsoft.com/library/dn627341.aspx)</ept><ph id=\"ph37\"/> to identify the fields to be serialized.",
          "pos": [
            172,
            443
          ]
        }
      ]
    },
    {
      "pos": [
        11287,
        11580
      ],
      "content": "In this example, objects (a <bpt id=\"p19\">**</bpt>SensorData<ept id=\"p19\">**</ept><ph id=\"ph38\"/> class with a member <bpt id=\"p20\">**</bpt>Location<ept id=\"p20\">**</ept><ph id=\"ph39\"/> struct) are serialized to a memory stream, and this stream is in turn deserialized. The result is then compared to the initial instance to confirm that the <bpt id=\"p21\">**</bpt>SensorData<ept id=\"p21\">**</ept><ph id=\"ph40\"/> object recovered is identical to the original.",
      "nodes": [
        {
          "content": "In this example, objects (a <bpt id=\"p19\">**</bpt>SensorData<ept id=\"p19\">**</ept><ph id=\"ph38\"/> class with a member <bpt id=\"p20\">**</bpt>Location<ept id=\"p20\">**</ept><ph id=\"ph39\"/> struct) are serialized to a memory stream, and this stream is in turn deserialized.",
          "pos": [
            0,
            269
          ]
        },
        {
          "content": "The result is then compared to the initial instance to confirm that the <bpt id=\"p21\">**</bpt>SensorData<ept id=\"p21\">**</ept><ph id=\"ph40\"/> object recovered is identical to the original.",
          "pos": [
            270,
            458
          ]
        }
      ]
    },
    {
      "pos": [
        11582,
        11897
      ],
      "content": "The schema in this example is assumed to be shared between the readers and writers, so the Avro object container format is not required. For an example of how to serialize and deserialize data into memory buffers by using reflection with the object container format when the schema must be shared with the data, see",
      "nodes": [
        {
          "content": "The schema in this example is assumed to be shared between the readers and writers, so the Avro object container format is not required.",
          "pos": [
            0,
            136
          ]
        },
        {
          "content": "For an example of how to serialize and deserialize data into memory buffers by using reflection with the object container format when the schema must be shared with the data, see",
          "pos": [
            137,
            315
          ]
        }
      ]
    },
    {
      "pos": [
        11919,
        11977
      ],
      "content": "Serialization using object container files with reflection"
    },
    {
      "pos": [
        11981,
        11982
      ],
      "content": "."
    },
    {
      "pos": [
        16310,
        16355
      ],
      "content": "Sample 2: Serialization with a generic record"
    },
    {
      "pos": [
        16357,
        16860
      ],
      "content": "A JSON schema can be explicitly specified in a generic record when reflection cannot be used because the data cannot be represented via .NET classes with a data contract. This method is generally slower than using reflection. In such cases, the schema for the data may also be dynamic, i.e., not be known at compile time. Data represented as comma-separated values (CSV) files whose schema is unknown until it is transformed to the Avro format at run time is an example of this sort of dynamic scenario.",
      "nodes": [
        {
          "content": "A JSON schema can be explicitly specified in a generic record when reflection cannot be used because the data cannot be represented via .NET classes with a data contract.",
          "pos": [
            0,
            170
          ]
        },
        {
          "content": "This method is generally slower than using reflection.",
          "pos": [
            171,
            225
          ]
        },
        {
          "content": "In such cases, the schema for the data may also be dynamic, i.e., not be known at compile time.",
          "pos": [
            226,
            321
          ]
        },
        {
          "content": "Data represented as comma-separated values (CSV) files whose schema is unknown until it is transformed to the Avro format at run time is an example of this sort of dynamic scenario.",
          "pos": [
            322,
            503
          ]
        }
      ]
    },
    {
      "pos": [
        16862,
        17231
      ],
      "content": "This example shows how to create and use an <bpt id=\"p22\">[</bpt><bpt id=\"p23\">**</bpt>AvroRecord<ept id=\"p23\">**</ept><ept id=\"p22\">](http://msdn.microsoft.com/library/microsoft.hadoop.avro.avrorecord.aspx)</ept><ph id=\"ph41\"/> to explicitly specify a JSON schema, how to populate it with the data, and then how to serialize and deserialize it. The result is then compared to the initial instance to confirm that the record recovered is identical to the original.",
      "nodes": [
        {
          "content": "This example shows how to create and use an <bpt id=\"p22\">[</bpt><bpt id=\"p23\">**</bpt>AvroRecord<ept id=\"p23\">**</ept><ept id=\"p22\">](http://msdn.microsoft.com/library/microsoft.hadoop.avro.avrorecord.aspx)</ept><ph id=\"ph41\"/> to explicitly specify a JSON schema, how to populate it with the data, and then how to serialize and deserialize it.",
          "pos": [
            0,
            345
          ]
        },
        {
          "content": "The result is then compared to the initial instance to confirm that the record recovered is identical to the original.",
          "pos": [
            346,
            464
          ]
        }
      ]
    },
    {
      "pos": [
        17233,
        17571
      ],
      "content": "The schema in this example is assumed to be shared between the readers and writers, so the Avro object container format is not required. For an example of how to serialize and deserialize data into memory buffers by using a generic record with the object container format when the schema must be included with the serialized data, see the",
      "nodes": [
        {
          "content": "The schema in this example is assumed to be shared between the readers and writers, so the Avro object container format is not required.",
          "pos": [
            0,
            136
          ]
        },
        {
          "content": "For an example of how to serialize and deserialize data into memory buffers by using a generic record with the object container format when the schema must be included with the serialized data, see the",
          "pos": [
            137,
            338
          ]
        }
      ]
    },
    {
      "pos": [
        17593,
        17655
      ],
      "content": "Serialization using object container files with generic record"
    },
    {
      "pos": [
        17660,
        17668
      ],
      "content": "example."
    },
    {
      "pos": [
        22674,
        22760
      ],
      "content": "Sample 3: Serialization using object container files and serialization with reflection"
    },
    {
      "pos": [
        22762,
        23268
      ],
      "content": "This example is similar to the scenario in the <ph id=\"ph42\">&lt;a href=\"#Scenario1\"&gt;</ph><ph id=\"ph43\"/> first example<ph id=\"ph44\">&lt;/a&gt;</ph>, where the schema is implicitly specified with reflection. The difference is that here, the schema is not assumed to be known to the reader that deserializes it. The <bpt id=\"p24\">**</bpt>SensorData<ept id=\"p24\">**</ept><ph id=\"ph45\"/> objects to be serialized and their implicitly specified schema are stored in an Avro object container file represented by the <bpt id=\"p25\">[</bpt><bpt id=\"p26\">**</bpt>AvroContainer<ept id=\"p26\">**</ept><ept id=\"p25\">](http://msdn.microsoft.com/library/microsoft.hadoop.avro.container.avrocontainer.aspx)</ept><ph id=\"ph46\"/> class.",
      "nodes": [
        {
          "content": "This example is similar to the scenario in the <ph id=\"ph42\">&lt;a href=\"#Scenario1\"&gt;</ph><ph id=\"ph43\"/> first example<ph id=\"ph44\">&lt;/a&gt;</ph>, where the schema is implicitly specified with reflection.",
          "pos": [
            0,
            210
          ]
        },
        {
          "content": "The difference is that here, the schema is not assumed to be known to the reader that deserializes it.",
          "pos": [
            211,
            313
          ]
        },
        {
          "content": "The <bpt id=\"p24\">**</bpt>SensorData<ept id=\"p24\">**</ept><ph id=\"ph45\"/> objects to be serialized and their implicitly specified schema are stored in an Avro object container file represented by the <bpt id=\"p25\">[</bpt><bpt id=\"p26\">**</bpt>AvroContainer<ept id=\"p26\">**</ept><ept id=\"p25\">](http://msdn.microsoft.com/library/microsoft.hadoop.avro.container.avrocontainer.aspx)</ept><ph id=\"ph46\"/> class.",
          "pos": [
            314,
            721
          ]
        }
      ]
    },
    {
      "pos": [
        23270,
        23577
      ],
      "content": "The data is serialized in this example with <bpt id=\"p27\">[</bpt><bpt id=\"p28\">**</bpt>SequentialWriter<ph id=\"ph47\">&lt;SensorData&gt;</ph><ept id=\"p28\">**</ept><ept id=\"p27\">](http://msdn.microsoft.com/library/dn627340.aspx)</ept><ph id=\"ph48\"/> and deserialized with <bpt id=\"p29\">[</bpt><bpt id=\"p30\">**</bpt>SequentialReader<ph id=\"ph49\">&lt;SensorData&gt;</ph><ept id=\"p30\">**</ept><ept id=\"p29\">](http://msdn.microsoft.com/library/dn627340.aspx)</ept>. The result then is compared to the initial instances to ensure identity.",
      "nodes": [
        {
          "content": "The data is serialized in this example with <bpt id=\"p27\">[</bpt><bpt id=\"p28\">**</bpt>SequentialWriter<ph id=\"ph47\">&lt;SensorData&gt;</ph><ept id=\"p28\">**</ept><ept id=\"p27\">](http://msdn.microsoft.com/library/dn627340.aspx)</ept><ph id=\"ph48\"/> and deserialized with <bpt id=\"p29\">[</bpt><bpt id=\"p30\">**</bpt>SequentialReader<ph id=\"ph49\">&lt;SensorData&gt;</ph><ept id=\"p30\">**</ept><ept id=\"p29\">](http://msdn.microsoft.com/library/dn627340.aspx)</ept>.",
          "pos": [
            0,
            459
          ]
        },
        {
          "content": "The result then is compared to the initial instances to ensure identity.",
          "pos": [
            460,
            532
          ]
        }
      ]
    },
    {
      "pos": [
        23579,
        23916
      ],
      "content": "The data in the object container file is compressed via the default <bpt id=\"p31\">[</bpt><bpt id=\"p32\">**</bpt>Deflate<ept id=\"p32\">**</ept><ept id=\"p31\">][deflate-100]</ept><ph id=\"ph50\"/> compression codec from .NET Framework 4. See the <ph id=\"ph51\">&lt;a href=\"#Scenario5\"&gt;</ph><ph id=\"ph52\"/> fifth example<ph id=\"ph53\">&lt;/a&gt;</ph><ph id=\"ph54\"/> in this topic to learn how to use a more recent and superior version of the <bpt id=\"p33\">[</bpt><bpt id=\"p34\">**</bpt>Deflate<ept id=\"p34\">**</ept><ept id=\"p33\">][deflate-110]</ept><ph id=\"ph55\"/> compression codec available in .NET Framework 4.5.",
      "nodes": [
        {
          "content": "The data in the object container file is compressed via the default <bpt id=\"p31\">[</bpt><bpt id=\"p32\">**</bpt>Deflate<ept id=\"p32\">**</ept><ept id=\"p31\">][deflate-100]</ept><ph id=\"ph50\"/> compression codec from .NET Framework 4.",
          "pos": [
            0,
            230
          ]
        },
        {
          "content": "See the <ph id=\"ph51\">&lt;a href=\"#Scenario5\"&gt;</ph><ph id=\"ph52\"/> fifth example<ph id=\"ph53\">&lt;/a&gt;</ph><ph id=\"ph54\"/> in this topic to learn how to use a more recent and superior version of the <bpt id=\"p33\">[</bpt><bpt id=\"p34\">**</bpt>Deflate<ept id=\"p34\">**</ept><ept id=\"p33\">][deflate-110]</ept><ph id=\"ph55\"/> compression codec available in .NET Framework 4.5.",
          "pos": [
            231,
            607
          ]
        }
      ]
    },
    {
      "pos": [
        33299,
        33389
      ],
      "content": "Sample 4: Serialization using object container files and serialization with generic record"
    },
    {
      "pos": [
        33391,
        33437
      ],
      "content": "This example is similar to the scenario in the"
    },
    {
      "pos": [
        33460,
        33474
      ],
      "content": "second example"
    },
    {
      "pos": [
        33478,
        33634
      ],
      "content": ", where the schema is explicitly specified with JSON. The difference is that here, the schema is not assumed to be known to the reader that deserializes it.",
      "nodes": [
        {
          "content": ", where the schema is explicitly specified with JSON.",
          "pos": [
            0,
            53
          ]
        },
        {
          "content": "The difference is that here, the schema is not assumed to be known to the reader that deserializes it.",
          "pos": [
            54,
            156
          ]
        }
      ]
    },
    {
      "pos": [
        33636,
        34321
      ],
      "content": "The test data set is collected into a list of <bpt id=\"p35\">[</bpt><bpt id=\"p36\">**</bpt>AvroRecord<ept id=\"p36\">**</ept><ept id=\"p35\">](http://msdn.microsoft.com/library/microsoft.hadoop.avro.avrorecord.aspx)</ept><ph id=\"ph56\"/> objects via an explicitly defined JSON schema and then stored in an object container file represented by the <bpt id=\"p37\">[</bpt><bpt id=\"p38\">**</bpt>AvroContainer<ept id=\"p38\">**</ept><ept id=\"p37\">](http://msdn.microsoft.com/library/microsoft.hadoop.avro.container.avrocontainer.aspx)</ept><ph id=\"ph57\"/> class. This container file creates a writer that is used to serialize the data, uncompressed, to a memory stream that is then saved to a file. The <bpt id=\"p39\">[</bpt><bpt id=\"p40\">**</bpt>Codec.Null<ept id=\"p40\">**</ept><ept id=\"p39\">](http://msdn.microsoft.com/library/microsoft.hadoop.avro.container.codec.null.aspx)</ept><ph id=\"ph58\"/> parameter used for creating the reader specifies that this data will not be compressed.",
      "nodes": [
        {
          "content": "The test data set is collected into a list of <bpt id=\"p35\">[</bpt><bpt id=\"p36\">**</bpt>AvroRecord<ept id=\"p36\">**</ept><ept id=\"p35\">](http://msdn.microsoft.com/library/microsoft.hadoop.avro.avrorecord.aspx)</ept><ph id=\"ph56\"/> objects via an explicitly defined JSON schema and then stored in an object container file represented by the <bpt id=\"p37\">[</bpt><bpt id=\"p38\">**</bpt>AvroContainer<ept id=\"p38\">**</ept><ept id=\"p37\">](http://msdn.microsoft.com/library/microsoft.hadoop.avro.container.avrocontainer.aspx)</ept><ph id=\"ph57\"/> class.",
          "pos": [
            0,
            547
          ]
        },
        {
          "content": "This container file creates a writer that is used to serialize the data, uncompressed, to a memory stream that is then saved to a file.",
          "pos": [
            548,
            683
          ]
        },
        {
          "content": "The <bpt id=\"p39\">[</bpt><bpt id=\"p40\">**</bpt>Codec.Null<ept id=\"p40\">**</ept><ept id=\"p39\">](http://msdn.microsoft.com/library/microsoft.hadoop.avro.container.codec.null.aspx)</ept><ph id=\"ph58\"/> parameter used for creating the reader specifies that this data will not be compressed.",
          "pos": [
            684,
            970
          ]
        }
      ]
    },
    {
      "pos": [
        34323,
        34505
      ],
      "content": "The data is then read from the file and deserialized into a collection of objects. This collection is compared to the initial list of Avro records to confirm that they are identical.",
      "nodes": [
        {
          "content": "The data is then read from the file and deserialized into a collection of objects.",
          "pos": [
            0,
            82
          ]
        },
        {
          "content": "This collection is compared to the initial list of Avro records to confirm that they are identical.",
          "pos": [
            83,
            182
          ]
        }
      ]
    },
    {
      "pos": [
        45635,
        45719
      ],
      "content": "Sample 5: Serialization using object container files with a custom compression codec"
    },
    {
      "pos": [
        45721,
        46000
      ],
      "content": "The fifth example shows how to how to use a custom compression codec for Avro object container files. A sample containing the code for this example can be downloaded from the <bpt id=\"p41\">[</bpt>Azure code samples<ept id=\"p41\">](http://code.msdn.microsoft.com/windowsazure/Serialize-data-with-the-67159111)</ept><ph id=\"ph59\"/> site.",
      "nodes": [
        {
          "content": "The fifth example shows how to how to use a custom compression codec for Avro object container files.",
          "pos": [
            0,
            101
          ]
        },
        {
          "content": "A sample containing the code for this example can be downloaded from the <bpt id=\"p41\">[</bpt>Azure code samples<ept id=\"p41\">](http://code.msdn.microsoft.com/windowsazure/Serialize-data-with-the-67159111)</ept><ph id=\"ph59\"/> site.",
          "pos": [
            102,
            334
          ]
        }
      ]
    },
    {
      "pos": [
        46002,
        46629
      ],
      "content": "The <bpt id=\"p42\">[</bpt>Avro Specification<ept id=\"p42\">](http://avro.apache.org/docs/current/spec.html#Required+Codecs)</ept><ph id=\"ph60\"/> allows usage of an optional compression codec (in addition to <bpt id=\"p43\">**</bpt>Null<ept id=\"p43\">**</ept><ph id=\"ph61\"/> and <bpt id=\"p44\">**</bpt>Deflate<ept id=\"p44\">**</ept><ph id=\"ph62\"/> defaults). This example is not implementing a completely new codec such as Snappy (mentioned as a supported optional codec in the <bpt id=\"p45\">[</bpt>Avro Specification<ept id=\"p45\">](http://avro.apache.org/docs/current/spec.html#snappy)</ept>). It shows how to use the .NET Framework 4.5 implementation of the <bpt id=\"p46\">[</bpt><bpt id=\"p47\">**</bpt>Deflate<ept id=\"p47\">**</ept><ept id=\"p46\">][deflate-110]</ept><ph id=\"ph63\"/> codec, which provides a better compression algorithm based on the <bpt id=\"p48\">[</bpt>zlib<ept id=\"p48\">](http://zlib.net/)</ept><ph id=\"ph64\"/> compression library than the default .NET Framework 4 version.",
      "nodes": [
        {
          "content": "The <bpt id=\"p42\">[</bpt>Avro Specification<ept id=\"p42\">](http://avro.apache.org/docs/current/spec.html#Required+Codecs)</ept><ph id=\"ph60\"/> allows usage of an optional compression codec (in addition to <bpt id=\"p43\">**</bpt>Null<ept id=\"p43\">**</ept><ph id=\"ph61\"/> and <bpt id=\"p44\">**</bpt>Deflate<ept id=\"p44\">**</ept><ph id=\"ph62\"/> defaults).",
          "pos": [
            0,
            350
          ]
        },
        {
          "content": "This example is not implementing a completely new codec such as Snappy (mentioned as a supported optional codec in the <bpt id=\"p45\">[</bpt>Avro Specification<ept id=\"p45\">](http://avro.apache.org/docs/current/spec.html#snappy)</ept>).",
          "pos": [
            351,
            586
          ]
        },
        {
          "content": "It shows how to use the .NET Framework 4.5 implementation of the <bpt id=\"p46\">[</bpt><bpt id=\"p47\">**</bpt>Deflate<ept id=\"p47\">**</ept><ept id=\"p46\">][deflate-110]</ept><ph id=\"ph63\"/> codec, which provides a better compression algorithm based on the <bpt id=\"p48\">[</bpt>zlib<ept id=\"p48\">](http://zlib.net/)</ept><ph id=\"ph64\"/> compression library than the default .NET Framework 4 version.",
          "pos": [
            587,
            982
          ]
        }
      ]
    },
    {
      "pos": [
        65455,
        65532
      ],
      "content": "Sample 6: Using Avro to upload data for the Microsoft Azure HDInsight service"
    },
    {
      "pos": [
        65534,
        65829
      ],
      "content": "The sixth example illustrates some programming techniques related to interacting with the Azure HDInsight service. A sample containing the code for this example can be downloaded from the <bpt id=\"p49\">[</bpt>Azure code samples<ept id=\"p49\">](https://code.msdn.microsoft.com/windowsazure/Using-Avro-to-upload-data-ae81b1e3)</ept><ph id=\"ph65\"/> site.",
      "nodes": [
        {
          "content": "The sixth example illustrates some programming techniques related to interacting with the Azure HDInsight service.",
          "pos": [
            0,
            114
          ]
        },
        {
          "content": "A sample containing the code for this example can be downloaded from the <bpt id=\"p49\">[</bpt>Azure code samples<ept id=\"p49\">](https://code.msdn.microsoft.com/windowsazure/Using-Avro-to-upload-data-ae81b1e3)</ept><ph id=\"ph65\"/> site.",
          "pos": [
            115,
            350
          ]
        }
      ]
    },
    {
      "pos": [
        65831,
        65861
      ],
      "content": "The sample does the following:"
    },
    {
      "pos": [
        65865,
        65915
      ],
      "content": "Connects to an existing HDInsight service cluster."
    },
    {
      "pos": [
        65918,
        66431
      ],
      "content": "Serializes several CSV files and uploads the result to Azure Blob storage. (The CSV files are distributed together with the sample and represent an extract from AMEX Stock historical data distributed by <bpt id=\"p50\">[</bpt>Infochimps<ept id=\"p50\">](http://www.infochimps.com/)</ept><ph id=\"ph66\"/> for the period 1970-2010. The sample reads CSV file data, converts the records to instances of the <bpt id=\"p51\">**</bpt>Stock<ept id=\"p51\">**</ept><ph id=\"ph67\"/> class, and then serializes them by using reflection. Stock type definition is created from a JSON schema via the Microsoft Avro Library code generation utility.",
      "nodes": [
        {
          "content": "Serializes several CSV files and uploads the result to Azure Blob storage.",
          "pos": [
            0,
            74
          ]
        },
        {
          "content": "(The CSV files are distributed together with the sample and represent an extract from AMEX Stock historical data distributed by <bpt id=\"p50\">[</bpt>Infochimps<ept id=\"p50\">](http://www.infochimps.com/)</ept><ph id=\"ph66\"/> for the period 1970-2010.",
          "pos": [
            75,
            324
          ]
        },
        {
          "content": "The sample reads CSV file data, converts the records to instances of the <bpt id=\"p51\">**</bpt>Stock<ept id=\"p51\">**</ept><ph id=\"ph67\"/> class, and then serializes them by using reflection.",
          "pos": [
            325,
            515
          ]
        },
        {
          "content": "Stock type definition is created from a JSON schema via the Microsoft Avro Library code generation utility.",
          "pos": [
            516,
            623
          ]
        }
      ]
    },
    {
      "pos": [
        66434,
        66545
      ],
      "content": "Creates a new external table called <bpt id=\"p52\">**</bpt>Stocks<ept id=\"p52\">**</ept><ph id=\"ph68\"/> in Hive, and links it to the data uploaded in the previous step."
    },
    {
      "pos": [
        66548,
        66605
      ],
      "content": "Executes a query by using Hive over the <bpt id=\"p53\">**</bpt>Stocks<ept id=\"p53\">**</ept><ph id=\"ph69\"/> table."
    },
    {
      "pos": [
        66607,
        66891
      ],
      "content": "In addition, the sample performs a clean-up procedure before and after performing major operations. During the clean-up, all of the related Azure Blob data and folders are removed, and the Hive table is dropped. You can also invoke the clean-up procedure from the sample command line.",
      "nodes": [
        {
          "content": "In addition, the sample performs a clean-up procedure before and after performing major operations.",
          "pos": [
            0,
            99
          ]
        },
        {
          "content": "During the clean-up, all of the related Azure Blob data and folders are removed, and the Hive table is dropped.",
          "pos": [
            100,
            211
          ]
        },
        {
          "content": "You can also invoke the clean-up procedure from the sample command line.",
          "pos": [
            212,
            284
          ]
        }
      ]
    },
    {
      "pos": [
        66893,
        66936
      ],
      "content": "The sample has the following prerequisites:"
    },
    {
      "pos": [
        66940,
        67003
      ],
      "content": "An active Microsoft Azure subscription and its subscription ID."
    },
    {
      "pos": [
        67006,
        67198
      ],
      "content": "A management certificate for the subscription with the corresponding private key. The certificate should be installed in the current user private storage on the machine used to run the sample.",
      "nodes": [
        {
          "content": "A management certificate for the subscription with the corresponding private key.",
          "pos": [
            0,
            81
          ]
        },
        {
          "content": "The certificate should be installed in the current user private storage on the machine used to run the sample.",
          "pos": [
            82,
            192
          ]
        }
      ]
    },
    {
      "pos": [
        67201,
        67229
      ],
      "content": "An active HDInsight cluster."
    },
    {
      "pos": [
        67232,
        67385
      ],
      "content": "An Azure Storage account linked to the HDInsight cluster from the previous prerequisite, together with the corresponding primary or secondary access key."
    },
    {
      "pos": [
        67387,
        67547
      ],
      "content": "All of the information from the prerequisites should be entered to the sample configuration file before the sample is run. There are two possible ways to do it:",
      "nodes": [
        {
          "content": "All of the information from the prerequisites should be entered to the sample configuration file before the sample is run.",
          "pos": [
            0,
            122
          ]
        },
        {
          "content": "There are two possible ways to do it:",
          "pos": [
            123,
            160
          ]
        }
      ]
    },
    {
      "pos": [
        67551,
        67630
      ],
      "content": "Edit the app.config file in the sample root directory and then build the sample"
    },
    {
      "pos": [
        67633,
        67718
      ],
      "content": "First build the sample, and then edit AvroHDISample.exe.config in the build directory"
    },
    {
      "pos": [
        67720,
        68045
      ],
      "content": "In both cases all edits should be done in the <bpt id=\"p54\">**</bpt><ph id=\"ph70\">&lt;appSettings&gt;</ph><ept id=\"p54\">**</ept><ph id=\"ph71\"/> settings section. Please follow the comments in the file.\nThe sample is run from the command line by executing the following command (where the .zip file with the sample was assumed to be extracted to C:\\AvroHDISample; if otherwise, use the relevant file path):",
      "nodes": [
        {
          "content": "In both cases all edits should be done in the <bpt id=\"p54\">**</bpt><ph id=\"ph70\">&lt;appSettings&gt;</ph><ept id=\"p54\">**</ept><ph id=\"ph71\"/> settings section.",
          "pos": [
            0,
            161
          ]
        },
        {
          "content": "Please follow the comments in the file.",
          "pos": [
            162,
            201
          ]
        },
        {
          "content": "The sample is run from the command line by executing the following command (where the .zip file with the sample was assumed to be extracted to C:\\AvroHDISample; if otherwise, use the relevant file path):",
          "pos": [
            202,
            405
          ]
        }
      ]
    },
    {
      "pos": [
        68092,
        68143
      ],
      "content": "To clean up the cluster, run the following command:"
    }
  ],
  "content": "<properties\n    pageTitle=\"Serialize data with the Microsoft Avro Library | Microsoft Azure\"\n    description=\"Learn how Azure HDInsight uses Avro to serialize big data.\"\n    services=\"hdinsight\"\n    documentationCenter=\"\"\n    tags=\"azure-portal\"\n    authors=\"mumian\" \n    manager=\"paulettm\"\n    editor=\"cgronlun\"/>\n\n<tags\n    ms.service=\"hdinsight\"\n    ms.workload=\"big-data\"\n    ms.tgt_pltfrm=\"na\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\"\n    ms.date=\"02/11/2016\"\n    ms.author=\"jgao\"/>\n\n\n# Serialize data in Hadoop with the Microsoft Avro Library\n\nThis topic shows how to use the <a href=\"https://hadoopsdk.codeplex.com/wikipage?title=Avro%20Library\" target=\"_blank\">Microsoft Avro Library</a> to serialize objects and other data structures into streams in order to persist them to memory, a database, or a file, and also how to deserialize them to recover the original objects.\n\n[AZURE.INCLUDE [windows-only](../../includes/hdinsight-windows-only.md)]\n\n##Apache Avro\nThe <a href=\"https://hadoopsdk.codeplex.com/wikipage?title=Avro%20Library\" target=\"_blank\">Microsoft Avro Library</a> implements the Apache Avro data serialization system for the Microsoft.NET environment. Apache Avro provides a compact binary data interchange format for serialization. It uses <a href=\"http://www.json.org\" target=\"_blank\">JSON</a> to define a language-agnostic schema that underwrites language interoperability. Data serialized in one language can be read in another. Currently C, C++, C#, Java, PHP, Python, and Ruby are supported. Detailed information on the format can be found in the <a href=\"http://avro.apache.org/docs/current/spec.html\" target=\"_blank\">Apache Avro Specification</a>. Note that the current version of the Microsoft Avro Library does not support the remote procedure calls (RPCs) part of this specification.\n\nThe serialized representation of an object in the Avro system consists of two parts: schema and actual value. The Avro schema describes the language-independent data model of the serialized data with JSON. It is present side-by-side with a binary representation of data. Having the schema separate from the binary representation permits each object to be written with no per-value overheads, making serialization fast and the representation small.\n\n##The Hadoop scenario\nThe Apache Avro serialization format is widely used in Azure HDInsight and other Apache Hadoop environments. Avro provides a convenient way to represent complex data structures within a Hadoop MapReduce job. The format of Avro files (Avro object container file) has been designed to support the distributed MapReduce programming model. The key feature that enables the distribution is that the files are “splittable” in the sense that one can seek any point in a file and start reading from a particular block.\n\n##Serialization in Avro Library\nThe .NET Library for Avro supports two ways of serializing objects:\n\n- **reflection** - The JSON schema for the types is automatically built from the data contract attributes of the .NET types to be serialized.\n- **generic record** - A JSON schema is explicitly specified in a record represented by the [**AvroRecord**](http://msdn.microsoft.com/library/microsoft.hadoop.avro.avrorecord.aspx) class when no .NET types are present to describe the schema for the data to be serialized.\n\nWhen the data schema is known to both the writer and reader of the stream, the data can be sent without its schema. In cases when an Avro object container file is used, the schema is stored within the file. Other parameters, such as the codec used for data compression, can be specified. These scenarios are outlined in more detail and illustrated in the code examples below.\n\n\n## Install Avro Library\n\nThe following are required before you install the libary:\n\n- <a href=\"http://www.microsoft.com/download/details.aspx?id=17851\" target=\"_blank\">Microsoft .NET Framework 4</a>\n- <a href=\"http://james.newtonking.com/json\" target=\"_blank\">Newtonsoft Json.NET</a> (6.0.4 or later)\n\nNote that the Newtonsoft.Json.dll dependency is downloaded automatically with the installation of the Microsoft Avro Library. The procedure for this is provided in the following section.\n\n\nThe Microsoft Avro Library is distributed as a NuGet package that can be installed from Visual Studio via the following procedure:\n\n1. Select the **Project** tab -> **Manage NuGet Packages...**\n2. Search for \"Microsoft.Hadoop.Avro\" in the **Search Online** box.\n3. Click the **Install** button next to **Microsoft Azure HDInsight Avro Library**.\n\nNote that the Newtonsoft.Json.dll (>=6.0.4) dependency is also downloaded automatically with the Microsoft Avro Library.\n\nYou may want to visit the <a href=\"https://hadoopsdk.codeplex.com/wikipage?title=Avro%20Library\" target=\"_blank\">Microsoft Avro Library home page</a> to read the current release notes.\n\n\nThe Microsoft Avro Library source code is available at the <a href=\"https://hadoopsdk.codeplex.com/wikipage?title=Avro%20Library\" target=\"_blank\">Microsoft Avro Library home page</a>.\n\n##Compile schemas using Avro Library\n\nThe Microsoft Avro Library contains a code generation utility that allows creating C# types automatically based on the previously defined JSON schema. The code generation utility is not distributed as a binary executable, but can be easily built via the following procedure:\n\n1. Download the .zip file with the latest version of HDInsight SDK source code from <a href=\"http://hadoopsdk.codeplex.com/SourceControl/latest\" target=\"_blank\">Microsoft .NET SDK For Hadoop</a>. (Click the **Download** icon.)\n\n2. Extract the HDInsight SDK to a directory on the machine with .NET Framework 4 installed and connected to the Internet for downloading necessary dependency NuGet packages. Below we will assume that the source code is extracted to C:\\SDK.\n\n3. Go to the folder C:\\SDK\\src\\Microsoft.Hadoop.Avro.Tools and run build.bat. (The file will call MSBuild from the 32-bit distribution of the .NET Framework. If you would like to use the 64-bit version, edit build.bat, following the comments inside the file.) Ensure that the build is successful. (On some systems, MSBuild may produce warnings. These warnings do not affect the utility as long as there are no build errors.)\n\n4. The compiled utility is located in C:\\SDK\\Bin\\Unsigned\\Release\\Microsoft.Hadoop.Avro.Tools.\n\n\nTo get familiar with the command-line syntax, execute the following command from the folder where the code generation utility is located: `Microsoft.Hadoop.Avro.Tools help /c:codegen`\n\nTo test the utility, you can generate C# classes from the sample JSON schema file provided with the source code. Execute the following command:\n\n    Microsoft.Hadoop.Avro.Tools codegen /i:C:\\SDK\\src\\Microsoft.Hadoop.Avro.Tools\\SampleJSON\\SampleJSONSchema.avsc /o:\n\nThis is supposed to produce two C# files in the current directory: SensorData.cs and Location.cs.\n\nTo understand the logic that the code generation utility is using while converting the JSON schema to C# types, see the file GenerationVerification.feature located in C:\\SDK\\src\\Microsoft.Hadoop.Avro.Tools\\Doc.\n\nPlease note that namespaces are extracted from the JSON schema, using the logic described in the file mentioned in the previous paragraph. Namespaces extracted from the schema take precedence over whatever is provided with the /n parameter in the utility command line. If you want to override the namespaces contained within the schema, use the /nf parameter. For example, to change all namespaces from the SampleJSONSchema.avsc to my.own.nspace, execute the following command:\n\n    Microsoft.Hadoop.Avro.Tools codegen /i:C:\\SDK\\src\\Microsoft.Hadoop.Avro.Tools\\SampleJSON\\SampleJSONSchema.avsc /o:. /nf:my.own.nspace\n\n## Samples\nSix examples provided in this topic illustrate different scenarios supported by the Microsoft Avro Library. The Microsoft Avro Library is designed to work with any stream. In these examples, data is manipulated via memory streams rather than file streams or databases for simplicity and consistency. The approach taken in a production environment will depend on the exact scenario requirements, data source and volume, performance constraints, and other factors.\n\nThe first two examples show how to serialize and deserialize data into memory stream buffers by using reflection and generic records. The schema in these two cases is assumed to be shared between the readers and writers out-of-band.\n\nThe third and fourth examples show how to serialize and deserialize data by using the Avro object container files. When data is stored in an Avro container file, its schema is always stored with it because the schema must be shared for deserialization.\n\nThe sample containing the first four examples can be downloaded from the <a href=\"http://code.msdn.microsoft.com/windowsazure/Serialize-data-with-the-86055923\" target=\"_blank\">Azure code samples</a> site.\n\nThe fifth example shows how to how to use a custom compression codec for Avro object container files. A sample containing the code for this example can be downloaded from the <a href=\"http://code.msdn.microsoft.com/windowsazure/Serialize-data-with-the-67159111\" target=\"_blank\">Azure code samples</a> site.\n\nThe sixth sample shows how to use Avro serialization to upload data to Azure Blob storage and then analyze it by using Hive with an HDInsight (Hadoop) cluster. It can be downloaded from the <a href=\"https://code.msdn.microsoft.com/windowsazure/Using-Avro-to-upload-data-ae81b1e3\" target=\"_blank\">Azure code samples</a> site.\n\nHere are links to the six samples discussed in the topic:\n\n * <a href=\"#Scenario1\">**Serialization with reflection**</a> - The JSON schema for types to be serialized is automatically built from the data contract attributes.\n * <a href=\"#Scenario2\">**Serialization with generic record**</a> - The JSON schema is explicitly specified in a record when no .NET type is available for reflection.\n * <a href=\"#Scenario3\">**Serialization using object container files with reflection**</a> - The JSON schema is automatically built and shared together with the serialized data via an Avro object container file.\n * <a href=\"#Scenario4\">**Serialization using object container files with generic record**</a> - The JSON schema is explicitly specified before the serialization and shared together with the data via an Avro object container file.\n * <a href=\"#Scenario5\">**Serialization using object container files with a custom compression codec**</a> - The example shows how to create an Avro object container file with a customized .NET implementation of the Deflate data compression codec.\n * <a href=\"#Scenario6\">**Using Avro to upload data for the Microsoft Azure HDInsight service**</a> - The example illustrates how Avro serialization interacts with the HDInsight service. An active Azure subscription and access to an Azure HDInsight cluster are required to run this example.\n\n###<a name=\"Scenario1\"></a>Sample 1: Serialization with reflection\n\nThe JSON schema for the types can be automatically built by the Microsoft Avro Library via reflection from the data contract attributes of the C# objects to be serialized. The Microsoft Avro Library creates an [**IAvroSeralizer<T>**](http://msdn.microsoft.com/library/dn627341.aspx) to identify the fields to be serialized.\n\nIn this example, objects (a **SensorData** class with a member **Location** struct) are serialized to a memory stream, and this stream is in turn deserialized. The result is then compared to the initial instance to confirm that the **SensorData** object recovered is identical to the original.\n\nThe schema in this example is assumed to be shared between the readers and writers, so the Avro object container format is not required. For an example of how to serialize and deserialize data into memory buffers by using reflection with the object container format when the schema must be shared with the data, see <a href=\"#Scenario3\">Serialization using object container files with reflection</a>.\n\n    namespace Microsoft.Hadoop.Avro.Sample\n    {\n        using System;\n        using System.Collections.Generic;\n        using System.IO;\n        using System.Linq;\n        using System.Runtime.Serialization;\n        using Microsoft.Hadoop.Avro.Container;\n        using Microsoft.Hadoop.Avro;\n\n        //Sample class used in serialization samples\n        [DataContract(Name = \"SensorDataValue\", Namespace = \"Sensors\")]\n        internal class SensorData\n        {\n            [DataMember(Name = \"Location\")]\n            public Location Position { get; set; }\n\n            [DataMember(Name = \"Value\")]\n            public byte[] Value { get; set; }\n        }\n\n        //Sample struct used in serialization samples\n        [DataContract]\n        internal struct Location\n        {\n            [DataMember]\n            public int Floor { get; set; }\n\n            [DataMember]\n            public int Room { get; set; }\n        }\n\n        //This class contains all methods demonstrating\n        //the usage of Microsoft Avro Library\n        public class AvroSample\n        {\n\n            //Serialize and deserialize sample data set represented as an object using reflection.\n            //No explicit schema definition is required - schema of serialized objects is automatically built.\n            public void SerializeDeserializeObjectUsingReflection()\n            {\n\n                Console.WriteLine(\"SERIALIZATION USING REFLECTION\\n\");\n                Console.WriteLine(\"Serializing Sample Data Set...\");\n\n                //Create a new AvroSerializer instance and specify a custom serialization strategy AvroDataContractResolver\n                //for serializing only properties attributed with DataContract/DateMember\n                var avroSerializer = AvroSerializer.Create<SensorData>();\n\n                //Create a memory stream buffer\n                using (var buffer = new MemoryStream())\n                {\n                    //Create a data set by using sample class and struct\n                    var expected = new SensorData { Value = new byte[] { 1, 2, 3, 4, 5 }, Position = new Location { Room = 243, Floor = 1 } };\n\n                    //Serialize the data to the specified stream\n                    avroSerializer.Serialize(buffer, expected);\n\n\n                    Console.WriteLine(\"Deserializing Sample Data Set...\");\n\n                    //Prepare the stream for deserializing the data\n                    buffer.Seek(0, SeekOrigin.Begin);\n\n                    //Deserialize data from the stream and cast it to the same type used for serialization\n                    var actual = avroSerializer.Deserialize(buffer);\n\n                    Console.WriteLine(\"Comparing Initial and Deserialized Data Sets...\");\n\n                    //Finally, verify that deserialized data matches the original one\n                    bool isEqual = this.Equal(expected, actual);\n\n                    Console.WriteLine(\"Result of Data Set Identity Comparison is {0}\", isEqual);\n\n                }\n            }\n\n            //\n            //Helper methods\n            //\n\n            //Comparing two SensorData objects\n            private bool Equal(SensorData left, SensorData right)\n            {\n                return left.Position.Equals(right.Position) && left.Value.SequenceEqual(right.Value);\n            }\n\n\n\n            static void Main()\n            {\n\n                string sectionDivider = \"---------------------------------------- \";\n\n                //Create an instance of AvroSample Class and invoke methods\n                //illustrating different serializing approaches\n                AvroSample Sample = new AvroSample();\n\n                //Serialization to memory using reflection\n                Sample.SerializeDeserializeObjectUsingReflection();\n\n                Console.WriteLine(sectionDivider);\n                Console.WriteLine(\"Press any key to exit.\");\n                Console.Read();\n            }\n        }\n    }\n    // The example is expected to display the following output:\n    // SERIALIZATION USING REFLECTION\n    //\n    // Serializing Sample Data Set...\n    // Deserializing Sample Data Set...\n    // Comparing Initial and Deserialized Data Sets...\n    // Result of Data Set Identity Comparison is True\n    // ----------------------------------------\n    // Press any key to exit.\n\n\n###Sample 2: Serialization with a generic record\n\nA JSON schema can be explicitly specified in a generic record when reflection cannot be used because the data cannot be represented via .NET classes with a data contract. This method is generally slower than using reflection. In such cases, the schema for the data may also be dynamic, i.e., not be known at compile time. Data represented as comma-separated values (CSV) files whose schema is unknown until it is transformed to the Avro format at run time is an example of this sort of dynamic scenario.\n\nThis example shows how to create and use an [**AvroRecord**](http://msdn.microsoft.com/library/microsoft.hadoop.avro.avrorecord.aspx) to explicitly specify a JSON schema, how to populate it with the data, and then how to serialize and deserialize it. The result is then compared to the initial instance to confirm that the record recovered is identical to the original.\n\nThe schema in this example is assumed to be shared between the readers and writers, so the Avro object container format is not required. For an example of how to serialize and deserialize data into memory buffers by using a generic record with the object container format when the schema must be included with the serialized data, see the <a href=\"#Scenario4\">Serialization using object container files with generic record</a> example.\n\n\n    namespace Microsoft.Hadoop.Avro.Sample\n    {\n    using System;\n    using System.Collections.Generic;\n    using System.IO;\n    using System.Linq;\n    using System.Runtime.Serialization;\n    using Microsoft.Hadoop.Avro.Container;\n    using Microsoft.Hadoop.Avro.Schema;\n    using Microsoft.Hadoop.Avro;\n\n    //This class contains all methods demonstrating\n    //the usage of Microsoft Avro Library\n    public class AvroSample\n    {\n\n        //Serialize and deserialize sample data set by using a generic record.\n        //A generic record is a special class with the schema explicitly defined in JSON.\n        //All serialized data should be mapped to the fields of the generic record,\n        //which in turn will be then serialized.\n        public void SerializeDeserializeObjectUsingGenericRecords()\n        {\n            Console.WriteLine(\"SERIALIZATION USING GENERIC RECORD\\n\");\n            Console.WriteLine(\"Defining the Schema and creating Sample Data Set...\");\n\n            //Define the schema in JSON\n            const string Schema = @\"{\n                                \"\"type\"\":\"\"record\"\",\n                                \"\"name\"\":\"\"Microsoft.Hadoop.Avro.Specifications.SensorData\"\",\n                                \"\"fields\"\":\n                                    [\n                                        {\n                                            \"\"name\"\":\"\"Location\"\",\n                                            \"\"type\"\":\n                                                {\n                                                    \"\"type\"\":\"\"record\"\",\n                                                    \"\"name\"\":\"\"Microsoft.Hadoop.Avro.Specifications.Location\"\",\n                                                    \"\"fields\"\":\n                                                        [\n                                                            { \"\"name\"\":\"\"Floor\"\", \"\"type\"\":\"\"int\"\" },\n                                                            { \"\"name\"\":\"\"Room\"\", \"\"type\"\":\"\"int\"\" }\n                                                        ]\n                                                }\n                                        },\n                                        { \"\"name\"\":\"\"Value\"\", \"\"type\"\":\"\"bytes\"\" }\n                                    ]\n                            }\";\n\n            //Create a generic serializer based on the schema\n            var serializer = AvroSerializer.CreateGeneric(Schema);\n            var rootSchema = serializer.WriterSchema as RecordSchema;\n\n            //Create a memory stream buffer\n            using (var stream = new MemoryStream())\n            {\n                //Create a generic record to represent the data\n                dynamic location = new AvroRecord(rootSchema.GetField(\"Location\").TypeSchema);\n                location.Floor = 1;\n                location.Room = 243;\n\n                dynamic expected = new AvroRecord(serializer.WriterSchema);\n                expected.Location = location;\n                expected.Value = new byte[] { 1, 2, 3, 4, 5 };\n\n                Console.WriteLine(\"Serializing Sample Data Set...\");\n\n                //Serialize the data\n                serializer.Serialize(stream, expected);\n\n                stream.Seek(0, SeekOrigin.Begin);\n\n                Console.WriteLine(\"Deserializing Sample Data Set...\");\n\n                //Deserialize the data into a generic record\n                dynamic actual = serializer.Deserialize(stream);\n\n                Console.WriteLine(\"Comparing Initial and Deserialized Data Sets...\");\n\n                //Finally, verify the results\n                bool isEqual = expected.Location.Floor.Equals(actual.Location.Floor);\n                isEqual = isEqual && expected.Location.Room.Equals(actual.Location.Room);\n                isEqual = isEqual && ((byte[])expected.Value).SequenceEqual((byte[])actual.Value);\n                Console.WriteLine(\"Result of Data Set Identity Comparison is {0}\", isEqual);\n            }\n        }\n\n        static void Main()\n        {\n\n            string sectionDivider = \"---------------------------------------- \";\n\n            //Create an instance of AvroSample class and invoke methods\n            //illustrating different serializing approaches\n            AvroSample Sample = new AvroSample();\n\n            //Serialization to memory using generic record\n            Sample.SerializeDeserializeObjectUsingGenericRecords();\n\n            Console.WriteLine(sectionDivider);\n            Console.WriteLine(\"Press any key to exit.\");\n            Console.Read();\n        }\n    }\n    }\n    // The example is expected to display the following output:\n    // SERIALIZATION USING GENERIC RECORD\n    //\n    // Defining the Schema and creating Sample Data Set...\n    // Serializing Sample Data Set...\n    // Deserializing Sample Data Set...\n    // Comparing Initial and Deserialized Data Sets...\n    // Result of Data Set Identity Comparison is True\n    // ----------------------------------------\n    // Press any key to exit.\n\n\n###Sample 3: Serialization using object container files and serialization with reflection\n\nThis example is similar to the scenario in the <a href=\"#Scenario1\"> first example</a>, where the schema is implicitly specified with reflection. The difference is that here, the schema is not assumed to be known to the reader that deserializes it. The **SensorData** objects to be serialized and their implicitly specified schema are stored in an Avro object container file represented by the [**AvroContainer**](http://msdn.microsoft.com/library/microsoft.hadoop.avro.container.avrocontainer.aspx) class.\n\nThe data is serialized in this example with [**SequentialWriter<SensorData>**](http://msdn.microsoft.com/library/dn627340.aspx) and deserialized with [**SequentialReader<SensorData>**](http://msdn.microsoft.com/library/dn627340.aspx). The result then is compared to the initial instances to ensure identity.\n\nThe data in the object container file is compressed via the default [**Deflate**][deflate-100] compression codec from .NET Framework 4. See the <a href=\"#Scenario5\"> fifth example</a> in this topic to learn how to use a more recent and superior version of the [**Deflate**][deflate-110] compression codec available in .NET Framework 4.5.\n\n    namespace Microsoft.Hadoop.Avro.Sample\n    {\n        using System;\n        using System.Collections.Generic;\n        using System.IO;\n        using System.Linq;\n        using System.Runtime.Serialization;\n        using Microsoft.Hadoop.Avro.Container;\n        using Microsoft.Hadoop.Avro;\n\n        //Sample class used in serialization samples\n        [DataContract(Name = \"SensorDataValue\", Namespace = \"Sensors\")]\n        internal class SensorData\n        {\n            [DataMember(Name = \"Location\")]\n            public Location Position { get; set; }\n\n            [DataMember(Name = \"Value\")]\n            public byte[] Value { get; set; }\n        }\n\n        //Sample struct used in serialization samples\n        [DataContract]\n        internal struct Location\n        {\n            [DataMember]\n            public int Floor { get; set; }\n\n            [DataMember]\n            public int Room { get; set; }\n        }\n\n        //This class contains all methods demonstrating\n        //the usage of Microsoft Avro Library\n        public class AvroSample\n        {\n\n            //Serializes and deserializes the sample data set by using reflection and Avro object container files.\n            //Serialized data is compressed with the Deflate codec.\n            public void SerializeDeserializeUsingObjectContainersReflection()\n            {\n\n                Console.WriteLine(\"SERIALIZATION USING REFLECTION AND AVRO OBJECT CONTAINER FILES\\n\");\n\n                //Path for Avro object container file\n                string path = \"AvroSampleReflectionDeflate.avro\";\n\n                //Create a data set by using sample class and struct\n                var testData = new List<SensorData>\n                        {\n                            new SensorData { Value = new byte[] { 1, 2, 3, 4, 5 }, Position = new Location { Room = 243, Floor = 1 } },\n                            new SensorData { Value = new byte[] { 6, 7, 8, 9 }, Position = new Location { Room = 244, Floor = 1 } }\n                        };\n\n                //Serializing and saving data to file.\n                //Creating a memory stream buffer.\n                using (var buffer = new MemoryStream())\n                {\n                    Console.WriteLine(\"Serializing Sample Data Set...\");\n\n                    //Create a SequentialWriter instance for type SensorData, which can serialize a sequence of SensorData objects to stream.\n                    //Data will be compressed using the Deflate codec.\n                    using (var w = AvroContainer.CreateWriter<SensorData>(buffer, Codec.Deflate))\n                    {\n                        using (var writer = new SequentialWriter<SensorData>(w, 24))\n                        {\n                            // Serialize the data to stream by using the sequential writer\n                            testData.ForEach(writer.Write);\n                        }\n                    }\n\n                    //Save stream to file\n                    Console.WriteLine(\"Saving serialized data to file...\");\n                    if (!WriteFile(buffer, path))\n                    {\n                        Console.WriteLine(\"Error during file operation. Quitting method\");\n                        return;\n                    }\n                }\n\n                //Reading and deserializing data.\n                //Creating a memory stream buffer.\n                using (var buffer = new MemoryStream())\n                {\n                    Console.WriteLine(\"Reading data from file...\");\n\n                    //Reading data from object container file\n                    if (!ReadFile(buffer, path))\n                    {\n                        Console.WriteLine(\"Error during file operation. Quitting method\");\n                        return;\n                    }\n\n                    Console.WriteLine(\"Deserializing Sample Data Set...\");\n\n                    //Prepare the stream for deserializing the data\n                    buffer.Seek(0, SeekOrigin.Begin);\n\n                    //Create a SequentialReader instance for type SensorData, which will deserialize all serialized objects from the given stream.\n                    //It allows iterating over the deserialized objects because it implements the IEnumerable<T> interface.\n                    using (var reader = new SequentialReader<SensorData>(\n                        AvroContainer.CreateReader<SensorData>(buffer, true)))\n                    {\n                        var results = reader.Objects;\n\n                        //Finally, verify that deserialized data matches the original one\n                        Console.WriteLine(\"Comparing Initial and Deserialized Data Sets...\");\n                        int count = 1;\n                        var pairs = testData.Zip(results, (serialized, deserialized) => new { expected = serialized, actual = deserialized });\n                        foreach (var pair in pairs)\n                        {\n                            bool isEqual = this.Equal(pair.expected, pair.actual);\n                            Console.WriteLine(\"For Pair {0} result of Data Set Identity Comparison is {1}\", count, isEqual);\n                            count++;\n                        }\n                    }\n                }\n\n                //Delete the file\n                RemoveFile(path);\n            }\n\n            //\n            //Helper methods\n            //\n\n            //Comparing two SensorData objects\n            private bool Equal(SensorData left, SensorData right)\n            {\n                return left.Position.Equals(right.Position) && left.Value.SequenceEqual(right.Value);\n            }\n\n            //Saving memory stream to a new file with the given path\n            private bool WriteFile(MemoryStream InputStream, string path)\n            {\n                if (!File.Exists(path))\n                {\n                    try\n                    {\n                        using (FileStream fs = File.Create(path))\n                        {\n                            InputStream.Seek(0, SeekOrigin.Begin);\n                            InputStream.CopyTo(fs);\n                        }\n                        return true;\n                    }\n                    catch (Exception e)\n                    {\n                        Console.WriteLine(\"The following exception was thrown during creation and writing to the file \\\"{0}\\\"\", path);\n                        Console.WriteLine(e.Message);\n                        return false;\n                    }\n                }\n                else\n                {\n                    Console.WriteLine(\"Can not create file \\\"{0}\\\". File already exists\", path);\n                    return false;\n\n                }\n            }\n\n            //Reading a file content by using the given path to a memory stream\n            private bool ReadFile(MemoryStream OutputStream, string path)\n            {\n                try\n                {\n                    using (FileStream fs = File.Open(path, FileMode.Open))\n                    {\n                        fs.CopyTo(OutputStream);\n                    }\n                    return true;\n                }\n                catch (Exception e)\n                {\n                    Console.WriteLine(\"The following exception was thrown during reading from the file \\\"{0}\\\"\", path);\n                    Console.WriteLine(e.Message);\n                    return false;\n                }\n            }\n\n            //Deleting file by using given path\n            private void RemoveFile(string path)\n            {\n                if (File.Exists(path))\n                {\n                    try\n                    {\n                        File.Delete(path);\n                    }\n                    catch (Exception e)\n                    {\n                        Console.WriteLine(\"The following exception was thrown during deleting the file \\\"{0}\\\"\", path);\n                        Console.WriteLine(e.Message);\n                    }\n                }\n                else\n                {\n                    Console.WriteLine(\"Can not delete file \\\"{0}\\\". File does not exist\", path);\n                }\n            }\n\n            static void Main()\n            {\n\n                string sectionDivider = \"---------------------------------------- \";\n\n                //Create an instance of AvroSample class and invoke methods\n                //illustrating different serializing approaches\n                AvroSample Sample = new AvroSample();\n\n                //Serialization using reflection to Avro object container file\n                Sample.SerializeDeserializeUsingObjectContainersReflection();\n\n                Console.WriteLine(sectionDivider);\n                Console.WriteLine(\"Press any key to exit.\");\n                Console.Read();\n            }\n        }\n    }\n    // The example is expected to display the following output:\n    // SERIALIZATION USING REFLECTION AND AVRO OBJECT CONTAINER FILES\n    //\n    // Serializing Sample Data Set...\n    // Saving serialized data to file...\n    // Reading data from file...\n    // Deserializing Sample Data Set...\n    // Comparing Initial and Deserialized Data Sets...\n    // For Pair 1 result of Data Set Identity Comparison is True\n    // For Pair 2 result of Data Set Identity Comparison is True\n    // ----------------------------------------\n    // Press any key to exit.\n\n\n###Sample 4: Serialization using object container files and serialization with generic record\n\nThis example is similar to the scenario in the <a href=\"#Scenario2\"> second example</a>, where the schema is explicitly specified with JSON. The difference is that here, the schema is not assumed to be known to the reader that deserializes it.\n\nThe test data set is collected into a list of [**AvroRecord**](http://msdn.microsoft.com/library/microsoft.hadoop.avro.avrorecord.aspx) objects via an explicitly defined JSON schema and then stored in an object container file represented by the [**AvroContainer**](http://msdn.microsoft.com/library/microsoft.hadoop.avro.container.avrocontainer.aspx) class. This container file creates a writer that is used to serialize the data, uncompressed, to a memory stream that is then saved to a file. The [**Codec.Null**](http://msdn.microsoft.com/library/microsoft.hadoop.avro.container.codec.null.aspx) parameter used for creating the reader specifies that this data will not be compressed.\n\nThe data is then read from the file and deserialized into a collection of objects. This collection is compared to the initial list of Avro records to confirm that they are identical.\n\n\n    namespace Microsoft.Hadoop.Avro.Sample\n    {\n        using System;\n        using System.Collections.Generic;\n        using System.IO;\n        using System.Linq;\n        using System.Runtime.Serialization;\n        using Microsoft.Hadoop.Avro.Container;\n        using Microsoft.Hadoop.Avro.Schema;\n        using Microsoft.Hadoop.Avro;\n\n        //This class contains all methods demonstrating\n        //the usage of Microsoft Avro Library\n        public class AvroSample\n        {\n\n            //Serializes and deserializes a sample data set by using a generic record and Avro object container files.\n            //Serialized data is not compressed.\n            public void SerializeDeserializeUsingObjectContainersGenericRecord()\n            {\n                Console.WriteLine(\"SERIALIZATION USING GENERIC RECORD AND AVRO OBJECT CONTAINER FILES\\n\");\n\n                //Path for Avro object container file\n                string path = \"AvroSampleGenericRecordNullCodec.avro\";\n\n                Console.WriteLine(\"Defining the Schema and creating Sample Data Set...\");\n\n                //Define the schema in JSON\n                const string Schema = @\"{\n                                \"\"type\"\":\"\"record\"\",\n                                \"\"name\"\":\"\"Microsoft.Hadoop.Avro.Specifications.SensorData\"\",\n                                \"\"fields\"\":\n                                    [\n                                        {\n                                            \"\"name\"\":\"\"Location\"\",\n                                            \"\"type\"\":\n                                                {\n                                                    \"\"type\"\":\"\"record\"\",\n                                                    \"\"name\"\":\"\"Microsoft.Hadoop.Avro.Specifications.Location\"\",\n                                                    \"\"fields\"\":\n                                                        [\n                                                            { \"\"name\"\":\"\"Floor\"\", \"\"type\"\":\"\"int\"\" },\n                                                            { \"\"name\"\":\"\"Room\"\", \"\"type\"\":\"\"int\"\" }\n                                                        ]\n                                                }\n                                        },\n                                        { \"\"name\"\":\"\"Value\"\", \"\"type\"\":\"\"bytes\"\" }\n                                    ]\n                            }\";\n\n                //Create a generic serializer based on the schema\n                var serializer = AvroSerializer.CreateGeneric(Schema);\n                var rootSchema = serializer.WriterSchema as RecordSchema;\n\n                //Create a generic record to represent the data\n                var testData = new List<AvroRecord>();\n\n                dynamic expected1 = new AvroRecord(rootSchema);\n                dynamic location1 = new AvroRecord(rootSchema.GetField(\"Location\").TypeSchema);\n                location1.Floor = 1;\n                location1.Room = 243;\n                expected1.Location = location1;\n                expected1.Value = new byte[] { 1, 2, 3, 4, 5 };\n                testData.Add(expected1);\n\n                dynamic expected2 = new AvroRecord(rootSchema);\n                dynamic location2 = new AvroRecord(rootSchema.GetField(\"Location\").TypeSchema);\n                location2.Floor = 1;\n                location2.Room = 244;\n                expected2.Location = location2;\n                expected2.Value = new byte[] { 6, 7, 8, 9 };\n                testData.Add(expected2);\n\n                //Serializing and saving data to file.\n                //Create a MemoryStream buffer.\n                using (var buffer = new MemoryStream())\n                {\n                    Console.WriteLine(\"Serializing Sample Data Set...\");\n\n                    //Create a SequentialWriter instance for type SensorData, which can serialize a sequence of SensorData objects to stream.\n                    //Data will not be compressed (Null compression codec).\n                    using (var writer = AvroContainer.CreateGenericWriter(Schema, buffer, Codec.Null))\n                    {\n                        using (var streamWriter = new SequentialWriter<object>(writer, 24))\n                        {\n                            // Serialize the data to stream by using the sequential writer\n                            testData.ForEach(streamWriter.Write);\n                        }\n                    }\n\n                    Console.WriteLine(\"Saving serialized data to file...\");\n\n                    //Save stream to file\n                    if (!WriteFile(buffer, path))\n                    {\n                        Console.WriteLine(\"Error during file operation. Quitting method\");\n                        return;\n                    }\n                }\n\n                //Reading and deserializing the data.\n                //Create a memory stream buffer.\n                using (var buffer = new MemoryStream())\n                {\n                    Console.WriteLine(\"Reading data from file...\");\n\n                    //Reading data from object container file\n                    if (!ReadFile(buffer, path))\n                    {\n                        Console.WriteLine(\"Error during file operation. Quitting method\");\n                        return;\n                    }\n\n                    Console.WriteLine(\"Deserializing Sample Data Set...\");\n\n                    //Prepare the stream for deserializing the data\n                    buffer.Seek(0, SeekOrigin.Begin);\n\n                    //Create a SequentialReader instance for type SensorData, which will deserialize all serialized objects from the given stream.\n                    //It allows iterating over the deserialized objects because it implements the IEnumerable<T> interface.\n                    using (var reader = AvroContainer.CreateGenericReader(buffer))\n                    {\n                        using (var streamReader = new SequentialReader<object>(reader))\n                        {\n                            var results = streamReader.Objects;\n\n                            Console.WriteLine(\"Comparing Initial and Deserialized Data Sets...\");\n\n                            //Finally, verify the results\n                            var pairs = testData.Zip(results, (serialized, deserialized) => new { expected = (dynamic)serialized, actual = (dynamic)deserialized });\n                            int count = 1;\n                            foreach (var pair in pairs)\n                            {\n                                bool isEqual = pair.expected.Location.Floor.Equals(pair.actual.Location.Floor);\n                                isEqual = isEqual && pair.expected.Location.Room.Equals(pair.actual.Location.Room);\n                                isEqual = isEqual && ((byte[])pair.expected.Value).SequenceEqual((byte[])pair.actual.Value);\n                                Console.WriteLine(\"For Pair {0} result of Data Set Identity Comparison is {1}\", count, isEqual.ToString());\n                                count++;\n                            }\n                        }\n                    }\n                }\n\n                //Delete the file\n                RemoveFile(path);\n            }\n\n            //\n            //Helper methods\n            //\n\n            //Saving memory stream to a new file with the given path\n            private bool WriteFile(MemoryStream InputStream, string path)\n            {\n                if (!File.Exists(path))\n                {\n                    try\n                    {\n                        using (FileStream fs = File.Create(path))\n                        {\n                            InputStream.Seek(0, SeekOrigin.Begin);\n                            InputStream.CopyTo(fs);\n                        }\n                        return true;\n                    }\n                    catch (Exception e)\n                    {\n                        Console.WriteLine(\"The following exception was thrown during creation and writing to the file \\\"{0}\\\"\", path);\n                        Console.WriteLine(e.Message);\n                        return false;\n                    }\n                }\n                else\n                {\n                    Console.WriteLine(\"Can not create file \\\"{0}\\\". File already exists\", path);\n                    return false;\n\n                }\n            }\n\n            //Reading a file content by using the given path to a memory stream\n            private bool ReadFile(MemoryStream OutputStream, string path)\n            {\n                try\n                {\n                    using (FileStream fs = File.Open(path, FileMode.Open))\n                    {\n                        fs.CopyTo(OutputStream);\n                    }\n                    return true;\n                }\n                catch (Exception e)\n                {\n                    Console.WriteLine(\"The following exception was thrown during reading from the file \\\"{0}\\\"\", path);\n                    Console.WriteLine(e.Message);\n                    return false;\n                }\n            }\n\n            //Deleting file by using the given path\n            private void RemoveFile(string path)\n            {\n                if (File.Exists(path))\n                {\n                    try\n                    {\n                        File.Delete(path);\n                    }\n                    catch (Exception e)\n                    {\n                        Console.WriteLine(\"The following exception was thrown during deleting the file \\\"{0}\\\"\", path);\n                        Console.WriteLine(e.Message);\n                    }\n                }\n                else\n                {\n                    Console.WriteLine(\"Can not delete file \\\"{0}\\\". File does not exist\", path);\n                }\n            }\n\n            static void Main()\n            {\n\n                string sectionDivider = \"---------------------------------------- \";\n\n                //Create an instance of the AvroSample class and invoke methods\n                //illustrating different serializing approaches\n                AvroSample Sample = new AvroSample();\n\n                //Serialization using generic record to Avro object container file\n                Sample.SerializeDeserializeUsingObjectContainersGenericRecord();\n\n                Console.WriteLine(sectionDivider);\n                Console.WriteLine(\"Press any key to exit.\");\n                Console.Read();\n            }\n        }\n    }\n    // The example is expected to display the following output:\n    // SERIALIZATION USING GENERIC RECORD AND AVRO OBJECT CONTAINER FILES\n    //\n    // Defining the Schema and creating Sample Data Set...\n    // Serializing Sample Data Set...\n    // Saving serialized data to file...\n    // Reading data from file...\n    // Deserializing Sample Data Set...\n    // Comparing Initial and Deserialized Data Sets...\n    // For Pair 1 result of Data Set Identity Comparison is True\n    // For Pair 2 result of Data Set Identity Comparison is True\n    // ----------------------------------------\n    // Press any key to exit.\n\n\n\n\n###Sample 5: Serialization using object container files with a custom compression codec\n\nThe fifth example shows how to how to use a custom compression codec for Avro object container files. A sample containing the code for this example can be downloaded from the [Azure code samples](http://code.msdn.microsoft.com/windowsazure/Serialize-data-with-the-67159111) site.\n\nThe [Avro Specification](http://avro.apache.org/docs/current/spec.html#Required+Codecs) allows usage of an optional compression codec (in addition to **Null** and **Deflate** defaults). This example is not implementing a completely new codec such as Snappy (mentioned as a supported optional codec in the [Avro Specification](http://avro.apache.org/docs/current/spec.html#snappy)). It shows how to use the .NET Framework 4.5 implementation of the [**Deflate**][deflate-110] codec, which provides a better compression algorithm based on the [zlib](http://zlib.net/) compression library than the default .NET Framework 4 version.\n\n\n    //\n    // This code needs to be compiled with the parameter Target Framework set as \".NET Framework 4.5\"\n    // to ensure the desired implementation of the Deflate compression algorithm is used.\n    // Ensure your C# project is set up accordingly.\n    //\n\n    namespace Microsoft.Hadoop.Avro.Sample\n    {\n        using System;\n        using System.Collections.Generic;\n        using System.Diagnostics;\n        using System.IO;\n        using System.IO.Compression;\n        using System.Linq;\n        using System.Runtime.Serialization;\n        using Microsoft.Hadoop.Avro.Container;\n        using Microsoft.Hadoop.Avro;\n\n        #region Defining objects for serialization\n        //Sample class used in serialization samples\n        [DataContract(Name = \"SensorDataValue\", Namespace = \"Sensors\")]\n        internal class SensorData\n        {\n            [DataMember(Name = \"Location\")]\n            public Location Position { get; set; }\n\n            [DataMember(Name = \"Value\")]\n            public byte[] Value { get; set; }\n        }\n\n        //Sample struct used in serialization samples\n        [DataContract]\n        internal struct Location\n        {\n            [DataMember]\n            public int Floor { get; set; }\n\n            [DataMember]\n            public int Room { get; set; }\n        }\n        #endregion\n\n        #region Defining custom codec based on .NET Framework V.4.5 Deflate\n        //Avro.NET codec class contains two methods,\n        //GetCompressedStreamOver(Stream uncompressed) and GetDecompressedStreamOver(Stream compressed),\n        //which are the key ones for data compression.\n        //To enable a custom codec, one needs to implement these methods for the required codec.\n\n        #region Defining Compression and Decompression Streams\n        //DeflateStream (class from System.IO.Compression namespace that implements Deflate algorithm)\n        //cannot be directly used for Avro because it does not support vital operations like Seek.\n        //Thus one needs to implement two classes inherited from stream\n        //(one for compressed and one for decompressed stream)\n        //that use Deflate compression and implement all required features.\n        internal sealed class CompressionStreamDeflate45 : Stream\n        {\n            private readonly Stream buffer;\n            private DeflateStream compressionStream;\n\n            public CompressionStreamDeflate45(Stream buffer)\n            {\n                Debug.Assert(buffer != null, \"Buffer is not allowed to be null.\");\n\n                this.compressionStream = new DeflateStream(buffer, CompressionLevel.Fastest, true);\n                this.buffer = buffer;\n            }\n\n            public override bool CanRead\n            {\n                get { return this.buffer.CanRead; }\n            }\n\n            public override bool CanSeek\n            {\n                get { return true; }\n            }\n\n            public override bool CanWrite\n            {\n                get { return this.buffer.CanWrite; }\n            }\n\n            public override void Flush()\n            {\n                this.compressionStream.Close();\n            }\n\n            public override long Length\n            {\n                get { return this.buffer.Length; }\n            }\n\n            public override long Position\n            {\n                get\n                {\n                    return this.buffer.Position;\n                }\n\n                set\n                {\n                    this.buffer.Position = value;\n                }\n            }\n\n            public override int Read(byte[] buffer, int offset, int count)\n            {\n                return this.buffer.Read(buffer, offset, count);\n            }\n\n            public override long Seek(long offset, SeekOrigin origin)\n            {\n                return this.buffer.Seek(offset, origin);\n            }\n\n            public override void SetLength(long value)\n            {\n                throw new NotSupportedException();\n            }\n\n            public override void Write(byte[] buffer, int offset, int count)\n            {\n                this.compressionStream.Write(buffer, offset, count);\n            }\n\n            protected override void Dispose(bool disposed)\n            {\n                base.Dispose(disposed);\n\n                if (disposed)\n                {\n                    this.compressionStream.Dispose();\n                    this.compressionStream = null;\n                }\n            }\n        }\n\n        internal sealed class DecompressionStreamDeflate45 : Stream\n        {\n            private readonly DeflateStream decompressed;\n\n            public DecompressionStreamDeflate45(Stream compressed)\n            {\n                this.decompressed = new DeflateStream(compressed, CompressionMode.Decompress, true);\n            }\n\n            public override bool CanRead\n            {\n                get { return true; }\n            }\n\n            public override bool CanSeek\n            {\n                get { return true; }\n            }\n\n            public override bool CanWrite\n            {\n                get { return false; }\n            }\n\n            public override void Flush()\n            {\n                this.decompressed.Close();\n            }\n\n            public override long Length\n            {\n                get { return this.decompressed.Length; }\n            }\n\n            public override long Position\n            {\n                get\n                {\n                    return this.decompressed.Position;\n                }\n\n                set\n                {\n                    throw new NotSupportedException();\n                }\n            }\n\n            public override int Read(byte[] buffer, int offset, int count)\n            {\n                return this.decompressed.Read(buffer, offset, count);\n            }\n\n            public override long Seek(long offset, SeekOrigin origin)\n            {\n                throw new NotSupportedException();\n            }\n\n            public override void SetLength(long value)\n            {\n                throw new NotSupportedException();\n            }\n\n            public override void Write(byte[] buffer, int offset, int count)\n            {\n                throw new NotSupportedException();\n            }\n\n            protected override void Dispose(bool disposing)\n            {\n                base.Dispose(disposing);\n\n                if (disposing)\n                {\n                    this.decompressed.Dispose();\n                }\n            }\n        }\n        #endregion\n\n        #region Define Codec\n        //Define the actual codec class containing the required methods for manipulating streams:\n        //GetCompressedStreamOver(Stream uncompressed) and GetDecompressedStreamOver(Stream compressed).\n        //Codec class uses classes for compressed and decompressed streams defined above.\n        internal sealed class DeflateCodec45 : Codec\n        {\n\n            //We merely use different IMPLEMENTATIONS of Deflate, so CodecName remains \"deflate\"\n            public static readonly string CodecName = \"deflate\";\n\n            public DeflateCodec45()\n                : base(CodecName)\n            {\n            }\n\n            public override Stream GetCompressedStreamOver(Stream decompressed)\n            {\n                if (decompressed == null)\n                {\n                    throw new ArgumentNullException(\"decompressed\");\n                }\n\n                return new CompressionStreamDeflate45(decompressed);\n            }\n\n            public override Stream GetDecompressedStreamOver(Stream compressed)\n            {\n                if (compressed == null)\n                {\n                    throw new ArgumentNullException(\"compressed\");\n                }\n\n                return new DecompressionStreamDeflate45(compressed);\n            }\n        }\n        #endregion\n\n        #region Define modified Codec Factory\n        //Define modified codec factory to be used in the reader.\n        //It will catch the attempt to use \"Deflate\" and provide  a custom codec.\n        //For all other cases, it will rely on the base class (CodecFactory).\n        internal sealed class CodecFactoryDeflate45 : CodecFactory\n        {\n\n            public override Codec Create(string codecName)\n            {\n                if (codecName == DeflateCodec45.CodecName)\n                    return new DeflateCodec45();\n                else\n                    return base.Create(codecName);\n            }\n        }\n        #endregion\n\n        #endregion\n\n        #region Sample Class with demonstration methods\n        //This class contains methods demonstrating\n        //the usage of Microsoft Avro Library\n        public class AvroSample\n        {\n\n            //Serializes and deserializes sample data set by using reflection and Avro object container files.\n            //Serialized data is compressed with the custom compression codec (Deflate of .NET Framework 4.5).\n            //\n            //This sample uses memory stream for all operations related to serialization, deserialization and\n            //object container manipulation, though file stream could be easily used.\n            public void SerializeDeserializeUsingObjectContainersReflectionCustomCodec()\n            {\n\n                Console.WriteLine(\"SERIALIZATION USING REFLECTION, AVRO OBJECT CONTAINER FILES AND CUSTOM CODEC\\n\");\n\n                //Path for Avro object container file\n                string path = \"AvroSampleReflectionDeflate45.avro\";\n\n                //Create a data set by using sample class and struct\n                var testData = new List<SensorData>\n                        {\n                            new SensorData { Value = new byte[] { 1, 2, 3, 4, 5 }, Position = new Location { Room = 243, Floor = 1 } },\n                            new SensorData { Value = new byte[] { 6, 7, 8, 9 }, Position = new Location { Room = 244, Floor = 1 } }\n                        };\n\n                //Serializing and saving data to file.\n                //Creating a memory stream buffer.\n                using (var buffer = new MemoryStream())\n                {\n                    Console.WriteLine(\"Serializing Sample Data Set...\");\n\n                    //Create a SequentialWriter instance for type SensorData, which can serialize a sequence of SensorData objects to stream.\n                    //Here the custom codec is introduced. For convenience, the next commented code line shows how to use built-in Deflate.\n                    //Note that because the sample deals with different IMPLEMENTATIONS of Deflate, built-in and custom codecs are interchangeable\n                    //in read-write operations.\n                    //using (var w = AvroContainer.CreateWriter<SensorData>(buffer, Codec.Deflate))\n                    using (var w = AvroContainer.CreateWriter<SensorData>(buffer, new DeflateCodec45()))\n                    {\n                        using (var writer = new SequentialWriter<SensorData>(w, 24))\n                        {\n                            // Serialize the data to stream using the sequential writer\n                            testData.ForEach(writer.Write);\n                        }\n                    }\n\n                    //Save stream to file\n                    Console.WriteLine(\"Saving serialized data to file...\");\n                    if (!WriteFile(buffer, path))\n                    {\n                        Console.WriteLine(\"Error during file operation. Quitting method\");\n                        return;\n                    }\n                }\n\n                //Reading and deserializing data.\n                //Creating a memory stream buffer.\n                using (var buffer = new MemoryStream())\n                {\n                    Console.WriteLine(\"Reading data from file...\");\n\n                    //Reading data from object container file\n                    if (!ReadFile(buffer, path))\n                    {\n                        Console.WriteLine(\"Error during file operation. Quitting method\");\n                        return;\n                    }\n\n                    Console.WriteLine(\"Deserializing Sample Data Set...\");\n\n                    //Prepare the stream for deserializing the data\n                    buffer.Seek(0, SeekOrigin.Begin);\n\n                    //Because of SequentialReader<T> constructor signature, an AvroSerializerSettings instance is required\n                    //when codec factory is explicitly specified.\n                    //You may comment the line below if you want to use built-in Deflate (see next comment).\n                    AvroSerializerSettings settings = new AvroSerializerSettings();\n\n                    //Create a SequentialReader instance for type SensorData, which will deserialize all serialized objects from the given stream.\n                    //It allows iterating over the deserialized objects because it implements the IEnumerable<T> interface.\n                    //Here the custom codec factory is introduced.\n                    //For convenience, the next commented code line shows how to use built-in Deflate\n                    //(no explicit Codec Factory parameter is required in this case).\n                    //Note that because the sample deals with different IMPLEMENTATIONS of Deflate, built-in and custom codecs are interchangeable\n                    //in read-write operations.\n                    //using (var reader = new SequentialReader<SensorData>(AvroContainer.CreateReader<SensorData>(buffer, true)))\n                    using (var reader = new SequentialReader<SensorData>(\n                        AvroContainer.CreateReader<SensorData>(buffer, true, settings, new CodecFactoryDeflate45())))\n                    {\n                        var results = reader.Objects;\n\n                        //Finally, verify that deserialized data matches the original one\n                        Console.WriteLine(\"Comparing Initial and Deserialized Data Sets...\");\n                        bool isEqual;\n                        int count = 1;\n                        var pairs = testData.Zip(results, (serialized, deserialized) => new { expected = serialized, actual = deserialized });\n                        foreach (var pair in pairs)\n                        {\n                            isEqual = this.Equal(pair.expected, pair.actual);\n                            Console.WriteLine(\"For Pair {0} result of Data Set Identity Comparison is {1}\", count, isEqual.ToString());\n                            count++;\n                        }\n                    }\n                }\n\n                //Delete the file\n                RemoveFile(path);\n            }\n        #endregion\n\n            #region Helper Methods\n\n            //Comparing two SensorData objects\n            private bool Equal(SensorData left, SensorData right)\n            {\n                return left.Position.Equals(right.Position) && left.Value.SequenceEqual(right.Value);\n            }\n\n            //Saving memory stream to a new file with the given path\n            private bool WriteFile(MemoryStream InputStream, string path)\n            {\n                if (!File.Exists(path))\n                {\n                    try\n                    {\n                        using (FileStream fs = File.Create(path))\n                        {\n                            InputStream.Seek(0, SeekOrigin.Begin);\n                            InputStream.CopyTo(fs);\n                        }\n                        return true;\n                    }\n                    catch (Exception e)\n                    {\n                        Console.WriteLine(\"The following exception was thrown during creation and writing to the file \\\"{0}\\\"\", path);\n                        Console.WriteLine(e.Message);\n                        return false;\n                    }\n                }\n                else\n                {\n                    Console.WriteLine(\"Can not create file \\\"{0}\\\". File already exists\", path);\n                    return false;\n\n                }\n            }\n\n            //Reading file content by using the given path to a memory stream\n            private bool ReadFile(MemoryStream OutputStream, string path)\n            {\n                try\n                {\n                    using (FileStream fs = File.Open(path, FileMode.Open))\n                    {\n                        fs.CopyTo(OutputStream);\n                    }\n                    return true;\n                }\n                catch (Exception e)\n                {\n                    Console.WriteLine(\"The following exception was thrown during reading from the file \\\"{0}\\\"\", path);\n                    Console.WriteLine(e.Message);\n                    return false;\n                }\n            }\n\n            //Deleting file by using given path\n            private void RemoveFile(string path)\n            {\n                if (File.Exists(path))\n                {\n                    try\n                    {\n                        File.Delete(path);\n                    }\n                    catch (Exception e)\n                    {\n                        Console.WriteLine(\"The following exception was thrown during deleting the file \\\"{0}\\\"\", path);\n                        Console.WriteLine(e.Message);\n                    }\n                }\n                else\n                {\n                    Console.WriteLine(\"Can not delete file \\\"{0}\\\". File does not exist\", path);\n                }\n            }\n            #endregion\n\n            static void Main()\n            {\n\n                string sectionDivider = \"---------------------------------------- \";\n\n                //Create an instance of AvroSample Class and invoke methods\n                //illustrating different serializing approaches\n                AvroSample Sample = new AvroSample();\n\n                //Serialization using reflection to Avro object container file using custom codec\n                Sample.SerializeDeserializeUsingObjectContainersReflectionCustomCodec();\n\n                Console.WriteLine(sectionDivider);\n                Console.WriteLine(\"Press any key to exit.\");\n                Console.Read();\n            }\n        }\n    }\n    // The example is expected to display the following output:\n    // SERIALIZATION USING REFLECTION, AVRO OBJECT CONTAINER FILES AND CUSTOM CODEC\n    //\n    // Serializing Sample Data Set...\n    // Saving serialized data to file...\n    // Reading data from file...\n    // Deserializing Sample Data Set...\n    // Comparing Initial and Deserialized Data Sets...\n    // For Pair 1 result of Data Set Identity Comparison is True\n    //For Pair 2 result of Data Set Identity Comparison is True\n    // ----------------------------------------\n    // Press any key to exit.\n\n###Sample 6: Using Avro to upload data for the Microsoft Azure HDInsight service\n\nThe sixth example illustrates some programming techniques related to interacting with the Azure HDInsight service. A sample containing the code for this example can be downloaded from the [Azure code samples](https://code.msdn.microsoft.com/windowsazure/Using-Avro-to-upload-data-ae81b1e3) site.\n\nThe sample does the following:\n\n* Connects to an existing HDInsight service cluster.\n* Serializes several CSV files and uploads the result to Azure Blob storage. (The CSV files are distributed together with the sample and represent an extract from AMEX Stock historical data distributed by [Infochimps](http://www.infochimps.com/) for the period 1970-2010. The sample reads CSV file data, converts the records to instances of the **Stock** class, and then serializes them by using reflection. Stock type definition is created from a JSON schema via the Microsoft Avro Library code generation utility.\n* Creates a new external table called **Stocks** in Hive, and links it to the data uploaded in the previous step.\n* Executes a query by using Hive over the **Stocks** table.\n\nIn addition, the sample performs a clean-up procedure before and after performing major operations. During the clean-up, all of the related Azure Blob data and folders are removed, and the Hive table is dropped. You can also invoke the clean-up procedure from the sample command line.\n\nThe sample has the following prerequisites:\n\n* An active Microsoft Azure subscription and its subscription ID.\n* A management certificate for the subscription with the corresponding private key. The certificate should be installed in the current user private storage on the machine used to run the sample.\n* An active HDInsight cluster.\n* An Azure Storage account linked to the HDInsight cluster from the previous prerequisite, together with the corresponding primary or secondary access key.\n\nAll of the information from the prerequisites should be entered to the sample configuration file before the sample is run. There are two possible ways to do it:\n\n* Edit the app.config file in the sample root directory and then build the sample\n* First build the sample, and then edit AvroHDISample.exe.config in the build directory\n\nIn both cases all edits should be done in the **<appSettings>** settings section. Please follow the comments in the file.\nThe sample is run from the command line by executing the following command (where the .zip file with the sample was assumed to be extracted to C:\\AvroHDISample; if otherwise, use the relevant file path):\n\n    AvroHDISample run C:\\AvroHDISample\\Data\n\nTo clean up the cluster, run the following command:\n\n    AvroHDISample clean\n\n[deflate-100]: http://msdn.microsoft.com/library/system.io.compression.deflatestream(v=vs.100).aspx\n[deflate-110]: http://msdn.microsoft.com/library/system.io.compression.deflatestream(v=vs.110).aspx\n"
}