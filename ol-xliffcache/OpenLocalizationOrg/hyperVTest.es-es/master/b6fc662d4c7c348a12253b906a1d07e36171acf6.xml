{
  "nodes": [
    {
      "pos": [
        26,
        91
      ],
      "content": "Use Hadoop Pig with Remote Desktop in HDInsight | Microsoft Azure"
    },
    {
      "pos": [
        109,
        250
      ],
      "content": "Learn how to use the Pig command to run Pig Latin statements from a Remote Desktop connection to a Windows-based Hadoop cluster in HDInsight."
    },
    {
      "pos": [
        567,
        612
      ],
      "content": "Run Pig jobs from a Remote Desktop connection"
    },
    {
      "pos": [
        692,
        981
      ],
      "content": "This document provides a walkthrough for using the Pig command to run Pig Latin statements from a Remote Desktop connection to a Windows-based HDInsight cluster. Pig Latin allows you to create MapReduce applications by describing data transformations, rather than map and reduce functions.",
      "nodes": [
        {
          "content": "This document provides a walkthrough for using the Pig command to run Pig Latin statements from a Remote Desktop connection to a Windows-based HDInsight cluster.",
          "pos": [
            0,
            161
          ]
        },
        {
          "content": "Pig Latin allows you to create MapReduce applications by describing data transformations, rather than map and reduce functions.",
          "pos": [
            162,
            289
          ]
        }
      ]
    },
    {
      "pos": [
        983,
        1013
      ],
      "content": "In this document, learn how to"
    },
    {
      "pos": [
        1015,
        1017
      ],
      "content": "##"
    },
    {
      "pos": [
        1036,
        1049
      ],
      "content": "Prerequisites"
    },
    {
      "pos": [
        1051,
        1118
      ],
      "content": "To complete the steps in this article, you will need the following."
    },
    {
      "pos": [
        1122,
        1177
      ],
      "content": "A Windows-based HDInsight (Hadoop on HDInsight) cluster"
    },
    {
      "pos": [
        1181,
        1242
      ],
      "content": "A client computer running Windows 10, Windows 8, or Windows 7"
    },
    {
      "pos": [
        1244,
        1246
      ],
      "content": "##"
    },
    {
      "pos": [
        1266,
        1293
      ],
      "content": "Connect with Remote Desktop"
    },
    {
      "pos": [
        1295,
        1489
      ],
      "content": "Enable Remote Desktop for the HDInsight cluster, then connect to it by following the instructions at <bpt id=\"p1\">[</bpt>Connect to HDInsight clusters using RDP<ept id=\"p1\">](hdinsight-administer-use-management-portal.md#rdp)</ept>."
    },
    {
      "pos": [
        1491,
        1493
      ],
      "content": "##"
    },
    {
      "pos": [
        1509,
        1528
      ],
      "content": "Use the Pig command"
    },
    {
      "pos": [
        1533,
        1644
      ],
      "content": "After you have a Remote Desktop connection, start the <bpt id=\"p2\">**</bpt>Hadoop Command Line<ept id=\"p2\">**</ept><ph id=\"ph3\"/> by using the icon on the desktop."
    },
    {
      "pos": [
        1649,
        1692
      ],
      "content": "Use the following to start the Pig command:"
    },
    {
      "pos": [
        1726,
        1771
      ],
      "content": "You will be presented with a <ph id=\"ph4\">`grunt&gt;`</ph><ph id=\"ph5\"/> prompt."
    },
    {
      "pos": [
        1776,
        1806
      ],
      "content": "Enter the following statement:"
    },
    {
      "pos": [
        1868,
        2012
      ],
      "content": "This command loads the contents of the sample.log file into the LOGS file. You can view the contents of the file by using the following command:",
      "nodes": [
        {
          "content": "This command loads the contents of the sample.log file into the LOGS file.",
          "pos": [
            0,
            74
          ]
        },
        {
          "content": "You can view the contents of the file by using the following command:",
          "pos": [
            75,
            144
          ]
        }
      ]
    },
    {
      "pos": [
        2037,
        2140
      ],
      "content": "Transform the data by applying a regular expression to extract only the logging level from each record:"
    },
    {
      "pos": [
        2260,
        2353
      ],
      "content": "You can use <bpt id=\"p3\">**</bpt>DUMP<ept id=\"p3\">**</ept><ph id=\"ph6\"/> to view the data after the transformation. In this case, <ph id=\"ph7\">`DUMP LEVELS;`</ph>.",
      "nodes": [
        {
          "content": "You can use <bpt id=\"p3\">**</bpt>DUMP<ept id=\"p3\">**</ept><ph id=\"ph6\"/> to view the data after the transformation.",
          "pos": [
            0,
            115
          ]
        },
        {
          "content": "In this case, <ph id=\"ph7\">`DUMP LEVELS;`</ph>.",
          "pos": [
            116,
            163
          ]
        }
      ]
    },
    {
      "pos": [
        2358,
        2495
      ],
      "content": "Continue applying transformations by using the following statements. Use <ph id=\"ph8\">`DUMP`</ph><ph id=\"ph9\"/> to view the result of the transformation after each step.",
      "nodes": [
        {
          "content": "Continue applying transformations by using the following statements.",
          "pos": [
            0,
            68
          ]
        },
        {
          "content": "Use <ph id=\"ph8\">`DUMP`</ph><ph id=\"ph9\"/> to view the result of the transformation after each step.",
          "pos": [
            69,
            169
          ]
        }
      ]
    },
    {
      "pos": [
        2501,
        3368
      ],
      "leadings": [
        "",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   "
      ],
      "content": "<ph id=\"ph10\">&lt;table&gt;</ph><ph id=\"ph11\">\n &lt;tr&gt;</ph><ph id=\"ph12\">\n &lt;th&gt;</ph>Statement<ph id=\"ph13\">&lt;/th&gt;</ph><ph id=\"ph14\">&lt;th&gt;</ph>What it does<ph id=\"ph15\">&lt;/th&gt;</ph><ph id=\"ph16\">\n &lt;/tr&gt;</ph><ph id=\"ph17\">\n &lt;tr&gt;</ph><ph id=\"ph18\">\n &lt;td&gt;</ph>FILTEREDLEVELS = FILTER LEVELS by LOGLEVEL is not null;<ph id=\"ph19\">&lt;/td&gt;</ph><ph id=\"ph20\">&lt;td&gt;</ph>Removes rows that contain a null value for the log level and stores the results into FILTEREDLEVELS.<ph id=\"ph21\">&lt;/td&gt;</ph><ph id=\"ph22\">\n &lt;/tr&gt;</ph><ph id=\"ph23\">\n &lt;tr&gt;</ph><ph id=\"ph24\">\n &lt;td&gt;</ph>GROUPEDLEVELS = GROUP FILTEREDLEVELS by LOGLEVEL;<ph id=\"ph25\">&lt;/td&gt;</ph><ph id=\"ph26\">&lt;td&gt;</ph>Groups the rows by log level and stores the results into GROUPEDLEVELS.<ph id=\"ph27\">&lt;/td&gt;</ph><ph id=\"ph28\">\n &lt;/tr&gt;</ph><ph id=\"ph29\">\n &lt;tr&gt;</ph><ph id=\"ph30\">\n &lt;td&gt;</ph>FREQUENCIES = foreach GROUPEDLEVELS generate group as LOGLEVEL, COUNT(FILTEREDLEVELS.LOGLEVEL) as COUNT;<ph id=\"ph31\">&lt;/td&gt;</ph><ph id=\"ph32\">&lt;td&gt;</ph>Creates a new set of data that contains each unique log level value and how many times it occurs. This is stored into FREQUENCIES<ph id=\"ph33\">&lt;/td&gt;</ph><ph id=\"ph34\">\n &lt;/tr&gt;</ph><ph id=\"ph35\">\n &lt;tr&gt;</ph><ph id=\"ph36\">\n &lt;td&gt;</ph>RESULT = order FREQUENCIES by COUNT desc;<ph id=\"ph37\">&lt;/td&gt;</ph><ph id=\"ph38\">&lt;td&gt;</ph>Orders the log levels by count (descending,) and stores into RESULT<ph id=\"ph39\">&lt;/td&gt;</ph><ph id=\"ph40\">\n &lt;/tr&gt;</ph><ph id=\"ph41\">\n &lt;/table&gt;</ph>",
      "nodes": [
        {
          "content": "<ph id=\"ph10\">&lt;table&gt;</ph><ph id=\"ph11\">\n &lt;tr&gt;</ph><ph id=\"ph12\">\n &lt;th&gt;</ph>Statement<ph id=\"ph13\">&lt;/th&gt;</ph><ph id=\"ph14\">&lt;th&gt;</ph>What it does<ph id=\"ph15\">&lt;/th&gt;</ph><ph id=\"ph16\">\n &lt;/tr&gt;</ph><ph id=\"ph17\">\n &lt;tr&gt;</ph><ph id=\"ph18\">\n &lt;td&gt;</ph>FILTEREDLEVELS = FILTER LEVELS by LOGLEVEL is not null;<ph id=\"ph19\">&lt;/td&gt;</ph><ph id=\"ph20\">&lt;td&gt;</ph>Removes rows that contain a null value for the log level and stores the results into FILTEREDLEVELS.<ph id=\"ph21\">&lt;/td&gt;</ph><ph id=\"ph22\">\n &lt;/tr&gt;</ph><ph id=\"ph23\">\n &lt;tr&gt;</ph><ph id=\"ph24\">\n &lt;td&gt;</ph>GROUPEDLEVELS = GROUP FILTEREDLEVELS by LOGLEVEL;<ph id=\"ph25\">&lt;/td&gt;</ph><ph id=\"ph26\">&lt;td&gt;</ph>Groups the rows by log level and stores the results into GROUPEDLEVELS.<ph id=\"ph27\">&lt;/td&gt;</ph><ph id=\"ph28\">\n &lt;/tr&gt;</ph><ph id=\"ph29\">\n &lt;tr&gt;</ph><ph id=\"ph30\">\n &lt;td&gt;</ph>FREQUENCIES = foreach GROUPEDLEVELS generate group as LOGLEVEL, COUNT(FILTEREDLEVELS.LOGLEVEL) as COUNT;<ph id=\"ph31\">&lt;/td&gt;</ph><ph id=\"ph32\">&lt;td&gt;</ph>Creates a new set of data that contains each unique log level value and how many times it occurs.",
          "pos": [
            0,
            1199
          ]
        },
        {
          "content": "This is stored into FREQUENCIES<ph id=\"ph33\">&lt;/td&gt;</ph><ph id=\"ph34\">\n &lt;/tr&gt;</ph><ph id=\"ph35\">\n &lt;tr&gt;</ph><ph id=\"ph36\">\n &lt;td&gt;</ph>RESULT = order FREQUENCIES by COUNT desc;<ph id=\"ph37\">&lt;/td&gt;</ph><ph id=\"ph38\">&lt;td&gt;</ph>Orders the log levels by count (descending,) and stores into RESULT<ph id=\"ph39\">&lt;/td&gt;</ph><ph id=\"ph40\">\n &lt;/tr&gt;</ph><ph id=\"ph41\">\n &lt;/table&gt;</ph>",
          "pos": [
            1200,
            1619
          ]
        }
      ]
    },
    {
      "pos": [
        3373,
        3601
      ],
      "content": "You can also save the results of a transformation by using the <ph id=\"ph42\">`STORE`</ph><ph id=\"ph43\"/> statement. For example, the following command saves the <ph id=\"ph44\">`RESULT`</ph><ph id=\"ph45\"/> to the <bpt id=\"p4\">**</bpt>/example/data/pigout<ept id=\"p4\">**</ept><ph id=\"ph46\"/> directory in the default storage container for your cluster:",
      "nodes": [
        {
          "content": "You can also save the results of a transformation by using the <ph id=\"ph42\">`STORE`</ph><ph id=\"ph43\"/> statement.",
          "pos": [
            0,
            115
          ]
        },
        {
          "content": "For example, the following command saves the <ph id=\"ph44\">`RESULT`</ph><ph id=\"ph45\"/> to the <bpt id=\"p4\">**</bpt>/example/data/pigout<ept id=\"p4\">**</ept><ph id=\"ph46\"/> directory in the default storage container for your cluster:",
          "pos": [
            116,
            349
          ]
        }
      ]
    },
    {
      "pos": [
        3666,
        3823
      ],
      "content": "<ph id=\"ph47\">[AZURE.NOTE]</ph><ph id=\"ph48\"/> The data is stored in the specified directory in files named <bpt id=\"p5\">**</bpt>part-nnnnn<ept id=\"p5\">**</ept>. If the directory already exists, you will receive an error message.",
      "nodes": [
        {
          "content": "<ph id=\"ph47\">[AZURE.NOTE]</ph><ph id=\"ph48\"/> The data is stored in the specified directory in files named <bpt id=\"p5\">**</bpt>part-nnnnn<ept id=\"p5\">**</ept>.",
          "pos": [
            0,
            161
          ]
        },
        {
          "content": "If the directory already exists, you will receive an error message.",
          "pos": [
            162,
            229
          ]
        }
      ]
    },
    {
      "pos": [
        3828,
        3884
      ],
      "content": "To exit the grunt prompt, enter the following statement."
    },
    {
      "pos": [
        3904,
        3925
      ],
      "content": "Pig Latin batch files"
    },
    {
      "pos": [
        3927,
        4005
      ],
      "content": "You can also use the Pig command to run Pig Latin that is contained in a file."
    },
    {
      "pos": [
        4010,
        4136
      ],
      "content": "After exiting the grunt prompt, open <bpt id=\"p6\">**</bpt>Notepad<ept id=\"p6\">**</ept><ph id=\"ph49\"/> and create a new file named <bpt id=\"p7\">**</bpt>pigbatch.pig<ept id=\"p7\">**</ept><ph id=\"ph50\"/> in the <bpt id=\"p8\">**</bpt>%PIG_HOME%<ept id=\"p8\">**</ept><ph id=\"ph51\"/> directory."
    },
    {
      "pos": [
        4141,
        4224
      ],
      "content": "Type or paste the following lines into the <bpt id=\"p9\">**</bpt>pigbatch.pig<ept id=\"p9\">**</ept><ph id=\"ph52\"/> file, and then save it:"
    },
    {
      "pos": [
        4704,
        4777
      ],
      "content": "Use the following to run the <bpt id=\"p10\">**</bpt>pigbatch.pig<ept id=\"p10\">**</ept><ph id=\"ph53\"/> file using the pig command."
    },
    {
      "pos": [
        4820,
        4966
      ],
      "content": "When the batch job completes, you should see the following output, which should be the same as when you used <ph id=\"ph54\">`DUMP RESULT;`</ph><ph id=\"ph55\"/> in the previous steps:"
    },
    {
      "pos": [
        5081,
        5083
      ],
      "content": "##"
    },
    {
      "pos": [
        5103,
        5110
      ],
      "content": "Summary"
    },
    {
      "pos": [
        5112,
        5252
      ],
      "content": "As you can see, the Pig command allows you to interactively run MapReduce operations, or run Pig Latin jobs that are stored in a batch file."
    },
    {
      "pos": [
        5254,
        5256
      ],
      "content": "##"
    },
    {
      "pos": [
        5278,
        5288
      ],
      "content": "Next steps"
    },
    {
      "pos": [
        5290,
        5337
      ],
      "content": "For general information about Pig in HDInsight:"
    },
    {
      "pos": [
        5341,
        5397
      ],
      "content": "<bpt id=\"p11\">[</bpt>Use Pig with Hadoop on HDInsight<ept id=\"p11\">](hdinsight-use-pig.md)</ept>"
    },
    {
      "pos": [
        5399,
        5470
      ],
      "content": "For information about other ways you can work with Hadoop on HDInsight:"
    },
    {
      "pos": [
        5474,
        5532
      ],
      "content": "<bpt id=\"p12\">[</bpt>Use Hive with Hadoop on HDInsight<ept id=\"p12\">](hdinsight-use-hive.md)</ept>"
    },
    {
      "pos": [
        5536,
        5604
      ],
      "content": "<bpt id=\"p13\">[</bpt>Use MapReduce with Hadoop on HDInsight<ept id=\"p13\">](hdinsight-use-mapreduce.md)</ept>"
    }
  ],
  "content": "<properties\n   pageTitle=\"Use Hadoop Pig with Remote Desktop in HDInsight | Microsoft Azure\"\n   description=\"Learn how to use the Pig command to run Pig Latin statements from a Remote Desktop connection to a Windows-based Hadoop cluster in HDInsight.\"\n   services=\"hdinsight\"\n   documentationCenter=\"\"\n   authors=\"Blackmist\"\n   manager=\"paulettm\"\n   editor=\"cgronlun\"\n    tags=\"azure-portal\"/>\n\n<tags\n   ms.service=\"hdinsight\"\n   ms.devlang=\"na\"\n   ms.topic=\"article\"\n   ms.tgt_pltfrm=\"na\"\n   ms.workload=\"big-data\"\n   ms.date=\"02/05/2016\"\n   ms.author=\"larryfr\"/>\n\n#Run Pig jobs from a Remote Desktop connection\n\n[AZURE.INCLUDE [pig-selector](../../includes/hdinsight-selector-use-pig.md)]\n\nThis document provides a walkthrough for using the Pig command to run Pig Latin statements from a Remote Desktop connection to a Windows-based HDInsight cluster. Pig Latin allows you to create MapReduce applications by describing data transformations, rather than map and reduce functions.\n\nIn this document, learn how to\n\n##<a id=\"prereq\"></a>Prerequisites\n\nTo complete the steps in this article, you will need the following.\n\n* A Windows-based HDInsight (Hadoop on HDInsight) cluster\n\n* A client computer running Windows 10, Windows 8, or Windows 7\n\n##<a id=\"connect\"></a>Connect with Remote Desktop\n\nEnable Remote Desktop for the HDInsight cluster, then connect to it by following the instructions at [Connect to HDInsight clusters using RDP](hdinsight-administer-use-management-portal.md#rdp).\n\n##<a id=\"pig\"></a>Use the Pig command\n\n2. After you have a Remote Desktop connection, start the **Hadoop Command Line** by using the icon on the desktop.\n\n2. Use the following to start the Pig command:\n\n        %pig_home%\\bin\\pig\n\n    You will be presented with a `grunt>` prompt.\n\n3. Enter the following statement:\n\n        LOGS = LOAD 'wasb:///example/data/sample.log';\n\n    This command loads the contents of the sample.log file into the LOGS file. You can view the contents of the file by using the following command:\n\n        DUMP LOGS;\n\n4. Transform the data by applying a regular expression to extract only the logging level from each record:\n\n        LEVELS = foreach LOGS generate REGEX_EXTRACT($0, '(TRACE|DEBUG|INFO|WARN|ERROR|FATAL)', 1)  as LOGLEVEL;\n\n    You can use **DUMP** to view the data after the transformation. In this case, `DUMP LEVELS;`.\n\n5. Continue applying transformations by using the following statements. Use `DUMP` to view the result of the transformation after each step.\n\n    <table>\n    <tr>\n    <th>Statement</th><th>What it does</th>\n    </tr>\n    <tr>\n    <td>FILTEREDLEVELS = FILTER LEVELS by LOGLEVEL is not null;</td><td>Removes rows that contain a null value for the log level and stores the results into FILTEREDLEVELS.</td>\n    </tr>\n    <tr>\n    <td>GROUPEDLEVELS = GROUP FILTEREDLEVELS by LOGLEVEL;</td><td>Groups the rows by log level and stores the results into GROUPEDLEVELS.</td>\n    </tr>\n    <tr>\n    <td>FREQUENCIES = foreach GROUPEDLEVELS generate group as LOGLEVEL, COUNT(FILTEREDLEVELS.LOGLEVEL) as COUNT;</td><td>Creates a new set of data that contains each unique log level value and how many times it occurs. This is stored into FREQUENCIES</td>\n    </tr>\n    <tr>\n    <td>RESULT = order FREQUENCIES by COUNT desc;</td><td>Orders the log levels by count (descending,) and stores into RESULT</td>\n    </tr>\n    </table>\n\n6. You can also save the results of a transformation by using the `STORE` statement. For example, the following command saves the `RESULT` to the **/example/data/pigout** directory in the default storage container for your cluster:\n\n        STORE RESULT into 'wasb:///example/data/pigout'\n\n    > [AZURE.NOTE] The data is stored in the specified directory in files named **part-nnnnn**. If the directory already exists, you will receive an error message.\n\n7. To exit the grunt prompt, enter the following statement.\n\n        QUIT;\n\n###Pig Latin batch files\n\nYou can also use the Pig command to run Pig Latin that is contained in a file.\n\n3. After exiting the grunt prompt, open **Notepad** and create a new file named **pigbatch.pig** in the **%PIG_HOME%** directory.\n\n4. Type or paste the following lines into the **pigbatch.pig** file, and then save it:\n\n        LOGS = LOAD 'wasb:///example/data/sample.log';\n        LEVELS = foreach LOGS generate REGEX_EXTRACT($0, '(TRACE|DEBUG|INFO|WARN|ERROR|FATAL)', 1)  as LOGLEVEL;\n        FILTEREDLEVELS = FILTER LEVELS by LOGLEVEL is not null;\n        GROUPEDLEVELS = GROUP FILTEREDLEVELS by LOGLEVEL;\n        FREQUENCIES = foreach GROUPEDLEVELS generate group as LOGLEVEL, COUNT(FILTEREDLEVELS.LOGLEVEL) as COUNT;\n        RESULT = order FREQUENCIES by COUNT desc;\n        DUMP RESULT;\n\n5. Use the following to run the **pigbatch.pig** file using the pig command.\n\n        pig %PIG_HOME%\\pigbatch.pig\n\n    When the batch job completes, you should see the following output, which should be the same as when you used `DUMP RESULT;` in the previous steps:\n\n        (TRACE,816)\n        (DEBUG,434)\n        (INFO,96)\n        (WARN,11)\n        (ERROR,6)\n        (FATAL,2)\n\n##<a id=\"summary\"></a>Summary\n\nAs you can see, the Pig command allows you to interactively run MapReduce operations, or run Pig Latin jobs that are stored in a batch file.\n\n##<a id=\"nextsteps\"></a>Next steps\n\nFor general information about Pig in HDInsight:\n\n* [Use Pig with Hadoop on HDInsight](hdinsight-use-pig.md)\n\nFor information about other ways you can work with Hadoop on HDInsight:\n\n* [Use Hive with Hadoop on HDInsight](hdinsight-use-hive.md)\n\n* [Use MapReduce with Hadoop on HDInsight](hdinsight-use-mapreduce.md)\n"
}