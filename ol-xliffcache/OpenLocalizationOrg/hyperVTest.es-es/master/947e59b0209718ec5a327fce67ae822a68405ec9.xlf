<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" source-language="en-US" target-language="es-es">
    <body>
      <group id="main" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Create and load data into Hive tables from Blob storage | Microsoft Azure</source>
          <target state="new">Create and load data into Hive tables from Blob storage | Microsoft Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Create Hive tables and load data in blob to hive tables</source>
          <target state="new">Create Hive tables and load data in blob to hive tables</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Create and load data into Hive tables from Azure blob storage</source>
          <target state="new">Create and load data into Hive tables from Azure blob storage</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>Introduction</source>
          <target state="new">Introduction</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>In <bpt id="p1">**</bpt>this document<ept id="p1">**</ept>, generic Hive queries that create Hive tables and load data from Azure blob storage are presented.</source>
          <target state="new">In <bpt id="p1">**</bpt>this document<ept id="p1">**</ept>, generic Hive queries that create Hive tables and load data from Azure blob storage are presented.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>Some guidance is also provided on partitioning Hive tables and on using the Optimized Row Columnar (ORC) formatting to improve query performance.</source>
          <target state="new">Some guidance is also provided on partitioning Hive tables and on using the Optimized Row Columnar (ORC) formatting to improve query performance.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>This <bpt id="p2">**</bpt>menu<ept id="p2">**</ept><ph id="ph2" /> links to topics that describe how to ingest data into target environments where the data can be stored and processed during the Cortana Analytics Process (CAPS).</source>
          <target state="new">This <bpt id="p2">**</bpt>menu<ept id="p2">**</ept><ph id="ph2" /> links to topics that describe how to ingest data into target environments where the data can be stored and processed during the Cortana Analytics Process (CAPS).</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>Prerequisites</source>
          <target state="new">Prerequisites</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>This article assumes that you have:</source>
          <target state="new">This article assumes that you have:</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>Created an Azure storage account.</source>
          <target state="new">Created an Azure storage account.</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>If you need instructions, see <bpt id="p3">[</bpt>Create an Azure Storage account<ept id="p3">](../hdinsight-get-started.md#storage)</ept></source>
          <target state="new">If you need instructions, see <bpt id="p3">[</bpt>Create an Azure Storage account<ept id="p3">](../hdinsight-get-started.md#storage)</ept></target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>Provisioned a customized Hadoop cluster with the HDInsight service.</source>
          <target state="new">Provisioned a customized Hadoop cluster with the HDInsight service.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>If you need instructions, see <bpt id="p4">[</bpt>Customize Azure HDInsight Hadoop clusters for advanced analytics<ept id="p4">](machine-learning-data-science-customize-hadoop-cluster.md)</ept>.</source>
          <target state="new">If you need instructions, see <bpt id="p4">[</bpt>Customize Azure HDInsight Hadoop clusters for advanced analytics<ept id="p4">](machine-learning-data-science-customize-hadoop-cluster.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>Enabled remote access to the cluster, logged in, and opened the Hadoop Command Line console.</source>
          <target state="new">Enabled remote access to the cluster, logged in, and opened the Hadoop Command Line console.</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>If you need instructions, see <bpt id="p5">[</bpt>Access the Head Node of Hadoop Cluster<ept id="p5">](machine-learning-data-science-customize-hadoop-cluster.md#headnode)</ept>.</source>
          <target state="new">If you need instructions, see <bpt id="p5">[</bpt>Access the Head Node of Hadoop Cluster<ept id="p5">](machine-learning-data-science-customize-hadoop-cluster.md#headnode)</ept>.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>Upload data to Azure blob storage</source>
          <target state="new">Upload data to Azure blob storage</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>If you created an Azure virtual machine by following the instructions provided in <bpt id="p6">[</bpt>Set up an Azure virtual machine for advanced analytics<ept id="p6">](machine-learning-data-science-setup-virtual-machine.md)</ept>, this script file should have been downloaded to the <bpt id="p7">*</bpt>C:\\Users\\\&lt;user name\&gt;\\Documents\\Data Science Scripts<ept id="p7">*</ept><ph id="ph4" /> directory on the virtual machine.</source>
          <target state="new">If you created an Azure virtual machine by following the instructions provided in <bpt id="p6">[</bpt>Set up an Azure virtual machine for advanced analytics<ept id="p6">](machine-learning-data-science-setup-virtual-machine.md)</ept>, this script file should have been downloaded to the <bpt id="p7">*</bpt>C:\\Users\\\&lt;user name\&gt;\\Documents\\Data Science Scripts<ept id="p7">*</ept><ph id="ph4" /> directory on the virtual machine.</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>These Hive queries only require that you plug in your own data schema and Azure blob storage configuration in the appropriate fields to be ready for submission.</source>
          <target state="new">These Hive queries only require that you plug in your own data schema and Azure blob storage configuration in the appropriate fields to be ready for submission.</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>We assume that the data for Hive tables is in an <bpt id="p8">**</bpt>uncompressed<ept id="p8">**</ept><ph id="ph5" /> tabular format, and that the data has been uploaded to the default (or to an additional) container of the storage account used by the Hadoop cluster.</source>
          <target state="new">We assume that the data for Hive tables is in an <bpt id="p8">**</bpt>uncompressed<ept id="p8">**</ept><ph id="ph5" /> tabular format, and that the data has been uploaded to the default (or to an additional) container of the storage account used by the Hadoop cluster.</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>If you want to practice on the <bpt id="p9">_</bpt>NYC Taxi Trip Data<ept id="p9">_</ept>, you need to first  download the 24 <ph id="ph6">&lt;a href="http://www.andresmh.com/nyctaxitrips/" target="_blank"&gt;</ph>NYC Taxi Trip Data<ph id="ph7">&lt;/a&gt;</ph><ph id="ph8" /> files (12 Trip files, and 12 Fare files), <bpt id="p10">**</bpt>unzip<ept id="p10">**</ept><ph id="ph9" /> all files into .csv files, and then upload them to the default (or appropriate container) of the Azure storage account that was created by the procedure outlined in the <bpt id="p11">[</bpt>Customize Azure HDInsight Hadoop clusters for Advanced Analytics Process and Technology<ept id="p11">](machine-learning-data-science-customize-hadoop-cluster.md)</ept><ph id="ph10" /> topic.</source>
          <target state="new">If you want to practice on the <bpt id="p9">_</bpt>NYC Taxi Trip Data<ept id="p9">_</ept>, you need to first  download the 24 <ph id="ph6">&lt;a href="http://www.andresmh.com/nyctaxitrips/" target="_blank"&gt;</ph>NYC Taxi Trip Data<ph id="ph7">&lt;/a&gt;</ph><ph id="ph8" /> files (12 Trip files, and 12 Fare files), <bpt id="p10">**</bpt>unzip<ept id="p10">**</ept><ph id="ph9" /> all files into .csv files, and then upload them to the default (or appropriate container) of the Azure storage account that was created by the procedure outlined in the <bpt id="p11">[</bpt>Customize Azure HDInsight Hadoop clusters for Advanced Analytics Process and Technology<ept id="p11">](machine-learning-data-science-customize-hadoop-cluster.md)</ept><ph id="ph10" /> topic.</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>The process to upload the .csv files to the default container on the storage account can be found on this <bpt id="p12">[</bpt>page<ept id="p12">](machine-learning-data-science-process-hive-walkthrough/#upload)</ept>.</source>
          <target state="new">The process to upload the .csv files to the default container on the storage account can be found on this <bpt id="p12">[</bpt>page<ept id="p12">](machine-learning-data-science-process-hive-walkthrough/#upload)</ept>.</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>How to Submit Hive Queries</source>
          <target state="new">How to Submit Hive Queries</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>Hive queries can be submitted by using:</source>
          <target state="new">Hive queries can be submitted by using:</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>the Hadoop Command Line on the headnode of the cluster</source>
          <target state="new">the Hadoop Command Line on the headnode of the cluster</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>the IPython Notebook</source>
          <target state="new">the IPython Notebook</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>the Hive Editor</source>
          <target state="new">the Hive Editor</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>Azure PowerShell scripts</source>
          <target state="new">Azure PowerShell scripts</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>Hive queries are SQL-like.</source>
          <target state="new">Hive queries are SQL-like.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>Users familiar with SQL may find the</source>
          <target state="new">Users familiar with SQL may find the</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>SQL-to-Hive Cheat Sheet</source>
          <target state="new">SQL-to-Hive Cheat Sheet</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>useful.</source>
          <target state="new">useful.</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>When submitting a Hive query, you can also control the destination of the output from Hive queries, whether it be on the screen or to a local file on the head node or to an Azure blob.</source>
          <target state="new">When submitting a Hive query, you can also control the destination of the output from Hive queries, whether it be on the screen or to a local file on the head node or to an Azure blob.</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>Through Hadoop Command Line console in Head Node of Hadoop Cluster</source>
          <target state="new">Through Hadoop Command Line console in Head Node of Hadoop Cluster</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>If the query is complex, submitting Hive queries directly from the head node of the Hadoop cluster typically leads to faster turn around than submitting it with a Hive Editor or by using Azure PowerShell scripts.</source>
          <target state="new">If the query is complex, submitting Hive queries directly from the head node of the Hadoop cluster typically leads to faster turn around than submitting it with a Hive Editor or by using Azure PowerShell scripts.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>Log in to the head node of the Hadoop cluster, open the Hadoop Command Line on the desktop of the head node, and enter command</source>
          <target state="new">Log in to the head node of the Hadoop cluster, open the Hadoop Command Line on the desktop of the head node, and enter command</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>Users have three ways to submit Hive queries in Hadoop Command Line console:</source>
          <target state="new">Users have three ways to submit Hive queries in Hadoop Command Line console:</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>directly from the Hadoop command line</source>
          <target state="new">directly from the Hadoop command line</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>using .hql files</source>
          <target state="new">using .hql files</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>from the Hive command console</source>
          <target state="new">from the Hive command console</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>Submit Hive queries directly from the Hadoop Command Line</source>
          <target state="new">Submit Hive queries directly from the Hadoop Command Line</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>Users can run command like</source>
          <target state="new">Users can run command like</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>to submit simple Hive queries directly in the Hadoop command line.</source>
          <target state="new">to submit simple Hive queries directly in the Hadoop command line.</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>Here is an example, where the red box outlines the command that submits the Hive query, and the green box outlines the output from the Hive query.</source>
          <target state="new">Here is an example, where the red box outlines the command that submits the Hive query, and the green box outlines the output from the Hive query.</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source><ph id="ph11">![</ph>Create workspace<ph id="ph12">](./media/machine-learning-data-science-process-hive-tables/run-hive-queries-1.png)</ph></source>
          <target state="new"><ph id="ph11">![</ph>Create workspace<ph id="ph12">](./media/machine-learning-data-science-process-hive-tables/run-hive-queries-1.png)</ph></target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>Submit Hive queries in .hql files</source>
          <target state="new">Submit Hive queries in .hql files</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>When the Hive query is more complicated and has multiple lines, editing queries in Hadoop command line or Hive command console is not practical.</source>
          <target state="new">When the Hive query is more complicated and has multiple lines, editing queries in Hadoop command line or Hive command console is not practical.</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>An alternative is to use a text editor in the head node of the Hadoop cluster and to save the Hive queries in a .hql file in a local directory of the head node.</source>
          <target state="new">An alternative is to use a text editor in the head node of the Hadoop cluster and to save the Hive queries in a .hql file in a local directory of the head node.</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>Then the Hive query in the .hql file can be submitted by using the <ph id="ph13">`-f`</ph><ph id="ph14" /> argument in the <ph id="ph15">`hive`</ph><ph id="ph16" /> command as follows:</source>
          <target state="new">Then the Hive query in the .hql file can be submitted by using the <ph id="ph13">`-f`</ph><ph id="ph14" /> argument in the <ph id="ph15">`hive`</ph><ph id="ph16" /> command as follows:</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>Suppress progress status screen print of Hive queries</source>
          <target state="new">Suppress progress status screen print of Hive queries</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>By default, after Hive query is submitted in the Hadoop Command Line console, the progress of the Map/Reduce job will be printed out on screen.</source>
          <target state="new">By default, after Hive query is submitted in the Hadoop Command Line console, the progress of the Map/Reduce job will be printed out on screen.</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>To suppress the screen print of the Map/Reduce job progress, you can use the argument <ph id="ph17">`-S`</ph><ph id="ph18" /> (case-sensitive) argument in the command line as follows:</source>
          <target state="new">To suppress the screen print of the Map/Reduce job progress, you can use the argument <ph id="ph17">`-S`</ph><ph id="ph18" /> (case-sensitive) argument in the command line as follows:</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>Submit Hive queries in Hive command console.</source>
          <target state="new">Submit Hive queries in Hive command console.</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>Users can also enter the Hive command console by running the  <ph id="ph19">`hive`</ph><ph id="ph20" /> command from the Hadoop command line, and then submit Hive queries from Hive command console at the <bpt id="p13">**</bpt>hive&gt;<ept id="p13">**</ept><ph id="ph21" /> prompt.</source>
          <target state="new">Users can also enter the Hive command console by running the  <ph id="ph19">`hive`</ph><ph id="ph20" /> command from the Hadoop command line, and then submit Hive queries from Hive command console at the <bpt id="p13">**</bpt>hive&gt;<ept id="p13">**</ept><ph id="ph21" /> prompt.</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>Here is an example.</source>
          <target state="new">Here is an example.</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source><ph id="ph22">![</ph>Create workspace<ph id="ph23">](./media/machine-learning-data-science-process-hive-tables/run-hive-queries-2.png)</ph></source>
          <target state="new"><ph id="ph22">![</ph>Create workspace<ph id="ph23">](./media/machine-learning-data-science-process-hive-tables/run-hive-queries-2.png)</ph></target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>In this example, the two red boxes highlight the commands used to enter the Hive command console, and the Hive query submitted in Hive command console, respectively.</source>
          <target state="new">In this example, the two red boxes highlight the commands used to enter the Hive command console, and the Hive query submitted in Hive command console, respectively.</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>The green box highlights the output from the Hive query.</source>
          <target state="new">The green box highlights the output from the Hive query.</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>The previous examples directly output the Hive query results on screen.</source>
          <target state="new">The previous examples directly output the Hive query results on screen.</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>Users can also write the output to a local file on the head node, or to an Azure blob.</source>
          <target state="new">Users can also write the output to a local file on the head node, or to an Azure blob.</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source>Then, users can use other tools to further analyze the output of from Hive queries.</source>
          <target state="new">Then, users can use other tools to further analyze the output of from Hive queries.</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>Output Hive query results to a local file.</source>
          <target state="new">Output Hive query results to a local file.</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>To output Hive query results to a local directory on the head node, users have to submit the Hive query in the Hadoop Command Line as follows:</source>
          <target state="new">To output Hive query results to a local directory on the head node, users have to submit the Hive query in the Hadoop Command Line as follows:</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>Output Hive query results to an Azure blob</source>
          <target state="new">Output Hive query results to an Azure blob</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>Users can also output the Hive query results to an Azure blob, within the default container of the Hadoop cluster.</source>
          <target state="new">Users can also output the Hive query results to an Azure blob, within the default container of the Hadoop cluster.</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>The Hive query to do this looks like this:</source>
          <target state="new">The Hive query to do this looks like this:</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>In the following example, the output of Hive query is written to a blob directory <ph id="ph24">`queryoutputdir`</ph><ph id="ph25" /> within the default container of the Hadoop cluster.</source>
          <target state="new">In the following example, the output of Hive query is written to a blob directory <ph id="ph24">`queryoutputdir`</ph><ph id="ph25" /> within the default container of the Hadoop cluster.</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source>Here, you must only provide the directory name, without the blob name.</source>
          <target state="new">Here, you must only provide the directory name, without the blob name.</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source>An error will be thrown out if you provide both the directory and the blob name, such as <bpt id="p14">*</bpt>wasb:///queryoutputdir/queryoutput.txt<ept id="p14">*</ept>.</source>
          <target state="new">An error will be thrown out if you provide both the directory and the blob name, such as <bpt id="p14">*</bpt>wasb:///queryoutputdir/queryoutput.txt<ept id="p14">*</ept>.</target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source><ph id="ph26">![</ph>Create workspace<ph id="ph27">](./media/machine-learning-data-science-process-hive-tables/output-hive-results-2.png)</ph></source>
          <target state="new"><ph id="ph26">![</ph>Create workspace<ph id="ph27">](./media/machine-learning-data-science-process-hive-tables/output-hive-results-2.png)</ph></target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>The output of the Hive query can be seen in blob storage by opening the default container of the Hadoop cluster using the Azure Storage Explorer (or equivalent) tool.</source>
          <target state="new">The output of the Hive query can be seen in blob storage by opening the default container of the Hadoop cluster using the Azure Storage Explorer (or equivalent) tool.</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>You can apply the filter (highlighted by red box) if you only want to retrieve a blob with specified letters in names.</source>
          <target state="new">You can apply the filter (highlighted by red box) if you only want to retrieve a blob with specified letters in names.</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source><ph id="ph28">![</ph>Create workspace<ph id="ph29">](./media/machine-learning-data-science-process-hive-tables/output-hive-results-3.png)</ph></source>
          <target state="new"><ph id="ph28">![</ph>Create workspace<ph id="ph29">](./media/machine-learning-data-science-process-hive-tables/output-hive-results-3.png)</ph></target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source>Through Hive Editor or Azure PowerShell Commands</source>
          <target state="new">Through Hive Editor or Azure PowerShell Commands</target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source>Users can also use the Query Console (Hive Editor) by entering the URL of the form</source>
          <target state="new">Users can also use the Query Console (Hive Editor) by entering the URL of the form</target>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source><bpt id="p15">*</bpt>https://&amp;#60;Hadoop cluster name&gt;.azurehdinsight.net/Home/HiveEditor<ept id="p15">*</ept></source>
          <target state="new"><bpt id="p15">*</bpt>https://&amp;#60;Hadoop cluster name&gt;.azurehdinsight.net/Home/HiveEditor<ept id="p15">*</ept></target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source>into a web browser.</source>
          <target state="new">into a web browser.</target>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source>Note that you will be asked to input the Hadoop cluster credentials to log in.</source>
          <target state="new">Note that you will be asked to input the Hadoop cluster credentials to log in.</target>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source>Alternatively, you can <bpt id="p16">[</bpt>Run Hive queries using PowerShell<ept id="p16">](../hdinsight/hdinsight-hadoop-use-hive-powershell.md)</ept>.</source>
          <target state="new">Alternatively, you can <bpt id="p16">[</bpt>Run Hive queries using PowerShell<ept id="p16">](../hdinsight/hdinsight-hadoop-use-hive-powershell.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source>Create Hive database and tables</source>
          <target state="new">Create Hive database and tables</target>
        </trans-unit>
        <trans-unit id="180" translate="yes" xml:space="preserve">
          <source>The Hive queries are shared in the <bpt id="p17">[</bpt>Github repository<ept id="p17">](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_create_db_tbls_load_data_generic.hql)</ept><ph id="ph30" /> and can be downloaded from there.</source>
          <target state="new">The Hive queries are shared in the <bpt id="p17">[</bpt>Github repository<ept id="p17">](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/DataScienceProcess/DataScienceScripts/sample_hive_create_db_tbls_load_data_generic.hql)</ept><ph id="ph30" /> and can be downloaded from there.</target>
        </trans-unit>
        <trans-unit id="181" translate="yes" xml:space="preserve">
          <source>Here is the Hive query that creates a Hive table.</source>
          <target state="new">Here is the Hive query that creates a Hive table.</target>
        </trans-unit>
        <trans-unit id="182" translate="yes" xml:space="preserve">
          <source>Here are the descriptions of the fields that users need to plug in and other configurations:</source>
          <target state="new">Here are the descriptions of the fields that users need to plug in and other configurations:</target>
        </trans-unit>
        <trans-unit id="183" translate="yes" xml:space="preserve">
          <source><bpt id="p18">**</bpt>&amp;#60;database name&gt;<ept id="p18">**</ept>: the name of the database users want to create.</source>
          <target state="new"><bpt id="p18">**</bpt>&amp;#60;database name&gt;<ept id="p18">**</ept>: the name of the database users want to create.</target>
        </trans-unit>
        <trans-unit id="184" translate="yes" xml:space="preserve">
          <source>If users just want to use the default database, the query <bpt id="p19">*</bpt>create database...<ept id="p19">*</ept><ph id="ph31" /> can be omitted.</source>
          <target state="new">If users just want to use the default database, the query <bpt id="p19">*</bpt>create database...<ept id="p19">*</ept><ph id="ph31" /> can be omitted.</target>
        </trans-unit>
        <trans-unit id="185" translate="yes" xml:space="preserve">
          <source><bpt id="p20">**</bpt>&amp;#60;table name&gt;<ept id="p20">**</ept>: the name of the table users want to create within the specified database.</source>
          <target state="new"><bpt id="p20">**</bpt>&amp;#60;table name&gt;<ept id="p20">**</ept>: the name of the table users want to create within the specified database.</target>
        </trans-unit>
        <trans-unit id="186" translate="yes" xml:space="preserve">
          <source>If users want to use the default database, the table can be directly referred by <bpt id="p21">*</bpt>&amp;#60;table name&gt;<ept id="p21">*</ept><ph id="ph32" /> without &amp;#60;database name&gt;.</source>
          <target state="new">If users want to use the default database, the table can be directly referred by <bpt id="p21">*</bpt>&amp;#60;table name&gt;<ept id="p21">*</ept><ph id="ph32" /> without &amp;#60;database name&gt;.</target>
        </trans-unit>
        <trans-unit id="187" translate="yes" xml:space="preserve">
          <source><bpt id="p22">**</bpt>&amp;#60;field separator&gt;<ept id="p22">**</ept>: the separator that delimits fields in the data file to be uploaded to the Hive table.</source>
          <target state="new"><bpt id="p22">**</bpt>&amp;#60;field separator&gt;<ept id="p22">**</ept>: the separator that delimits fields in the data file to be uploaded to the Hive table.</target>
        </trans-unit>
        <trans-unit id="188" translate="yes" xml:space="preserve">
          <source><bpt id="p23">**</bpt>&amp;#60;line separator&gt;<ept id="p23">**</ept>: the separator that delimits lines in the data file.</source>
          <target state="new"><bpt id="p23">**</bpt>&amp;#60;line separator&gt;<ept id="p23">**</ept>: the separator that delimits lines in the data file.</target>
        </trans-unit>
        <trans-unit id="189" translate="yes" xml:space="preserve">
          <source><bpt id="p24">**</bpt>&amp;#60;storage location&gt;<ept id="p24">**</ept>: the Azure storage location to save the data of Hive tables.</source>
          <target state="new"><bpt id="p24">**</bpt>&amp;#60;storage location&gt;<ept id="p24">**</ept>: the Azure storage location to save the data of Hive tables.</target>
        </trans-unit>
        <trans-unit id="190" translate="yes" xml:space="preserve">
          <source>If users do not specify <bpt id="p25">*</bpt>LOCATION &amp;#60;storage location&gt;<ept id="p25">*</ept>, the database and the tables are stored in <bpt id="p26">*</bpt>hive/warehouse/<ept id="p26">*</ept><ph id="ph33" /> directory in the default container of the Hive cluster by default.</source>
          <target state="new">If users do not specify <bpt id="p25">*</bpt>LOCATION &amp;#60;storage location&gt;<ept id="p25">*</ept>, the database and the tables are stored in <bpt id="p26">*</bpt>hive/warehouse/<ept id="p26">*</ept><ph id="ph33" /> directory in the default container of the Hive cluster by default.</target>
        </trans-unit>
        <trans-unit id="191" translate="yes" xml:space="preserve">
          <source>If a user wants to specify the storage location, the storage location has to be within the default container for the database and tables.</source>
          <target state="new">If a user wants to specify the storage location, the storage location has to be within the default container for the database and tables.</target>
        </trans-unit>
        <trans-unit id="192" translate="yes" xml:space="preserve">
          <source>This location has to be referred as location relative to the default container of the cluster in the format of <bpt id="p27">*</bpt>'wasb:///&amp;#60;directory 1&gt;/'<ept id="p27">*</ept><ph id="ph34" /> or <bpt id="p28">*</bpt>'wasb:///&amp;#60;directory 1&gt;/&amp;#60;directory 2&gt;/'<ept id="p28">*</ept>, etc. After the query is executed, the relative directories will be created within the default container.</source>
          <target state="new">This location has to be referred as location relative to the default container of the cluster in the format of <bpt id="p27">*</bpt>'wasb:///&amp;#60;directory 1&gt;/'<ept id="p27">*</ept><ph id="ph34" /> or <bpt id="p28">*</bpt>'wasb:///&amp;#60;directory 1&gt;/&amp;#60;directory 2&gt;/'<ept id="p28">*</ept>, etc. After the query is executed, the relative directories will be created within the default container.</target>
        </trans-unit>
        <trans-unit id="193" translate="yes" xml:space="preserve">
          <source><bpt id="p29">**</bpt>TBLPROPERTIES("skip.header.line.count"="1")<ept id="p29">**</ept>: If the data file has a header line, users have to add this property <bpt id="p30">**</bpt>at the end<ept id="p30">**</ept><ph id="ph35" /> of the <bpt id="p31">*</bpt>create table<ept id="p31">*</ept><ph id="ph36" /> query.</source>
          <target state="new"><bpt id="p29">**</bpt>TBLPROPERTIES("skip.header.line.count"="1")<ept id="p29">**</ept>: If the data file has a header line, users have to add this property <bpt id="p30">**</bpt>at the end<ept id="p30">**</ept><ph id="ph35" /> of the <bpt id="p31">*</bpt>create table<ept id="p31">*</ept><ph id="ph36" /> query.</target>
        </trans-unit>
        <trans-unit id="194" translate="yes" xml:space="preserve">
          <source>Otherwise, the header line will be loaded as a record to the table.</source>
          <target state="new">Otherwise, the header line will be loaded as a record to the table.</target>
        </trans-unit>
        <trans-unit id="195" translate="yes" xml:space="preserve">
          <source>If the data file does not have a header line, this configuration can be omitted in the query.</source>
          <target state="new">If the data file does not have a header line, this configuration can be omitted in the query.</target>
        </trans-unit>
        <trans-unit id="196" translate="yes" xml:space="preserve">
          <source>Load data to Hive tables</source>
          <target state="new">Load data to Hive tables</target>
        </trans-unit>
        <trans-unit id="197" translate="yes" xml:space="preserve">
          <source>Here is the Hive query that loads data into a Hive table.</source>
          <target state="new">Here is the Hive query that loads data into a Hive table.</target>
        </trans-unit>
        <trans-unit id="198" translate="yes" xml:space="preserve">
          <source><bpt id="p32">**</bpt>&amp;#60;path to blob data&gt;<ept id="p32">**</ept>: If the blob file to be uploaded to the Hive table is in the default container of the HDInsight Hadoop cluster, the <bpt id="p33">*</bpt>&amp;#60;path to blob data&gt;<ept id="p33">*</ept><ph id="ph37" /> should be in the format <bpt id="p34">*</bpt>'wasb:///&amp;#60;directory in this container&gt;/&amp;#60;blob file name&gt;'<ept id="p34">*</ept>.</source>
          <target state="new"><bpt id="p32">**</bpt>&amp;#60;path to blob data&gt;<ept id="p32">**</ept>: If the blob file to be uploaded to the Hive table is in the default container of the HDInsight Hadoop cluster, the <bpt id="p33">*</bpt>&amp;#60;path to blob data&gt;<ept id="p33">*</ept><ph id="ph37" /> should be in the format <bpt id="p34">*</bpt>'wasb:///&amp;#60;directory in this container&gt;/&amp;#60;blob file name&gt;'<ept id="p34">*</ept>.</target>
        </trans-unit>
        <trans-unit id="199" translate="yes" xml:space="preserve">
          <source>The blob file can also be in an additional container of the HDInsight Hadoop cluster.</source>
          <target state="new">The blob file can also be in an additional container of the HDInsight Hadoop cluster.</target>
        </trans-unit>
        <trans-unit id="200" translate="yes" xml:space="preserve">
          <source>In this case, <bpt id="p35">*</bpt>&amp;#60;path to blob data&gt;<ept id="p35">*</ept><ph id="ph38" /> should be in the format <bpt id="p36">*</bpt>'wasb://&amp;#60;container name&gt;@&amp;#60;storage account name&gt;.blob.core.windows.net/&amp;#60;blob file name&gt;'<ept id="p36">*</ept>.</source>
          <target state="new">In this case, <bpt id="p35">*</bpt>&amp;#60;path to blob data&gt;<ept id="p35">*</ept><ph id="ph38" /> should be in the format <bpt id="p36">*</bpt>'wasb://&amp;#60;container name&gt;@&amp;#60;storage account name&gt;.blob.core.windows.net/&amp;#60;blob file name&gt;'<ept id="p36">*</ept>.</target>
        </trans-unit>
        <trans-unit id="201" translate="yes" xml:space="preserve">
          <source><ph id="ph39">[AZURE.NOTE]</ph><ph id="ph40" /> The blob data to be uploaded to Hive table has to be in the default or additional container of the storage account for the Hadoop cluster.</source>
          <target state="new"><ph id="ph39">[AZURE.NOTE]</ph><ph id="ph40" /> The blob data to be uploaded to Hive table has to be in the default or additional container of the storage account for the Hadoop cluster.</target>
        </trans-unit>
        <trans-unit id="202" translate="yes" xml:space="preserve">
          <source>Otherwise, the <bpt id="p37">*</bpt>LOAD DATA<ept id="p37">*</ept><ph id="ph41" /> query will fail complaining that it cannot access the data.</source>
          <target state="new">Otherwise, the <bpt id="p37">*</bpt>LOAD DATA<ept id="p37">*</ept><ph id="ph41" /> query will fail complaining that it cannot access the data.</target>
        </trans-unit>
        <trans-unit id="203" translate="yes" xml:space="preserve">
          <source>Advanced topics: partitioned table and store Hive data in ORC format</source>
          <target state="new">Advanced topics: partitioned table and store Hive data in ORC format</target>
        </trans-unit>
        <trans-unit id="204" translate="yes" xml:space="preserve">
          <source>If the data is large, partitioning the table is beneficial for queries that only need to scan a few partitions of the table.</source>
          <target state="new">If the data is large, partitioning the table is beneficial for queries that only need to scan a few partitions of the table.</target>
        </trans-unit>
        <trans-unit id="205" translate="yes" xml:space="preserve">
          <source>For instance, it is reasonable to partition the log data of a web site by dates.</source>
          <target state="new">For instance, it is reasonable to partition the log data of a web site by dates.</target>
        </trans-unit>
        <trans-unit id="206" translate="yes" xml:space="preserve">
          <source>In addition to partitioning Hive tables, it is also beneficial to store the Hive data in the Optimized Row Columnar (ORC) format.</source>
          <target state="new">In addition to partitioning Hive tables, it is also beneficial to store the Hive data in the Optimized Row Columnar (ORC) format.</target>
        </trans-unit>
        <trans-unit id="207" translate="yes" xml:space="preserve">
          <source>For more information on ORC formatting, see</source>
          <target state="new">For more information on ORC formatting, see</target>
        </trans-unit>
        <trans-unit id="208" translate="yes" xml:space="preserve">
          <source>Using ORC files improves performance when Hive is reading, writing, and processing data</source>
          <target state="new">Using ORC files improves performance when Hive is reading, writing, and processing data</target>
        </trans-unit>
        <trans-unit id="210" translate="yes" xml:space="preserve">
          <source>Partitioned table</source>
          <target state="new">Partitioned table</target>
        </trans-unit>
        <trans-unit id="211" translate="yes" xml:space="preserve">
          <source>Here is the Hive query that creates a partitioned table and loads data into it.</source>
          <target state="new">Here is the Hive query that creates a partitioned table and loads data into it.</target>
        </trans-unit>
        <trans-unit id="212" translate="yes" xml:space="preserve">
          <source>When querying partitioned tables, it is recommended to add the partition condition in the <bpt id="p38">**</bpt>beginning<ept id="p38">**</ept><ph id="ph42" /> of the <ph id="ph43">`where`</ph><ph id="ph44" /> clause as this improves the efficacy of searching significantly.</source>
          <target state="new">When querying partitioned tables, it is recommended to add the partition condition in the <bpt id="p38">**</bpt>beginning<ept id="p38">**</ept><ph id="ph42" /> of the <ph id="ph43">`where`</ph><ph id="ph44" /> clause as this improves the efficacy of searching significantly.</target>
        </trans-unit>
        <trans-unit id="213" translate="yes" xml:space="preserve">
          <source>Store Hive data in ORC format</source>
          <target state="new">Store Hive data in ORC format</target>
        </trans-unit>
        <trans-unit id="214" translate="yes" xml:space="preserve">
          <source>Users cannot directly load data from blob storage into Hive tables that is stored in the ORC format.</source>
          <target state="new">Users cannot directly load data from blob storage into Hive tables that is stored in the ORC format.</target>
        </trans-unit>
        <trans-unit id="215" translate="yes" xml:space="preserve">
          <source>Here are the steps that the users need to take in order to load data from Azure blobs to Hive tables stored in ORC format.</source>
          <target state="new">Here are the steps that the users need to take in order to load data from Azure blobs to Hive tables stored in ORC format.</target>
        </trans-unit>
        <trans-unit id="216" translate="yes" xml:space="preserve">
          <source>Create an external table <bpt id="p39">**</bpt>STORED AS TEXTFILE<ept id="p39">**</ept><ph id="ph45" /> and load data from blob storage to the table.</source>
          <target state="new">Create an external table <bpt id="p39">**</bpt>STORED AS TEXTFILE<ept id="p39">**</ept><ph id="ph45" /> and load data from blob storage to the table.</target>
        </trans-unit>
        <trans-unit id="217" translate="yes" xml:space="preserve">
          <source>Create an internal table with the same schema as the external table in step 1, with the same field delimiter, and store the Hive data in the ORC format.</source>
          <target state="new">Create an internal table with the same schema as the external table in step 1, with the same field delimiter, and store the Hive data in the ORC format.</target>
        </trans-unit>
        <trans-unit id="218" translate="yes" xml:space="preserve">
          <source>Select data from the external table in step 1 and insert into the ORC table</source>
          <target state="new">Select data from the external table in step 1 and insert into the ORC table</target>
        </trans-unit>
        <trans-unit id="219" translate="yes" xml:space="preserve">
          <source><ph id="ph46">[AZURE.NOTE]</ph><ph id="ph47" /> If the TEXTFILE table <bpt id="p40">*</bpt>&amp;#60;database name&gt;.&amp;#60;external textfile table name&gt;<ept id="p40">*</ept><ph id="ph48" /> has partitions, in STEP 3, the <ph id="ph49">`SELECT * FROM &lt;database name&gt;.&lt;external textfile table name&gt;`</ph><ph id="ph50" /> command will select the partition variable as a field in the returned data set.</source>
          <target state="new"><ph id="ph46">[AZURE.NOTE]</ph><ph id="ph47" /> If the TEXTFILE table <bpt id="p40">*</bpt>&amp;#60;database name&gt;.&amp;#60;external textfile table name&gt;<ept id="p40">*</ept><ph id="ph48" /> has partitions, in STEP 3, the <ph id="ph49">`SELECT * FROM &lt;database name&gt;.&lt;external textfile table name&gt;`</ph><ph id="ph50" /> command will select the partition variable as a field in the returned data set.</target>
        </trans-unit>
        <trans-unit id="220" translate="yes" xml:space="preserve">
          <source>Inserting it into the <bpt id="p41">*</bpt>&amp;#60;database name&gt;.&amp;#60;ORC table name&gt;<ept id="p41">*</ept><ph id="ph51" /> will fail since <bpt id="p42">*</bpt>&amp;#60;database name&gt;.&amp;#60;ORC table name&gt;<ept id="p42">*</ept><ph id="ph52" /> does not have the partition variable as a field in the table schema.</source>
          <target state="new">Inserting it into the <bpt id="p41">*</bpt>&amp;#60;database name&gt;.&amp;#60;ORC table name&gt;<ept id="p41">*</ept><ph id="ph51" /> will fail since <bpt id="p42">*</bpt>&amp;#60;database name&gt;.&amp;#60;ORC table name&gt;<ept id="p42">*</ept><ph id="ph52" /> does not have the partition variable as a field in the table schema.</target>
        </trans-unit>
        <trans-unit id="221" translate="yes" xml:space="preserve">
          <source>In this case, users need to specifically select the fields to be inserted to <bpt id="p43">*</bpt>&amp;#60;database name&gt;.&amp;#60;ORC table name&gt;<ept id="p43">*</ept><ph id="ph53" /> as follows:</source>
          <target state="new">In this case, users need to specifically select the fields to be inserted to <bpt id="p43">*</bpt>&amp;#60;database name&gt;.&amp;#60;ORC table name&gt;<ept id="p43">*</ept><ph id="ph53" /> as follows:</target>
        </trans-unit>
        <trans-unit id="222" translate="yes" xml:space="preserve">
          <source>It is safe to drop the <bpt id="p44">*</bpt>&amp;#60;external textfile table name&gt;<ept id="p44">*</ept><ph id="ph54" /> when using the following query after all data has been inserted into <bpt id="p45">*</bpt>&amp;#60;database name&gt;.&amp;#60;ORC table name&gt;<ept id="p45">*</ept>:</source>
          <target state="new">It is safe to drop the <bpt id="p44">*</bpt>&amp;#60;external textfile table name&gt;<ept id="p44">*</ept><ph id="ph54" /> when using the following query after all data has been inserted into <bpt id="p45">*</bpt>&amp;#60;database name&gt;.&amp;#60;ORC table name&gt;<ept id="p45">*</ept>:</target>
        </trans-unit>
        <trans-unit id="223" translate="yes" xml:space="preserve">
          <source>After following this procedure, you should have a table with data in the ORC format ready to use.</source>
          <target state="new">After following this procedure, you should have a table with data in the ORC format ready to use.</target>
        </trans-unit>
        <trans-unit id="224" translate="yes" xml:space="preserve">
          <source>Tuning sections should go here</source>
          <target state="new">Tuning sections should go here</target>
        </trans-unit>
        <trans-unit id="225" translate="yes" xml:space="preserve">
          <source>In the final section, parameters that users can tune so that the performance of Hive queries can be improved are discussed.</source>
          <target state="new">In the final section, parameters that users can tune so that the performance of Hive queries can be improved are discussed.</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">947e59b0209718ec5a327fce67ae822a68405ec9</xliffext:olfilehash>
  </header>
</xliff>