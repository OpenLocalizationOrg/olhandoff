<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" source-language="en-US" target-language="es-es">
    <body>
      <group id="main" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Use Beeline to work with Hive on HDInsight (Hadoop) | Microsoft Azure</source>
          <target state="new">Use Beeline to work with Hive on HDInsight (Hadoop) | Microsoft Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Learn how to use SSH to connect to a Hadoop cluster in HDInsight, and then interactively submit Hive queries by using Beeline.</source>
          <target state="new">Learn how to use SSH to connect to a Hadoop cluster in HDInsight, and then interactively submit Hive queries by using Beeline.</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Beeline is a utility for working with HiveServer2 over JDBC.</source>
          <target state="new">Beeline is a utility for working with HiveServer2 over JDBC.</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>Use Hive with Hadoop in HDInsight with Beeline</source>
          <target state="new">Use Hive with Hadoop in HDInsight with Beeline</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>In this article, you will learn how to use Secure Shell (SSH) to connect to a Linux-based HDInsight cluster, and then interactively submit Hive queries by using the <bpt id="p1">[</bpt>Beeline<ept id="p1">](https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-Beeline–NewCommandLineShell)</ept><ph id="ph3" /> command-line tool.</source>
          <target state="new">In this article, you will learn how to use Secure Shell (SSH) to connect to a Linux-based HDInsight cluster, and then interactively submit Hive queries by using the <bpt id="p1">[</bpt>Beeline<ept id="p1">](https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-Beeline–NewCommandLineShell)</ept><ph id="ph3" /> command-line tool.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source><ph id="ph4">[AZURE.NOTE]</ph><ph id="ph5" /> Beeline used JDBC to connect to Hive.</source>
          <target state="new"><ph id="ph4">[AZURE.NOTE]</ph><ph id="ph5" /> Beeline used JDBC to connect to Hive.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>For more information on using JDBC with Hive, see <bpt id="p2">[</bpt>Connect to Hive on Azure HDInsight using the Hive JDBC driver<ept id="p2">](hdinsight-connect-hive-jdbc-driver.md)</ept>.</source>
          <target state="new">For more information on using JDBC with Hive, see <bpt id="p2">[</bpt>Connect to Hive on Azure HDInsight using the Hive JDBC driver<ept id="p2">](hdinsight-connect-hive-jdbc-driver.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>Prerequisites</source>
          <target state="new">Prerequisites</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>To complete the steps in this article, you will need the following:</source>
          <target state="new">To complete the steps in this article, you will need the following:</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>A Linux-based Hadoop on HDInsight cluster.</source>
          <target state="new">A Linux-based Hadoop on HDInsight cluster.</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>An SSH client.</source>
          <target state="new">An SSH client.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>Linux, Unix, and Mac OS should come with an SSH client.</source>
          <target state="new">Linux, Unix, and Mac OS should come with an SSH client.</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>Windows users must download a client, such as <bpt id="p3">[</bpt>PuTTY<ept id="p3">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.</source>
          <target state="new">Windows users must download a client, such as <bpt id="p3">[</bpt>PuTTY<ept id="p3">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>Connect with SSH</source>
          <target state="new">Connect with SSH</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>Connect to the fully qualified domain name (FQDN) of your HDInsight cluster by using the SSH command.</source>
          <target state="new">Connect to the fully qualified domain name (FQDN) of your HDInsight cluster by using the SSH command.</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>The FQDN will be the name you gave the cluster, then <bpt id="p4">**</bpt>.azurehdinsight.net<ept id="p4">**</ept>.</source>
          <target state="new">The FQDN will be the name you gave the cluster, then <bpt id="p4">**</bpt>.azurehdinsight.net<ept id="p4">**</ept>.</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>For example, the following would connect to a cluster named <bpt id="p5">**</bpt>myhdinsight<ept id="p5">**</ept>:</source>
          <target state="new">For example, the following would connect to a cluster named <bpt id="p5">**</bpt>myhdinsight<ept id="p5">**</ept>:</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source><bpt id="p6">**</bpt>If you provided a certificate key for SSH authentication<ept id="p6">**</ept><ph id="ph6" /> when you created the HDInsight cluster, you may need to specify the location of the private key on your client system:</source>
          <target state="new"><bpt id="p6">**</bpt>If you provided a certificate key for SSH authentication<ept id="p6">**</ept><ph id="ph6" /> when you created the HDInsight cluster, you may need to specify the location of the private key on your client system:</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source><bpt id="p7">**</bpt>If you provided a password for SSH authentication<ept id="p7">**</ept><ph id="ph7" /> when you created the HDInsight cluster, you will need to provide the password when prompted.</source>
          <target state="new"><bpt id="p7">**</bpt>If you provided a password for SSH authentication<ept id="p7">**</ept><ph id="ph7" /> when you created the HDInsight cluster, you will need to provide the password when prompted.</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>For more information on using SSH with HDInsight, see <bpt id="p8">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Linux, OS X, and Unix<ept id="p8">](hdinsight-hadoop-linux-use-ssh-unix.md)</ept>.</source>
          <target state="new">For more information on using SSH with HDInsight, see <bpt id="p8">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Linux, OS X, and Unix<ept id="p8">](hdinsight-hadoop-linux-use-ssh-unix.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>PuTTY (Windows-based clients)</source>
          <target state="new">PuTTY (Windows-based clients)</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>Windows does not provide a built-in SSH client.</source>
          <target state="new">Windows does not provide a built-in SSH client.</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>We recommend using <bpt id="p9">**</bpt>PuTTY<ept id="p9">**</ept>, which can be downloaded from <bpt id="p10">[</bpt>http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html<ept id="p10">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.</source>
          <target state="new">We recommend using <bpt id="p9">**</bpt>PuTTY<ept id="p9">**</ept>, which can be downloaded from <bpt id="p10">[</bpt>http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html<ept id="p10">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>For more information on using PuTTY, see <bpt id="p11">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Windows <ept id="p11">](hdinsight-hadoop-linux-use-ssh-windows.md)</ept>.</source>
          <target state="new">For more information on using PuTTY, see <bpt id="p11">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Windows <ept id="p11">](hdinsight-hadoop-linux-use-ssh-windows.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>Use the Beeline command</source>
          <target state="new">Use the Beeline command</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>Once connected, use the following to get the hostname of the head node:</source>
          <target state="new">Once connected, use the following to get the hostname of the head node:</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>Save the returned host name, as it will be used later when connecting to HiveServer2 from Beeline.</source>
          <target state="new">Save the returned host name, as it will be used later when connecting to HiveServer2 from Beeline.</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>Start the Hive CLI by using the following command:</source>
          <target state="new">Start the Hive CLI by using the following command:</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>From the <ph id="ph8">`beeline&gt;`</ph><ph id="ph9" /> prompt, use the following to connect to the HiveServer2 service.</source>
          <target state="new">From the <ph id="ph8">`beeline&gt;`</ph><ph id="ph9" /> prompt, use the following to connect to the HiveServer2 service.</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>Replace <bpt id="p12">__</bpt>HOSTNAME<ept id="p12">__</ept><ph id="ph10" /> with the host name returned for the head node ealier:</source>
          <target state="new">Replace <bpt id="p12">__</bpt>HOSTNAME<ept id="p12">__</ept><ph id="ph10" /> with the host name returned for the head node ealier:</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>When prompted, enter the password for the administrator (admin) account for your HDInsight cluster.</source>
          <target state="new">When prompted, enter the password for the administrator (admin) account for your HDInsight cluster.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>Once the connection is established, the prompt will change to the following:</source>
          <target state="new">Once the connection is established, the prompt will change to the following:</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>Beeline commands usually begin with a <ph id="ph11">`!`</ph><ph id="ph12" /> character, for example <ph id="ph13">`!help`</ph><ph id="ph14" /> displays help.</source>
          <target state="new">Beeline commands usually begin with a <ph id="ph11">`!`</ph><ph id="ph12" /> character, for example <ph id="ph13">`!help`</ph><ph id="ph14" /> displays help.</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>However the <ph id="ph15">`!`</ph><ph id="ph16" /> can often be ommited.</source>
          <target state="new">However the <ph id="ph15">`!`</ph><ph id="ph16" /> can often be ommited.</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>For example, <ph id="ph17">`help`</ph><ph id="ph18" /> will also work.</source>
          <target state="new">For example, <ph id="ph17">`help`</ph><ph id="ph18" /> will also work.</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>If you view help, you will notice <ph id="ph19">`!sql`</ph>, which is used to execute HiveQL statements.</source>
          <target state="new">If you view help, you will notice <ph id="ph19">`!sql`</ph>, which is used to execute HiveQL statements.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>However, HiveQL is so commonly used that you can ommit the preceeding <ph id="ph20">`!sql`</ph>.</source>
          <target state="new">However, HiveQL is so commonly used that you can ommit the preceeding <ph id="ph20">`!sql`</ph>.</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>The following two statements have exactly the same results; displaying the tables currently available through Hive:</source>
          <target state="new">The following two statements have exactly the same results; displaying the tables currently available through Hive:</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>On a new cluster, only one table should be listed: <bpt id="p13">__</bpt>hivesampletable<ept id="p13">__</ept>.</source>
          <target state="new">On a new cluster, only one table should be listed: <bpt id="p13">__</bpt>hivesampletable<ept id="p13">__</ept>.</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>Use the following to display the schema for the hivesampletable:</source>
          <target state="new">Use the following to display the schema for the hivesampletable:</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>This will return the following information:</source>
          <target state="new">This will return the following information:</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>This displays the columns in the table.</source>
          <target state="new">This displays the columns in the table.</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>While we could perform some queries against this data, let's instead create a brand new table to demonstrate how to load data into Hive and apply a schema.</source>
          <target state="new">While we could perform some queries against this data, let's instead create a brand new table to demonstrate how to load data into Hive and apply a schema.</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>Enter the following statements to create a new table named <bpt id="p14">**</bpt>log4jLogs<ept id="p14">**</ept><ph id="ph21" /> by using sample data provided with the HDInsight cluster:</source>
          <target state="new">Enter the following statements to create a new table named <bpt id="p14">**</bpt>log4jLogs<ept id="p14">**</ept><ph id="ph21" /> by using sample data provided with the HDInsight cluster:</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>These statements perform the following actions:</source>
          <target state="new">These statements perform the following actions:</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source><bpt id="p15">**</bpt>DROP TABLE<ept id="p15">**</ept><ph id="ph22" /> - Deletes the table and the data file, in case the table already exists.</source>
          <target state="new"><bpt id="p15">**</bpt>DROP TABLE<ept id="p15">**</ept><ph id="ph22" /> - Deletes the table and the data file, in case the table already exists.</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source><bpt id="p16">**</bpt>CREATE EXTERNAL TABLE<ept id="p16">**</ept><ph id="ph23" /> - Creates a new 'external' table in Hive.</source>
          <target state="new"><bpt id="p16">**</bpt>CREATE EXTERNAL TABLE<ept id="p16">**</ept><ph id="ph23" /> - Creates a new 'external' table in Hive.</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>External tables only store the table definition in Hive.</source>
          <target state="new">External tables only store the table definition in Hive.</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>The data is left in the original location.</source>
          <target state="new">The data is left in the original location.</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source><bpt id="p17">**</bpt>ROW FORMAT<ept id="p17">**</ept><ph id="ph24" /> - Tells Hive how the data is formatted.</source>
          <target state="new"><bpt id="p17">**</bpt>ROW FORMAT<ept id="p17">**</ept><ph id="ph24" /> - Tells Hive how the data is formatted.</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>In this case, the fields in each log are separated by a space.</source>
          <target state="new">In this case, the fields in each log are separated by a space.</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source><bpt id="p18">**</bpt>STORED AS TEXTFILE LOCATION<ept id="p18">**</ept><ph id="ph25" /> - Tells Hive where the data is stored (the example/data directory), and that it is stored as text.</source>
          <target state="new"><bpt id="p18">**</bpt>STORED AS TEXTFILE LOCATION<ept id="p18">**</ept><ph id="ph25" /> - Tells Hive where the data is stored (the example/data directory), and that it is stored as text.</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source><bpt id="p19">**</bpt>SELECT<ept id="p19">**</ept><ph id="ph26" /> - Selects a count of all rows where column <bpt id="p20">**</bpt>t4<ept id="p20">**</ept><ph id="ph27" /> contains the value <bpt id="p21">**</bpt>[ERROR]<ept id="p21">**</ept>.</source>
          <target state="new"><bpt id="p19">**</bpt>SELECT<ept id="p19">**</ept><ph id="ph26" /> - Selects a count of all rows where column <bpt id="p20">**</bpt>t4<ept id="p20">**</ept><ph id="ph27" /> contains the value <bpt id="p21">**</bpt>[ERROR]<ept id="p21">**</ept>.</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>This should return a value of <bpt id="p22">**</bpt>3<ept id="p22">**</ept><ph id="ph28" /> as there are three rows that contain this value.</source>
          <target state="new">This should return a value of <bpt id="p22">**</bpt>3<ept id="p22">**</ept><ph id="ph28" /> as there are three rows that contain this value.</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source><bpt id="p23">**</bpt>INPUT__FILE__NAME LIKE '%.log'<ept id="p23">**</ept><ph id="ph29" /> - Tells Hive that we should only return data from files ending in .log.</source>
          <target state="new"><bpt id="p23">**</bpt>INPUT__FILE__NAME LIKE '%.log'<ept id="p23">**</ept><ph id="ph29" /> - Tells Hive that we should only return data from files ending in .log.</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>Normally, you would only have data with the same schema within the same folder when querying with hive, however this example log file is stored with other data formats.</source>
          <target state="new">Normally, you would only have data with the same schema within the same folder when querying with hive, however this example log file is stored with other data formats.</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source><ph id="ph30">[AZURE.NOTE]</ph><ph id="ph31" /> External tables should be used when you expect the underlying data to be updated by an external source, such as an automated data upload process, or by another MapReduce operation, but always want Hive queries to use the latest data.</source>
          <target state="new"><ph id="ph30">[AZURE.NOTE]</ph><ph id="ph31" /> External tables should be used when you expect the underlying data to be updated by an external source, such as an automated data upload process, or by another MapReduce operation, but always want Hive queries to use the latest data.</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>Dropping an external table does <bpt id="p24">**</bpt>not<ept id="p24">**</ept><ph id="ph32" /> delete the data, only the table definition.</source>
          <target state="new">Dropping an external table does <bpt id="p24">**</bpt>not<ept id="p24">**</ept><ph id="ph32" /> delete the data, only the table definition.</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>The output of this command should be similar to the following:</source>
          <target state="new">The output of this command should be similar to the following:</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>To exit Beeline, use <ph id="ph33">`!quit`</ph>.</source>
          <target state="new">To exit Beeline, use <ph id="ph33">`!quit`</ph>.</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>Run a HiveQL file</source>
          <target state="new">Run a HiveQL file</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>Beeline can also be used to run a file that contains HiveQL statements.</source>
          <target state="new">Beeline can also be used to run a file that contains HiveQL statements.</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source>Use the following steps to create a file, then run it using Beeline.</source>
          <target state="new">Use the following steps to create a file, then run it using Beeline.</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source>Use the following command to create a new file named <bpt id="p25">__</bpt>query.hql<ept id="p25">__</ept>:</source>
          <target state="new">Use the following command to create a new file named <bpt id="p25">__</bpt>query.hql<ept id="p25">__</ept>:</target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source>Once the editor opens, use the following as the contents of the file.</source>
          <target state="new">Once the editor opens, use the following as the contents of the file.</target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>This query will create a new 'internal' table named <bpt id="p26">**</bpt>errorLogs<ept id="p26">**</ept>:</source>
          <target state="new">This query will create a new 'internal' table named <bpt id="p26">**</bpt>errorLogs<ept id="p26">**</ept>:</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>These statements perform the following actions:</source>
          <target state="new">These statements perform the following actions:</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source><bpt id="p27">**</bpt>CREATE TABLE IF NOT EXISTS<ept id="p27">**</ept><ph id="ph34" /> - Creates a table, if it does not already exist.</source>
          <target state="new"><bpt id="p27">**</bpt>CREATE TABLE IF NOT EXISTS<ept id="p27">**</ept><ph id="ph34" /> - Creates a table, if it does not already exist.</target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source>Since the <bpt id="p28">**</bpt>EXTERNAL<ept id="p28">**</ept><ph id="ph35" /> keyword is not used, this is an internal table, which is stored in the Hive data warehouse and is managed completely by Hive.</source>
          <target state="new">Since the <bpt id="p28">**</bpt>EXTERNAL<ept id="p28">**</ept><ph id="ph35" /> keyword is not used, this is an internal table, which is stored in the Hive data warehouse and is managed completely by Hive.</target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source><bpt id="p29">**</bpt>STORED AS ORC<ept id="p29">**</ept><ph id="ph36" /> - Stores the data in Optimized Row Columnar (ORC) format.</source>
          <target state="new"><bpt id="p29">**</bpt>STORED AS ORC<ept id="p29">**</ept><ph id="ph36" /> - Stores the data in Optimized Row Columnar (ORC) format.</target>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source>This is a highly optimized and efficient format for storing Hive data.</source>
          <target state="new">This is a highly optimized and efficient format for storing Hive data.</target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source><bpt id="p30">**</bpt>INSERT OVERWRITE ... SELECT<ept id="p30">**</ept><ph id="ph37" /> - Selects rows from the <bpt id="p31">**</bpt>log4jLogs<ept id="p31">**</ept><ph id="ph38" /> table that contain <bpt id="p32">**</bpt>[ERROR]<ept id="p32">**</ept>, then inserts the data into the <bpt id="p33">**</bpt>errorLogs<ept id="p33">**</ept><ph id="ph39" /> table.</source>
          <target state="new"><bpt id="p30">**</bpt>INSERT OVERWRITE ... SELECT<ept id="p30">**</ept><ph id="ph37" /> - Selects rows from the <bpt id="p31">**</bpt>log4jLogs<ept id="p31">**</ept><ph id="ph38" /> table that contain <bpt id="p32">**</bpt>[ERROR]<ept id="p32">**</ept>, then inserts the data into the <bpt id="p33">**</bpt>errorLogs<ept id="p33">**</ept><ph id="ph39" /> table.</target>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source><ph id="ph40">[AZURE.NOTE]</ph><ph id="ph41" /> Unlike external tables, dropping an internal table will delete the underlying data as well.</source>
          <target state="new"><ph id="ph40">[AZURE.NOTE]</ph><ph id="ph41" /> Unlike external tables, dropping an internal table will delete the underlying data as well.</target>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source>To save the file, use <bpt id="p34">__</bpt>Ctrl<ept id="p34">__</ept>+_<bpt id="p35">__</bpt>X<ept id="p35">__</ept>, then enter <bpt id="p36">__</bpt>Y<ept id="p36">__</ept>, and finally <bpt id="p37">__</bpt>Enter<ept id="p37">__</ept>.</source>
          <target state="new">To save the file, use <bpt id="p34">__</bpt>Ctrl<ept id="p34">__</ept>+_<bpt id="p35">__</bpt>X<ept id="p35">__</ept>, then enter <bpt id="p36">__</bpt>Y<ept id="p36">__</ept>, and finally <bpt id="p37">__</bpt>Enter<ept id="p37">__</ept>.</target>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source>Use the following to run the file using Beeline.</source>
          <target state="new">Use the following to run the file using Beeline.</target>
        </trans-unit>
        <trans-unit id="180" translate="yes" xml:space="preserve">
          <source>Replease <bpt id="p38">__</bpt>HOSTNAME<ept id="p38">__</ept><ph id="ph42" /> with the name obtained earlier for the head node, and <bpt id="p39">__</bpt>PASSWORD<ept id="p39">__</ept><ph id="ph43" /> with the password for the admin account:</source>
          <target state="new">Replease <bpt id="p38">__</bpt>HOSTNAME<ept id="p38">__</ept><ph id="ph42" /> with the name obtained earlier for the head node, and <bpt id="p39">__</bpt>PASSWORD<ept id="p39">__</ept><ph id="ph43" /> with the password for the admin account:</target>
        </trans-unit>
        <trans-unit id="181" translate="yes" xml:space="preserve">
          <source>To verify that the <bpt id="p40">**</bpt>errorLogs<ept id="p40">**</ept><ph id="ph44" /> table was created, start Beeline and connect to HiveServer2, then use the following statement to return all the rows from <bpt id="p41">**</bpt>errorLogs<ept id="p41">**</ept>:</source>
          <target state="new">To verify that the <bpt id="p40">**</bpt>errorLogs<ept id="p40">**</ept><ph id="ph44" /> table was created, start Beeline and connect to HiveServer2, then use the following statement to return all the rows from <bpt id="p41">**</bpt>errorLogs<ept id="p41">**</ept>:</target>
        </trans-unit>
        <trans-unit id="182" translate="yes" xml:space="preserve">
          <source>Three rows of data should be returned, all containing <bpt id="p42">**</bpt>[ERROR]<ept id="p42">**</ept><ph id="ph45" /> in column t4:</source>
          <target state="new">Three rows of data should be returned, all containing <bpt id="p42">**</bpt>[ERROR]<ept id="p42">**</ept><ph id="ph45" /> in column t4:</target>
        </trans-unit>
        <trans-unit id="184" translate="yes" xml:space="preserve">
          <source>Summary</source>
          <target state="new">Summary</target>
        </trans-unit>
        <trans-unit id="185" translate="yes" xml:space="preserve">
          <source>As you can see, the Beeline command provides an easy way to interactively run Hive queries on an HDInsight cluster.</source>
          <target state="new">As you can see, the Beeline command provides an easy way to interactively run Hive queries on an HDInsight cluster.</target>
        </trans-unit>
        <trans-unit id="187" translate="yes" xml:space="preserve">
          <source>Next steps</source>
          <target state="new">Next steps</target>
        </trans-unit>
        <trans-unit id="188" translate="yes" xml:space="preserve">
          <source>For general information on Hive in HDInsight:</source>
          <target state="new">For general information on Hive in HDInsight:</target>
        </trans-unit>
        <trans-unit id="189" translate="yes" xml:space="preserve">
          <source><bpt id="p43">[</bpt>Use Hive with Hadoop on HDInsight<ept id="p43">](hdinsight-use-hive.md)</ept></source>
          <target state="new"><bpt id="p43">[</bpt>Use Hive with Hadoop on HDInsight<ept id="p43">](hdinsight-use-hive.md)</ept></target>
        </trans-unit>
        <trans-unit id="190" translate="yes" xml:space="preserve">
          <source>For information on other ways you can work with Hadoop on HDInsight:</source>
          <target state="new">For information on other ways you can work with Hadoop on HDInsight:</target>
        </trans-unit>
        <trans-unit id="191" translate="yes" xml:space="preserve">
          <source><bpt id="p44">[</bpt>Use Pig with Hadoop on HDInsight<ept id="p44">](hdinsight-use-pig.md)</ept></source>
          <target state="new"><bpt id="p44">[</bpt>Use Pig with Hadoop on HDInsight<ept id="p44">](hdinsight-use-pig.md)</ept></target>
        </trans-unit>
        <trans-unit id="192" translate="yes" xml:space="preserve">
          <source><bpt id="p45">[</bpt>Use MapReduce with Hadoop on HDInsight<ept id="p45">](hdinsight-use-mapreduce.md)</ept></source>
          <target state="new"><bpt id="p45">[</bpt>Use MapReduce with Hadoop on HDInsight<ept id="p45">](hdinsight-use-mapreduce.md)</ept></target>
        </trans-unit>
        <trans-unit id="193" translate="yes" xml:space="preserve">
          <source>If you are using Tez with Hive, see the following documents for debugging information:</source>
          <target state="new">If you are using Tez with Hive, see the following documents for debugging information:</target>
        </trans-unit>
        <trans-unit id="194" translate="yes" xml:space="preserve">
          <source><bpt id="p46">[</bpt>Use the Tez UI on Windows-based HDInsight<ept id="p46">](hdinsight-debug-tez-ui.md)</ept></source>
          <target state="new"><bpt id="p46">[</bpt>Use the Tez UI on Windows-based HDInsight<ept id="p46">](hdinsight-debug-tez-ui.md)</ept></target>
        </trans-unit>
        <trans-unit id="195" translate="yes" xml:space="preserve">
          <source><bpt id="p47">[</bpt>Use the Ambari Tez view on Linux-based HDInsight<ept id="p47">](hdinsight-debug-ambari-tez-view.md)</ept></source>
          <target state="new"><bpt id="p47">[</bpt>Use the Ambari Tez view on Linux-based HDInsight<ept id="p47">](hdinsight-debug-ambari-tez-view.md)</ept></target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">f208ec252dedb59081a2f384e952ba53f15c6df4</xliffext:olfilehash>
  </header>
</xliff>