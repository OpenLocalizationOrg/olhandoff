{
  "nodes": [
    {
      "pos": [
        26,
        97
      ],
      "content": "MapReduce and SSH connection with Hadoop in HDInsight | Microsoft Azure"
    },
    {
      "pos": [
        115,
        184
      ],
      "content": "Learn how to use SSH to run MapReduce jobs using Hadoop on HDInsight."
    },
    {
      "pos": [
        501,
        548
      ],
      "content": "Use MapReduce with Hadoop on HDInsight with SSH"
    },
    {
      "pos": [
        640,
        802
      ],
      "content": "In this article, you will learn how to use Secure Shell (SSH) to connect to a Hadoop on HDInsight cluster and then submit MapReduce jobs by using Hadoop commands."
    },
    {
      "pos": [
        806,
        988
      ],
      "content": "<ph id=\"ph3\">[AZURE.NOTE]</ph><ph id=\"ph4\"/> If you are already familiar with using Linux-based Hadoop servers, but you are new to HDInsight, see <bpt id=\"p1\">[</bpt>Linux-based HDInsight tips<ept id=\"p1\">](hdinsight-hadoop-linux-information.md)</ept>."
    },
    {
      "pos": [
        990,
        992
      ],
      "content": "##"
    },
    {
      "pos": [
        1011,
        1024
      ],
      "content": "Prerequisites"
    },
    {
      "pos": [
        1026,
        1093
      ],
      "content": "To complete the steps in this article, you will need the following:"
    },
    {
      "pos": [
        1097,
        1150
      ],
      "content": "A Linux-based HDInsight (Hadoop on HDInsight) cluster"
    },
    {
      "pos": [
        1154,
        1359
      ],
      "content": "An SSH client. Linux, Unix, and Mac operating systems should come with an SSH client. Windows users must download a client, such as <bpt id=\"p2\">[</bpt>PuTTY<ept id=\"p2\">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.",
      "nodes": [
        {
          "content": "An SSH client.",
          "pos": [
            0,
            14
          ]
        },
        {
          "content": "Linux, Unix, and Mac operating systems should come with an SSH client.",
          "pos": [
            15,
            85
          ]
        },
        {
          "content": "Windows users must download a client, such as <bpt id=\"p2\">[</bpt>PuTTY<ept id=\"p2\">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.",
          "pos": [
            86,
            243
          ]
        }
      ]
    },
    {
      "pos": [
        1361,
        1363
      ],
      "content": "##"
    },
    {
      "pos": [
        1379,
        1395
      ],
      "content": "Connect with SSH"
    },
    {
      "pos": [
        1397,
        1660
      ],
      "content": "Connect to the fully qualified domain name (FQDN) of your HDInsight cluster by using the SSH command. The FQDN will be the name you gave the cluster, followed by <bpt id=\"p3\">**</bpt>.azurehdinsight.net<ept id=\"p3\">**</ept>. For example, the following would connect to a cluster named <bpt id=\"p4\">**</bpt>myhdinsight<ept id=\"p4\">**</ept>:",
      "nodes": [
        {
          "content": "Connect to the fully qualified domain name (FQDN) of your HDInsight cluster by using the SSH command.",
          "pos": [
            0,
            101
          ]
        },
        {
          "content": "The FQDN will be the name you gave the cluster, followed by <bpt id=\"p3\">**</bpt>.azurehdinsight.net<ept id=\"p3\">**</ept>.",
          "pos": [
            102,
            224
          ]
        },
        {
          "content": "For example, the following would connect to a cluster named <bpt id=\"p4\">**</bpt>myhdinsight<ept id=\"p4\">**</ept>:",
          "pos": [
            225,
            339
          ]
        }
      ]
    },
    {
      "pos": [
        1712,
        1904
      ],
      "content": "<bpt id=\"p5\">**</bpt>If you provided a certificate key for SSH authentication<ept id=\"p5\">**</ept><ph id=\"ph5\"/> when you created the HDInsight cluster, you may need to specify the location of the private key on your client system, for example:"
    },
    {
      "pos": [
        1971,
        2117
      ],
      "content": "<bpt id=\"p6\">**</bpt>If you provided a password for SSH authentication<ept id=\"p6\">**</ept><ph id=\"ph6\"/> when you created the HDInsight cluster, you will need to provide the password when prompted."
    },
    {
      "pos": [
        2119,
        2287
      ],
      "content": "For more information on using SSH with HDInsight, see <bpt id=\"p7\">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Linux, OS X, and Unix<ept id=\"p7\">](hdinsight-hadoop-linux-use-ssh-unix.md)</ept>."
    },
    {
      "pos": [
        2292,
        2315
      ],
      "content": "PuTTY (Windows clients)"
    },
    {
      "pos": [
        2317,
        2555
      ],
      "content": "Windows does not provide a built-in SSH client. We recommend using <bpt id=\"p8\">**</bpt>PuTTY<ept id=\"p8\">**</ept>, which can be downloaded from <bpt id=\"p9\">[</bpt>http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html<ept id=\"p9\">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.",
      "nodes": [
        {
          "content": "Windows does not provide a built-in SSH client.",
          "pos": [
            0,
            47
          ]
        },
        {
          "content": "We recommend using <bpt id=\"p8\">**</bpt>PuTTY<ept id=\"p8\">**</ept>, which can be downloaded from <bpt id=\"p9\">[</bpt>http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html<ept id=\"p9\">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.",
          "pos": [
            48,
            314
          ]
        }
      ]
    },
    {
      "pos": [
        2557,
        2702
      ],
      "content": "For more information on using PuTTY, see <bpt id=\"p10\">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Windows <ept id=\"p10\">](hdinsight-hadoop-linux-use-ssh-windows.md)</ept>."
    },
    {
      "pos": [
        2704,
        2706
      ],
      "content": "##"
    },
    {
      "pos": [
        2725,
        2744
      ],
      "content": "Use Hadoop commands"
    },
    {
      "pos": [
        2749,
        2861
      ],
      "content": "After you are connected to the HDInsight cluster, use the following <bpt id=\"p11\">**</bpt>Hadoop<ept id=\"p11\">**</ept><ph id=\"ph7\"/> command to start a MapReduce job:"
    },
    {
      "pos": [
        3048,
        3294
      ],
      "content": "This starts the <bpt id=\"p12\">**</bpt>wordcount<ept id=\"p12\">**</ept><ph id=\"ph8\"/> class, which is contained in the <bpt id=\"p13\">**</bpt>hadoop-mapreduce-examples.jar<ept id=\"p13\">**</ept><ph id=\"ph9\"/> file. As input, it uses the <bpt id=\"p14\">**</bpt>wasb://example/data/gutenberg/davinci.txt<ept id=\"p14\">**</ept><ph id=\"ph10\"/> document, and output is stored at <bpt id=\"p15\">**</bpt>wasb:///example/data/WordCountOutput<ept id=\"p15\">**</ept>.",
      "nodes": [
        {
          "content": "This starts the <bpt id=\"p12\">**</bpt>wordcount<ept id=\"p12\">**</ept><ph id=\"ph8\"/> class, which is contained in the <bpt id=\"p13\">**</bpt>hadoop-mapreduce-examples.jar<ept id=\"p13\">**</ept><ph id=\"ph9\"/> file.",
          "pos": [
            0,
            210
          ]
        },
        {
          "content": "As input, it uses the <bpt id=\"p14\">**</bpt>wasb://example/data/gutenberg/davinci.txt<ept id=\"p14\">**</ept><ph id=\"ph10\"/> document, and output is stored at <bpt id=\"p15\">**</bpt>wasb:///example/data/WordCountOutput<ept id=\"p15\">**</ept>.",
          "pos": [
            211,
            449
          ]
        }
      ]
    },
    {
      "pos": [
        3302,
        3454
      ],
      "content": "<ph id=\"ph11\">[AZURE.NOTE]</ph><ph id=\"ph12\"/> For more information about this MapReduce job and the example data, see <bpt id=\"p16\">[</bpt>Use MapReduce in Hadoop on HDInsight<ept id=\"p16\">](hdinsight-use-mapreduce.md)</ept>."
    },
    {
      "pos": [
        3459,
        3573
      ],
      "content": "The job emits details as it processes, and it returns information similar to the following when the job completes:"
    },
    {
      "pos": [
        3706,
        3840
      ],
      "content": "When the job completes, use the following command to list the output files that are stored at <bpt id=\"p17\">**</bpt>wasb://example/data/WordCountOutput<ept id=\"p17\">**</ept>:"
    },
    {
      "pos": [
        3905,
        4030
      ],
      "content": "This should display two files, <bpt id=\"p18\">**</bpt>_SUCCESS<ept id=\"p18\">**</ept><ph id=\"ph13\"/> and <bpt id=\"p19\">**</bpt>part-r-00000<ept id=\"p19\">**</ept>. The <bpt id=\"p20\">**</bpt>part-r-00000<ept id=\"p20\">**</ept><ph id=\"ph14\"/> file contains the output for this job.",
      "nodes": [
        {
          "content": "This should display two files, <bpt id=\"p18\">**</bpt>_SUCCESS<ept id=\"p18\">**</ept><ph id=\"ph13\"/> and <bpt id=\"p19\">**</bpt>part-r-00000<ept id=\"p19\">**</ept>.",
          "pos": [
            0,
            160
          ]
        },
        {
          "content": "The <bpt id=\"p20\">**</bpt>part-r-00000<ept id=\"p20\">**</ept><ph id=\"ph14\"/> file contains the output for this job.",
          "pos": [
            161,
            275
          ]
        }
      ]
    },
    {
      "pos": [
        4038,
        4196
      ],
      "content": "<ph id=\"ph15\">[AZURE.NOTE]</ph><ph id=\"ph16\"/> Some MapReduce jobs may split the results across multiple <bpt id=\"p21\">**</bpt>part-r-#####<ept id=\"p21\">**</ept><ph id=\"ph17\"/> files. If so, use the ##### suffix to indicate the order of the files.",
      "nodes": [
        {
          "content": "<ph id=\"ph15\">[AZURE.NOTE]</ph><ph id=\"ph16\"/> Some MapReduce jobs may split the results across multiple <bpt id=\"p21\">**</bpt>part-r-#####<ept id=\"p21\">**</ept><ph id=\"ph17\"/> files.",
          "pos": [
            0,
            183
          ]
        },
        {
          "content": "If so, use the ##### suffix to indicate the order of the files.",
          "pos": [
            184,
            247
          ]
        }
      ]
    },
    {
      "pos": [
        4201,
        4247
      ],
      "content": "To view the output, use the following command:"
    },
    {
      "pos": [
        4326,
        4555
      ],
      "content": "This displays a list of the words that are contained in the <bpt id=\"p22\">**</bpt>wasb://example/data/gutenberg/davinci.txt<ept id=\"p22\">**</ept><ph id=\"ph18\"/> file and the number of times each word occured. The following is an example of the data that will be contained in the file:",
      "nodes": [
        {
          "content": "This displays a list of the words that are contained in the <bpt id=\"p22\">**</bpt>wasb://example/data/gutenberg/davinci.txt<ept id=\"p22\">**</ept><ph id=\"ph18\"/> file and the number of times each word occured.",
          "pos": [
            0,
            208
          ]
        },
        {
          "content": "The following is an example of the data that will be contained in the file:",
          "pos": [
            209,
            284
          ]
        }
      ]
    },
    {
      "pos": [
        4740,
        4742
      ],
      "content": "##"
    },
    {
      "pos": [
        4762,
        4769
      ],
      "content": "Summary"
    },
    {
      "pos": [
        4771,
        4898
      ],
      "content": "As you can see, Hadoop commands provide an easy way to run MapReduce jobs in an HDInsight cluster and then view the job output."
    },
    {
      "pos": [
        4900,
        4902
      ],
      "content": "##"
    },
    {
      "pos": [
        4924,
        4934
      ],
      "content": "Next steps"
    },
    {
      "pos": [
        4936,
        4994
      ],
      "content": "For general information about MapReduce jobs in HDInsight:"
    },
    {
      "pos": [
        4998,
        5061
      ],
      "content": "<bpt id=\"p23\">[</bpt>Use MapReduce on HDInsight Hadoop<ept id=\"p23\">](hdinsight-use-mapreduce.md)</ept>"
    },
    {
      "pos": [
        5063,
        5134
      ],
      "content": "For information about other ways you can work with Hadoop on HDInsight:"
    },
    {
      "pos": [
        5138,
        5196
      ],
      "content": "<bpt id=\"p24\">[</bpt>Use Hive with Hadoop on HDInsight<ept id=\"p24\">](hdinsight-use-hive.md)</ept>"
    },
    {
      "pos": [
        5200,
        5256
      ],
      "content": "<bpt id=\"p25\">[</bpt>Use Pig with Hadoop on HDInsight<ept id=\"p25\">](hdinsight-use-pig.md)</ept>"
    }
  ],
  "content": "<properties\n   pageTitle=\"MapReduce and SSH connection with Hadoop in HDInsight | Microsoft Azure\"\n   description=\"Learn how to use SSH to run MapReduce jobs using Hadoop on HDInsight.\"\n   services=\"hdinsight\"\n   documentationCenter=\"\"\n   authors=\"Blackmist\"\n   manager=\"paulettm\"\n   editor=\"cgronlun\"\n   tags=\"azure-portal\"/>\n\n<tags\n   ms.service=\"hdinsight\"\n   ms.devlang=\"na\"\n   ms.topic=\"article\"\n   ms.tgt_pltfrm=\"na\"\n   ms.workload=\"big-data\"\n   ms.date=\"01/08/2016\"\n   ms.author=\"larryfr\"/>\n\n# Use MapReduce with Hadoop on HDInsight with SSH\n\n[AZURE.INCLUDE [mapreduce-selector](../../includes/hdinsight-selector-use-mapreduce.md)]\n\nIn this article, you will learn how to use Secure Shell (SSH) to connect to a Hadoop on HDInsight cluster and then submit MapReduce jobs by using Hadoop commands.\n\n> [AZURE.NOTE] If you are already familiar with using Linux-based Hadoop servers, but you are new to HDInsight, see [Linux-based HDInsight tips](hdinsight-hadoop-linux-information.md).\n\n##<a id=\"prereq\"></a>Prerequisites\n\nTo complete the steps in this article, you will need the following:\n\n* A Linux-based HDInsight (Hadoop on HDInsight) cluster\n\n* An SSH client. Linux, Unix, and Mac operating systems should come with an SSH client. Windows users must download a client, such as [PuTTY](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html).\n\n##<a id=\"ssh\"></a>Connect with SSH\n\nConnect to the fully qualified domain name (FQDN) of your HDInsight cluster by using the SSH command. The FQDN will be the name you gave the cluster, followed by **.azurehdinsight.net**. For example, the following would connect to a cluster named **myhdinsight**:\n\n    ssh admin@myhdinsight-ssh.azurehdinsight.net\n\n**If you provided a certificate key for SSH authentication** when you created the HDInsight cluster, you may need to specify the location of the private key on your client system, for example:\n\n    ssh admin@myhdinsight-ssh.azurehdinsight.net -i ~/mykey.key\n\n**If you provided a password for SSH authentication** when you created the HDInsight cluster, you will need to provide the password when prompted.\n\nFor more information on using SSH with HDInsight, see [Use SSH with Linux-based Hadoop on HDInsight from Linux, OS X, and Unix](hdinsight-hadoop-linux-use-ssh-unix.md).\n\n###PuTTY (Windows clients)\n\nWindows does not provide a built-in SSH client. We recommend using **PuTTY**, which can be downloaded from [http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html).\n\nFor more information on using PuTTY, see [Use SSH with Linux-based Hadoop on HDInsight from Windows ](hdinsight-hadoop-linux-use-ssh-windows.md).\n\n##<a id=\"hadoop\"></a>Use Hadoop commands\n\n1. After you are connected to the HDInsight cluster, use the following **Hadoop** command to start a MapReduce job:\n\n        hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar wordcount wasb:///example/data/gutenberg/davinci.txt wasb:///example/data/WordCountOutput\n\n    This starts the **wordcount** class, which is contained in the **hadoop-mapreduce-examples.jar** file. As input, it uses the **wasb://example/data/gutenberg/davinci.txt** document, and output is stored at **wasb:///example/data/WordCountOutput**.\n\n    > [AZURE.NOTE] For more information about this MapReduce job and the example data, see [Use MapReduce in Hadoop on HDInsight](hdinsight-use-mapreduce.md).\n\n2. The job emits details as it processes, and it returns information similar to the following when the job completes:\n\n        File Input Format Counters\n        Bytes Read=1395666\n        File Output Format Counters\n        Bytes Written=337623\n\n3. When the job completes, use the following command to list the output files that are stored at **wasb://example/data/WordCountOutput**:\n\n        hdfs dfs -ls wasb:///example/data/WordCountOutput\n\n    This should display two files, **_SUCCESS** and **part-r-00000**. The **part-r-00000** file contains the output for this job.\n\n    > [AZURE.NOTE] Some MapReduce jobs may split the results across multiple **part-r-#####** files. If so, use the ##### suffix to indicate the order of the files.\n\n4. To view the output, use the following command:\n\n        hdfs dfs -cat wasb:///example/data/WordCountOutput/part-r-00000\n\n    This displays a list of the words that are contained in the **wasb://example/data/gutenberg/davinci.txt** file and the number of times each word occured. The following is an example of the data that will be contained in the file:\n\n        wreathed        3\n        wreathing       1\n        wreaths         1\n        wrecked         3\n        wrenching       1\n        wretched        6\n        wriggling       1\n\n##<a id=\"summary\"></a>Summary\n\nAs you can see, Hadoop commands provide an easy way to run MapReduce jobs in an HDInsight cluster and then view the job output.\n\n##<a id=\"nextsteps\"></a>Next steps\n\nFor general information about MapReduce jobs in HDInsight:\n\n* [Use MapReduce on HDInsight Hadoop](hdinsight-use-mapreduce.md)\n\nFor information about other ways you can work with Hadoop on HDInsight:\n\n* [Use Hive with Hadoop on HDInsight](hdinsight-use-hive.md)\n\n* [Use Pig with Hadoop on HDInsight](hdinsight-use-pig.md)\n"
}