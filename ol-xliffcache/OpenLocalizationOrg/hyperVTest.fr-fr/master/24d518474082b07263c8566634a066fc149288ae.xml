{
  "nodes": [
    {
      "pos": [
        27,
        89
      ],
      "content": "Availability of Hadoop clusters in HDInsight | Microsoft Azure"
    },
    {
      "pos": [
        108,
        193
      ],
      "content": "HDInsight deploys highly available and reliable clusters with an addtional head node."
    },
    {
      "pos": [
        523,
        583
      ],
      "content": "Availability and reliability of Hadoop clusters in HDInsight"
    },
    {
      "pos": [
        586,
        960
      ],
      "content": "HDInsight allows customers to deploy a variety of cluster types, for different data analytics workloads. Cluster types offered today are Hadoop clusters for query and analysis workloads, HBase clusters for NoSQL workloads, and Storm clusters for real time event processing workloads. Within a given cluster type, there are different roles for the various nodes. For example:",
      "nodes": [
        {
          "content": "HDInsight allows customers to deploy a variety of cluster types, for different data analytics workloads.",
          "pos": [
            0,
            104
          ]
        },
        {
          "content": "Cluster types offered today are Hadoop clusters for query and analysis workloads, HBase clusters for NoSQL workloads, and Storm clusters for real time event processing workloads.",
          "pos": [
            105,
            283
          ]
        },
        {
          "content": "Within a given cluster type, there are different roles for the various nodes.",
          "pos": [
            284,
            361
          ]
        },
        {
          "content": "For example:",
          "pos": [
            362,
            374
          ]
        }
      ]
    },
    {
      "pos": [
        966,
        1024
      ],
      "content": "Hadoop clusters for HDInsight are deployed with two roles:"
    },
    {
      "pos": [
        1031,
        1050
      ],
      "content": "Head node (2 nodes)"
    },
    {
      "pos": [
        1057,
        1084
      ],
      "content": "Data node (at least 1 node)"
    },
    {
      "pos": [
        1088,
        1147
      ],
      "content": "HBase clusters for HDInsight are deployed with three roles:"
    },
    {
      "pos": [
        1154,
        1176
      ],
      "content": "Head servers (2 nodes)"
    },
    {
      "pos": [
        1183,
        1215
      ],
      "content": "Region servers (at least 1 node)"
    },
    {
      "pos": [
        1222,
        1254
      ],
      "content": "Master/Zookeeper nodes (3 nodes)"
    },
    {
      "pos": [
        1258,
        1317
      ],
      "content": "Storm clusters for HDInsight are deployed with three roles:"
    },
    {
      "pos": [
        1324,
        1346
      ],
      "content": "Nimbus nodes (2 nodes)"
    },
    {
      "pos": [
        1353,
        1389
      ],
      "content": "Supervisor servers (at least 1 node)"
    },
    {
      "pos": [
        1396,
        1421
      ],
      "content": "Zookeeper nodes (3 nodes)"
    },
    {
      "pos": [
        1423,
        1911
      ],
      "content": "Standard implementations of Hadoop clusters typically have a single head node. HDInsight removes this single point of failure with the addition of a secondary head node /head server/Nimbus node to increase the availability and reliability of the service needed to manage workloads. These head  nodes/head servers/Nimbus nodes are designed to manage the failure of worker nodes smoothly, but any outages of master services running on the head node would cause the cluster to cease to work.",
      "nodes": [
        {
          "content": "Standard implementations of Hadoop clusters typically have a single head node.",
          "pos": [
            0,
            78
          ]
        },
        {
          "content": "HDInsight removes this single point of failure with the addition of a secondary head node /head server/Nimbus node to increase the availability and reliability of the service needed to manage workloads.",
          "pos": [
            79,
            281
          ]
        },
        {
          "content": "These head  nodes/head servers/Nimbus nodes are designed to manage the failure of worker nodes smoothly, but any outages of master services running on the head node would cause the cluster to cease to work.",
          "pos": [
            282,
            488
          ]
        }
      ]
    },
    {
      "pos": [
        1914,
        2202
      ],
      "content": "<bpt id=\"p1\">[</bpt>ZooKeeper<ept id=\"p1\">](http://zookeeper.apache.org/ )</ept><ph id=\"ph2\"/> nodes (ZKs) have been added and are used for leader election of head nodes and to insure that worker nodes and gateways (GWs) know when to fail over to the secondary head node (Head Node1) when the active head node (Head Node0) becomes inactive."
    },
    {
      "pos": [
        2204,
        2375
      ],
      "content": "<ph id=\"ph3\">![</ph>Diagram of the highly reliable head nodes in the HDInsight Hadoop implementation.<ph id=\"ph4\">](./media/hdinsight-high-availability/hadoop.high.availability.architecture.diagram.png)</ph>"
    },
    {
      "pos": [
        2383,
        2420
      ],
      "content": "Check active head node service status"
    },
    {
      "pos": [
        2421,
        3180
      ],
      "content": "To determine which head node is active and to check on the status of the services running on that head node, you must connect to the Hadoop cluster by using the Remote Desktop Protocol (RDP). For the RDP instructions, see <bpt id=\"p2\">[</bpt>Manage Hadoop clusters in HDInsight by using the Azure Portal<ept id=\"p2\">](hdinsight-administer-use-management-portal.md#connect-to-hdinsight-clusters-by-using-rdp)</ept>. Once you have remoted into the cluster, double-click on the **Hadoop Service Available ** icon located on the desktop to obtain status about which head node the Namenode, Jobtracker, Templeton, Oozieservice, Metastore, and Hiveserver2 services are running, or for HDI 3.0, the Namenode, Resource Manager, History Server, Templeton, Oozieservice, Metastore, and Hiveserver2 services.",
      "nodes": [
        {
          "content": "To determine which head node is active and to check on the status of the services running on that head node, you must connect to the Hadoop cluster by using the Remote Desktop Protocol (RDP).",
          "pos": [
            0,
            191
          ]
        },
        {
          "content": "For the RDP instructions, see <bpt id=\"p2\">[</bpt>Manage Hadoop clusters in HDInsight by using the Azure Portal<ept id=\"p2\">](hdinsight-administer-use-management-portal.md#connect-to-hdinsight-clusters-by-using-rdp)</ept>.",
          "pos": [
            192,
            414
          ]
        },
        {
          "content": "Once you have remoted into the cluster, double-click on the **Hadoop Service Available ** icon located on the desktop to obtain status about which head node the Namenode, Jobtracker, Templeton, Oozieservice, Metastore, and Hiveserver2 services are running, or for HDI 3.0, the Namenode, Resource Manager, History Server, Templeton, Oozieservice, Metastore, and Hiveserver2 services.",
          "pos": [
            415,
            797
          ]
        }
      ]
    },
    {
      "pos": [
        3263,
        3318
      ],
      "content": "On the screenshot, the active head node is <bpt id=\"p3\">*</bpt>headnode0<ept id=\"p3\">*</ept>."
    },
    {
      "pos": [
        3323,
        3366
      ],
      "content": "Access log files on the secondary head node"
    },
    {
      "pos": [
        3368,
        3874
      ],
      "content": "To access job logs on the secondary head node in the event that it has become the active head node, browsing the JobTracker UI still works as it does for the primary active node. To access JobTracker, you must connect to the Hadoop cluster by using RDP as described in the previous section. Once you have remoted into the cluster, double-click on the <bpt id=\"p4\">**</bpt>Hadoop Name Node Status<ept id=\"p4\">**</ept><ph id=\"ph6\"/> icon located on the desktop and then click on the <bpt id=\"p5\">**</bpt>NameNode logs<ept id=\"p5\">**</ept><ph id=\"ph7\"/> to get to the directory of logs on the secondary head node.",
      "nodes": [
        {
          "content": "To access job logs on the secondary head node in the event that it has become the active head node, browsing the JobTracker UI still works as it does for the primary active node.",
          "pos": [
            0,
            178
          ]
        },
        {
          "content": "To access JobTracker, you must connect to the Hadoop cluster by using RDP as described in the previous section.",
          "pos": [
            179,
            290
          ]
        },
        {
          "content": "Once you have remoted into the cluster, double-click on the <bpt id=\"p4\">**</bpt>Hadoop Name Node Status<ept id=\"p4\">**</ept><ph id=\"ph6\"/> icon located on the desktop and then click on the <bpt id=\"p5\">**</bpt>NameNode logs<ept id=\"p5\">**</ept><ph id=\"ph7\"/> to get to the directory of logs on the secondary head node.",
          "pos": [
            291,
            610
          ]
        }
      ]
    },
    {
      "pos": [
        3953,
        3977
      ],
      "content": "Configure head node size"
    },
    {
      "pos": [
        3978,
        4290
      ],
      "content": "The head nodes are allocated as large virtual machines (VMs) by default. This size is adequate for the management of most Hadoop jobs run on the cluster. But there are scenarios that may require extra-large VMs for the head nodes. One example is when the cluster has to manage a large number of small Oozie jobs.",
      "nodes": [
        {
          "content": "The head nodes are allocated as large virtual machines (VMs) by default.",
          "pos": [
            0,
            72
          ]
        },
        {
          "content": "This size is adequate for the management of most Hadoop jobs run on the cluster.",
          "pos": [
            73,
            153
          ]
        },
        {
          "content": "But there are scenarios that may require extra-large VMs for the head nodes.",
          "pos": [
            154,
            230
          ]
        },
        {
          "content": "One example is when the cluster has to manage a large number of small Oozie jobs.",
          "pos": [
            231,
            312
          ]
        }
      ]
    },
    {
      "pos": [
        4292,
        4388
      ],
      "content": "Extra-large VMs can be configured by using either Azure PowerShell cmdlets or the HDInsight SDK."
    },
    {
      "pos": [
        4390,
        4733
      ],
      "content": "The creation and provisioning of a cluster by using Azure PowerShell is documented in <bpt id=\"p6\">[</bpt>Administer HDInsight using PowerShell<ept id=\"p6\">](hdinsight-administer-use-powershell.md)</ept>. The configuration of an extra-large head node requires the addition of the <ph id=\"ph9\">`-HeadNodeVMSize ExtraLarge`</ph><ph id=\"ph10\"/> parameter to the <ph id=\"ph11\">`New-AzureRmHDInsightcluster`</ph><ph id=\"ph12\"/> cmdlet used in this code.",
      "nodes": [
        {
          "content": "The creation and provisioning of a cluster by using Azure PowerShell is documented in <bpt id=\"p6\">[</bpt>Administer HDInsight using PowerShell<ept id=\"p6\">](hdinsight-administer-use-powershell.md)</ept>.",
          "pos": [
            0,
            204
          ]
        },
        {
          "content": "The configuration of an extra-large head node requires the addition of the <ph id=\"ph9\">`-HeadNodeVMSize ExtraLarge`</ph><ph id=\"ph10\"/> parameter to the <ph id=\"ph11\">`New-AzureRmHDInsightcluster`</ph><ph id=\"ph12\"/> cmdlet used in this code.",
          "pos": [
            205,
            448
          ]
        }
      ]
    },
    {
      "pos": [
        5325,
        5686
      ],
      "content": "For the SDK, the story is similar. The creation and provisioning of a cluster by using the SDK is documented in <bpt id=\"p7\">[</bpt>Using HDInsight .NET SDK<ept id=\"p7\">](hdinsight-provision-clusters.md#sdk)</ept>. The configuration of an extra-large head node requires the addition of the <ph id=\"ph13\">`HeadNodeSize = NodeVMSize.ExtraLarge`</ph><ph id=\"ph14\"/> parameter to the <ph id=\"ph15\">`ClusterCreateParameters()`</ph><ph id=\"ph16\"/> method used in this code.",
      "nodes": [
        {
          "content": "For the SDK, the story is similar.",
          "pos": [
            0,
            34
          ]
        },
        {
          "content": "The creation and provisioning of a cluster by using the SDK is documented in <bpt id=\"p7\">[</bpt>Using HDInsight .NET SDK<ept id=\"p7\">](hdinsight-provision-clusters.md#sdk)</ept>.",
          "pos": [
            35,
            214
          ]
        },
        {
          "content": "The configuration of an extra-large head node requires the addition of the <ph id=\"ph13\">`HeadNodeSize = NodeVMSize.ExtraLarge`</ph><ph id=\"ph14\"/> parameter to the <ph id=\"ph15\">`ClusterCreateParameters()`</ph><ph id=\"ph16\"/> method used in this code.",
          "pos": [
            215,
            467
          ]
        }
      ]
    },
    {
      "pos": [
        6248,
        6258
      ],
      "content": "Next Steps"
    },
    {
      "pos": [
        6262,
        6311
      ],
      "content": "<bpt id=\"p8\">[</bpt>Apache ZooKeeper<ept id=\"p8\">](http://zookeeper.apache.org/ )</ept>"
    },
    {
      "pos": [
        6314,
        6406
      ],
      "content": "<bpt id=\"p9\">[</bpt>Connect to HDInsight clusters using RDP<ept id=\"p9\">](hdinsight-administer-use-management-portal.md#rdp)</ept>"
    },
    {
      "pos": [
        6409,
        6472
      ],
      "content": "<bpt id=\"p10\">[</bpt>Using HDInsight .NET SDK<ept id=\"p10\">](hdinsight-provision-clusters.md#sdk)</ept>"
    }
  ],
  "content": "<properties\n    pageTitle=\"Availability of Hadoop clusters in HDInsight | Microsoft Azure\"\n    description=\"HDInsight deploys highly available and reliable clusters with an addtional head node.\"\n    services=\"hdinsight\"\n    tags=\"azure-portal\"\n    editor=\"cgronlun\"\n    manager=\"paulettm\"\n    authors=\"mumian\"\n    documentationCenter=\"\"/>\n\n<tags\n    ms.service=\"hdinsight\"\n    ms.workload=\"big-data\"\n    ms.tgt_pltfrm=\"na\"\n    ms.devlang=\"multiple\"\n    ms.topic=\"article\"\n    ms.date=\"02/04/2016\"\n    ms.author=\"jgao\"/>\n\n\n#Availability and reliability of Hadoop clusters in HDInsight\n\n\nHDInsight allows customers to deploy a variety of cluster types, for different data analytics workloads. Cluster types offered today are Hadoop clusters for query and analysis workloads, HBase clusters for NoSQL workloads, and Storm clusters for real time event processing workloads. Within a given cluster type, there are different roles for the various nodes. For example:\n\n\n\n- Hadoop clusters for HDInsight are deployed with two roles:\n    - Head node (2 nodes)\n    - Data node (at least 1 node)\n\n- HBase clusters for HDInsight are deployed with three roles:\n    - Head servers (2 nodes)\n    - Region servers (at least 1 node)\n    - Master/Zookeeper nodes (3 nodes)\n\n- Storm clusters for HDInsight are deployed with three roles:\n    - Nimbus nodes (2 nodes)\n    - Supervisor servers (at least 1 node)\n    - Zookeeper nodes (3 nodes)\n\nStandard implementations of Hadoop clusters typically have a single head node. HDInsight removes this single point of failure with the addition of a secondary head node /head server/Nimbus node to increase the availability and reliability of the service needed to manage workloads. These head  nodes/head servers/Nimbus nodes are designed to manage the failure of worker nodes smoothly, but any outages of master services running on the head node would cause the cluster to cease to work.\n\n\n[ZooKeeper](http://zookeeper.apache.org/ ) nodes (ZKs) have been added and are used for leader election of head nodes and to insure that worker nodes and gateways (GWs) know when to fail over to the secondary head node (Head Node1) when the active head node (Head Node0) becomes inactive.\n\n![Diagram of the highly reliable head nodes in the HDInsight Hadoop implementation.](./media/hdinsight-high-availability/hadoop.high.availability.architecture.diagram.png)\n\n\n\n\n## Check active head node service status\nTo determine which head node is active and to check on the status of the services running on that head node, you must connect to the Hadoop cluster by using the Remote Desktop Protocol (RDP). For the RDP instructions, see [Manage Hadoop clusters in HDInsight by using the Azure Portal](hdinsight-administer-use-management-portal.md#connect-to-hdinsight-clusters-by-using-rdp). Once you have remoted into the cluster, double-click on the **Hadoop Service Available ** icon located on the desktop to obtain status about which head node the Namenode, Jobtracker, Templeton, Oozieservice, Metastore, and Hiveserver2 services are running, or for HDI 3.0, the Namenode, Resource Manager, History Server, Templeton, Oozieservice, Metastore, and Hiveserver2 services.\n\n![](./media/hdinsight-high-availability/Hadoop.Service.Availability.Status.png)\n\nOn the screenshot, the active head node is *headnode0*.\n\n## Access log files on the secondary head node\n\nTo access job logs on the secondary head node in the event that it has become the active head node, browsing the JobTracker UI still works as it does for the primary active node. To access JobTracker, you must connect to the Hadoop cluster by using RDP as described in the previous section. Once you have remoted into the cluster, double-click on the **Hadoop Name Node Status** icon located on the desktop and then click on the **NameNode logs** to get to the directory of logs on the secondary head node.\n\n![](./media/hdinsight-high-availability/Hadoop.Head.Node.Log.Files.png)\n\n\n## Configure head node size\nThe head nodes are allocated as large virtual machines (VMs) by default. This size is adequate for the management of most Hadoop jobs run on the cluster. But there are scenarios that may require extra-large VMs for the head nodes. One example is when the cluster has to manage a large number of small Oozie jobs.\n\nExtra-large VMs can be configured by using either Azure PowerShell cmdlets or the HDInsight SDK.\n\nThe creation and provisioning of a cluster by using Azure PowerShell is documented in [Administer HDInsight using PowerShell](hdinsight-administer-use-powershell.md). The configuration of an extra-large head node requires the addition of the `-HeadNodeVMSize ExtraLarge` parameter to the `New-AzureRmHDInsightcluster` cmdlet used in this code.\n\n    # Create a new HDInsight cluster in Azure PowerShell\n    # Configured with an ExtraLarge head-node VM\n    New-AzureRmHDInsightCluster `\n                -ResourceGroupName $resourceGroupName `\n                -ClusterName $clusterName ` \n                -Location $location `\n                -HeadNodeVMSize ExtraLarge `\n                -DefaultStorageAccountName \"$storageAccountName.blob.core.windows.net\" `\n                -DefaultStorageAccountKey $storageAccountKey `\n                -DefaultStorageContainerName $containerName  `\n                -ClusterSizeInNodes $clusterNodes\n\nFor the SDK, the story is similar. The creation and provisioning of a cluster by using the SDK is documented in [Using HDInsight .NET SDK](hdinsight-provision-clusters.md#sdk). The configuration of an extra-large head node requires the addition of the `HeadNodeSize = NodeVMSize.ExtraLarge` parameter to the `ClusterCreateParameters()` method used in this code.\n\n    # Create a new HDInsight cluster with the HDInsight SDK\n    # Configured with an ExtraLarge head-node VM\n    ClusterCreateParameters clusterInfo = new ClusterCreateParameters()\n    {\n        Name = clustername,\n        Location = location,\n        HeadNodeSize = NodeVMSize.ExtraLarge,\n        DefaultStorageAccountName = storageaccountname,\n        DefaultStorageAccountKey = storageaccountkey,\n        DefaultStorageContainer = containername,\n        UserName = username,\n        Password = password,\n        ClusterSizeInNodes = clustersize\n    };\n\n\n## Next Steps\n\n- [Apache ZooKeeper](http://zookeeper.apache.org/ )\n- [Connect to HDInsight clusters using RDP](hdinsight-administer-use-management-portal.md#rdp)\n- [Using HDInsight .NET SDK](hdinsight-provision-clusters.md#sdk)\n"
}