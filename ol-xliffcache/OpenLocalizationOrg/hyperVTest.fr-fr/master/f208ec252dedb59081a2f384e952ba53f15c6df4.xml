{
  "nodes": [
    {
      "pos": [
        26,
        95
      ],
      "content": "Use Beeline to work with Hive on HDInsight (Hadoop) | Microsoft Azure"
    },
    {
      "pos": [
        113,
        300
      ],
      "content": "Learn how to use SSH to connect to a Hadoop cluster in HDInsight, and then interactively submit Hive queries by using Beeline. Beeline is a utility for working with HiveServer2 over JDBC.",
      "nodes": [
        {
          "content": "Learn how to use SSH to connect to a Hadoop cluster in HDInsight, and then interactively submit Hive queries by using Beeline.",
          "pos": [
            0,
            126
          ]
        },
        {
          "content": "Beeline is a utility for working with HiveServer2 over JDBC.",
          "pos": [
            127,
            187
          ]
        }
      ]
    },
    {
      "pos": [
        617,
        663
      ],
      "content": "Use Hive with Hadoop in HDInsight with Beeline"
    },
    {
      "pos": [
        745,
        1055
      ],
      "content": "In this article, you will learn how to use Secure Shell (SSH) to connect to a Linux-based HDInsight cluster, and then interactively submit Hive queries by using the <bpt id=\"p1\">[</bpt>Beeline<ept id=\"p1\">](https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-Beeline–NewCommandLineShell)</ept><ph id=\"ph3\"/> command-line tool."
    },
    {
      "pos": [
        1059,
        1263
      ],
      "content": "<ph id=\"ph4\">[AZURE.NOTE]</ph><ph id=\"ph5\"/> Beeline used JDBC to connect to Hive. For more information on using JDBC with Hive, see <bpt id=\"p2\">[</bpt>Connect to Hive on Azure HDInsight using the Hive JDBC driver<ept id=\"p2\">](hdinsight-connect-hive-jdbc-driver.md)</ept>.",
      "nodes": [
        {
          "content": "<ph id=\"ph4\">[AZURE.NOTE]</ph><ph id=\"ph5\"/> Beeline used JDBC to connect to Hive.",
          "pos": [
            0,
            82
          ]
        },
        {
          "content": "For more information on using JDBC with Hive, see <bpt id=\"p2\">[</bpt>Connect to Hive on Azure HDInsight using the Hive JDBC driver<ept id=\"p2\">](hdinsight-connect-hive-jdbc-driver.md)</ept>.",
          "pos": [
            83,
            274
          ]
        }
      ]
    },
    {
      "pos": [
        1265,
        1267
      ],
      "content": "##"
    },
    {
      "pos": [
        1286,
        1299
      ],
      "content": "Prerequisites"
    },
    {
      "pos": [
        1301,
        1368
      ],
      "content": "To complete the steps in this article, you will need the following:"
    },
    {
      "pos": [
        1372,
        1414
      ],
      "content": "A Linux-based Hadoop on HDInsight cluster."
    },
    {
      "pos": [
        1418,
        1608
      ],
      "content": "An SSH client. Linux, Unix, and Mac OS should come with an SSH client. Windows users must download a client, such as <bpt id=\"p3\">[</bpt>PuTTY<ept id=\"p3\">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.",
      "nodes": [
        {
          "content": "An SSH client.",
          "pos": [
            0,
            14
          ]
        },
        {
          "content": "Linux, Unix, and Mac OS should come with an SSH client.",
          "pos": [
            15,
            70
          ]
        },
        {
          "content": "Windows users must download a client, such as <bpt id=\"p3\">[</bpt>PuTTY<ept id=\"p3\">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.",
          "pos": [
            71,
            228
          ]
        }
      ]
    },
    {
      "pos": [
        1610,
        1612
      ],
      "content": "##"
    },
    {
      "pos": [
        1628,
        1644
      ],
      "content": "Connect with SSH"
    },
    {
      "pos": [
        1646,
        1902
      ],
      "content": "Connect to the fully qualified domain name (FQDN) of your HDInsight cluster by using the SSH command. The FQDN will be the name you gave the cluster, then <bpt id=\"p4\">**</bpt>.azurehdinsight.net<ept id=\"p4\">**</ept>. For example, the following would connect to a cluster named <bpt id=\"p5\">**</bpt>myhdinsight<ept id=\"p5\">**</ept>:",
      "nodes": [
        {
          "content": "Connect to the fully qualified domain name (FQDN) of your HDInsight cluster by using the SSH command.",
          "pos": [
            0,
            101
          ]
        },
        {
          "content": "The FQDN will be the name you gave the cluster, then <bpt id=\"p4\">**</bpt>.azurehdinsight.net<ept id=\"p4\">**</ept>.",
          "pos": [
            102,
            217
          ]
        },
        {
          "content": "For example, the following would connect to a cluster named <bpt id=\"p5\">**</bpt>myhdinsight<ept id=\"p5\">**</ept>:",
          "pos": [
            218,
            332
          ]
        }
      ]
    },
    {
      "pos": [
        1954,
        2133
      ],
      "content": "<bpt id=\"p6\">**</bpt>If you provided a certificate key for SSH authentication<ept id=\"p6\">**</ept><ph id=\"ph6\"/> when you created the HDInsight cluster, you may need to specify the location of the private key on your client system:"
    },
    {
      "pos": [
        2200,
        2346
      ],
      "content": "<bpt id=\"p7\">**</bpt>If you provided a password for SSH authentication<ept id=\"p7\">**</ept><ph id=\"ph7\"/> when you created the HDInsight cluster, you will need to provide the password when prompted."
    },
    {
      "pos": [
        2348,
        2516
      ],
      "content": "For more information on using SSH with HDInsight, see <bpt id=\"p8\">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Linux, OS X, and Unix<ept id=\"p8\">](hdinsight-hadoop-linux-use-ssh-unix.md)</ept>."
    },
    {
      "pos": [
        2521,
        2550
      ],
      "content": "PuTTY (Windows-based clients)"
    },
    {
      "pos": [
        2552,
        2790
      ],
      "content": "Windows does not provide a built-in SSH client. We recommend using <bpt id=\"p9\">**</bpt>PuTTY<ept id=\"p9\">**</ept>, which can be downloaded from <bpt id=\"p10\">[</bpt>http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html<ept id=\"p10\">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.",
      "nodes": [
        {
          "content": "Windows does not provide a built-in SSH client.",
          "pos": [
            0,
            47
          ]
        },
        {
          "content": "We recommend using <bpt id=\"p9\">**</bpt>PuTTY<ept id=\"p9\">**</ept>, which can be downloaded from <bpt id=\"p10\">[</bpt>http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html<ept id=\"p10\">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.",
          "pos": [
            48,
            316
          ]
        }
      ]
    },
    {
      "pos": [
        2792,
        2937
      ],
      "content": "For more information on using PuTTY, see <bpt id=\"p11\">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Windows <ept id=\"p11\">](hdinsight-hadoop-linux-use-ssh-windows.md)</ept>."
    },
    {
      "pos": [
        2939,
        2941
      ],
      "content": "##"
    },
    {
      "pos": [
        2961,
        2984
      ],
      "content": "Use the Beeline command"
    },
    {
      "pos": [
        2989,
        3060
      ],
      "content": "Once connected, use the following to get the hostname of the head node:"
    },
    {
      "pos": [
        3091,
        3189
      ],
      "content": "Save the returned host name, as it will be used later when connecting to HiveServer2 from Beeline."
    },
    {
      "pos": [
        3198,
        3248
      ],
      "content": "Start the Hive CLI by using the following command:"
    },
    {
      "pos": [
        3270,
        3429
      ],
      "content": "From the <ph id=\"ph8\">`beeline&gt;`</ph><ph id=\"ph9\"/> prompt, use the following to connect to the HiveServer2 service. Replace <bpt id=\"p12\">__</bpt>HOSTNAME<ept id=\"p12\">__</ept><ph id=\"ph10\"/> with the host name returned for the head node ealier:",
      "nodes": [
        {
          "content": "From the <ph id=\"ph8\">`beeline&gt;`</ph><ph id=\"ph9\"/> prompt, use the following to connect to the HiveServer2 service.",
          "pos": [
            0,
            119
          ]
        },
        {
          "content": "Replace <bpt id=\"p12\">__</bpt>HOSTNAME<ept id=\"p12\">__</ept><ph id=\"ph10\"/> with the host name returned for the head node ealier:",
          "pos": [
            120,
            249
          ]
        }
      ]
    },
    {
      "pos": [
        3507,
        3683
      ],
      "content": "When prompted, enter the password for the administrator (admin) account for your HDInsight cluster. Once the connection is established, the prompt will change to the following:",
      "nodes": [
        {
          "content": "When prompted, enter the password for the administrator (admin) account for your HDInsight cluster.",
          "pos": [
            0,
            99
          ]
        },
        {
          "content": "Once the connection is established, the prompt will change to the following:",
          "pos": [
            100,
            176
          ]
        }
      ]
    },
    {
      "pos": [
        3731,
        3892
      ],
      "content": "Beeline commands usually begin with a <ph id=\"ph11\">`!`</ph><ph id=\"ph12\"/> character, for example <ph id=\"ph13\">`!help`</ph><ph id=\"ph14\"/> displays help. However the <ph id=\"ph15\">`!`</ph><ph id=\"ph16\"/> can often be ommited. For example, <ph id=\"ph17\">`help`</ph><ph id=\"ph18\"/> will also work.",
      "nodes": [
        {
          "content": "Beeline commands usually begin with a <ph id=\"ph11\">`!`</ph><ph id=\"ph12\"/> character, for example <ph id=\"ph13\">`!help`</ph><ph id=\"ph14\"/> displays help.",
          "pos": [
            0,
            155
          ]
        },
        {
          "content": "However the <ph id=\"ph15\">`!`</ph><ph id=\"ph16\"/> can often be ommited.",
          "pos": [
            156,
            227
          ]
        },
        {
          "content": "For example, <ph id=\"ph17\">`help`</ph><ph id=\"ph18\"/> will also work.",
          "pos": [
            228,
            297
          ]
        }
      ]
    },
    {
      "pos": [
        3898,
        4177
      ],
      "content": "If you view help, you will notice <ph id=\"ph19\">`!sql`</ph>, which is used to execute HiveQL statements. However, HiveQL is so commonly used that you can ommit the preceeding <ph id=\"ph20\">`!sql`</ph>. The following two statements have exactly the same results; displaying the tables currently available through Hive:",
      "nodes": [
        {
          "content": "If you view help, you will notice <ph id=\"ph19\">`!sql`</ph>, which is used to execute HiveQL statements.",
          "pos": [
            0,
            104
          ]
        },
        {
          "content": "However, HiveQL is so commonly used that you can ommit the preceeding <ph id=\"ph20\">`!sql`</ph>.",
          "pos": [
            105,
            201
          ]
        },
        {
          "content": "The following two statements have exactly the same results; displaying the tables currently available through Hive:",
          "pos": [
            202,
            317
          ]
        }
      ]
    },
    {
      "pos": [
        4239,
        4310
      ],
      "content": "On a new cluster, only one table should be listed: <bpt id=\"p13\">__</bpt>hivesampletable<ept id=\"p13\">__</ept>."
    },
    {
      "pos": [
        4315,
        4379
      ],
      "content": "Use the following to display the schema for the hivesampletable:"
    },
    {
      "pos": [
        4428,
        4471
      ],
      "content": "This will return the following information:"
    },
    {
      "pos": [
        5361,
        5556
      ],
      "content": "This displays the columns in the table. While we could perform some queries against this data, let's instead create a brand new table to demonstrate how to load data into Hive and apply a schema.",
      "nodes": [
        {
          "content": "This displays the columns in the table.",
          "pos": [
            0,
            39
          ]
        },
        {
          "content": "While we could perform some queries against this data, let's instead create a brand new table to demonstrate how to load data into Hive and apply a schema.",
          "pos": [
            40,
            195
          ]
        }
      ]
    },
    {
      "pos": [
        5565,
        5695
      ],
      "content": "Enter the following statements to create a new table named <bpt id=\"p14\">**</bpt>log4jLogs<ept id=\"p14\">**</ept><ph id=\"ph21\"/> by using sample data provided with the HDInsight cluster:"
    },
    {
      "pos": [
        6093,
        6140
      ],
      "content": "These statements perform the following actions:"
    },
    {
      "pos": [
        6148,
        6235
      ],
      "content": "<bpt id=\"p15\">**</bpt>DROP TABLE<ept id=\"p15\">**</ept><ph id=\"ph22\"/> - Deletes the table and the data file, in case the table already exists."
    },
    {
      "pos": [
        6242,
        6409
      ],
      "content": "<bpt id=\"p16\">**</bpt>CREATE EXTERNAL TABLE<ept id=\"p16\">**</ept><ph id=\"ph23\"/> - Creates a new 'external' table in Hive. External tables only store the table definition in Hive. The data is left in the original location.",
      "nodes": [
        {
          "content": "<bpt id=\"p16\">**</bpt>CREATE EXTERNAL TABLE<ept id=\"p16\">**</ept><ph id=\"ph23\"/> - Creates a new 'external' table in Hive.",
          "pos": [
            0,
            122
          ]
        },
        {
          "content": "External tables only store the table definition in Hive.",
          "pos": [
            123,
            179
          ]
        },
        {
          "content": "The data is left in the original location.",
          "pos": [
            180,
            222
          ]
        }
      ]
    },
    {
      "pos": [
        6416,
        6533
      ],
      "content": "<bpt id=\"p17\">**</bpt>ROW FORMAT<ept id=\"p17\">**</ept><ph id=\"ph24\"/> - Tells Hive how the data is formatted. In this case, the fields in each log are separated by a space.",
      "nodes": [
        {
          "content": "<bpt id=\"p17\">**</bpt>ROW FORMAT<ept id=\"p17\">**</ept><ph id=\"ph24\"/> - Tells Hive how the data is formatted.",
          "pos": [
            0,
            109
          ]
        },
        {
          "content": "In this case, the fields in each log are separated by a space.",
          "pos": [
            110,
            172
          ]
        }
      ]
    },
    {
      "pos": [
        6540,
        6670
      ],
      "content": "<bpt id=\"p18\">**</bpt>STORED AS TEXTFILE LOCATION<ept id=\"p18\">**</ept><ph id=\"ph25\"/> - Tells Hive where the data is stored (the example/data directory), and that it is stored as text."
    },
    {
      "pos": [
        6677,
        6854
      ],
      "content": "<bpt id=\"p19\">**</bpt>SELECT<ept id=\"p19\">**</ept><ph id=\"ph26\"/> - Selects a count of all rows where column <bpt id=\"p20\">**</bpt>t4<ept id=\"p20\">**</ept><ph id=\"ph27\"/> contains the value <bpt id=\"p21\">**</bpt>[ERROR]<ept id=\"p21\">**</ept>. This should return a value of <bpt id=\"p22\">**</bpt>3<ept id=\"p22\">**</ept><ph id=\"ph28\"/> as there are three rows that contain this value.",
      "nodes": [
        {
          "content": "<bpt id=\"p19\">**</bpt>SELECT<ept id=\"p19\">**</ept><ph id=\"ph26\"/> - Selects a count of all rows where column <bpt id=\"p20\">**</bpt>t4<ept id=\"p20\">**</ept><ph id=\"ph27\"/> contains the value <bpt id=\"p21\">**</bpt>[ERROR]<ept id=\"p21\">**</ept>.",
          "pos": [
            0,
            242
          ]
        },
        {
          "content": "This should return a value of <bpt id=\"p22\">**</bpt>3<ept id=\"p22\">**</ept><ph id=\"ph28\"/> as there are three rows that contain this value.",
          "pos": [
            243,
            382
          ]
        }
      ]
    },
    {
      "pos": [
        6861,
        7136
      ],
      "content": "<bpt id=\"p23\">**</bpt>INPUT__FILE__NAME LIKE '%.log'<ept id=\"p23\">**</ept><ph id=\"ph29\"/> - Tells Hive that we should only return data from files ending in .log. Normally, you would only have data with the same schema within the same folder when querying with hive, however this example log file is stored with other data formats.",
      "nodes": [
        {
          "content": "<bpt id=\"p23\">**</bpt>INPUT__FILE__NAME LIKE '%.log'<ept id=\"p23\">**</ept><ph id=\"ph29\"/> - Tells Hive that we should only return data from files ending in .log.",
          "pos": [
            0,
            161
          ]
        },
        {
          "content": "Normally, you would only have data with the same schema within the same folder when querying with hive, however this example log file is stored with other data formats.",
          "pos": [
            162,
            330
          ]
        }
      ]
    },
    {
      "pos": [
        7144,
        7390
      ],
      "content": "<ph id=\"ph30\">[AZURE.NOTE]</ph><ph id=\"ph31\"/> External tables should be used when you expect the underlying data to be updated by an external source, such as an automated data upload process, or by another MapReduce operation, but always want Hive queries to use the latest data."
    },
    {
      "pos": [
        7403,
        7486
      ],
      "content": "Dropping an external table does <bpt id=\"p24\">**</bpt>not<ept id=\"p24\">**</ept><ph id=\"ph32\"/> delete the data, only the table definition."
    },
    {
      "pos": [
        7496,
        7558
      ],
      "content": "The output of this command should be similar to the following:"
    },
    {
      "pos": [
        8447,
        8476
      ],
      "content": "To exit Beeline, use <ph id=\"ph33\">`!quit`</ph>."
    },
    {
      "pos": [
        8478,
        8480
      ],
      "content": "##"
    },
    {
      "pos": [
        8497,
        8514
      ],
      "content": "Run a HiveQL file"
    },
    {
      "pos": [
        8516,
        8656
      ],
      "content": "Beeline can also be used to run a file that contains HiveQL statements. Use the following steps to create a file, then run it using Beeline.",
      "nodes": [
        {
          "content": "Beeline can also be used to run a file that contains HiveQL statements.",
          "pos": [
            0,
            71
          ]
        },
        {
          "content": "Use the following steps to create a file, then run it using Beeline.",
          "pos": [
            72,
            140
          ]
        }
      ]
    },
    {
      "pos": [
        8661,
        8728
      ],
      "content": "Use the following command to create a new file named <bpt id=\"p25\">__</bpt>query.hql<ept id=\"p25\">__</ept>:"
    },
    {
      "pos": [
        8765,
        8901
      ],
      "content": "Once the editor opens, use the following as the contents of the file. This query will create a new 'internal' table named <bpt id=\"p26\">**</bpt>errorLogs<ept id=\"p26\">**</ept>:",
      "nodes": [
        {
          "content": "Once the editor opens, use the following as the contents of the file.",
          "pos": [
            0,
            69
          ]
        },
        {
          "content": "This query will create a new 'internal' table named <bpt id=\"p26\">**</bpt>errorLogs<ept id=\"p26\">**</ept>:",
          "pos": [
            70,
            176
          ]
        }
      ]
    },
    {
      "pos": [
        9193,
        9240
      ],
      "content": "These statements perform the following actions:"
    },
    {
      "pos": [
        9248,
        9476
      ],
      "content": "<bpt id=\"p27\">**</bpt>CREATE TABLE IF NOT EXISTS<ept id=\"p27\">**</ept><ph id=\"ph34\"/> - Creates a table, if it does not already exist. Since the <bpt id=\"p28\">**</bpt>EXTERNAL<ept id=\"p28\">**</ept><ph id=\"ph35\"/> keyword is not used, this is an internal table, which is stored in the Hive data warehouse and is managed completely by Hive.",
      "nodes": [
        {
          "content": "<bpt id=\"p27\">**</bpt>CREATE TABLE IF NOT EXISTS<ept id=\"p27\">**</ept><ph id=\"ph34\"/> - Creates a table, if it does not already exist.",
          "pos": [
            0,
            134
          ]
        },
        {
          "content": "Since the <bpt id=\"p28\">**</bpt>EXTERNAL<ept id=\"p28\">**</ept><ph id=\"ph35\"/> keyword is not used, this is an internal table, which is stored in the Hive data warehouse and is managed completely by Hive.",
          "pos": [
            135,
            338
          ]
        }
      ]
    },
    {
      "pos": [
        9483,
        9629
      ],
      "content": "<bpt id=\"p29\">**</bpt>STORED AS ORC<ept id=\"p29\">**</ept><ph id=\"ph36\"/> - Stores the data in Optimized Row Columnar (ORC) format. This is a highly optimized and efficient format for storing Hive data.",
      "nodes": [
        {
          "content": "<bpt id=\"p29\">**</bpt>STORED AS ORC<ept id=\"p29\">**</ept><ph id=\"ph36\"/> - Stores the data in Optimized Row Columnar (ORC) format.",
          "pos": [
            0,
            130
          ]
        },
        {
          "content": "This is a highly optimized and efficient format for storing Hive data.",
          "pos": [
            131,
            201
          ]
        }
      ]
    },
    {
      "pos": [
        9636,
        9789
      ],
      "content": "<bpt id=\"p30\">**</bpt>INSERT OVERWRITE ... SELECT<ept id=\"p30\">**</ept><ph id=\"ph37\"/> - Selects rows from the <bpt id=\"p31\">**</bpt>log4jLogs<ept id=\"p31\">**</ept><ph id=\"ph38\"/> table that contain <bpt id=\"p32\">**</bpt>[ERROR]<ept id=\"p32\">**</ept>, then inserts the data into the <bpt id=\"p33\">**</bpt>errorLogs<ept id=\"p33\">**</ept><ph id=\"ph39\"/> table."
    },
    {
      "pos": [
        9801,
        9905
      ],
      "content": "<ph id=\"ph40\">[AZURE.NOTE]</ph><ph id=\"ph41\"/> Unlike external tables, dropping an internal table will delete the underlying data as well."
    },
    {
      "pos": [
        9914,
        9993
      ],
      "content": "To save the file, use <bpt id=\"p34\">__</bpt>Ctrl<ept id=\"p34\">__</ept>+_<bpt id=\"p35\">__</bpt>X<ept id=\"p35\">__</ept>, then enter <bpt id=\"p36\">__</bpt>Y<ept id=\"p36\">__</ept>, and finally <bpt id=\"p37\">__</bpt>Enter<ept id=\"p37\">__</ept>."
    },
    {
      "pos": [
        9998,
        10176
      ],
      "content": "Use the following to run the file using Beeline. Replease <bpt id=\"p38\">__</bpt>HOSTNAME<ept id=\"p38\">__</ept><ph id=\"ph42\"/> with the name obtained earlier for the head node, and <bpt id=\"p39\">__</bpt>PASSWORD<ept id=\"p39\">__</ept><ph id=\"ph43\"/> with the password for the admin account:",
      "nodes": [
        {
          "content": "Use the following to run the file using Beeline.",
          "pos": [
            0,
            48
          ]
        },
        {
          "content": "Replease <bpt id=\"p38\">__</bpt>HOSTNAME<ept id=\"p38\">__</ept><ph id=\"ph42\"/> with the name obtained earlier for the head node, and <bpt id=\"p39\">__</bpt>PASSWORD<ept id=\"p39\">__</ept><ph id=\"ph43\"/> with the password for the admin account:",
          "pos": [
            49,
            288
          ]
        }
      ]
    },
    {
      "pos": [
        10285,
        10454
      ],
      "content": "To verify that the <bpt id=\"p40\">**</bpt>errorLogs<ept id=\"p40\">**</ept><ph id=\"ph44\"/> table was created, start Beeline and connect to HiveServer2, then use the following statement to return all the rows from <bpt id=\"p41\">**</bpt>errorLogs<ept id=\"p41\">**</ept>:"
    },
    {
      "pos": [
        10494,
        10573
      ],
      "content": "Three rows of data should be returned, all containing <bpt id=\"p42\">**</bpt>[ERROR]<ept id=\"p42\">**</ept><ph id=\"ph45\"/> in column t4:"
    },
    {
      "pos": [
        11483,
        11485
      ],
      "content": "##"
    },
    {
      "pos": [
        11505,
        11512
      ],
      "content": "Summary"
    },
    {
      "pos": [
        11514,
        11629
      ],
      "content": "As you can see, the Beeline command provides an easy way to interactively run Hive queries on an HDInsight cluster."
    },
    {
      "pos": [
        11631,
        11633
      ],
      "content": "##"
    },
    {
      "pos": [
        11655,
        11665
      ],
      "content": "Next steps"
    },
    {
      "pos": [
        11667,
        11712
      ],
      "content": "For general information on Hive in HDInsight:"
    },
    {
      "pos": [
        11716,
        11774
      ],
      "content": "<bpt id=\"p43\">[</bpt>Use Hive with Hadoop on HDInsight<ept id=\"p43\">](hdinsight-use-hive.md)</ept>"
    },
    {
      "pos": [
        11776,
        11844
      ],
      "content": "For information on other ways you can work with Hadoop on HDInsight:"
    },
    {
      "pos": [
        11848,
        11904
      ],
      "content": "<bpt id=\"p44\">[</bpt>Use Pig with Hadoop on HDInsight<ept id=\"p44\">](hdinsight-use-pig.md)</ept>"
    },
    {
      "pos": [
        11908,
        11976
      ],
      "content": "<bpt id=\"p45\">[</bpt>Use MapReduce with Hadoop on HDInsight<ept id=\"p45\">](hdinsight-use-mapreduce.md)</ept>"
    },
    {
      "pos": [
        11978,
        12064
      ],
      "content": "If you are using Tez with Hive, see the following documents for debugging information:"
    },
    {
      "pos": [
        12068,
        12138
      ],
      "content": "<bpt id=\"p46\">[</bpt>Use the Tez UI on Windows-based HDInsight<ept id=\"p46\">](hdinsight-debug-tez-ui.md)</ept>"
    },
    {
      "pos": [
        12142,
        12228
      ],
      "content": "<bpt id=\"p47\">[</bpt>Use the Ambari Tez view on Linux-based HDInsight<ept id=\"p47\">](hdinsight-debug-ambari-tez-view.md)</ept>"
    }
  ],
  "content": "<properties\n   pageTitle=\"Use Beeline to work with Hive on HDInsight (Hadoop) | Microsoft Azure\"\n   description=\"Learn how to use SSH to connect to a Hadoop cluster in HDInsight, and then interactively submit Hive queries by using Beeline. Beeline is a utility for working with HiveServer2 over JDBC.\"\n   services=\"hdinsight\"\n   documentationCenter=\"\"\n   authors=\"Blackmist\"\n   manager=\"paulettm\"\n   editor=\"cgronlun\"\n    tags=\"azure-portal\"/>\n\n<tags\n   ms.service=\"hdinsight\"\n   ms.devlang=\"na\"\n   ms.topic=\"article\"\n   ms.tgt_pltfrm=\"na\"\n   ms.workload=\"big-data\"\n   ms.date=\"02/16/2016\"\n   ms.author=\"larryfr\"/>\n\n#Use Hive with Hadoop in HDInsight with Beeline\n\n[AZURE.INCLUDE [hive-selector](../../includes/hdinsight-selector-use-hive.md)]\n\nIn this article, you will learn how to use Secure Shell (SSH) to connect to a Linux-based HDInsight cluster, and then interactively submit Hive queries by using the [Beeline](https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-Beeline–NewCommandLineShell) command-line tool.\n\n> [AZURE.NOTE] Beeline used JDBC to connect to Hive. For more information on using JDBC with Hive, see [Connect to Hive on Azure HDInsight using the Hive JDBC driver](hdinsight-connect-hive-jdbc-driver.md).\n\n##<a id=\"prereq\"></a>Prerequisites\n\nTo complete the steps in this article, you will need the following:\n\n* A Linux-based Hadoop on HDInsight cluster.\n\n* An SSH client. Linux, Unix, and Mac OS should come with an SSH client. Windows users must download a client, such as [PuTTY](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html).\n\n##<a id=\"ssh\"></a>Connect with SSH\n\nConnect to the fully qualified domain name (FQDN) of your HDInsight cluster by using the SSH command. The FQDN will be the name you gave the cluster, then **.azurehdinsight.net**. For example, the following would connect to a cluster named **myhdinsight**:\n\n    ssh admin@myhdinsight-ssh.azurehdinsight.net\n\n**If you provided a certificate key for SSH authentication** when you created the HDInsight cluster, you may need to specify the location of the private key on your client system:\n\n    ssh admin@myhdinsight-ssh.azurehdinsight.net -i ~/mykey.key\n\n**If you provided a password for SSH authentication** when you created the HDInsight cluster, you will need to provide the password when prompted.\n\nFor more information on using SSH with HDInsight, see [Use SSH with Linux-based Hadoop on HDInsight from Linux, OS X, and Unix](hdinsight-hadoop-linux-use-ssh-unix.md).\n\n###PuTTY (Windows-based clients)\n\nWindows does not provide a built-in SSH client. We recommend using **PuTTY**, which can be downloaded from [http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html).\n\nFor more information on using PuTTY, see [Use SSH with Linux-based Hadoop on HDInsight from Windows ](hdinsight-hadoop-linux-use-ssh-windows.md).\n\n##<a id=\"beeline\"></a>Use the Beeline command\n\n1. Once connected, use the following to get the hostname of the head node:\n\n        hostname -f\n    \n    Save the returned host name, as it will be used later when connecting to HiveServer2 from Beeline.\n    \n2. Start the Hive CLI by using the following command:\n\n        beeline\n\n2. From the `beeline>` prompt, use the following to connect to the HiveServer2 service. Replace __HOSTNAME__ with the host name returned for the head node ealier:\n\n        !connect jdbc:hive2://HOSTNAME:10001/;transportMode=http admin\n\n    When prompted, enter the password for the administrator (admin) account for your HDInsight cluster. Once the connection is established, the prompt will change to the following:\n    \n        jdbc:hive2://HOSTNAME:10001/>\n\n3. Beeline commands usually begin with a `!` character, for example `!help` displays help. However the `!` can often be ommited. For example, `help` will also work.\n\n    If you view help, you will notice `!sql`, which is used to execute HiveQL statements. However, HiveQL is so commonly used that you can ommit the preceeding `!sql`. The following two statements have exactly the same results; displaying the tables currently available through Hive:\n    \n        !sql show tables;\n        show tables;\n    \n    On a new cluster, only one table should be listed: __hivesampletable__.\n\n4. Use the following to display the schema for the hivesampletable:\n\n        describe hivesampletable;\n        \n    This will return the following information:\n    \n        +-----------------------+------------+----------+--+\n        |       col_name        | data_type  | comment  |\n        +-----------------------+------------+----------+--+\n        | clientid              | string     |          |\n        | querytime             | string     |          |\n        | market                | string     |          |\n        | deviceplatform        | string     |          |\n        | devicemake            | string     |          |\n        | devicemodel           | string     |          |\n        | state                 | string     |          |\n        | country               | string     |          |\n        | querydwelltime        | double     |          |\n        | sessionid             | bigint     |          |\n        | sessionpagevieworder  | bigint     |          |\n        +-----------------------+------------+----------+--+\n\n    This displays the columns in the table. While we could perform some queries against this data, let's instead create a brand new table to demonstrate how to load data into Hive and apply a schema.\n    \n5. Enter the following statements to create a new table named **log4jLogs** by using sample data provided with the HDInsight cluster:\n\n        DROP TABLE log4jLogs;\n        CREATE EXTERNAL TABLE log4jLogs (t1 string, t2 string, t3 string, t4 string, t5 string, t6 string, t7 string)\n        ROW FORMAT DELIMITED FIELDS TERMINATED BY ' '\n        STORED AS TEXTFILE LOCATION 'wasb:///example/data/';\n        SELECT t4 AS sev, COUNT(*) AS count FROM log4jLogs WHERE t4 = '[ERROR]' AND INPUT__FILE__NAME LIKE '%.log' GROUP BY t4;\n\n    These statements perform the following actions:\n\n    * **DROP TABLE** - Deletes the table and the data file, in case the table already exists.\n    * **CREATE EXTERNAL TABLE** - Creates a new 'external' table in Hive. External tables only store the table definition in Hive. The data is left in the original location.\n    * **ROW FORMAT** - Tells Hive how the data is formatted. In this case, the fields in each log are separated by a space.\n    * **STORED AS TEXTFILE LOCATION** - Tells Hive where the data is stored (the example/data directory), and that it is stored as text.\n    * **SELECT** - Selects a count of all rows where column **t4** contains the value **[ERROR]**. This should return a value of **3** as there are three rows that contain this value.\n    * **INPUT__FILE__NAME LIKE '%.log'** - Tells Hive that we should only return data from files ending in .log. Normally, you would only have data with the same schema within the same folder when querying with hive, however this example log file is stored with other data formats.\n\n    > [AZURE.NOTE] External tables should be used when you expect the underlying data to be updated by an external source, such as an automated data upload process, or by another MapReduce operation, but always want Hive queries to use the latest data.\n    >\n    > Dropping an external table does **not** delete the data, only the table definition.\n    \n    The output of this command should be similar to the following:\n    \n        INFO  : Tez session hasn't been created yet. Opening session\n        INFO  :\n        \n        INFO  : Status: Running (Executing on YARN cluster with App id application_1443698635933_0001)\n        \n        INFO  : Map 1: -/-      Reducer 2: 0/1\n        INFO  : Map 1: 0/1      Reducer 2: 0/1\n        INFO  : Map 1: 0/1      Reducer 2: 0/1\n        INFO  : Map 1: 0/1      Reducer 2: 0/1\n        INFO  : Map 1: 0/1      Reducer 2: 0/1\n        INFO  : Map 1: 0(+1)/1  Reducer 2: 0/1\n        INFO  : Map 1: 0(+1)/1  Reducer 2: 0/1\n        INFO  : Map 1: 1/1      Reducer 2: 0/1\n        INFO  : Map 1: 1/1      Reducer 2: 0(+1)/1\n        INFO  : Map 1: 1/1      Reducer 2: 1/1\n        +----------+--------+--+\n        |   sev    | count  |\n        +----------+--------+--+\n        | [ERROR]  | 3      |\n        +----------+--------+--+\n        1 row selected (47.351 seconds)\n\n4. To exit Beeline, use `!quit`.\n\n##<a id=\"file\"></a>Run a HiveQL file\n\nBeeline can also be used to run a file that contains HiveQL statements. Use the following steps to create a file, then run it using Beeline.\n\n1. Use the following command to create a new file named __query.hql__:\n\n        nano query.hql\n        \n2. Once the editor opens, use the following as the contents of the file. This query will create a new 'internal' table named **errorLogs**:\n\n        CREATE TABLE IF NOT EXISTS errorLogs (t1 string, t2 string, t3 string, t4 string, t5 string, t6 string, t7 string) STORED AS ORC;\n        INSERT OVERWRITE TABLE errorLogs SELECT t1, t2, t3, t4, t5, t6, t7 FROM log4jLogs WHERE t4 = '[ERROR]' AND INPUT__FILE__NAME LIKE '%.log';\n\n    These statements perform the following actions:\n\n    * **CREATE TABLE IF NOT EXISTS** - Creates a table, if it does not already exist. Since the **EXTERNAL** keyword is not used, this is an internal table, which is stored in the Hive data warehouse and is managed completely by Hive.\n    * **STORED AS ORC** - Stores the data in Optimized Row Columnar (ORC) format. This is a highly optimized and efficient format for storing Hive data.\n    * **INSERT OVERWRITE ... SELECT** - Selects rows from the **log4jLogs** table that contain **[ERROR]**, then inserts the data into the **errorLogs** table.\n    \n    > [AZURE.NOTE] Unlike external tables, dropping an internal table will delete the underlying data as well.\n    \n3. To save the file, use __Ctrl__+___X__, then enter __Y__, and finally __Enter__.\n\n4. Use the following to run the file using Beeline. Replease __HOSTNAME__ with the name obtained earlier for the head node, and __PASSWORD__ with the password for the admin account:\n\n        beeline -u 'jdbc:hive2://HOSTNAME:10001/;transportMode=http' -n admin -p PASSWORD -f query.hql\n\n5. To verify that the **errorLogs** table was created, start Beeline and connect to HiveServer2, then use the following statement to return all the rows from **errorLogs**:\n\n        SELECT * from errorLogs;\n\n    Three rows of data should be returned, all containing **[ERROR]** in column t4:\n    \n        +---------------+---------------+---------------+---------------+---------------+---------------+---------------+--+\n        | errorlogs.t1  | errorlogs.t2  | errorlogs.t3  | errorlogs.t4  | errorlogs.t5  | errorlogs.t6  | errorlogs.t7  |\n        +---------------+---------------+---------------+---------------+---------------+---------------+---------------+--+\n        | 2012-02-03    | 18:35:34      | SampleClass0  | [ERROR]       | incorrect     | id            |               |\n        | 2012-02-03    | 18:55:54      | SampleClass1  | [ERROR]       | incorrect     | id            |               |\n        | 2012-02-03    | 19:25:27      | SampleClass4  | [ERROR]       | incorrect     | id            |               |\n        +---------------+---------------+---------------+---------------+---------------+---------------+---------------+--+\n        3 rows selected (1.538 seconds)\n\n##<a id=\"summary\"></a>Summary\n\nAs you can see, the Beeline command provides an easy way to interactively run Hive queries on an HDInsight cluster.\n\n##<a id=\"nextsteps\"></a>Next steps\n\nFor general information on Hive in HDInsight:\n\n* [Use Hive with Hadoop on HDInsight](hdinsight-use-hive.md)\n\nFor information on other ways you can work with Hadoop on HDInsight:\n\n* [Use Pig with Hadoop on HDInsight](hdinsight-use-pig.md)\n\n* [Use MapReduce with Hadoop on HDInsight](hdinsight-use-mapreduce.md)\n\nIf you are using Tez with Hive, see the following documents for debugging information:\n\n* [Use the Tez UI on Windows-based HDInsight](hdinsight-debug-tez-ui.md)\n\n* [Use the Ambari Tez view on Linux-based HDInsight](hdinsight-debug-ambari-tez-view.md)\n\n[hdinsight-sdk-documentation]: http://msdnstage.redmond.corp.microsoft.com/library/dn479185.aspx\n\n[azure-purchase-options]: http://azure.microsoft.com/pricing/purchase-options/\n[azure-member-offers]: http://azure.microsoft.com/pricing/member-offers/\n[azure-free-trial]: http://azure.microsoft.com/pricing/free-trial/\n\n[apache-tez]: http://tez.apache.org\n[apache-hive]: http://hive.apache.org/\n[apache-log4j]: http://en.wikipedia.org/wiki/Log4j\n[hive-on-tez-wiki]: https://cwiki.apache.org/confluence/display/Hive/Hive+on+Tez\n[import-to-excel]: http://azure.microsoft.com/documentation/articles/hdinsight-connect-excel-power-query/\n\n\n[hdinsight-use-oozie]: hdinsight-use-oozie.md\n[hdinsight-analyze-flight-data]: hdinsight-analyze-flight-delay-data.md\n\n[putty]: http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html\n\n\n[hdinsight-provision]: hdinsight-provision-clusters.md\n[hdinsight-submit-jobs]: hdinsight-submit-hadoop-jobs-programmatically.md\n[hdinsight-upload-data]: hdinsight-upload-data.md\n\n\n[powershell-here-strings]: http://technet.microsoft.com/library/ee692792.aspx\n\n"
}