<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" source-language="en-US" target-language="fr-fr">
    <body>
      <group id="main" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Out of memory error (OOM) - Hive settings | Microsoft Azure</source>
          <target state="new">Out of memory error (OOM) - Hive settings | Microsoft Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Fix an out of memory error (OOM) from a Hive query in Hadoop in HDInsight.</source>
          <target state="new">Fix an out of memory error (OOM) from a Hive query in Hadoop in HDInsight.</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>The customer scenario is a query across many large tables.</source>
          <target state="new">The customer scenario is a query across many large tables.</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>Fix an Out of Memory (OOM) error with Hive memory settings in Hadoop in Azure HDInsight</source>
          <target state="new">Fix an Out of Memory (OOM) error with Hive memory settings in Hadoop in Azure HDInsight</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>One of the common problems our customers face is getting an Out of Memory (OOM) error when using Hive.</source>
          <target state="new">One of the common problems our customers face is getting an Out of Memory (OOM) error when using Hive.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>This article describes a customer scenario and the Hive settings we recommended to fix the issue.</source>
          <target state="new">This article describes a customer scenario and the Hive settings we recommended to fix the issue.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>Scenario: Hive query across large tables</source>
          <target state="new">Scenario: Hive query across large tables</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>A customer ran the query below using Hive.</source>
          <target state="new">A customer ran the query below using Hive.</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>Some nuances of this query:</source>
          <target state="new">Some nuances of this query:</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>T1 is an alias to a big table, TABLE1, which has lots of STRING column types.</source>
          <target state="new">T1 is an alias to a big table, TABLE1, which has lots of STRING column types.</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>Other tables are not that big but do have a large number of columns.</source>
          <target state="new">Other tables are not that big but do have a large number of columns.</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>All tables are joining each other, in some cases with multiple columns in TABLE1 and others.</source>
          <target state="new">All tables are joining each other, in some cases with multiple columns in TABLE1 and others.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>When the customer ran the query using Hive on MapReduce on a 24 node A3 cluster, the query ran in about 26 minutes.</source>
          <target state="new">When the customer ran the query using Hive on MapReduce on a 24 node A3 cluster, the query ran in about 26 minutes.</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>The customer noticed the following warning messages when the query was run using Hive on MapReduce:</source>
          <target state="new">The customer noticed the following warning messages when the query was run using Hive on MapReduce:</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>Because the query finished executing in about 26 minutes, the customer ignored these warnings and instead started to focus on how to improve the this query’s performance further.</source>
          <target state="new">Because the query finished executing in about 26 minutes, the customer ignored these warnings and instead started to focus on how to improve the this query’s performance further.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>The customer consulted <bpt id="p1">[</bpt>Optimize Hive queries for Hadoop in HDInsight<ept id="p1">](hdinsight-hadoop-optimize-hive-query.md)</ept>, and decided to use Tez execution engine.</source>
          <target state="new">The customer consulted <bpt id="p1">[</bpt>Optimize Hive queries for Hadoop in HDInsight<ept id="p1">](hdinsight-hadoop-optimize-hive-query.md)</ept>, and decided to use Tez execution engine.</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>Once the same query was run with the Tez setting enabled the query ran for 15 minutes, and then threw the following error:</source>
          <target state="new">Once the same query was run with the Tez setting enabled the query ran for 15 minutes, and then threw the following error:</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>The customer then decided to use a bigger VM (i.e. D12) thinking a bigger VM would have more heap space.</source>
          <target state="new">The customer then decided to use a bigger VM (i.e. D12) thinking a bigger VM would have more heap space.</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>Even then, the customer continued to see the error.</source>
          <target state="new">Even then, the customer continued to see the error.</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>The customer reached out to the HDInsight team for help in debugging this issue.</source>
          <target state="new">The customer reached out to the HDInsight team for help in debugging this issue.</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>Debug the Out of Memory (OOM) error</source>
          <target state="new">Debug the Out of Memory (OOM) error</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>Our support and engineering teams together found one of the issues causing the Out of Memory (OOM) error was a <bpt id="p2">[</bpt>known issue described in the Apache JIRA<ept id="p2">](https://issues.apache.org/jira/browse/HIVE-8306)</ept>.</source>
          <target state="new">Our support and engineering teams together found one of the issues causing the Out of Memory (OOM) error was a <bpt id="p2">[</bpt>known issue described in the Apache JIRA<ept id="p2">](https://issues.apache.org/jira/browse/HIVE-8306)</ept>.</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>From the description in the JIRA:</source>
          <target state="new">From the description in the JIRA:</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>We confirmed that <bpt id="p3">**</bpt>hive.auto.convert.join.noconditionaltask<ept id="p3">**</ept><ph id="ph2" /> was indeed set to <bpt id="p4">**</bpt>true<ept id="p4">**</ept><ph id="ph3" /> by looking under hive-site.xml file:</source>
          <target state="new">We confirmed that <bpt id="p3">**</bpt>hive.auto.convert.join.noconditionaltask<ept id="p3">**</ept><ph id="ph2" /> was indeed set to <bpt id="p4">**</bpt>true<ept id="p4">**</ept><ph id="ph3" /> by looking under hive-site.xml file:</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>Based on the warning and the JIRA, our hypothesis was Map Join was the cause of the Java Heap Space OOM error.</source>
          <target state="new">Based on the warning and the JIRA, our hypothesis was Map Join was the cause of the Java Heap Space OOM error.</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>So we dug deeper into this issue.</source>
          <target state="new">So we dug deeper into this issue.</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>As explained in the blog post <bpt id="p5">[</bpt>Hadoop Yarn memory settings in HDInsight<ept id="p5">](http://blogs.msdn.com/b/shanyu/archive/2014/07/31/hadoop-yarn-memory-settings-in-hdinsigh.aspx)</ept>, when Tez execution engine is used the heap space used actually belongs to the Tez container.</source>
          <target state="new">As explained in the blog post <bpt id="p5">[</bpt>Hadoop Yarn memory settings in HDInsight<ept id="p5">](http://blogs.msdn.com/b/shanyu/archive/2014/07/31/hadoop-yarn-memory-settings-in-hdinsigh.aspx)</ept>, when Tez execution engine is used the heap space used actually belongs to the Tez container.</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>See the image below describing the Tez container memory.</source>
          <target state="new">See the image below describing the Tez container memory.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source><ph id="ph4">![</ph>Tez container memory diagram: Hive out of memory error  OOM<ph id="ph5">](./media/hdinsight-hadoop-hive-out-of-memory-error-oom/hive-out-of-memory-error-oom-tez-container-memory.png)</ph></source>
          <target state="new"><ph id="ph4">![</ph>Tez container memory diagram: Hive out of memory error  OOM<ph id="ph5">](./media/hdinsight-hadoop-hive-out-of-memory-error-oom/hive-out-of-memory-error-oom-tez-container-memory.png)</ph></target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>As the blog post suggests, the following two memory settings define the container memory for the heap: <bpt id="p6">**</bpt>hive.tez.container.size<ept id="p6">**</ept><ph id="ph6" /> and <bpt id="p7">**</bpt>hive.tez.java.opts<ept id="p7">**</ept>.</source>
          <target state="new">As the blog post suggests, the following two memory settings define the container memory for the heap: <bpt id="p6">**</bpt>hive.tez.container.size<ept id="p6">**</ept><ph id="ph6" /> and <bpt id="p7">**</bpt>hive.tez.java.opts<ept id="p7">**</ept>.</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>From our experience, the OOM exception does not mean the container size is too small.</source>
          <target state="new">From our experience, the OOM exception does not mean the container size is too small.</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>It means the Java heap size (hive.tez.java.opts) is too small.</source>
          <target state="new">It means the Java heap size (hive.tez.java.opts) is too small.</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>So whenever you see OOM, you can try to increase <bpt id="p8">**</bpt>hive.tez.java.opts<ept id="p8">**</ept>.</source>
          <target state="new">So whenever you see OOM, you can try to increase <bpt id="p8">**</bpt>hive.tez.java.opts<ept id="p8">**</ept>.</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>If needed you might have to increase <bpt id="p9">**</bpt>hive.tez.container.size<ept id="p9">**</ept>.</source>
          <target state="new">If needed you might have to increase <bpt id="p9">**</bpt>hive.tez.container.size<ept id="p9">**</ept>.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>The <bpt id="p10">**</bpt>java.opts<ept id="p10">**</ept><ph id="ph7" /> setting should be around 80% of <bpt id="p11">**</bpt>container.size<ept id="p11">**</ept>.</source>
          <target state="new">The <bpt id="p10">**</bpt>java.opts<ept id="p10">**</ept><ph id="ph7" /> setting should be around 80% of <bpt id="p11">**</bpt>container.size<ept id="p11">**</ept>.</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source><ph id="ph8">[AZURE.NOTE]</ph><ph id="ph9" />  The setting <bpt id="p12">**</bpt>hive.tez.java.opts<ept id="p12">**</ept><ph id="ph10" /> must always be smaller than <bpt id="p13">**</bpt>hive.tez.container.size<ept id="p13">**</ept>.</source>
          <target state="new"><ph id="ph8">[AZURE.NOTE]</ph><ph id="ph9" />  The setting <bpt id="p12">**</bpt>hive.tez.java.opts<ept id="p12">**</ept><ph id="ph10" /> must always be smaller than <bpt id="p13">**</bpt>hive.tez.container.size<ept id="p13">**</ept>.</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>Since a D12 machine has 28GB memory, we decided to use a container size of 10GB (10240MB) and assign 80% to java.opts.</source>
          <target state="new">Since a D12 machine has 28GB memory, we decided to use a container size of 10GB (10240MB) and assign 80% to java.opts.</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>This was done on the Hive console using the setting below:</source>
          <target state="new">This was done on the Hive console using the setting below:</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>Based on these settings, the query successfully ran in under ten minutes.</source>
          <target state="new">Based on these settings, the query successfully ran in under ten minutes.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>Conclusion: OOM errors and container size</source>
          <target state="new">Conclusion: OOM errors and container size</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>Getting an OOM error doesn't necessarily mean the container size is too small.</source>
          <target state="new">Getting an OOM error doesn't necessarily mean the container size is too small.</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>Instead, you should configure the memory settings so that the heap size is increased and is at least 80% of the container memory size.</source>
          <target state="new">Instead, you should configure the memory settings so that the heap size is increased and is at least 80% of the container memory size.</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">7f84d482948e14f34a2e3c07ef9058868754906a</xliffext:olfilehash>
  </header>
</xliff>