<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" source-language="en-US" target-language="fr-fr">
    <body>
      <group id="main" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Use Python with Hive and Pig in HDInsight | Microsoft Azure</source>
          <target state="new">Use Python with Hive and Pig in HDInsight | Microsoft Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Learn how to use Python User Defined Functions (UDF) from Hive and Pig in HDInsight, the Hadoop technology stack on Azure.</source>
          <target state="new">Learn how to use Python User Defined Functions (UDF) from Hive and Pig in HDInsight, the Hadoop technology stack on Azure.</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Use Python with Hive and Pig in HDInsight</source>
          <target state="new">Use Python with Hive and Pig in HDInsight</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>Hive and Pig are great for working with data in HDInsight, but sometimes you need a more general purpose language.</source>
          <target state="new">Hive and Pig are great for working with data in HDInsight, but sometimes you need a more general purpose language.</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>Both Hive and Pig allow you to create User Defined Functions (UDF) using a variety of programming languages.</source>
          <target state="new">Both Hive and Pig allow you to create User Defined Functions (UDF) using a variety of programming languages.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>In this article, you will learn how to use a Python UDF from Hive and Pig.</source>
          <target state="new">In this article, you will learn how to use a Python UDF from Hive and Pig.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source><ph id="ph2">[AZURE.NOTE]</ph><ph id="ph3" /> The steps in this article apply to HDInsight cluster versions 2.1, 3.0, 3.1, and 3.2.</source>
          <target state="new"><ph id="ph2">[AZURE.NOTE]</ph><ph id="ph3" /> The steps in this article apply to HDInsight cluster versions 2.1, 3.0, 3.1, and 3.2.</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>Requirements</source>
          <target state="new">Requirements</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>An HDInsight cluster (Windows or Linux-based)</source>
          <target state="new">An HDInsight cluster (Windows or Linux-based)</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>A text editor</source>
          <target state="new">A text editor</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source><ph id="ph4">[AZURE.IMPORTANT]</ph><ph id="ph5" /> If you are using a Linux-based HDInsight server, but creating the Python files on a Windows client, you must use an editor that uses LF as a line ending.</source>
          <target state="new"><ph id="ph4">[AZURE.IMPORTANT]</ph><ph id="ph5" /> If you are using a Linux-based HDInsight server, but creating the Python files on a Windows client, you must use an editor that uses LF as a line ending.</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>If you are not sure whether your editor uses LF or CRLF, see the <bpt id="p1">[</bpt>Troubleshooting<ept id="p1">](#troubleshooting)</ept><ph id="ph6" /> section for steps on removing the CR character using utilities on the HDInsight cluster.</source>
          <target state="new">If you are not sure whether your editor uses LF or CRLF, see the <bpt id="p1">[</bpt>Troubleshooting<ept id="p1">](#troubleshooting)</ept><ph id="ph6" /> section for steps on removing the CR character using utilities on the HDInsight cluster.</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>Python on HDInsight</source>
          <target state="new">Python on HDInsight</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>Python2.7 is installed by default on HDInsight 3.0 and later clusters.</source>
          <target state="new">Python2.7 is installed by default on HDInsight 3.0 and later clusters.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>Hive can be used with this version of Python for stream processing (data is passed between Hive and Python using STDOUT/STDIN).</source>
          <target state="new">Hive can be used with this version of Python for stream processing (data is passed between Hive and Python using STDOUT/STDIN).</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>HDInsight also includes Jython, which is a Python implementation written in Java.</source>
          <target state="new">HDInsight also includes Jython, which is a Python implementation written in Java.</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>Pig understands how to talk to Jython without having to resort to streaming, so it's preferable when using Pig.</source>
          <target state="new">Pig understands how to talk to Jython without having to resort to streaming, so it's preferable when using Pig.</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>Hive and Python</source>
          <target state="new">Hive and Python</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>Python can be used as a UDF from Hive through the HiveQL <bpt id="p2">**</bpt>TRANSFORM<ept id="p2">**</ept><ph id="ph7" /> statement.</source>
          <target state="new">Python can be used as a UDF from Hive through the HiveQL <bpt id="p2">**</bpt>TRANSFORM<ept id="p2">**</ept><ph id="ph7" /> statement.</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>For example, the following HiveQL invokes a Python script stored in the <bpt id="p3">**</bpt>streaming.py<ept id="p3">**</ept><ph id="ph8" /> file.</source>
          <target state="new">For example, the following HiveQL invokes a Python script stored in the <bpt id="p3">**</bpt>streaming.py<ept id="p3">**</ept><ph id="ph8" /> file.</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source><bpt id="p4">**</bpt>Linux-based HDInsight<ept id="p4">**</ept></source>
          <target state="new"><bpt id="p4">**</bpt>Linux-based HDInsight<ept id="p4">**</ept></target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source><bpt id="p5">**</bpt>Windows-based HDInsight<ept id="p5">**</ept></source>
          <target state="new"><bpt id="p5">**</bpt>Windows-based HDInsight<ept id="p5">**</ept></target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source><ph id="ph9">[AZURE.NOTE]</ph><ph id="ph10" /> On Windows-based HDInsight clusters, the <bpt id="p6">**</bpt>USING<ept id="p6">**</ept><ph id="ph11" /> clause must specify the full path to python.exe.</source>
          <target state="new"><ph id="ph9">[AZURE.NOTE]</ph><ph id="ph10" /> On Windows-based HDInsight clusters, the <bpt id="p6">**</bpt>USING<ept id="p6">**</ept><ph id="ph11" /> clause must specify the full path to python.exe.</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>This is always <ph id="ph12">`D:\Python27\python.exe`</ph>.</source>
          <target state="new">This is always <ph id="ph12">`D:\Python27\python.exe`</ph>.</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>Here's what this example does:</source>
          <target state="new">Here's what this example does:</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>The <bpt id="p7">**</bpt>add file<ept id="p7">**</ept><ph id="ph13" /> statement at the beginning of the file adds the <bpt id="p8">**</bpt>streaming.py<ept id="p8">**</ept><ph id="ph14" /> file to the distributed cache, so it's accessible by all nodes in the cluster.</source>
          <target state="new">The <bpt id="p7">**</bpt>add file<ept id="p7">**</ept><ph id="ph13" /> statement at the beginning of the file adds the <bpt id="p8">**</bpt>streaming.py<ept id="p8">**</ept><ph id="ph14" /> file to the distributed cache, so it's accessible by all nodes in the cluster.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>The  <bpt id="p9">**</bpt>SELECT TRANSFORM ... USING<ept id="p9">**</ept><ph id="ph15" /> statement selects data from the <bpt id="p10">**</bpt>hivesampletable<ept id="p10">**</ept>, and passes clientid, devicemake, and devicemodel to the <bpt id="p11">**</bpt>streaming.py<ept id="p11">**</ept><ph id="ph16" /> script.</source>
          <target state="new">The  <bpt id="p9">**</bpt>SELECT TRANSFORM ... USING<ept id="p9">**</ept><ph id="ph15" /> statement selects data from the <bpt id="p10">**</bpt>hivesampletable<ept id="p10">**</ept>, and passes clientid, devicemake, and devicemodel to the <bpt id="p11">**</bpt>streaming.py<ept id="p11">**</ept><ph id="ph16" /> script.</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>The <bpt id="p12">**</bpt>AS<ept id="p12">**</ept><ph id="ph17" /> clause describes the fields returned from <bpt id="p13">**</bpt>streaming.py<ept id="p13">**</ept></source>
          <target state="new">The <bpt id="p12">**</bpt>AS<ept id="p12">**</ept><ph id="ph17" /> clause describes the fields returned from <bpt id="p13">**</bpt>streaming.py<ept id="p13">**</ept></target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source><ph id="ph18">&lt;a name="streamingpy"&gt;</ph><ph id="ph19">&lt;/a&gt;</ph>
Here's the <bpt id="p14">**</bpt>streaming.py<ept id="p14">**</ept><ph id="ph20" /> file used by the HiveQL example.</source>
          <target state="new"><ph id="ph18">&lt;a name="streamingpy"&gt;</ph><ph id="ph19">&lt;/a&gt;</ph>
Here's the <bpt id="p14">**</bpt>streaming.py<ept id="p14">**</ept><ph id="ph20" /> file used by the HiveQL example.</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>Since we are using streaming, this script has to do the following:</source>
          <target state="new">Since we are using streaming, this script has to do the following:</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>Read data from STDIN.</source>
          <target state="new">Read data from STDIN.</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>This is accomplished by using <ph id="ph21">`sys.stdin.readline()`</ph><ph id="ph22" /> in this example.</source>
          <target state="new">This is accomplished by using <ph id="ph21">`sys.stdin.readline()`</ph><ph id="ph22" /> in this example.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>The trailing newline character is removed using <ph id="ph23">`string.strip(line, "\n ")`</ph>, since we just want the text data and not the end of line indicator.</source>
          <target state="new">The trailing newline character is removed using <ph id="ph23">`string.strip(line, "\n ")`</ph>, since we just want the text data and not the end of line indicator.</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>When doing stream processing, a single line contains all the values with a tab character between each value.</source>
          <target state="new">When doing stream processing, a single line contains all the values with a tab character between each value.</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>So <ph id="ph24">`string.split(line, "\t")`</ph><ph id="ph25" /> can be used to split the input at each tab, returning just the fields.</source>
          <target state="new">So <ph id="ph24">`string.split(line, "\t")`</ph><ph id="ph25" /> can be used to split the input at each tab, returning just the fields.</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>When processing is complete, the output must be written to STDOUT as a single line, with a tab between each field.</source>
          <target state="new">When processing is complete, the output must be written to STDOUT as a single line, with a tab between each field.</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>This is accomplished by using <ph id="ph26">`print "\t".join([clientid, phone_label, hashlib.md5(phone_label).hexdigest()])`</ph>.</source>
          <target state="new">This is accomplished by using <ph id="ph26">`print "\t".join([clientid, phone_label, hashlib.md5(phone_label).hexdigest()])`</ph>.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>This all occurs within a <ph id="ph27">`while`</ph><ph id="ph28" /> loop, that will repeat until no <ph id="ph29">`line`</ph><ph id="ph30" /> is read, at which point <ph id="ph31">`break`</ph><ph id="ph32" /> exits the loop and the script terminates.</source>
          <target state="new">This all occurs within a <ph id="ph27">`while`</ph><ph id="ph28" /> loop, that will repeat until no <ph id="ph29">`line`</ph><ph id="ph30" /> is read, at which point <ph id="ph31">`break`</ph><ph id="ph32" /> exits the loop and the script terminates.</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>Beyond that, the script just concatenates the input values for <ph id="ph33">`devicemake`</ph><ph id="ph34" /> and <ph id="ph35">`devicemodel`</ph>, and calculates a hash of the concatenated value.</source>
          <target state="new">Beyond that, the script just concatenates the input values for <ph id="ph33">`devicemake`</ph><ph id="ph34" /> and <ph id="ph35">`devicemodel`</ph>, and calculates a hash of the concatenated value.</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>Pretty simple, but it describes the basics of how any Python script invoked from Hive should function: Loop, read input until there is no more, break each line of input apart at the tabs, process, write a single line of tab delimited output.</source>
          <target state="new">Pretty simple, but it describes the basics of how any Python script invoked from Hive should function: Loop, read input until there is no more, break each line of input apart at the tabs, process, write a single line of tab delimited output.</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>See <bpt id="p15">[</bpt>Running the examples<ept id="p15">](#running)</ept><ph id="ph36" /> for how to run this example on your HDInsight cluster.</source>
          <target state="new">See <bpt id="p15">[</bpt>Running the examples<ept id="p15">](#running)</ept><ph id="ph36" /> for how to run this example on your HDInsight cluster.</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>Pig and Python</source>
          <target state="new">Pig and Python</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>A Python script can be used as a UDF from Pig through the <bpt id="p16">**</bpt>GENERATE<ept id="p16">**</ept><ph id="ph37" /> statement.</source>
          <target state="new">A Python script can be used as a UDF from Pig through the <bpt id="p16">**</bpt>GENERATE<ept id="p16">**</ept><ph id="ph37" /> statement.</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>For example, the following example uses a Python script stored in the <bpt id="p17">**</bpt>jython.py<ept id="p17">**</ept><ph id="ph38" /> file.</source>
          <target state="new">For example, the following example uses a Python script stored in the <bpt id="p17">**</bpt>jython.py<ept id="p17">**</ept><ph id="ph38" /> file.</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>Here's how this example works:</source>
          <target state="new">Here's how this example works:</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>It registers the file containing the Python script (<bpt id="p18">**</bpt>jython.py<ept id="p18">**</ept>,) using <bpt id="p19">**</bpt>Jython<ept id="p19">**</ept>, and exposes it to Pig as <bpt id="p20">**</bpt>myfuncs<ept id="p20">**</ept>.</source>
          <target state="new">It registers the file containing the Python script (<bpt id="p18">**</bpt>jython.py<ept id="p18">**</ept>,) using <bpt id="p19">**</bpt>Jython<ept id="p19">**</ept>, and exposes it to Pig as <bpt id="p20">**</bpt>myfuncs<ept id="p20">**</ept>.</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>Jython is a Python implementation in Java, and runs in the same Java Virtual machine as Pig.</source>
          <target state="new">Jython is a Python implementation in Java, and runs in the same Java Virtual machine as Pig.</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>This allows us to treat the Python script like a traditional function call vs. the streaming approach used with Hive.</source>
          <target state="new">This allows us to treat the Python script like a traditional function call vs. the streaming approach used with Hive.</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>The next line loads the sample data file, <bpt id="p21">**</bpt>sample.log<ept id="p21">**</ept><ph id="ph39" /> into <bpt id="p22">**</bpt>LOGS<ept id="p22">**</ept>.</source>
          <target state="new">The next line loads the sample data file, <bpt id="p21">**</bpt>sample.log<ept id="p21">**</ept><ph id="ph39" /> into <bpt id="p22">**</bpt>LOGS<ept id="p22">**</ept>.</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>Since this log file doesn't have a consistent schema, it also defines each record (<bpt id="p23">**</bpt>LINE<ept id="p23">**</ept><ph id="ph40" /> in this case,) as a <bpt id="p24">**</bpt>chararray<ept id="p24">**</ept>.</source>
          <target state="new">Since this log file doesn't have a consistent schema, it also defines each record (<bpt id="p23">**</bpt>LINE<ept id="p23">**</ept><ph id="ph40" /> in this case,) as a <bpt id="p24">**</bpt>chararray<ept id="p24">**</ept>.</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>Chararray is, essentially, a string.</source>
          <target state="new">Chararray is, essentially, a string.</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source>The third line filters out any null values, storing the result of the operation into <bpt id="p25">**</bpt>LOG<ept id="p25">**</ept>.</source>
          <target state="new">The third line filters out any null values, storing the result of the operation into <bpt id="p25">**</bpt>LOG<ept id="p25">**</ept>.</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>Next, it iterates over the records in <bpt id="p26">**</bpt>LOG<ept id="p26">**</ept><ph id="ph41" /> and uses <bpt id="p27">**</bpt>GENERATE<ept id="p27">**</ept><ph id="ph42" /> to invoke the <bpt id="p28">**</bpt>create_structure<ept id="p28">**</ept><ph id="ph43" /> method contained in the <bpt id="p29">**</bpt>jython.py<ept id="p29">**</ept><ph id="ph44" /> script loaded as <bpt id="p30">**</bpt>myfuncs<ept id="p30">**</ept>.</source>
          <target state="new">Next, it iterates over the records in <bpt id="p26">**</bpt>LOG<ept id="p26">**</ept><ph id="ph41" /> and uses <bpt id="p27">**</bpt>GENERATE<ept id="p27">**</ept><ph id="ph42" /> to invoke the <bpt id="p28">**</bpt>create_structure<ept id="p28">**</ept><ph id="ph43" /> method contained in the <bpt id="p29">**</bpt>jython.py<ept id="p29">**</ept><ph id="ph44" /> script loaded as <bpt id="p30">**</bpt>myfuncs<ept id="p30">**</ept>.</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source><bpt id="p31">**</bpt>LINE<ept id="p31">**</ept><ph id="ph45" /> is used to pass the current record to the function.</source>
          <target state="new"><bpt id="p31">**</bpt>LINE<ept id="p31">**</ept><ph id="ph45" /> is used to pass the current record to the function.</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>Finally, the outputs are dumped to STDOUT using the <bpt id="p32">**</bpt>DUMP<ept id="p32">**</ept><ph id="ph46" /> command.</source>
          <target state="new">Finally, the outputs are dumped to STDOUT using the <bpt id="p32">**</bpt>DUMP<ept id="p32">**</ept><ph id="ph46" /> command.</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>This is just to immediately show the results after the operation completes; in a real script you would normally <bpt id="p33">**</bpt>STORE<ept id="p33">**</ept><ph id="ph47" /> the data into a new file.</source>
          <target state="new">This is just to immediately show the results after the operation completes; in a real script you would normally <bpt id="p33">**</bpt>STORE<ept id="p33">**</ept><ph id="ph47" /> the data into a new file.</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source><ph id="ph48">&lt;a name="jythonpy"&gt;</ph><ph id="ph49">&lt;/a&gt;</ph>
Here's the <bpt id="p34">**</bpt>jython.py<ept id="p34">**</ept><ph id="ph50" /> file used by the Pig example:</source>
          <target state="new"><ph id="ph48">&lt;a name="jythonpy"&gt;</ph><ph id="ph49">&lt;/a&gt;</ph>
Here's the <bpt id="p34">**</bpt>jython.py<ept id="p34">**</ept><ph id="ph50" /> file used by the Pig example:</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>Remember that we previously just defined the <bpt id="p35">**</bpt>LINE<ept id="p35">**</ept><ph id="ph51" /> input as a chararray because there was no consistent schema for the input?</source>
          <target state="new">Remember that we previously just defined the <bpt id="p35">**</bpt>LINE<ept id="p35">**</ept><ph id="ph51" /> input as a chararray because there was no consistent schema for the input?</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>What the <bpt id="p36">**</bpt>jython.py<ept id="p36">**</ept><ph id="ph52" /> does is to transform the data into a consistent schema for output.</source>
          <target state="new">What the <bpt id="p36">**</bpt>jython.py<ept id="p36">**</ept><ph id="ph52" /> does is to transform the data into a consistent schema for output.</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>It works like this:</source>
          <target state="new">It works like this:</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>The <bpt id="p37">**</bpt>@outputSchema<ept id="p37">**</ept><ph id="ph53" /> statement defines the format of the data that will be returned to Pig.</source>
          <target state="new">The <bpt id="p37">**</bpt>@outputSchema<ept id="p37">**</ept><ph id="ph53" /> statement defines the format of the data that will be returned to Pig.</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>In this case, it's a <bpt id="p38">**</bpt>data bag<ept id="p38">**</ept>, which is a Pig data type.</source>
          <target state="new">In this case, it's a <bpt id="p38">**</bpt>data bag<ept id="p38">**</ept>, which is a Pig data type.</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>The bag contains the following fields, all of which are chararray (strings):</source>
          <target state="new">The bag contains the following fields, all of which are chararray (strings):</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source>date - the date the log entry was created</source>
          <target state="new">date - the date the log entry was created</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source>time - the time the log entry was created</source>
          <target state="new">time - the time the log entry was created</target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source>classname - the class name the entry was created for</source>
          <target state="new">classname - the class name the entry was created for</target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>level - the log level</source>
          <target state="new">level - the log level</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>detail - verbose details for the log entry</source>
          <target state="new">detail - verbose details for the log entry</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source>Next, the <bpt id="p39">**</bpt>def create_structure(input)<ept id="p39">**</ept><ph id="ph54" /> defines the function that Pig will pass line items to.</source>
          <target state="new">Next, the <bpt id="p39">**</bpt>def create_structure(input)<ept id="p39">**</ept><ph id="ph54" /> defines the function that Pig will pass line items to.</target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source>The example data, <bpt id="p40">**</bpt>sample.log<ept id="p40">**</ept>, mostly conforms to the date, time, classname, level, and detail schema we want to return.</source>
          <target state="new">The example data, <bpt id="p40">**</bpt>sample.log<ept id="p40">**</ept>, mostly conforms to the date, time, classname, level, and detail schema we want to return.</target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source>But it also contains a few lines that begin with the string '<bpt id="p41">*</bpt>java.lang.Exception<ept id="p41">*</ept>' that need to be modified to match the schema.</source>
          <target state="new">But it also contains a few lines that begin with the string '<bpt id="p41">*</bpt>java.lang.Exception<ept id="p41">*</ept>' that need to be modified to match the schema.</target>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source>The <bpt id="p42">**</bpt>if<ept id="p42">**</ept><ph id="ph55" /> statement checks for those, then massages the input data to move the '<bpt id="p43">*</bpt>java.lang.Exception<ept id="p43">*</ept>' string to the end, bringing the data in-line with our expected output schema.</source>
          <target state="new">The <bpt id="p42">**</bpt>if<ept id="p42">**</ept><ph id="ph55" /> statement checks for those, then massages the input data to move the '<bpt id="p43">*</bpt>java.lang.Exception<ept id="p43">*</ept>' string to the end, bringing the data in-line with our expected output schema.</target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source>Next, the <bpt id="p44">**</bpt>split<ept id="p44">**</ept><ph id="ph56" /> command is used to split the data at the first four space characters.</source>
          <target state="new">Next, the <bpt id="p44">**</bpt>split<ept id="p44">**</ept><ph id="ph56" /> command is used to split the data at the first four space characters.</target>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source>This results in five values, which are assigned into <bpt id="p45">**</bpt>date<ept id="p45">**</ept>, <bpt id="p46">**</bpt>time<ept id="p46">**</ept>, <bpt id="p47">**</bpt>classname<ept id="p47">**</ept>, <bpt id="p48">**</bpt>level<ept id="p48">**</ept>, and <bpt id="p49">**</bpt>detail<ept id="p49">**</ept>.</source>
          <target state="new">This results in five values, which are assigned into <bpt id="p45">**</bpt>date<ept id="p45">**</ept>, <bpt id="p46">**</bpt>time<ept id="p46">**</ept>, <bpt id="p47">**</bpt>classname<ept id="p47">**</ept>, <bpt id="p48">**</bpt>level<ept id="p48">**</ept>, and <bpt id="p49">**</bpt>detail<ept id="p49">**</ept>.</target>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source>Finally, the values are returned to Pig.</source>
          <target state="new">Finally, the values are returned to Pig.</target>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source>When the data is returned to Pig, it will have a consistent schema as defined in the <bpt id="p50">**</bpt>@outputSchema<ept id="p50">**</ept><ph id="ph57" /> statement.</source>
          <target state="new">When the data is returned to Pig, it will have a consistent schema as defined in the <bpt id="p50">**</bpt>@outputSchema<ept id="p50">**</ept><ph id="ph57" /> statement.</target>
        </trans-unit>
        <trans-unit id="181" translate="yes" xml:space="preserve">
          <source>Running the examples</source>
          <target state="new">Running the examples</target>
        </trans-unit>
        <trans-unit id="182" translate="yes" xml:space="preserve">
          <source>If you are using a Linux-based HDInsight cluster, use the <bpt id="p51">**</bpt>SSH<ept id="p51">**</ept><ph id="ph58" /> steps below.</source>
          <target state="new">If you are using a Linux-based HDInsight cluster, use the <bpt id="p51">**</bpt>SSH<ept id="p51">**</ept><ph id="ph58" /> steps below.</target>
        </trans-unit>
        <trans-unit id="183" translate="yes" xml:space="preserve">
          <source>If you are using a Windows-based HDInsight cluster and a Windows client, use the <bpt id="p52">**</bpt>PowerShell<ept id="p52">**</ept><ph id="ph59" /> steps.</source>
          <target state="new">If you are using a Windows-based HDInsight cluster and a Windows client, use the <bpt id="p52">**</bpt>PowerShell<ept id="p52">**</ept><ph id="ph59" /> steps.</target>
        </trans-unit>
        <trans-unit id="184" translate="yes" xml:space="preserve">
          <source>SSH</source>
          <target state="new">SSH</target>
        </trans-unit>
        <trans-unit id="185" translate="yes" xml:space="preserve">
          <source>For more information on using SSH, see</source>
          <target state="new">For more information on using SSH, see</target>
        </trans-unit>
        <trans-unit id="186" translate="yes" xml:space="preserve">
          <source>Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X</source>
          <target state="new">Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X</target>
        </trans-unit>
        <trans-unit id="187" translate="yes" xml:space="preserve">
          <source>or</source>
          <target state="new">or</target>
        </trans-unit>
        <trans-unit id="188" translate="yes" xml:space="preserve">
          <source>Use SSH with Linux-based Hadoop on HDInsight from Windows</source>
          <target state="new">Use SSH with Linux-based Hadoop on HDInsight from Windows</target>
        </trans-unit>
        <trans-unit id="190" translate="yes" xml:space="preserve">
          <source>Using the Python examples <bpt id="p53">[</bpt>streaming.py<ept id="p53">](#streamingpy)</ept><ph id="ph60" /> and <bpt id="p54">[</bpt>jython.py<ept id="p54">](#jythonpy)</ept>, create local copies of the files on your development machine.</source>
          <target state="new">Using the Python examples <bpt id="p53">[</bpt>streaming.py<ept id="p53">](#streamingpy)</ept><ph id="ph60" /> and <bpt id="p54">[</bpt>jython.py<ept id="p54">](#jythonpy)</ept>, create local copies of the files on your development machine.</target>
        </trans-unit>
        <trans-unit id="191" translate="yes" xml:space="preserve">
          <source>Use <ph id="ph61">`scp`</ph><ph id="ph62" /> to copy the files to your HDInsight cluster.</source>
          <target state="new">Use <ph id="ph61">`scp`</ph><ph id="ph62" /> to copy the files to your HDInsight cluster.</target>
        </trans-unit>
        <trans-unit id="192" translate="yes" xml:space="preserve">
          <source>For example, the following would copy the files to a cluster named <bpt id="p55">**</bpt>mycluster<ept id="p55">**</ept>.</source>
          <target state="new">For example, the following would copy the files to a cluster named <bpt id="p55">**</bpt>mycluster<ept id="p55">**</ept>.</target>
        </trans-unit>
        <trans-unit id="193" translate="yes" xml:space="preserve">
          <source>Use SSH to connect to the cluster.</source>
          <target state="new">Use SSH to connect to the cluster.</target>
        </trans-unit>
        <trans-unit id="194" translate="yes" xml:space="preserve">
          <source>For example, the following would connect to a cluster named <bpt id="p56">**</bpt>mycluster<ept id="p56">**</ept><ph id="ph63" /> as user <bpt id="p57">**</bpt>myuser<ept id="p57">**</ept>.</source>
          <target state="new">For example, the following would connect to a cluster named <bpt id="p56">**</bpt>mycluster<ept id="p56">**</ept><ph id="ph63" /> as user <bpt id="p57">**</bpt>myuser<ept id="p57">**</ept>.</target>
        </trans-unit>
        <trans-unit id="195" translate="yes" xml:space="preserve">
          <source>From the SSH session, add the python files uploaded previously to the WASB storage for the cluster.</source>
          <target state="new">From the SSH session, add the python files uploaded previously to the WASB storage for the cluster.</target>
        </trans-unit>
        <trans-unit id="196" translate="yes" xml:space="preserve">
          <source>After uploading the files, use the following steps to run the Hive and Pig jobs.</source>
          <target state="new">After uploading the files, use the following steps to run the Hive and Pig jobs.</target>
        </trans-unit>
        <trans-unit id="197" translate="yes" xml:space="preserve">
          <source>Hive</source>
          <target state="new">Hive</target>
        </trans-unit>
        <trans-unit id="198" translate="yes" xml:space="preserve">
          <source>Use the <ph id="ph64">`hive`</ph><ph id="ph65" /> command to start the hive shell.</source>
          <target state="new">Use the <ph id="ph64">`hive`</ph><ph id="ph65" /> command to start the hive shell.</target>
        </trans-unit>
        <trans-unit id="199" translate="yes" xml:space="preserve">
          <source>You should see a <ph id="ph66">`hive&gt;`</ph><ph id="ph67" /> prompt once the shell has loaded.</source>
          <target state="new">You should see a <ph id="ph66">`hive&gt;`</ph><ph id="ph67" /> prompt once the shell has loaded.</target>
        </trans-unit>
        <trans-unit id="200" translate="yes" xml:space="preserve">
          <source>Enter the following at the <ph id="ph68">`hive&gt;`</ph><ph id="ph69" /> prompt.</source>
          <target state="new">Enter the following at the <ph id="ph68">`hive&gt;`</ph><ph id="ph69" /> prompt.</target>
        </trans-unit>
        <trans-unit id="201" translate="yes" xml:space="preserve">
          <source>After entering the last line, the job should start.</source>
          <target state="new">After entering the last line, the job should start.</target>
        </trans-unit>
        <trans-unit id="202" translate="yes" xml:space="preserve">
          <source>Eventually it will return output similar to the following.</source>
          <target state="new">Eventually it will return output similar to the following.</target>
        </trans-unit>
        <trans-unit id="203" translate="yes" xml:space="preserve">
          <source>Pig</source>
          <target state="new">Pig</target>
        </trans-unit>
        <trans-unit id="204" translate="yes" xml:space="preserve">
          <source>Use the <ph id="ph70">`pig`</ph><ph id="ph71" /> command to start the shell.</source>
          <target state="new">Use the <ph id="ph70">`pig`</ph><ph id="ph71" /> command to start the shell.</target>
        </trans-unit>
        <trans-unit id="205" translate="yes" xml:space="preserve">
          <source>You should see a <ph id="ph72">`grunt&gt;`</ph><ph id="ph73" /> prompt once the shell has loaded.</source>
          <target state="new">You should see a <ph id="ph72">`grunt&gt;`</ph><ph id="ph73" /> prompt once the shell has loaded.</target>
        </trans-unit>
        <trans-unit id="206" translate="yes" xml:space="preserve">
          <source>Enter the following statements at the <ph id="ph74">`grunt&gt;`</ph><ph id="ph75" /> prompt.</source>
          <target state="new">Enter the following statements at the <ph id="ph74">`grunt&gt;`</ph><ph id="ph75" /> prompt.</target>
        </trans-unit>
        <trans-unit id="207" translate="yes" xml:space="preserve">
          <source>After entering the following line,the job should start.</source>
          <target state="new">After entering the following line,the job should start.</target>
        </trans-unit>
        <trans-unit id="208" translate="yes" xml:space="preserve">
          <source>Eventually it will return output similar to the following.</source>
          <target state="new">Eventually it will return output similar to the following.</target>
        </trans-unit>
        <trans-unit id="209" translate="yes" xml:space="preserve">
          <source>PowerShell</source>
          <target state="new">PowerShell</target>
        </trans-unit>
        <trans-unit id="210" translate="yes" xml:space="preserve">
          <source>These steps use Azure PowerShell.</source>
          <target state="new">These steps use Azure PowerShell.</target>
        </trans-unit>
        <trans-unit id="211" translate="yes" xml:space="preserve">
          <source>If this is not already installed and configured on your development machine, see <bpt id="p58">[</bpt>How to install and configure Azure PowerShell<ept id="p58">](../powershell-install-configure.md)</ept><ph id="ph76" /> before using the following steps.</source>
          <target state="new">If this is not already installed and configured on your development machine, see <bpt id="p58">[</bpt>How to install and configure Azure PowerShell<ept id="p58">](../powershell-install-configure.md)</ept><ph id="ph76" /> before using the following steps.</target>
        </trans-unit>
        <trans-unit id="212" translate="yes" xml:space="preserve">
          <source>Using the Python examples <bpt id="p59">[</bpt>streaming.py<ept id="p59">](#streamingpy)</ept><ph id="ph77" /> and <bpt id="p60">[</bpt>jython.py<ept id="p60">](#jythonpy)</ept>, create local copies of the files on your development machine.</source>
          <target state="new">Using the Python examples <bpt id="p59">[</bpt>streaming.py<ept id="p59">](#streamingpy)</ept><ph id="ph77" /> and <bpt id="p60">[</bpt>jython.py<ept id="p60">](#jythonpy)</ept>, create local copies of the files on your development machine.</target>
        </trans-unit>
        <trans-unit id="213" translate="yes" xml:space="preserve">
          <source>Use  the following PowerShell script to upload the <bpt id="p61">**</bpt>streaming.py<ept id="p61">**</ept><ph id="ph78" /> and <bpt id="p62">**</bpt>jython.py<ept id="p62">**</ept><ph id="ph79" /> files to the server.</source>
          <target state="new">Use  the following PowerShell script to upload the <bpt id="p61">**</bpt>streaming.py<ept id="p61">**</ept><ph id="ph78" /> and <bpt id="p62">**</bpt>jython.py<ept id="p62">**</ept><ph id="ph79" /> files to the server.</target>
        </trans-unit>
        <trans-unit id="214" translate="yes" xml:space="preserve">
          <source>Substitute the name of your Azure HDInsight cluster, and the path to the <bpt id="p63">**</bpt>streaming.py<ept id="p63">**</ept><ph id="ph80" /> and <bpt id="p64">**</bpt>jython.py<ept id="p64">**</ept><ph id="ph81" /> files on the first three lines of the script.</source>
          <target state="new">Substitute the name of your Azure HDInsight cluster, and the path to the <bpt id="p63">**</bpt>streaming.py<ept id="p63">**</ept><ph id="ph80" /> and <bpt id="p64">**</bpt>jython.py<ept id="p64">**</ept><ph id="ph81" /> files on the first three lines of the script.</target>
        </trans-unit>
        <trans-unit id="215" translate="yes" xml:space="preserve">
          <source>This script retrieves information for your HDInsight cluster, then extracts the account and key for the default storage account, and uploads the files to the root of the container.</source>
          <target state="new">This script retrieves information for your HDInsight cluster, then extracts the account and key for the default storage account, and uploads the files to the root of the container.</target>
        </trans-unit>
        <trans-unit id="216" translate="yes" xml:space="preserve">
          <source><ph id="ph82">[AZURE.NOTE]</ph><ph id="ph83" /> Other methods of uploading the scripts can be found in the <bpt id="p65">[</bpt>Upload data for Hadoop jobs in HDInsight<ept id="p65">](hdinsight-upload-data.md)</ept><ph id="ph84" /> document.</source>
          <target state="new"><ph id="ph82">[AZURE.NOTE]</ph><ph id="ph83" /> Other methods of uploading the scripts can be found in the <bpt id="p65">[</bpt>Upload data for Hadoop jobs in HDInsight<ept id="p65">](hdinsight-upload-data.md)</ept><ph id="ph84" /> document.</target>
        </trans-unit>
        <trans-unit id="217" translate="yes" xml:space="preserve">
          <source>After uploading the files, use the following PowerShell scripts to start the jobs.</source>
          <target state="new">After uploading the files, use the following PowerShell scripts to start the jobs.</target>
        </trans-unit>
        <trans-unit id="218" translate="yes" xml:space="preserve">
          <source>When the job completes, the output should be written to the PowerShell console.</source>
          <target state="new">When the job completes, the output should be written to the PowerShell console.</target>
        </trans-unit>
        <trans-unit id="219" translate="yes" xml:space="preserve">
          <source>Hive</source>
          <target state="new">Hive</target>
        </trans-unit>
        <trans-unit id="220" translate="yes" xml:space="preserve">
          <source>The following script will run the <bpt id="p66">__</bpt>streaming.py<ept id="p66">__</ept><ph id="ph85" /> script.</source>
          <target state="new">The following script will run the <bpt id="p66">__</bpt>streaming.py<ept id="p66">__</ept><ph id="ph85" /> script.</target>
        </trans-unit>
        <trans-unit id="221" translate="yes" xml:space="preserve">
          <source>Before running, it will prompt you for the HTTPs/Admin account information for your HDInsight cluster.</source>
          <target state="new">Before running, it will prompt you for the HTTPs/Admin account information for your HDInsight cluster.</target>
        </trans-unit>
        <trans-unit id="222" translate="yes" xml:space="preserve">
          <source>The output for the <bpt id="p67">**</bpt>Hive<ept id="p67">**</ept><ph id="ph86" /> job should appear similar to the following:</source>
          <target state="new">The output for the <bpt id="p67">**</bpt>Hive<ept id="p67">**</ept><ph id="ph86" /> job should appear similar to the following:</target>
        </trans-unit>
        <trans-unit id="223" translate="yes" xml:space="preserve">
          <source>Pig</source>
          <target state="new">Pig</target>
        </trans-unit>
        <trans-unit id="224" translate="yes" xml:space="preserve">
          <source>The following will use the <bpt id="p68">__</bpt>jython.py<ept id="p68">__</ept><ph id="ph87" /> script.</source>
          <target state="new">The following will use the <bpt id="p68">__</bpt>jython.py<ept id="p68">__</ept><ph id="ph87" /> script.</target>
        </trans-unit>
        <trans-unit id="225" translate="yes" xml:space="preserve">
          <source>Before running, it will prompt you for the HTTPs/Admin information for the HDInsight cluster.</source>
          <target state="new">Before running, it will prompt you for the HTTPs/Admin information for the HDInsight cluster.</target>
        </trans-unit>
        <trans-unit id="226" translate="yes" xml:space="preserve">
          <source>The output for the <bpt id="p69">**</bpt>Pig<ept id="p69">**</ept><ph id="ph88" /> job should appear similar to the following:</source>
          <target state="new">The output for the <bpt id="p69">**</bpt>Pig<ept id="p69">**</ept><ph id="ph88" /> job should appear similar to the following:</target>
        </trans-unit>
        <trans-unit id="228" translate="yes" xml:space="preserve">
          <source>Troubleshooting</source>
          <target state="new">Troubleshooting</target>
        </trans-unit>
        <trans-unit id="229" translate="yes" xml:space="preserve">
          <source>Errors when running jobs</source>
          <target state="new">Errors when running jobs</target>
        </trans-unit>
        <trans-unit id="230" translate="yes" xml:space="preserve">
          <source>When running the hive job, you may encounter an error similar to the following:</source>
          <target state="new">When running the hive job, you may encounter an error similar to the following:</target>
        </trans-unit>
        <trans-unit id="231" translate="yes" xml:space="preserve">
          <source>This problem may be caused by the line endings in the streaming.py file.</source>
          <target state="new">This problem may be caused by the line endings in the streaming.py file.</target>
        </trans-unit>
        <trans-unit id="232" translate="yes" xml:space="preserve">
          <source>Many Windows editors default to using CRLF as the line ending, but Linux applications usually expect LF.</source>
          <target state="new">Many Windows editors default to using CRLF as the line ending, but Linux applications usually expect LF.</target>
        </trans-unit>
        <trans-unit id="233" translate="yes" xml:space="preserve">
          <source>If you are using an editor that cannot create LF line endings, or are unsure what line endings are being used, use the following PowerShell statements to remove the CR characters before uploading the file to HDInsight:</source>
          <target state="new">If you are using an editor that cannot create LF line endings, or are unsure what line endings are being used, use the following PowerShell statements to remove the CR characters before uploading the file to HDInsight:</target>
        </trans-unit>
        <trans-unit id="234" translate="yes" xml:space="preserve">
          <source>PowerShell scripts</source>
          <target state="new">PowerShell scripts</target>
        </trans-unit>
        <trans-unit id="235" translate="yes" xml:space="preserve">
          <source>Both of the example PowerShell scripts used to run the examples contain a commented line that will display error output for the job.</source>
          <target state="new">Both of the example PowerShell scripts used to run the examples contain a commented line that will display error output for the job.</target>
        </trans-unit>
        <trans-unit id="236" translate="yes" xml:space="preserve">
          <source>If you are not seeing the expected output for the job, uncomment the following line and see if the error information indicates a problem.</source>
          <target state="new">If you are not seeing the expected output for the job, uncomment the following line and see if the error information indicates a problem.</target>
        </trans-unit>
        <trans-unit id="237" translate="yes" xml:space="preserve">
          <source>The error information (STDERR,) and the result of the job (STDOUT,) are also logged to the default blob container for your clusters at the following locations.</source>
          <target state="new">The error information (STDERR,) and the result of the job (STDOUT,) are also logged to the default blob container for your clusters at the following locations.</target>
        </trans-unit>
        <trans-unit id="238" translate="yes" xml:space="preserve">
          <source>For this job..</source>
          <target state="new">For this job..</target>
        </trans-unit>
        <trans-unit id="239" translate="yes" xml:space="preserve">
          <source>Look at these files in the blob container</source>
          <target state="new">Look at these files in the blob container</target>
        </trans-unit>
        <trans-unit id="240" translate="yes" xml:space="preserve">
          <source>Hive</source>
          <target state="new">Hive</target>
        </trans-unit>
        <trans-unit id="241" translate="yes" xml:space="preserve">
          <source>/HivePython/stderr</source>
          <target state="new">/HivePython/stderr</target>
        </trans-unit>
        <trans-unit id="242" translate="yes" xml:space="preserve">
          <source>/HivePython/stdout</source>
          <target state="new">/HivePython/stdout</target>
        </trans-unit>
        <trans-unit id="243" translate="yes" xml:space="preserve">
          <source>Pig</source>
          <target state="new">Pig</target>
        </trans-unit>
        <trans-unit id="244" translate="yes" xml:space="preserve">
          <source>/PigPython/stderr</source>
          <target state="new">/PigPython/stderr</target>
        </trans-unit>
        <trans-unit id="245" translate="yes" xml:space="preserve">
          <source>/PigPython/stdout</source>
          <target state="new">/PigPython/stdout</target>
        </trans-unit>
        <trans-unit id="247" translate="yes" xml:space="preserve">
          <source>Next steps</source>
          <target state="new">Next steps</target>
        </trans-unit>
        <trans-unit id="248" translate="yes" xml:space="preserve">
          <source>If you need to load Python modules that aren't provided by default, see <bpt id="p70">[</bpt>How to deploy a module to Azure HDInsight<ept id="p70">](http://blogs.msdn.com/b/benjguin/archive/2014/03/03/how-to-deploy-a-python-module-to-windows-azure-hdinsight.aspx)</ept><ph id="ph89" /> for an example of how to do this.</source>
          <target state="new">If you need to load Python modules that aren't provided by default, see <bpt id="p70">[</bpt>How to deploy a module to Azure HDInsight<ept id="p70">](http://blogs.msdn.com/b/benjguin/archive/2014/03/03/how-to-deploy-a-python-module-to-windows-azure-hdinsight.aspx)</ept><ph id="ph89" /> for an example of how to do this.</target>
        </trans-unit>
        <trans-unit id="249" translate="yes" xml:space="preserve">
          <source>For other ways to use Pig, Hive, and to learn about using MapReduce, see the following.</source>
          <target state="new">For other ways to use Pig, Hive, and to learn about using MapReduce, see the following.</target>
        </trans-unit>
        <trans-unit id="250" translate="yes" xml:space="preserve">
          <source><bpt id="p71">[</bpt>Use Hive with HDInsight<ept id="p71">](hdinsight-use-hive.md)</ept></source>
          <target state="new"><bpt id="p71">[</bpt>Use Hive with HDInsight<ept id="p71">](hdinsight-use-hive.md)</ept></target>
        </trans-unit>
        <trans-unit id="251" translate="yes" xml:space="preserve">
          <source><bpt id="p72">[</bpt>Use Pig with HDInsight<ept id="p72">](hdinsight-use-pig.md)</ept></source>
          <target state="new"><bpt id="p72">[</bpt>Use Pig with HDInsight<ept id="p72">](hdinsight-use-pig.md)</ept></target>
        </trans-unit>
        <trans-unit id="252" translate="yes" xml:space="preserve">
          <source><bpt id="p73">[</bpt>Use MapReduce with HDInsight<ept id="p73">](hdinsight-use-mapreduce.md)</ept></source>
          <target state="new"><bpt id="p73">[</bpt>Use MapReduce with HDInsight<ept id="p73">](hdinsight-use-mapreduce.md)</ept></target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">7ef60b3b5c0500a3af65b5b4fb50853291970a51</xliffext:olfilehash>
  </header>
</xliff>