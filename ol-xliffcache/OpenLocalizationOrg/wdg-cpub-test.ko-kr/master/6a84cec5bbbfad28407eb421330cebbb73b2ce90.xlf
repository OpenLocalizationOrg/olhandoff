<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="md" source-language="en-US" target-language="ko-kr">
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Description: Use speech recognition to provide input, specify an action or command, and accomplish tasks.</source>
          <target state="new">Description: Use speech recognition to provide input, specify an action or command, and accomplish tasks.</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>title: Speech recognition</source>
          <target state="new">title: Speech recognition</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>ms.assetid: 553C0FB7-35BC-4894-9EF1-906139E17552</source>
          <target state="new">ms.assetid: 553C0FB7-35BC-4894-9EF1-906139E17552</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>label: Speech recognition</source>
          <target state="new">label: Speech recognition</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>template: detail.hbs</source>
          <target state="new">template: detail.hbs</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>Speech recognition</source>
          <target state="new">Speech recognition</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>\[ Updated for UWP apps on Windows 10.</source>
          <target state="new">\[ Updated for UWP apps on Windows 10.</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>For Windows 8.x articles, see the <bpt id="p1">[</bpt>archive<ept id="p1">](http://go.microsoft.com/fwlink/p/?linkid=619132)</ept> \]</source>
          <target state="new">For Windows 8.x articles, see the <bpt id="p1">[</bpt>archive<ept id="p1">](http://go.microsoft.com/fwlink/p/?linkid=619132)</ept> \]</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>Use speech recognition to provide input, specify an action or command, and accomplish tasks.</source>
          <target state="new">Use speech recognition to provide input, specify an action or command, and accomplish tasks.</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>Important APIs</source>
          <target state="new">Important APIs</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>Windows.Media.SpeechRecognition</source>
          <target state="new">Windows.Media.SpeechRecognition</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>Speech recognition is made up of a speech runtime, recognition APIs for programming the runtime, ready-to-use grammars for dictation and web search, and a default system UI that helps users discover and use speech recognition features.</source>
          <target state="new">Speech recognition is made up of a speech runtime, recognition APIs for programming the runtime, ready-to-use grammars for dictation and web search, and a default system UI that helps users discover and use speech recognition features.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;span id="Set_up_the_audio_feed"&gt;</ph><ph id="ph2">&lt;/span&gt;</ph><ph id="ph3">&lt;span id="set_up_the_audio_feed"&gt;</ph><ph id="ph4">&lt;/span&gt;</ph><ph id="ph5">&lt;span id="SET_UP_THE_AUDIO_FEED"&gt;</ph><ph id="ph6">&lt;/span&gt;</ph>Set up the audio feed</source>
          <target state="new"><ph id="ph1">&lt;span id="Set_up_the_audio_feed"&gt;</ph><ph id="ph2">&lt;/span&gt;</ph><ph id="ph3">&lt;span id="set_up_the_audio_feed"&gt;</ph><ph id="ph4">&lt;/span&gt;</ph><ph id="ph5">&lt;span id="SET_UP_THE_AUDIO_FEED"&gt;</ph><ph id="ph6">&lt;/span&gt;</ph>Set up the audio feed</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>Ensure that your device has a microphone or the equivalent.</source>
          <target state="new">Ensure that your device has a microphone or the equivalent.</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>Set the <bpt id="p1">**</bpt>Microphone<ept id="p1">**</ept> device capability (<bpt id="p2">[</bpt><bpt id="p3">**</bpt>DeviceCapability<ept id="p3">**</ept><ept id="p2">](https://msdn.microsoft.com/library/windows/apps/br211430)</ept>) in the <bpt id="p4">[</bpt>App package manifest<ept id="p4">](https://msdn.microsoft.com/library/windows/apps/br211474)</ept> (<bpt id="p5">**</bpt>package.appxmanifest<ept id="p5">**</ept> file) to get access to the microphone’s audio feed.</source>
          <target state="new">Set the <bpt id="p1">**</bpt>Microphone<ept id="p1">**</ept> device capability (<bpt id="p2">[</bpt><bpt id="p3">**</bpt>DeviceCapability<ept id="p3">**</ept><ept id="p2">](https://msdn.microsoft.com/library/windows/apps/br211430)</ept>) in the <bpt id="p4">[</bpt>App package manifest<ept id="p4">](https://msdn.microsoft.com/library/windows/apps/br211474)</ept> (<bpt id="p5">**</bpt>package.appxmanifest<ept id="p5">**</ept> file) to get access to the microphone’s audio feed.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>This allows the app to record audio from connected microphones.</source>
          <target state="new">This allows the app to record audio from connected microphones.</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>See <bpt id="p1">[</bpt>App capability declarations<ept id="p1">](https://msdn.microsoft.com/library/windows/apps/mt270968)</ept>.</source>
          <target state="new">See <bpt id="p1">[</bpt>App capability declarations<ept id="p1">](https://msdn.microsoft.com/library/windows/apps/mt270968)</ept>.</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;span id="Recognize_speech_input"&gt;</ph><ph id="ph2">&lt;/span&gt;</ph><ph id="ph3">&lt;span id="recognize_speech_input"&gt;</ph><ph id="ph4">&lt;/span&gt;</ph><ph id="ph5">&lt;span id="RECOGNIZE_SPEECH_INPUT"&gt;</ph><ph id="ph6">&lt;/span&gt;</ph>Recognize speech input</source>
          <target state="new"><ph id="ph1">&lt;span id="Recognize_speech_input"&gt;</ph><ph id="ph2">&lt;/span&gt;</ph><ph id="ph3">&lt;span id="recognize_speech_input"&gt;</ph><ph id="ph4">&lt;/span&gt;</ph><ph id="ph5">&lt;span id="RECOGNIZE_SPEECH_INPUT"&gt;</ph><ph id="ph6">&lt;/span&gt;</ph>Recognize speech input</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>A <bpt id="p1">*</bpt>constraint<ept id="p1">*</ept> defines the words and phrases (vocabulary) that an app recognizes in speech input.</source>
          <target state="new">A <bpt id="p1">*</bpt>constraint<ept id="p1">*</ept> defines the words and phrases (vocabulary) that an app recognizes in speech input.</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>Constraints are at the core of speech recognition and give your app great over the accuracy of speech recognition.</source>
          <target state="new">Constraints are at the core of speech recognition and give your app great over the accuracy of speech recognition.</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>You can use various types of constraints when performing speech recognition:</source>
          <target state="new">You can use various types of constraints when performing speech recognition:</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Predefined grammars<ept id="p1">**</ept> (<bpt id="p2">[</bpt><bpt id="p3">**</bpt>SpeechRecognitionTopicConstraint<ept id="p3">**</ept><ept id="p2">](https://msdn.microsoft.com/library/windows/apps/dn631446)</ept>).</source>
          <target state="new"><bpt id="p1">**</bpt>Predefined grammars<ept id="p1">**</ept> (<bpt id="p2">[</bpt><bpt id="p3">**</bpt>SpeechRecognitionTopicConstraint<ept id="p3">**</ept><ept id="p2">](https://msdn.microsoft.com/library/windows/apps/dn631446)</ept>).</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>Predefined dictation and web-search grammars provide speech recognition for your app without requiring you to author a grammar.</source>
          <target state="new">Predefined dictation and web-search grammars provide speech recognition for your app without requiring you to author a grammar.</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>When using these grammars, speech recognition is performed by a remote web service and the results are returned to the device.</source>
          <target state="new">When using these grammars, speech recognition is performed by a remote web service and the results are returned to the device.</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>The default free-text dictation grammar can recognize most words and phrases that a user can say in a particular language, and is optimized to recognize short phrases.</source>
          <target state="new">The default free-text dictation grammar can recognize most words and phrases that a user can say in a particular language, and is optimized to recognize short phrases.</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>The predefined dictation grammar is used if you don't specify any constraints for your <bpt id="p1">[</bpt><bpt id="p2">**</bpt>SpeechRecognizer<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn653226)</ept> object.</source>
          <target state="new">The predefined dictation grammar is used if you don't specify any constraints for your <bpt id="p1">[</bpt><bpt id="p2">**</bpt>SpeechRecognizer<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn653226)</ept> object.</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>Free-text dictation is useful when you don't want to limit the kinds of things a user can say.</source>
          <target state="new">Free-text dictation is useful when you don't want to limit the kinds of things a user can say.</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>Typical uses include creating notes or dictating the content for a message.</source>
          <target state="new">Typical uses include creating notes or dictating the content for a message.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>The web-search grammar, like a dictation grammar, contains a large number of words and phrases that a user might say.</source>
          <target state="new">The web-search grammar, like a dictation grammar, contains a large number of words and phrases that a user might say.</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>However, it is optimized to recognize terms that people typically use when searching the web.</source>
          <target state="new">However, it is optimized to recognize terms that people typically use when searching the web.</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Note<ept id="p1">**</ept>  Because predefined dictation and web-search grammars can be large, and because they are online (not on the device), performance might not be as fast as with a custom grammar installed on the device.</source>
          <target state="new"><bpt id="p1">**</bpt>Note<ept id="p1">**</ept>  Because predefined dictation and web-search grammars can be large, and because they are online (not on the device), performance might not be as fast as with a custom grammar installed on the device.</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Programmatic list constraints<ept id="p1">**</ept> (<bpt id="p2">[</bpt><bpt id="p3">**</bpt>SpeechRecognitionListConstraint<ept id="p3">**</ept><ept id="p2">](https://msdn.microsoft.com/library/windows/apps/dn631421)</ept>).</source>
          <target state="new"><bpt id="p1">**</bpt>Programmatic list constraints<ept id="p1">**</ept> (<bpt id="p2">[</bpt><bpt id="p3">**</bpt>SpeechRecognitionListConstraint<ept id="p3">**</ept><ept id="p2">](https://msdn.microsoft.com/library/windows/apps/dn631421)</ept>).</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>Programmatic list constraints provide a lightweight approach to creating simple grammars using a list of words or phrases.</source>
          <target state="new">Programmatic list constraints provide a lightweight approach to creating simple grammars using a list of words or phrases.</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>A list constraint works well for recognizing short, distinct phrases.</source>
          <target state="new">A list constraint works well for recognizing short, distinct phrases.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>Explicitly specifying all words in a grammar also improves recognition accuracy, as the speech recognition engine must only process speech to confirm a match.</source>
          <target state="new">Explicitly specifying all words in a grammar also improves recognition accuracy, as the speech recognition engine must only process speech to confirm a match.</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>The list can also be programmatically updated.</source>
          <target state="new">The list can also be programmatically updated.</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>A list constraint consists of an array of strings that represents speech input that your app will accept for a recognition operation.</source>
          <target state="new">A list constraint consists of an array of strings that represents speech input that your app will accept for a recognition operation.</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>You can create a list constraint in your app by creating a speech-recognition list-constraint object and passing an array of strings.</source>
          <target state="new">You can create a list constraint in your app by creating a speech-recognition list-constraint object and passing an array of strings.</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>Then, add that object to the constraints collection of the recognizer.</source>
          <target state="new">Then, add that object to the constraints collection of the recognizer.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>Recognition is successful when the speech recognizer recognizes any one of the strings in the array.</source>
          <target state="new">Recognition is successful when the speech recognizer recognizes any one of the strings in the array.</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>SRGS grammars<ept id="p1">**</ept> (<bpt id="p2">[</bpt><bpt id="p3">**</bpt>SpeechRecognitionGrammarFileConstraint<ept id="p3">**</ept><ept id="p2">](https://msdn.microsoft.com/library/windows/apps/dn631412)</ept>).</source>
          <target state="new"><bpt id="p1">**</bpt>SRGS grammars<ept id="p1">**</ept> (<bpt id="p2">[</bpt><bpt id="p3">**</bpt>SpeechRecognitionGrammarFileConstraint<ept id="p3">**</ept><ept id="p2">](https://msdn.microsoft.com/library/windows/apps/dn631412)</ept>).</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>An Speech Recognition Grammar Specification (SRGS) grammar is a static document that, unlike a programmatic list constraint, uses the XML format defined by the <bpt id="p1">[</bpt>SRGS Version 1.0<ept id="p1">](http://go.microsoft.com/fwlink/p/?LinkID=262302)</ept>.</source>
          <target state="new">An Speech Recognition Grammar Specification (SRGS) grammar is a static document that, unlike a programmatic list constraint, uses the XML format defined by the <bpt id="p1">[</bpt>SRGS Version 1.0<ept id="p1">](http://go.microsoft.com/fwlink/p/?LinkID=262302)</ept>.</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>An SRGS grammar provides the greatest control over the speech recognition experience by letting you capture multiple semantic meanings in a single recognition.</source>
          <target state="new">An SRGS grammar provides the greatest control over the speech recognition experience by letting you capture multiple semantic meanings in a single recognition.</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Voice command constraints<ept id="p1">**</ept> (<bpt id="p2">[</bpt><bpt id="p3">**</bpt>SpeechRecognitionVoiceCommandDefinitionConstraint<ept id="p3">**</ept><ept id="p2">](https://msdn.microsoft.com/library/windows/apps/dn653220)</ept>)</source>
          <target state="new"><bpt id="p1">**</bpt>Voice command constraints<ept id="p1">**</ept> (<bpt id="p2">[</bpt><bpt id="p3">**</bpt>SpeechRecognitionVoiceCommandDefinitionConstraint<ept id="p3">**</ept><ept id="p2">](https://msdn.microsoft.com/library/windows/apps/dn653220)</ept>)</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>Use a Voice Command Definition (VCD) XML file to define the commands that the user can say to initiate actions when activating your app.</source>
          <target state="new">Use a Voice Command Definition (VCD) XML file to define the commands that the user can say to initiate actions when activating your app.</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>For more detail, see <bpt id="p1">[</bpt>Launch a foreground app with voice commands in Cortana<ept id="p1">](launch-a-foreground-app-with-voice-commands-in-cortana.md)</ept>.</source>
          <target state="new">For more detail, see <bpt id="p1">[</bpt>Launch a foreground app with voice commands in Cortana<ept id="p1">](launch-a-foreground-app-with-voice-commands-in-cortana.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Note<ept id="p1">**</ept>  Which type of constraint type you use depends on the complexity of the recognition experience you want to create.</source>
          <target state="new"><bpt id="p1">**</bpt>Note<ept id="p1">**</ept>  Which type of constraint type you use depends on the complexity of the recognition experience you want to create.</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>Any could be the best choice for a specific recognition task, and you might find uses for all types of constraints in your app.</source>
          <target state="new">Any could be the best choice for a specific recognition task, and you might find uses for all types of constraints in your app.</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>To get started with constraints, see <bpt id="p1">[</bpt>Define custom recognition constraints<ept id="p1">](define-custom-recognition-constraints.md)</ept>.</source>
          <target state="new">To get started with constraints, see <bpt id="p1">[</bpt>Define custom recognition constraints<ept id="p1">](define-custom-recognition-constraints.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>The predefined Universal Windows app dictation grammar recognizes most words and short phrases in a language.</source>
          <target state="new">The predefined Universal Windows app dictation grammar recognizes most words and short phrases in a language.</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>It is activated by default when a speech recognizer object is instantiated without custom constraints.</source>
          <target state="new">It is activated by default when a speech recognizer object is instantiated without custom constraints.</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>In this example, we show how to:</source>
          <target state="new">In this example, we show how to:</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>Create a speech recognizer.</source>
          <target state="new">Create a speech recognizer.</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>Compile the default Universal Windows app constraints (no grammars have been added to the speech recognizer's grammar set).</source>
          <target state="new">Compile the default Universal Windows app constraints (no grammars have been added to the speech recognizer's grammar set).</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source>Start listening for speech by using the basic recognition UI and TTS feedback provided by the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>RecognizeWithUIAsync<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn653245)</ept> method.</source>
          <target state="new">Start listening for speech by using the basic recognition UI and TTS feedback provided by the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>RecognizeWithUIAsync<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn653245)</ept> method.</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>Use the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>RecognizeAsync<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn653244)</ept> method if the default UI is not required.</source>
          <target state="new">Use the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>RecognizeAsync<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn653244)</ept> method if the default UI is not required.</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;span id="Customize_the_recognition_UI"&gt;</ph><ph id="ph2">&lt;/span&gt;</ph><ph id="ph3">&lt;span id="customize_the_recognition_ui"&gt;</ph><ph id="ph4">&lt;/span&gt;</ph><ph id="ph5">&lt;span id="CUSTOMIZE_THE_RECOGNITION_UI"&gt;</ph><ph id="ph6">&lt;/span&gt;</ph>Customize the recognition UI</source>
          <target state="new"><ph id="ph1">&lt;span id="Customize_the_recognition_UI"&gt;</ph><ph id="ph2">&lt;/span&gt;</ph><ph id="ph3">&lt;span id="customize_the_recognition_ui"&gt;</ph><ph id="ph4">&lt;/span&gt;</ph><ph id="ph5">&lt;span id="CUSTOMIZE_THE_RECOGNITION_UI"&gt;</ph><ph id="ph6">&lt;/span&gt;</ph>Customize the recognition UI</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>When your app attempts speech recognition by calling <bpt id="p1">[</bpt><bpt id="p2">**</bpt>SpeechRecognizer.RecognizeWithUIAsync<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn653245)</ept>, several screens are shown in the following order.</source>
          <target state="new">When your app attempts speech recognition by calling <bpt id="p1">[</bpt><bpt id="p2">**</bpt>SpeechRecognizer.RecognizeWithUIAsync<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn653245)</ept>, several screens are shown in the following order.</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>If you're using a constraint based on a predefined grammar (dictation or web search):</source>
          <target state="new">If you're using a constraint based on a predefined grammar (dictation or web search):</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>Listening<ept id="p1">**</ept> screen.</source>
          <target state="new">The <bpt id="p1">**</bpt>Listening<ept id="p1">**</ept> screen.</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>Thinking<ept id="p1">**</ept> screen.</source>
          <target state="new">The <bpt id="p1">**</bpt>Thinking<ept id="p1">**</ept> screen.</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>Heard you say<ept id="p1">**</ept> screen or the error screen.</source>
          <target state="new">The <bpt id="p1">**</bpt>Heard you say<ept id="p1">**</ept> screen or the error screen.</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>If you're using a constraint based on a list of words or phrases, or a constraint based on a SRGS grammar file:</source>
          <target state="new">If you're using a constraint based on a list of words or phrases, or a constraint based on a SRGS grammar file:</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>Listening<ept id="p1">**</ept> screen.</source>
          <target state="new">The <bpt id="p1">**</bpt>Listening<ept id="p1">**</ept> screen.</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>Did you say<ept id="p1">**</ept> screen, if what the user said could be interpreted as more than one potential result.</source>
          <target state="new">The <bpt id="p1">**</bpt>Did you say<ept id="p1">**</ept> screen, if what the user said could be interpreted as more than one potential result.</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>Heard you say<ept id="p1">**</ept> screen or the error screen.</source>
          <target state="new">The <bpt id="p1">**</bpt>Heard you say<ept id="p1">**</ept> screen or the error screen.</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source>The following image shows an example of the flow between screens for a speech recognizer that uses a constraint based on a SRGS grammar file.</source>
          <target state="new">The following image shows an example of the flow between screens for a speech recognizer that uses a constraint based on a SRGS grammar file.</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source>In this example, speech recognition was successful.</source>
          <target state="new">In this example, speech recognition was successful.</target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source>initial recognition screen for a constraint based on a sgrs grammar file</source>
          <target state="new">initial recognition screen for a constraint based on a sgrs grammar file</target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>intermediate recognition screen for a constraint based on a sgrs grammar file</source>
          <target state="new">intermediate recognition screen for a constraint based on a sgrs grammar file</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>final recognition screen for a constraint based on a sgrs grammar file</source>
          <target state="new">final recognition screen for a constraint based on a sgrs grammar file</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>Listening<ept id="p1">**</ept> screen can provide examples of words or phrases that the app can recognize.</source>
          <target state="new">The <bpt id="p1">**</bpt>Listening<ept id="p1">**</ept> screen can provide examples of words or phrases that the app can recognize.</target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source>Here, we show how to use the properties of the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>SpeechRecognizerUIOptions<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn653234)</ept> class (obtained by calling the <bpt id="p3">[</bpt><bpt id="p4">**</bpt>SpeechRecognizer.UIOptions<ept id="p4">**</ept><ept id="p3">](https://msdn.microsoft.com/library/windows/apps/dn653254)</ept> property) to customize content on the <bpt id="p5">**</bpt>Listening<ept id="p5">**</ept> screen.</source>
          <target state="new">Here, we show how to use the properties of the <bpt id="p1">[</bpt><bpt id="p2">**</bpt>SpeechRecognizerUIOptions<ept id="p2">**</ept><ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn653234)</ept> class (obtained by calling the <bpt id="p3">[</bpt><bpt id="p4">**</bpt>SpeechRecognizer.UIOptions<ept id="p4">**</ept><ept id="p3">](https://msdn.microsoft.com/library/windows/apps/dn653254)</ept> property) to customize content on the <bpt id="p5">**</bpt>Listening<ept id="p5">**</ept> screen.</target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source><ph id="ph1">&lt;span id="related_topics"&gt;</ph><ph id="ph2">&lt;/span&gt;</ph>Related articles</source>
          <target state="new"><ph id="ph1">&lt;span id="related_topics"&gt;</ph><ph id="ph2">&lt;/span&gt;</ph>Related articles</target>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source>Developers</source>
          <target state="new">Developers</target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>Speech interactions<ept id="p1">](speech-interactions.md)</ept></source>
          <target state="new"><bpt id="p1">[</bpt>Speech interactions<ept id="p1">](speech-interactions.md)</ept></target>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Designers<ept id="p1">**</ept></source>
          <target state="new"><bpt id="p1">**</bpt>Designers<ept id="p1">**</ept></target>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>Speech design guidelines<ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn596121)</ept></source>
          <target state="new"><bpt id="p1">[</bpt>Speech design guidelines<ept id="p1">](https://msdn.microsoft.com/library/windows/apps/dn596121)</ept></target>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Samples<ept id="p1">**</ept></source>
          <target state="new"><bpt id="p1">**</bpt>Samples<ept id="p1">**</ept></target>
        </trans-unit>
        <trans-unit id="180" translate="yes" xml:space="preserve">
          <source>Speech recognition and speech synthesis sample</source>
          <target state="new">Speech recognition and speech synthesis sample</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">
    </xliffext:oltranslationpriority>
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">6a84cec5bbbfad28407eb421330cebbb73b2ce90</xliffext:olfilehash>
  </header>
</xliff>