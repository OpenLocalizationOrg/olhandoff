{
  "nodes": [
    {
      "pos": [
        27,
        97
      ],
      "content": "Feature engineering in the Cortana Analytics Process | Microsoft Azure"
    },
    {
      "pos": [
        117,
        248
      ],
      "content": "Explains the purposes of feature engineering and provides examples of its role in the data enhancement process of machine learning."
    },
    {
      "pos": [
        581,
        633
      ],
      "content": "Feature engineering in the Cortana Analytics Process"
    },
    {
      "pos": [
        636,
        798
      ],
      "content": "Feature engineering attempts to increase the predictive power of learning algorithms by creating features from raw data that help facilitate the learning process."
    },
    {
      "pos": [
        800,
        1147
      ],
      "content": "<ph id=\"ph2\">[AZURE.INCLUDE [cap-create-features-data-selector](../../includes/cap-create-features-selector.md)]</ph>\nThis <bpt id=\"p1\">**</bpt>menu<ept id=\"p1\">**</ept><ph id=\"ph3\"/> links to topics that describe how to create features for data in various environments. This task is a step in the <bpt id=\"p2\">[</bpt>Cortana Analytics Process (CAP)<ept id=\"p2\">](https://azure.microsoft.com/documentation/learning-paths/cortana-analytics-process/)</ept>.",
      "nodes": [
        {
          "content": "<ph id=\"ph2\">[AZURE.INCLUDE [cap-create-features-data-selector](../../includes/cap-create-features-selector.md)]</ph>\nThis <bpt id=\"p1\">**</bpt>menu<ept id=\"p1\">**</ept><ph id=\"ph3\"/> links to topics that describe how to create features for data in various environments.",
          "pos": [
            0,
            270
          ]
        },
        {
          "content": "This task is a step in the <bpt id=\"p2\">[</bpt>Cortana Analytics Process (CAP)<ept id=\"p2\">](https://azure.microsoft.com/documentation/learning-paths/cortana-analytics-process/)</ept>.",
          "pos": [
            271,
            455
          ]
        }
      ]
    },
    {
      "pos": [
        1151,
        1163
      ],
      "content": "Introduction"
    },
    {
      "pos": [
        1165,
        1368
      ],
      "content": "This topic explains the purposes of feature engineering and provides examples of its role in the data enhancement process of machine learning. These examples are drawn from Azure Machine Learning Studio.",
      "nodes": [
        {
          "content": "This topic explains the purposes of feature engineering and provides examples of its role in the data enhancement process of machine learning.",
          "pos": [
            0,
            142
          ]
        },
        {
          "content": "These examples are drawn from Azure Machine Learning Studio.",
          "pos": [
            143,
            203
          ]
        }
      ]
    },
    {
      "pos": [
        1371,
        1653
      ],
      "content": "The engineering and selection of features is one part of the CAP process outlined in the <bpt id=\"p3\">[</bpt>What is the Cortana Analytics Process?<ept id=\"p3\">](machine-learning-data-science-the-cortana-analytics-process.md)</ept><ph id=\"ph4\"/> Feature engineering and selection are parts of the <bpt id=\"p4\">**</bpt>Develop features<ept id=\"p4\">**</ept><ph id=\"ph5\"/> step of the CAP."
    },
    {
      "pos": [
        1657,
        1850
      ],
      "content": "<bpt id=\"p5\">**</bpt>feature engineering<ept id=\"p5\">**</ept>: This process attempts to create additional relevant features from the existing raw features in the data, and to increase the predictive power of the learning algorithm."
    },
    {
      "pos": [
        1853,
        2005
      ],
      "content": "<bpt id=\"p6\">**</bpt>feature selection<ept id=\"p6\">**</ept>: This process selects the key subset of original data features in an attempt to reduce the dimensionality of the training problem."
    },
    {
      "pos": [
        2007,
        2209
      ],
      "content": "Normally <bpt id=\"p7\">**</bpt>feature engineering<ept id=\"p7\">**</ept><ph id=\"ph6\"/> is applied first to generate additional features, and then the <bpt id=\"p8\">**</bpt>feature selection<ept id=\"p8\">**</ept><ph id=\"ph7\"/> step is performed to eliminate irrelevant, redundant, or highly correlated features."
    },
    {
      "pos": [
        2211,
        2646
      ],
      "content": "The training data used in machine learning can often be enhanced by extraction of features from the raw data collected. An example of an engineered feature in the context of learning how to classify the images of handwritten characters is creation of a bit density map constructed from the raw bit distribution data. This map can help locate the edges of the characters more efficiently than simply using the raw distribution directly.",
      "nodes": [
        {
          "content": "The training data used in machine learning can often be enhanced by extraction of features from the raw data collected.",
          "pos": [
            0,
            119
          ]
        },
        {
          "content": "An example of an engineered feature in the context of learning how to classify the images of handwritten characters is creation of a bit density map constructed from the raw bit distribution data.",
          "pos": [
            120,
            316
          ]
        },
        {
          "content": "This map can help locate the edges of the characters more efficiently than simply using the raw distribution directly.",
          "pos": [
            317,
            435
          ]
        }
      ]
    },
    {
      "pos": [
        2747,
        2801
      ],
      "content": "Creating Features from Your Data - Feature Engineering"
    },
    {
      "pos": [
        2803,
        3357
      ],
      "content": "The training data consists of a matrix composed of examples (records or observations stored in rows), each of which has a set of features (variables or fields stored in columns). The features specified in the experimental design are expected to characterize the patterns in the data. Although many of the raw data fields can be directly included in the selected feature set used to train a model, it is often the case that additional (engineered) features need to be constructed from the features in the raw data to generate an enhanced training dataset.",
      "nodes": [
        {
          "content": "The training data consists of a matrix composed of examples (records or observations stored in rows), each of which has a set of features (variables or fields stored in columns).",
          "pos": [
            0,
            178
          ]
        },
        {
          "content": "The features specified in the experimental design are expected to characterize the patterns in the data.",
          "pos": [
            179,
            283
          ]
        },
        {
          "content": "Although many of the raw data fields can be directly included in the selected feature set used to train a model, it is often the case that additional (engineered) features need to be constructed from the features in the raw data to generate an enhanced training dataset.",
          "pos": [
            284,
            554
          ]
        }
      ]
    },
    {
      "pos": [
        3359,
        3822
      ],
      "content": "What kind of features should be created to enhance the dataset when training a model? Engineered features that enhance the training provide information that better differentiates the patterns in the data. We expect the new features to provide additional information that is not clearly captured or easily apparent in the original or existing feature set. But this process is something of an art. Sound and productive decisions often require some domain expertise.",
      "nodes": [
        {
          "content": "What kind of features should be created to enhance the dataset when training a model?",
          "pos": [
            0,
            85
          ]
        },
        {
          "content": "Engineered features that enhance the training provide information that better differentiates the patterns in the data.",
          "pos": [
            86,
            204
          ]
        },
        {
          "content": "We expect the new features to provide additional information that is not clearly captured or easily apparent in the original or existing feature set.",
          "pos": [
            205,
            354
          ]
        },
        {
          "content": "But this process is something of an art.",
          "pos": [
            355,
            395
          ]
        },
        {
          "content": "Sound and productive decisions often require some domain expertise.",
          "pos": [
            396,
            463
          ]
        }
      ]
    },
    {
      "pos": [
        3824,
        3984
      ],
      "content": "When starting with Azure Machine Learning, it is easiest to grasp this process concretely using samples provided in the Studio. Two examples are presented here:",
      "nodes": [
        {
          "content": "When starting with Azure Machine Learning, it is easiest to grasp this process concretely using samples provided in the Studio.",
          "pos": [
            0,
            127
          ]
        },
        {
          "content": "Two examples are presented here:",
          "pos": [
            128,
            160
          ]
        }
      ]
    },
    {
      "pos": [
        3988,
        4180
      ],
      "content": "A regression example <bpt id=\"p9\">[</bpt>Prediction of the number of bike rentals<ept id=\"p9\">](../machine-learning-sample-prediction-of-number-of-bike-rentals.md)</ept><ph id=\"ph9\"/> in a supervised experiment where the target values are known"
    },
    {
      "pos": [
        4183,
        4323
      ],
      "content": "A text mining classification example using <bpt id=\"p10\">[</bpt>Feature Hashing<ept id=\"p10\">](https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/)</ept>"
    },
    {
      "pos": [
        4329,
        4385
      ],
      "content": "Example 1: Adding Temporal Features for Regression Model"
    },
    {
      "pos": [
        4391,
        5313
      ],
      "content": "Let's use the experiment \"Demand forecasting of bikes\" in Azure Machine Learning Studio to demonstrate how to engineer features for a regression task. The objective of this experiment is to predict the demand for the bikes, that is, the number of bike rentals within a specific month/day/hour. The dataset \"Bike Rental UCI dataset\" is used as the raw input data. This dataset is based on real data from the Capital Bikeshare company that maintains a bike rental network in Washington DC in the United States. The dataset represents the number of bike rentals within a specific hour of a day in the years 2011 and year 2012 and contains 17379 rows and 17 columns. The raw feature set contains weather conditions (temperature/humidity/wind speed) and the type of the day (holiday/weekday). The field to predict is \"cnt\", a count which represents the bike rentals within a specific hour and which ranges ranges from 1 to 977.",
      "nodes": [
        {
          "content": "Let's use the experiment \"Demand forecasting of bikes\" in Azure Machine Learning Studio to demonstrate how to engineer features for a regression task.",
          "pos": [
            0,
            150
          ]
        },
        {
          "content": "The objective of this experiment is to predict the demand for the bikes, that is, the number of bike rentals within a specific month/day/hour.",
          "pos": [
            151,
            293
          ]
        },
        {
          "content": "The dataset \"Bike Rental UCI dataset\" is used as the raw input data.",
          "pos": [
            294,
            362
          ]
        },
        {
          "content": "This dataset is based on real data from the Capital Bikeshare company that maintains a bike rental network in Washington DC in the United States.",
          "pos": [
            363,
            508
          ]
        },
        {
          "content": "The dataset represents the number of bike rentals within a specific hour of a day in the years 2011 and year 2012 and contains 17379 rows and 17 columns.",
          "pos": [
            509,
            662
          ]
        },
        {
          "content": "The raw feature set contains weather conditions (temperature/humidity/wind speed) and the type of the day (holiday/weekday).",
          "pos": [
            663,
            787
          ]
        },
        {
          "content": "The field to predict is \"cnt\", a count which represents the bike rentals within a specific hour and which ranges ranges from 1 to 977.",
          "pos": [
            788,
            922
          ]
        }
      ]
    },
    {
      "pos": [
        5315,
        5635
      ],
      "content": "With the goal of constructing effective features in the training data, four regression models are built using the same algorithm but with four different training datasets. The four datasets represent the same raw input data, but with an increasing number of features set. These features are grouped into four categories:",
      "nodes": [
        {
          "content": "With the goal of constructing effective features in the training data, four regression models are built using the same algorithm but with four different training datasets.",
          "pos": [
            0,
            171
          ]
        },
        {
          "content": "The four datasets represent the same raw input data, but with an increasing number of features set.",
          "pos": [
            172,
            271
          ]
        },
        {
          "content": "These features are grouped into four categories:",
          "pos": [
            272,
            320
          ]
        }
      ]
    },
    {
      "pos": [
        5640,
        5712
      ],
      "content": "A = weather + holiday + weekday + weekend features for the predicted day"
    },
    {
      "pos": [
        5716,
        5785
      ],
      "content": "B = number of bikes that were rented in each of the previous 12 hours"
    },
    {
      "pos": [
        5789,
        5874
      ],
      "content": "C = number of bikes that were rented in each of the previous 12 days at the same hour"
    },
    {
      "pos": [
        5878,
        5981
      ],
      "content": "D = number of bikes that were rented in each of the previous 12 weeks at the same hour and the same day"
    },
    {
      "pos": [
        5983,
        6446
      ],
      "content": "Besides feature set A, which already exist in the original raw data, the other three sets of features are created through the feature engineering process. Feature set B captures very recent demand for the bikes. Feature set C captures the demand for bikes at a particular hour. Feature set D captures demand for bikes at particular hour and particular day of the week. The four training datasets each includes feature set A, A+B, A+B+C, and A+B+C+D, respectively.",
      "nodes": [
        {
          "content": "Besides feature set A, which already exist in the original raw data, the other three sets of features are created through the feature engineering process.",
          "pos": [
            0,
            154
          ]
        },
        {
          "content": "Feature set B captures very recent demand for the bikes.",
          "pos": [
            155,
            211
          ]
        },
        {
          "content": "Feature set C captures the demand for bikes at a particular hour.",
          "pos": [
            212,
            277
          ]
        },
        {
          "content": "Feature set D captures demand for bikes at particular hour and particular day of the week.",
          "pos": [
            278,
            368
          ]
        },
        {
          "content": "The four training datasets each includes feature set A, A+B, A+B+C, and A+B+C+D, respectively.",
          "pos": [
            369,
            463
          ]
        }
      ]
    },
    {
      "pos": [
        6448,
        6987
      ],
      "content": "In the Azure Machine Learning experiment, these four training datasets are formed via four branches from the pre-processed input dataset. Except the left most branch, each of these branches contains an <bpt id=\"p11\">[</bpt>Execute R Script<ept id=\"p11\">](https://msdn.microsoft.com/library/azure/30806023-392b-42e0-94d6-6b775a6e0fd5/)</ept><ph id=\"ph10\"/> module, in which a set of derived features (feature set B, C, and D) are respectively constructed and appended to the imported dataset. The following figure demonstrates the R script used to create feature set B in the second left branch.",
      "nodes": [
        {
          "content": "In the Azure Machine Learning experiment, these four training datasets are formed via four branches from the pre-processed input dataset.",
          "pos": [
            0,
            137
          ]
        },
        {
          "content": "Except the left most branch, each of these branches contains an <bpt id=\"p11\">[</bpt>Execute R Script<ept id=\"p11\">](https://msdn.microsoft.com/library/azure/30806023-392b-42e0-94d6-6b775a6e0fd5/)</ept><ph id=\"ph10\"/> module, in which a set of derived features (feature set B, C, and D) are respectively constructed and appended to the imported dataset.",
          "pos": [
            138,
            491
          ]
        },
        {
          "content": "The following figure demonstrates the R script used to create feature set B in the second left branch.",
          "pos": [
            492,
            594
          ]
        }
      ]
    },
    {
      "pos": [
        6989,
        7086
      ],
      "content": "<ph id=\"ph11\">![</ph>create features<ph id=\"ph12\">](./media/machine-learning-data-science-create-features/addFeature-Rscripts.png)</ph>"
    },
    {
      "pos": [
        7088,
        7544
      ],
      "content": "The comparison of the performance results of the four models are summarized in the following table. The best results are shown by features A+B+C. Note that the error rate decreases when additional feature set are included in the training data. It verifies our presumption that the feature set B, C provide additional relevant information for the regression task. But adding the D feature does not seem to provide any additional reduction in the error rate.",
      "nodes": [
        {
          "content": "The comparison of the performance results of the four models are summarized in the following table.",
          "pos": [
            0,
            99
          ]
        },
        {
          "content": "The best results are shown by features A+B+C.",
          "pos": [
            100,
            145
          ]
        },
        {
          "content": "Note that the error rate decreases when additional feature set are included in the training data.",
          "pos": [
            146,
            243
          ]
        },
        {
          "content": "It verifies our presumption that the feature set B, C provide additional relevant information for the regression task.",
          "pos": [
            244,
            362
          ]
        },
        {
          "content": "But adding the D feature does not seem to provide any additional reduction in the error rate.",
          "pos": [
            363,
            456
          ]
        }
      ]
    },
    {
      "pos": [
        7546,
        7633
      ],
      "content": "<ph id=\"ph13\">![</ph>result comparison<ph id=\"ph14\">](./media/machine-learning-data-science-create-features/result1.png)</ph>"
    },
    {
      "pos": [
        7663,
        7706
      ],
      "content": "Example 2: Creating Features in Text Mining"
    },
    {
      "pos": [
        7710,
        8360
      ],
      "content": "Feature engineering is widely applied in tasks related to text mining, such as document classification and sentiment analysis. For example, when we want to classify documents into several categories, a typical assumption is that the word/phrases included in one doc category are less likely to occur in another doc category. In another words, the frequency of the words/phrases distribution is able to characterize different document categories. In text mining applications, because individual pieces of text-contents usually serve as the input data, the feature engineering process is needed to create the features involving word/phrase frequencies.",
      "nodes": [
        {
          "content": "Feature engineering is widely applied in tasks related to text mining, such as document classification and sentiment analysis.",
          "pos": [
            0,
            126
          ]
        },
        {
          "content": "For example, when we want to classify documents into several categories, a typical assumption is that the word/phrases included in one doc category are less likely to occur in another doc category.",
          "pos": [
            127,
            324
          ]
        },
        {
          "content": "In another words, the frequency of the words/phrases distribution is able to characterize different document categories.",
          "pos": [
            325,
            445
          ]
        },
        {
          "content": "In text mining applications, because individual pieces of text-contents usually serve as the input data, the feature engineering process is needed to create the features involving word/phrase frequencies.",
          "pos": [
            446,
            650
          ]
        }
      ]
    },
    {
      "pos": [
        8362,
        8686
      ],
      "content": "To achieve this task, a technique called <bpt id=\"p12\">**</bpt>feature hashing<ept id=\"p12\">**</ept><ph id=\"ph15\"/> is applied to efficiently turn arbitrary text features into indices. Instead of associating each text feature (words/phrases) to a particular index, this method functions by applying a hash function to the features and using their hash values as indices directly.",
      "nodes": [
        {
          "content": "To achieve this task, a technique called <bpt id=\"p12\">**</bpt>feature hashing<ept id=\"p12\">**</ept><ph id=\"ph15\"/> is applied to efficiently turn arbitrary text features into indices.",
          "pos": [
            0,
            184
          ]
        },
        {
          "content": "Instead of associating each text feature (words/phrases) to a particular index, this method functions by applying a hash function to the features and using their hash values as indices directly.",
          "pos": [
            185,
            379
          ]
        }
      ]
    },
    {
      "pos": [
        8688,
        9379
      ],
      "content": "In Azure Machine Learning, there is a <bpt id=\"p13\">[</bpt>Feature Hashing<ept id=\"p13\">](https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/)</ept><ph id=\"ph16\"/> module that creates these word/phrase features conveniently. Following figure shows an example of using this module. The input dataset contains two columns: the book rating ranging from 1 to 5, and the actual review content. The goal of this <bpt id=\"p14\">[</bpt>Feature Hashing<ept id=\"p14\">](https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/)</ept><ph id=\"ph17\"/> module is to retrieve a bunch of new features that show the occurrence frequency of the corresponding word(s)/phrase(s) within the particular book review. To use this module, we need to complete the following steps:",
      "nodes": [
        {
          "content": "In Azure Machine Learning, there is a <bpt id=\"p13\">[</bpt>Feature Hashing<ept id=\"p13\">](https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/)</ept><ph id=\"ph16\"/> module that creates these word/phrase features conveniently.",
          "pos": [
            0,
            251
          ]
        },
        {
          "content": "Following figure shows an example of using this module.",
          "pos": [
            252,
            307
          ]
        },
        {
          "content": "The input dataset contains two columns: the book rating ranging from 1 to 5, and the actual review content.",
          "pos": [
            308,
            415
          ]
        },
        {
          "content": "The goal of this <bpt id=\"p14\">[</bpt>Feature Hashing<ept id=\"p14\">](https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/)</ept><ph id=\"ph17\"/> module is to retrieve a bunch of new features that show the occurrence frequency of the corresponding word(s)/phrase(s) within the particular book review.",
          "pos": [
            416,
            740
          ]
        },
        {
          "content": "To use this module, we need to complete the following steps:",
          "pos": [
            741,
            801
          ]
        }
      ]
    },
    {
      "pos": [
        9383,
        9462
      ],
      "content": "First, select the column that contains the input text (\"Col2\" in this example)."
    },
    {
      "pos": [
        9465,
        9773
      ],
      "content": "Second, set the \"Hashing bitsize\" to 8, which means 2^8=256 features will be created. The word/phase in all the text will be hashed to 256 indices. The parameter \"Hashing bitsize\" ranges from 1 to 31. The word(s)/phrase(s) are less likely to be hashed into the same index if setting it to be a larger number.",
      "nodes": [
        {
          "content": "Second, set the \"Hashing bitsize\" to 8, which means 2^8=256 features will be created.",
          "pos": [
            0,
            85
          ]
        },
        {
          "content": "The word/phase in all the text will be hashed to 256 indices.",
          "pos": [
            86,
            147
          ]
        },
        {
          "content": "The parameter \"Hashing bitsize\" ranges from 1 to 31.",
          "pos": [
            148,
            200
          ]
        },
        {
          "content": "The word(s)/phrase(s) are less likely to be hashed into the same index if setting it to be a larger number.",
          "pos": [
            201,
            308
          ]
        }
      ]
    },
    {
      "pos": [
        9776,
        10104
      ],
      "content": "Third, set the parameter \"N-grams\" to 2. This gets the occurrence frequency of unigrams (a feature for every single word) and bigrams (a feature for every pair of adjacent words) from the input text. The parameter \"N-grams\" ranges from 0 to 10, which indicates the maximum number of sequential words to be included in a feature.",
      "nodes": [
        {
          "content": "Third, set the parameter \"N-grams\" to 2.",
          "pos": [
            0,
            40
          ]
        },
        {
          "content": "This gets the occurrence frequency of unigrams (a feature for every single word) and bigrams (a feature for every pair of adjacent words) from the input text.",
          "pos": [
            41,
            199
          ]
        },
        {
          "content": "The parameter \"N-grams\" ranges from 0 to 10, which indicates the maximum number of sequential words to be included in a feature.",
          "pos": [
            200,
            328
          ]
        }
      ]
    },
    {
      "pos": [
        10108,
        10211
      ],
      "content": "<ph id=\"ph18\">![</ph>\"Feature Hashing\" module<ph id=\"ph19\">](./media/machine-learning-data-science-create-features/feature-Hashing1.png)</ph>"
    },
    {
      "pos": [
        10213,
        10277
      ],
      "content": "The following figure shows what the these new feature look like."
    },
    {
      "pos": [
        10279,
        10383
      ],
      "content": "<ph id=\"ph20\">![</ph>\"Feature Hashing\" example<ph id=\"ph21\">](./media/machine-learning-data-science-create-features/feature-Hashing2.png)</ph>"
    },
    {
      "pos": [
        10389,
        10399
      ],
      "content": "Conclusion"
    },
    {
      "pos": [
        10401,
        11077
      ],
      "content": "Engineered and selected features increase the efficiency of the training process which attempts to extract the key information contained in the data. They also improve the power of these models to classify the input data accurately and to predict outcomes of interest more robustly. Feature engineering and selection can also combine to make the learning more computationally tractable. It does so by enhancing and then reducing the number of features needed to calibrate or train a model. Mathematically speaking, the features selected to train the model are a minimal set of independent variables that explain the patterns in the data and then predict outcomes successfully.",
      "nodes": [
        {
          "content": "Engineered and selected features increase the efficiency of the training process which attempts to extract the key information contained in the data.",
          "pos": [
            0,
            149
          ]
        },
        {
          "content": "They also improve the power of these models to classify the input data accurately and to predict outcomes of interest more robustly.",
          "pos": [
            150,
            282
          ]
        },
        {
          "content": "Feature engineering and selection can also combine to make the learning more computationally tractable.",
          "pos": [
            283,
            386
          ]
        },
        {
          "content": "It does so by enhancing and then reducing the number of features needed to calibrate or train a model.",
          "pos": [
            387,
            489
          ]
        },
        {
          "content": "Mathematically speaking, the features selected to train the model are a minimal set of independent variables that explain the patterns in the data and then predict outcomes successfully.",
          "pos": [
            490,
            676
          ]
        }
      ]
    },
    {
      "pos": [
        11079,
        11298
      ],
      "content": "Note that it is not always necessarily to perform feature engineering or feature selection. Whether it is needed or not depends on the data we have or collect, the algorithm we pick, and the objective of the experiment.",
      "nodes": [
        {
          "content": "Note that it is not always necessarily to perform feature engineering or feature selection.",
          "pos": [
            0,
            91
          ]
        },
        {
          "content": "Whether it is needed or not depends on the data we have or collect, the algorithm we pick, and the objective of the experiment.",
          "pos": [
            92,
            219
          ]
        }
      ]
    }
  ],
  "content": "<properties\n    pageTitle=\"Feature engineering in the Cortana Analytics Process | Microsoft Azure\" \n    description=\"Explains the purposes of feature engineering and provides examples of its role in the data enhancement process of machine learning.\"\n    services=\"machine-learning\"\n    documentationCenter=\"\"\n    authors=\"bradsev\"\n    manager=\"paulettm\"\n    editor=\"cgronlun\"/>\n\n<tags\n    ms.service=\"machine-learning\"\n    ms.workload=\"data-services\"\n    ms.tgt_pltfrm=\"na\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\"\n    ms.date=\"02/05/2016\"\n    ms.author=\"zhangya;bradsev\" />\n\n\n# Feature engineering in the Cortana Analytics Process \n\nFeature engineering attempts to increase the predictive power of learning algorithms by creating features from raw data that help facilitate the learning process.\n\n[AZURE.INCLUDE [cap-create-features-data-selector](../../includes/cap-create-features-selector.md)]\nThis **menu** links to topics that describe how to create features for data in various environments. This task is a step in the [Cortana Analytics Process (CAP)](https://azure.microsoft.com/documentation/learning-paths/cortana-analytics-process/).\n\n##Introduction\n\nThis topic explains the purposes of feature engineering and provides examples of its role in the data enhancement process of machine learning. These examples are drawn from Azure Machine Learning Studio. \n\nThe engineering and selection of features is one part of the CAP process outlined in the [What is the Cortana Analytics Process?](machine-learning-data-science-the-cortana-analytics-process.md) Feature engineering and selection are parts of the **Develop features** step of the CAP. \n* **feature engineering**: This process attempts to create additional relevant features from the existing raw features in the data, and to increase the predictive power of the learning algorithm.\n* **feature selection**: This process selects the key subset of original data features in an attempt to reduce the dimensionality of the training problem.\n\nNormally **feature engineering** is applied first to generate additional features, and then the **feature selection** step is performed to eliminate irrelevant, redundant, or highly correlated features.\n\nThe training data used in machine learning can often be enhanced by extraction of features from the raw data collected. An example of an engineered feature in the context of learning how to classify the images of handwritten characters is creation of a bit density map constructed from the raw bit distribution data. This map can help locate the edges of the characters more efficiently than simply using the raw distribution directly.\n\n\n[AZURE.INCLUDE [machine-learning-free-trial](../../includes/machine-learning-free-trial.md)]\n\n\n## Creating Features from Your Data - Feature Engineering\n\nThe training data consists of a matrix composed of examples (records or observations stored in rows), each of which has a set of features (variables or fields stored in columns). The features specified in the experimental design are expected to characterize the patterns in the data. Although many of the raw data fields can be directly included in the selected feature set used to train a model, it is often the case that additional (engineered) features need to be constructed from the features in the raw data to generate an enhanced training dataset.\n\nWhat kind of features should be created to enhance the dataset when training a model? Engineered features that enhance the training provide information that better differentiates the patterns in the data. We expect the new features to provide additional information that is not clearly captured or easily apparent in the original or existing feature set. But this process is something of an art. Sound and productive decisions often require some domain expertise.\n\nWhen starting with Azure Machine Learning, it is easiest to grasp this process concretely using samples provided in the Studio. Two examples are presented here:\n\n* A regression example [Prediction of the number of bike rentals](../machine-learning-sample-prediction-of-number-of-bike-rentals.md) in a supervised experiment where the target values are known\n* A text mining classification example using [Feature Hashing](https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/)\n\n### Example 1: Adding Temporal Features for Regression Model ###\n\nLet's use the experiment \"Demand forecasting of bikes\" in Azure Machine Learning Studio to demonstrate how to engineer features for a regression task. The objective of this experiment is to predict the demand for the bikes, that is, the number of bike rentals within a specific month/day/hour. The dataset \"Bike Rental UCI dataset\" is used as the raw input data. This dataset is based on real data from the Capital Bikeshare company that maintains a bike rental network in Washington DC in the United States. The dataset represents the number of bike rentals within a specific hour of a day in the years 2011 and year 2012 and contains 17379 rows and 17 columns. The raw feature set contains weather conditions (temperature/humidity/wind speed) and the type of the day (holiday/weekday). The field to predict is \"cnt\", a count which represents the bike rentals within a specific hour and which ranges ranges from 1 to 977.\n\nWith the goal of constructing effective features in the training data, four regression models are built using the same algorithm but with four different training datasets. The four datasets represent the same raw input data, but with an increasing number of features set. These features are grouped into four categories:\n\n1. A = weather + holiday + weekday + weekend features for the predicted day\n2. B = number of bikes that were rented in each of the previous 12 hours\n3. C = number of bikes that were rented in each of the previous 12 days at the same hour\n4. D = number of bikes that were rented in each of the previous 12 weeks at the same hour and the same day\n\nBesides feature set A, which already exist in the original raw data, the other three sets of features are created through the feature engineering process. Feature set B captures very recent demand for the bikes. Feature set C captures the demand for bikes at a particular hour. Feature set D captures demand for bikes at particular hour and particular day of the week. The four training datasets each includes feature set A, A+B, A+B+C, and A+B+C+D, respectively.\n\nIn the Azure Machine Learning experiment, these four training datasets are formed via four branches from the pre-processed input dataset. Except the left most branch, each of these branches contains an [Execute R Script](https://msdn.microsoft.com/library/azure/30806023-392b-42e0-94d6-6b775a6e0fd5/) module, in which a set of derived features (feature set B, C, and D) are respectively constructed and appended to the imported dataset. The following figure demonstrates the R script used to create feature set B in the second left branch.\n\n![create features](./media/machine-learning-data-science-create-features/addFeature-Rscripts.png)\n\nThe comparison of the performance results of the four models are summarized in the following table. The best results are shown by features A+B+C. Note that the error rate decreases when additional feature set are included in the training data. It verifies our presumption that the feature set B, C provide additional relevant information for the regression task. But adding the D feature does not seem to provide any additional reduction in the error rate.\n\n![result comparison](./media/machine-learning-data-science-create-features/result1.png)\n\n### <a name=\"example2\"></a> Example 2: Creating Features in Text Mining  \n\nFeature engineering is widely applied in tasks related to text mining, such as document classification and sentiment analysis. For example, when we want to classify documents into several categories, a typical assumption is that the word/phrases included in one doc category are less likely to occur in another doc category. In another words, the frequency of the words/phrases distribution is able to characterize different document categories. In text mining applications, because individual pieces of text-contents usually serve as the input data, the feature engineering process is needed to create the features involving word/phrase frequencies.\n\nTo achieve this task, a technique called **feature hashing** is applied to efficiently turn arbitrary text features into indices. Instead of associating each text feature (words/phrases) to a particular index, this method functions by applying a hash function to the features and using their hash values as indices directly.\n\nIn Azure Machine Learning, there is a [Feature Hashing](https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/) module that creates these word/phrase features conveniently. Following figure shows an example of using this module. The input dataset contains two columns: the book rating ranging from 1 to 5, and the actual review content. The goal of this [Feature Hashing](https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/) module is to retrieve a bunch of new features that show the occurrence frequency of the corresponding word(s)/phrase(s) within the particular book review. To use this module, we need to complete the following steps:\n\n* First, select the column that contains the input text (\"Col2\" in this example).\n* Second, set the \"Hashing bitsize\" to 8, which means 2^8=256 features will be created. The word/phase in all the text will be hashed to 256 indices. The parameter \"Hashing bitsize\" ranges from 1 to 31. The word(s)/phrase(s) are less likely to be hashed into the same index if setting it to be a larger number.\n* Third, set the parameter \"N-grams\" to 2. This gets the occurrence frequency of unigrams (a feature for every single word) and bigrams (a feature for every pair of adjacent words) from the input text. The parameter \"N-grams\" ranges from 0 to 10, which indicates the maximum number of sequential words to be included in a feature.  \n\n![\"Feature Hashing\" module](./media/machine-learning-data-science-create-features/feature-Hashing1.png)\n\nThe following figure shows what the these new feature look like.\n\n![\"Feature Hashing\" example](./media/machine-learning-data-science-create-features/feature-Hashing2.png)\n\n\n## Conclusion\n\nEngineered and selected features increase the efficiency of the training process which attempts to extract the key information contained in the data. They also improve the power of these models to classify the input data accurately and to predict outcomes of interest more robustly. Feature engineering and selection can also combine to make the learning more computationally tractable. It does so by enhancing and then reducing the number of features needed to calibrate or train a model. Mathematically speaking, the features selected to train the model are a minimal set of independent variables that explain the patterns in the data and then predict outcomes successfully.\n\nNote that it is not always necessarily to perform feature engineering or feature selection. Whether it is needed or not depends on the data we have or collect, the algorithm we pick, and the objective of the experiment.\n \n"
}