{
  "nodes": [
    {
      "pos": [
        28,
        114
      ],
      "content": "Use Apache Spark to build machine learning applications on HDInsight | Microsoft Azure"
    },
    {
      "pos": [
        134,
        240
      ],
      "content": "Step-by-step instructions on how to use notebooks with Apache Spark to build machine learning applications"
    },
    {
      "pos": [
        581,
        686
      ],
      "content": "Machine learning: Predictive analysis on food inspection data using MLlib with Spark on HDInsight (Linux)"
    },
    {
      "pos": [
        690,
        1191
      ],
      "content": "<ph id=\"ph2\">[AZURE.TIP]</ph><ph id=\"ph3\"/> This tutorial is also available as a Jupyter notebook on a Spark (Linux) cluster that you create in HDInsight. The notebook experience lets you run the Python snippets from the notebook itself. To perform the tutorial from within a notebook, create a Spark cluster, launch a Jupyter notebook (<ph id=\"ph4\">`https://CLUSTERNAME.azurehdinsight.net/jupyter`</ph>), and then run the notebook <bpt id=\"p1\">**</bpt>Spark Machine Learning - Predictive analysis on food inspection data using MLLib.ipynb<ept id=\"p1\">**</ept><ph id=\"ph5\"/> under the <bpt id=\"p2\">**</bpt>Python<ept id=\"p2\">**</ept><ph id=\"ph6\"/> folder.",
      "nodes": [
        {
          "content": "<ph id=\"ph2\">[AZURE.TIP]</ph><ph id=\"ph3\"/> This tutorial is also available as a Jupyter notebook on a Spark (Linux) cluster that you create in HDInsight.",
          "pos": [
            0,
            154
          ]
        },
        {
          "content": "The notebook experience lets you run the Python snippets from the notebook itself.",
          "pos": [
            155,
            237
          ]
        },
        {
          "content": "To perform the tutorial from within a notebook, create a Spark cluster, launch a Jupyter notebook (<ph id=\"ph4\">`https://CLUSTERNAME.azurehdinsight.net/jupyter`</ph>), and then run the notebook <bpt id=\"p1\">**</bpt>Spark Machine Learning - Predictive analysis on food inspection data using MLLib.ipynb<ept id=\"p1\">**</ept><ph id=\"ph5\"/> under the <bpt id=\"p2\">**</bpt>Python<ept id=\"p2\">**</ept><ph id=\"ph6\"/> folder.",
          "pos": [
            238,
            655
          ]
        }
      ]
    },
    {
      "pos": [
        1194,
        1499
      ],
      "content": "This article demonstrates how to use <bpt id=\"p3\">**</bpt>MLLib<ept id=\"p3\">**</ept>, Spark's built-in machine learning libraries, to perform a simple predictive analysis on an open dataset. MLLib is a core Spark library that provides a number of utilities that are useful for machine learning tasks, including utilities that are suitable for:",
      "nodes": [
        {
          "content": "This article demonstrates how to use <bpt id=\"p3\">**</bpt>MLLib<ept id=\"p3\">**</ept>, Spark's built-in machine learning libraries, to perform a simple predictive analysis on an open dataset.",
          "pos": [
            0,
            190
          ]
        },
        {
          "content": "MLLib is a core Spark library that provides a number of utilities that are useful for machine learning tasks, including utilities that are suitable for:",
          "pos": [
            191,
            343
          ]
        }
      ]
    },
    {
      "pos": [
        1503,
        1517
      ],
      "content": "Classification"
    },
    {
      "pos": [
        1521,
        1531
      ],
      "content": "Regression"
    },
    {
      "pos": [
        1535,
        1545
      ],
      "content": "Clustering"
    },
    {
      "pos": [
        1549,
        1563
      ],
      "content": "Topic modeling"
    },
    {
      "pos": [
        1567,
        1640
      ],
      "content": "Singular value decomposition (SVD) and principal component analysis (PCA)"
    },
    {
      "pos": [
        1644,
        1696
      ],
      "content": "Hypothesis testing and calculating sample statistics"
    },
    {
      "pos": [
        1698,
        1786
      ],
      "content": "This article presents a simple approach to <bpt id=\"p4\">*</bpt>classification<ept id=\"p4\">*</ept><ph id=\"ph7\"/> through logistic regression."
    },
    {
      "pos": [
        1791,
        1839
      ],
      "content": "What are classification and logistic regression?"
    },
    {
      "pos": [
        1841,
        2271
      ],
      "content": "<bpt id=\"p5\">*</bpt>Classification<ept id=\"p5\">*</ept>, a very common machine learning task, is the process of sorting input data into categories. It is the job of a classification algorithm to figure out how to assign \"labels\" to input data that you provide. For example, you could think of a machine learning algorithm that accepts stock information as input and divides the stock into two categories: stocks which you should sell and stocks which you should retain.",
      "nodes": [
        {
          "content": "<bpt id=\"p5\">*</bpt>Classification<ept id=\"p5\">*</ept>, a very common machine learning task, is the process of sorting input data into categories.",
          "pos": [
            0,
            146
          ]
        },
        {
          "content": "It is the job of a classification algorithm to figure out how to assign \"labels\" to input data that you provide.",
          "pos": [
            147,
            259
          ]
        },
        {
          "content": "For example, you could think of a machine learning algorithm that accepts stock information as input and divides the stock into two categories: stocks which you should sell and stocks which you should retain.",
          "pos": [
            260,
            468
          ]
        }
      ]
    },
    {
      "pos": [
        2273,
        2580
      ],
      "content": "Logistic regression is the algorithm that you use for classification. Spark's logistic regression API is useful for <bpt id=\"p6\">*</bpt>binary classification<ept id=\"p6\">*</ept>, or classifying input data into one of two groups. For more information about logistic regressions, see <bpt id=\"p7\">[</bpt>Wikipedia<ept id=\"p7\">](https://en.wikipedia.org/wiki/Logistic_regression)</ept>.",
      "nodes": [
        {
          "content": "Logistic regression is the algorithm that you use for classification.",
          "pos": [
            0,
            69
          ]
        },
        {
          "content": "Spark's logistic regression API is useful for <bpt id=\"p6\">*</bpt>binary classification<ept id=\"p6\">*</ept>, or classifying input data into one of two groups.",
          "pos": [
            70,
            228
          ]
        },
        {
          "content": "For more information about logistic regressions, see <bpt id=\"p7\">[</bpt>Wikipedia<ept id=\"p7\">](https://en.wikipedia.org/wiki/Logistic_regression)</ept>.",
          "pos": [
            229,
            383
          ]
        }
      ]
    },
    {
      "pos": [
        2582,
        2759
      ],
      "content": "In summary, the process of logistic regression produces a <bpt id=\"p8\">*</bpt>logistic function<ept id=\"p8\">*</ept><ph id=\"ph8\"/> that can be used to predict the probability that an input vector belongs in one group or the other."
    },
    {
      "pos": [
        2766,
        2815
      ],
      "content": "What are we trying to accomplish in this article?"
    },
    {
      "pos": [
        2817,
        3254
      ],
      "content": "You will use Spark to perform some predictive analysis on food inspection data (<bpt id=\"p9\">**</bpt>Food_Inspections1.csv<ept id=\"p9\">**</ept>) that was acquired through the <bpt id=\"p10\">[</bpt>City of Chicago data portal<ept id=\"p10\">](https://data.cityofchicago.org/)</ept>. This dataset contains information about food inspections that were conducted in Chicago, including information about each food establishment that was inspected, the violations that were found (if any), and the results of the inspection.",
      "nodes": [
        {
          "content": "You will use Spark to perform some predictive analysis on food inspection data (<bpt id=\"p9\">**</bpt>Food_Inspections1.csv<ept id=\"p9\">**</ept>) that was acquired through the <bpt id=\"p10\">[</bpt>City of Chicago data portal<ept id=\"p10\">](https://data.cityofchicago.org/)</ept>.",
          "pos": [
            0,
            278
          ]
        },
        {
          "content": "This dataset contains information about food inspections that were conducted in Chicago, including information about each food establishment that was inspected, the violations that were found (if any), and the results of the inspection.",
          "pos": [
            279,
            515
          ]
        }
      ]
    },
    {
      "pos": [
        3257,
        3352
      ],
      "content": "In the steps below, you develop a model to see what it takes to pass or fail a food inspection."
    },
    {
      "pos": [
        3358,
        3421
      ],
      "content": "Start building a machine learning application using Spark MLlib"
    },
    {
      "pos": [
        3426,
        3667
      ],
      "content": "From the <bpt id=\"p11\">[</bpt>Azure Preview Portal<ept id=\"p11\">](https://portal.azure.com/)</ept>, from the startboard, click the tile for your Spark cluster (if you pinned it to the startboard). You can also navigate to your cluster under <bpt id=\"p12\">**</bpt>Browse All<ept id=\"p12\">**</ept><ph id=\"ph9\"/> &gt; <bpt id=\"p13\">**</bpt>HDInsight Clusters<ept id=\"p13\">**</ept>.",
      "nodes": [
        {
          "content": "From the <bpt id=\"p11\">[</bpt>Azure Preview Portal<ept id=\"p11\">](https://portal.azure.com/)</ept>, from the startboard, click the tile for your Spark cluster (if you pinned it to the startboard).",
          "pos": [
            0,
            196
          ]
        },
        {
          "content": "You can also navigate to your cluster under <bpt id=\"p12\">**</bpt>Browse All<ept id=\"p12\">**</ept><ph id=\"ph9\"/> &gt; <bpt id=\"p13\">**</bpt>HDInsight Clusters<ept id=\"p13\">**</ept>.",
          "pos": [
            197,
            378
          ]
        }
      ]
    },
    {
      "pos": [
        3675,
        3860
      ],
      "content": "From the Spark cluster blade, click <bpt id=\"p14\">**</bpt>Quick Links<ept id=\"p14\">**</ept>, and then from the <bpt id=\"p15\">**</bpt>Cluster Dashboard<ept id=\"p15\">**</ept><ph id=\"ph10\"/> blade, click <bpt id=\"p16\">**</bpt>Jupyter Notebook<ept id=\"p16\">**</ept>. If prompted, enter the admin credentials for the cluster.",
      "nodes": [
        {
          "content": "From the Spark cluster blade, click <bpt id=\"p14\">**</bpt>Quick Links<ept id=\"p14\">**</ept>, and then from the <bpt id=\"p15\">**</bpt>Cluster Dashboard<ept id=\"p15\">**</ept><ph id=\"ph10\"/> blade, click <bpt id=\"p16\">**</bpt>Jupyter Notebook<ept id=\"p16\">**</ept>.",
          "pos": [
            0,
            262
          ]
        },
        {
          "content": "If prompted, enter the admin credentials for the cluster.",
          "pos": [
            263,
            320
          ]
        }
      ]
    },
    {
      "pos": [
        3868,
        4038
      ],
      "content": "<ph id=\"ph11\">[AZURE.NOTE]</ph><ph id=\"ph12\"/> You may also reach the Jupyter Notebook for your cluster by opening the following URL in your browser. Replace <bpt id=\"p17\">__</bpt>CLUSTERNAME<ept id=\"p17\">__</ept><ph id=\"ph13\"/> with the name of your cluster:",
      "nodes": [
        {
          "content": "<ph id=\"ph11\">[AZURE.NOTE]</ph><ph id=\"ph12\"/> You may also reach the Jupyter Notebook for your cluster by opening the following URL in your browser.",
          "pos": [
            0,
            149
          ]
        },
        {
          "content": "Replace <bpt id=\"p17\">__</bpt>CLUSTERNAME<ept id=\"p17\">__</ept><ph id=\"ph13\"/> with the name of your cluster:",
          "pos": [
            150,
            259
          ]
        }
      ]
    },
    {
      "pos": [
        4104,
        4170
      ],
      "content": "Create a new notebook. Click <bpt id=\"p18\">**</bpt>New<ept id=\"p18\">**</ept>, and then click <bpt id=\"p19\">**</bpt>Python 2<ept id=\"p19\">**</ept>.",
      "nodes": [
        {
          "content": "Create a new notebook.",
          "pos": [
            0,
            22
          ]
        },
        {
          "content": "Click <bpt id=\"p18\">**</bpt>New<ept id=\"p18\">**</ept>, and then click <bpt id=\"p19\">**</bpt>Python 2<ept id=\"p19\">**</ept>.",
          "pos": [
            23,
            146
          ]
        }
      ]
    },
    {
      "pos": [
        4176,
        4344
      ],
      "content": "<ph id=\"ph15\">![</ph>Create a new Jupyter notebook<ph id=\"ph16\">](./media/hdinsight-apache-spark-machine-learning-mllib-ipython/hdispark.note.jupyter.createnotebook.png \"Create a new Jupyter notebook\")</ph>"
    },
    {
      "pos": [
        4349,
        4477
      ],
      "content": "A new notebook is created and opened with the name Untitled.pynb. Click the notebook name at the top, and enter a friendly name.",
      "nodes": [
        {
          "content": "A new notebook is created and opened with the name Untitled.pynb.",
          "pos": [
            0,
            65
          ]
        },
        {
          "content": "Click the notebook name at the top, and enter a friendly name.",
          "pos": [
            66,
            128
          ]
        }
      ]
    },
    {
      "pos": [
        4483,
        4654
      ],
      "content": "<ph id=\"ph17\">![</ph>Provide a name for the notebook<ph id=\"ph18\">](./media/hdinsight-apache-spark-machine-learning-mllib-ipython/hdispark.note.jupyter.notebook.name.png \"Provide a name for the notebook\")</ph>"
    },
    {
      "pos": [
        4659,
        4835
      ],
      "content": "Start building your machine learning application. You should start by by setting up the Pyspark environment. To do so, place the cursor in the cell and press <bpt id=\"p20\">**</bpt>SHIFT + ENTER<ept id=\"p20\">**</ept>.",
      "nodes": [
        {
          "content": "Start building your machine learning application.",
          "pos": [
            0,
            49
          ]
        },
        {
          "content": "You should start by by setting up the Pyspark environment.",
          "pos": [
            50,
            108
          ]
        },
        {
          "content": "To do so, place the cursor in the cell and press <bpt id=\"p20\">**</bpt>SHIFT + ENTER<ept id=\"p20\">**</ept>.",
          "pos": [
            109,
            216
          ]
        }
      ]
    },
    {
      "pos": [
        5537,
        5565
      ],
      "content": "Construct an input dataframe"
    },
    {
      "pos": [
        5567,
        5888
      ],
      "content": "We already have a SQLContext that we can use to perform transformations on structured data. The first task is to load the sample data ((<bpt id=\"p21\">**</bpt>Food_Inspections1.csv<ept id=\"p21\">**</ept>)) into a Spark SQL <bpt id=\"p22\">*</bpt>dataframe<ept id=\"p22\">*</ept>. The snippets below assume that the data is already uploaded to the default storage container associated with the Spark cluster.",
      "nodes": [
        {
          "content": "We already have a SQLContext that we can use to perform transformations on structured data.",
          "pos": [
            0,
            91
          ]
        },
        {
          "content": "The first task is to load the sample data ((<bpt id=\"p21\">**</bpt>Food_Inspections1.csv<ept id=\"p21\">**</ept>)) into a Spark SQL <bpt id=\"p22\">*</bpt>dataframe<ept id=\"p22\">*</ept>.",
          "pos": [
            92,
            273
          ]
        },
        {
          "content": "The snippets below assume that the data is already uploaded to the default storage container associated with the Spark cluster.",
          "pos": [
            274,
            401
          ]
        }
      ]
    },
    {
      "pos": [
        5894,
        6100
      ],
      "content": "Because the raw data is in a CSV format, we need to use the Spark context to pull every line of the file into memory as unstructured text; then, you use Python's CSV library to parse each line individually."
    },
    {
      "pos": [
        6478,
        6588
      ],
      "content": "We now have the CSV file as an RDD. Let us retrieve one row from the RDD to understand the schema of the data.",
      "nodes": [
        {
          "content": "We now have the CSV file as an RDD.",
          "pos": [
            0,
            35
          ]
        },
        {
          "content": "Let us retrieve one row from the RDD to understand the schema of the data.",
          "pos": [
            36,
            110
          ]
        }
      ]
    },
    {
      "pos": [
        9236,
        9573
      ],
      "content": "The above output gives us an idea of the schema of the input file; the file includes the name of every establishment, the type of establishment, the address, the data of the inspections, and the location, among other things. Let's select a few columns that will be useful for our predictive analysis and group the results as a dataframe.",
      "nodes": [
        {
          "content": "The above output gives us an idea of the schema of the input file; the file includes the name of every establishment, the type of establishment, the address, the data of the inspections, and the location, among other things.",
          "pos": [
            0,
            224
          ]
        },
        {
          "content": "Let's select a few columns that will be useful for our predictive analysis and group the results as a dataframe.",
          "pos": [
            225,
            337
          ]
        }
      ]
    },
    {
      "pos": [
        9971,
        10184
      ],
      "content": "We now have a <bpt id=\"p23\">*</bpt>dataframe<ept id=\"p23\">*</ept>, <ph id=\"ph19\">`df`</ph><ph id=\"ph20\"/> on which we can perform our analysis. We've included 4 columns of interest in the dataframe: <bpt id=\"p24\">**</bpt>id<ept id=\"p24\">**</ept>, <bpt id=\"p25\">**</bpt>name<ept id=\"p25\">**</ept>, <bpt id=\"p26\">**</bpt>results<ept id=\"p26\">**</ept>, and <bpt id=\"p27\">**</bpt>violations<ept id=\"p27\">**</ept>. Let's get a small sample of the data:",
      "nodes": [
        {
          "content": "We now have a <bpt id=\"p23\">*</bpt>dataframe<ept id=\"p23\">*</ept>, <ph id=\"ph19\">`df`</ph><ph id=\"ph20\"/> on which we can perform our analysis.",
          "pos": [
            0,
            143
          ]
        },
        {
          "content": "We've included 4 columns of interest in the dataframe: <bpt id=\"p24\">**</bpt>id<ept id=\"p24\">**</ept>, <bpt id=\"p25\">**</bpt>name<ept id=\"p25\">**</ept>, <bpt id=\"p26\">**</bpt>results<ept id=\"p26\">**</ept>, and <bpt id=\"p27\">**</bpt>violations<ept id=\"p27\">**</ept>.",
          "pos": [
            144,
            409
          ]
        },
        {
          "content": "Let's get a small sample of the data:",
          "pos": [
            410,
            447
          ]
        }
      ]
    },
    {
      "pos": [
        10949,
        10968
      ],
      "content": "Understand the data"
    },
    {
      "pos": [
        10970,
        11096
      ],
      "content": "Let's start to get a sense of what our dataset contains. For example, what are the different values in the <bpt id=\"p28\">**</bpt>results<ept id=\"p28\">**</ept><ph id=\"ph21\"/> column?",
      "nodes": [
        {
          "content": "Let's start to get a sense of what our dataset contains.",
          "pos": [
            0,
            56
          ]
        },
        {
          "content": "For example, what are the different values in the <bpt id=\"p28\">**</bpt>results<ept id=\"p28\">**</ept><ph id=\"ph21\"/> column?",
          "pos": [
            57,
            181
          ]
        }
      ]
    },
    {
      "pos": [
        11148,
        11192
      ],
      "content": "You should see an output like the following:"
    },
    {
      "pos": [
        11515,
        11597
      ],
      "content": "A quick visualization can help us reason about the distribution of these outcomes."
    },
    {
      "pos": [
        11929,
        11973
      ],
      "content": "You should see an output like the following:"
    },
    {
      "pos": [
        11980,
        12075
      ],
      "content": "<ph id=\"ph22\">![</ph>Result output<ph id=\"ph23\">](./media/hdinsight-apache-spark-machine-learning-mllib-ipython/output_13_1.png)</ph>"
    },
    {
      "pos": [
        12078,
        12151
      ],
      "content": "You can see that there are 5 distinct results that an inspection can have"
    },
    {
      "pos": [
        12155,
        12175
      ],
      "content": "Business not located"
    },
    {
      "pos": [
        12179,
        12183
      ],
      "content": "Fail"
    },
    {
      "pos": [
        12186,
        12190
      ],
      "content": "Pass"
    },
    {
      "pos": [
        12193,
        12215
      ],
      "content": "Pss w/ conditions, and"
    },
    {
      "pos": [
        12218,
        12233
      ],
      "content": "Out of Business"
    },
    {
      "pos": [
        12236,
        12815
      ],
      "content": "Let us develop a model that can guess the outcome of a food inspection, given the violations. Since logistic regression is a binary classification method, it makes sense to group our data into two categories: <bpt id=\"p29\">**</bpt>Fail<ept id=\"p29\">**</ept><ph id=\"ph24\"/> and <bpt id=\"p30\">**</bpt>Pass<ept id=\"p30\">**</ept>. A \"Pass w/ Conditions\" is still a Pass, so when we train the model, we will consider the two results equivalent. Data with the other results (\"Business Not Located\", \"Out of Business\") are not useful so we will remove them from our training set. This should be okay since these two categories make up a very small percentage of the results anyway.",
      "nodes": [
        {
          "content": "Let us develop a model that can guess the outcome of a food inspection, given the violations.",
          "pos": [
            0,
            93
          ]
        },
        {
          "content": "Since logistic regression is a binary classification method, it makes sense to group our data into two categories: <bpt id=\"p29\">**</bpt>Fail<ept id=\"p29\">**</ept><ph id=\"ph24\"/> and <bpt id=\"p30\">**</bpt>Pass<ept id=\"p30\">**</ept>.",
          "pos": [
            94,
            326
          ]
        },
        {
          "content": "A \"Pass w/ Conditions\" is still a Pass, so when we train the model, we will consider the two results equivalent.",
          "pos": [
            327,
            439
          ]
        },
        {
          "content": "Data with the other results (\"Business Not Located\", \"Out of Business\") are not useful so we will remove them from our training set.",
          "pos": [
            440,
            572
          ]
        },
        {
          "content": "This should be okay since these two categories make up a very small percentage of the results anyway.",
          "pos": [
            573,
            674
          ]
        }
      ]
    },
    {
      "pos": [
        12817,
        13189
      ],
      "content": "Let us go ahead and convert our existing dataframe(<ph id=\"ph25\">`df`</ph>) into a new dataframe where each inspection is represented as a label-violations pair. In our case, a label of <ph id=\"ph26\">`0.0`</ph><ph id=\"ph27\"/> represents a failure, a label of <ph id=\"ph28\">`1.0`</ph><ph id=\"ph29\"/> represents a success, and a label of <ph id=\"ph30\">`-1.0`</ph><ph id=\"ph31\"/> represents some results besides those two. We will filter those other results out when computing the new data frame.",
      "nodes": [
        {
          "content": "Let us go ahead and convert our existing dataframe(<ph id=\"ph25\">`df`</ph>) into a new dataframe where each inspection is represented as a label-violations pair.",
          "pos": [
            0,
            161
          ]
        },
        {
          "content": "In our case, a label of <ph id=\"ph26\">`0.0`</ph><ph id=\"ph27\"/> represents a failure, a label of <ph id=\"ph28\">`1.0`</ph><ph id=\"ph29\"/> represents a success, and a label of <ph id=\"ph30\">`-1.0`</ph><ph id=\"ph31\"/> represents some results besides those two.",
          "pos": [
            162,
            419
          ]
        },
        {
          "content": "We will filter those other results out when computing the new data frame.",
          "pos": [
            420,
            493
          ]
        }
      ]
    },
    {
      "pos": [
        13545,
        13616
      ],
      "content": "Let's retrieve one row from the labeled data to see what it looks like."
    },
    {
      "pos": [
        13645,
        13689
      ],
      "content": "You should see an output like the following:"
    },
    {
      "pos": [
        15325,
        15384
      ],
      "content": "Create a logistic regression model from the input dataframe"
    },
    {
      "pos": [
        15386,
        15871
      ],
      "content": "Our final task is to convert the labeled data into a format that can be analyzed by logistic regression. The input to a logistic regression algorithm should be a set of <bpt id=\"p31\">*</bpt>label-feature vector pairs<ept id=\"p31\">*</ept>, where the \"feature vector\" is a vector of numbers that represents the input point in some way. So, we need a way to convert the \"violations\" column, which is semi-structured and contains a lot of comments in free-text, to an array of real numbers that a machine could easily understand.",
      "nodes": [
        {
          "content": "Our final task is to convert the labeled data into a format that can be analyzed by logistic regression.",
          "pos": [
            0,
            104
          ]
        },
        {
          "content": "The input to a logistic regression algorithm should be a set of <bpt id=\"p31\">*</bpt>label-feature vector pairs<ept id=\"p31\">*</ept>, where the \"feature vector\" is a vector of numbers that represents the input point in some way.",
          "pos": [
            105,
            333
          ]
        },
        {
          "content": "So, we need a way to convert the \"violations\" column, which is semi-structured and contains a lot of comments in free-text, to an array of real numbers that a machine could easily understand.",
          "pos": [
            334,
            525
          ]
        }
      ]
    },
    {
      "pos": [
        15874,
        16139
      ],
      "content": "One standard machine learning approach for processing natural language is to assign each distinct word an \"index\", and then pass a vector to the machine learning algorithm such that each index's value contains the relative frequency of that word in the text string."
    },
    {
      "pos": [
        16142,
        16520
      ],
      "content": "MLLib provides an easy way to perform this operation. First, we'll \"tokenize\" each violations string to get the individual words in each string, and then we'll use a <ph id=\"ph32\">`HashingTF`</ph><ph id=\"ph33\"/> to convert each set of tokens into a feature vector which can then be passed to the logistic regression algorithm to construct a model. We'll conduct all of these steps in sequence using a \"pipeline\".",
      "nodes": [
        {
          "content": "MLLib provides an easy way to perform this operation.",
          "pos": [
            0,
            53
          ]
        },
        {
          "content": "First, we'll \"tokenize\" each violations string to get the individual words in each string, and then we'll use a <ph id=\"ph32\">`HashingTF`</ph><ph id=\"ph33\"/> to convert each set of tokens into a feature vector which can then be passed to the logistic regression algorithm to construct a model.",
          "pos": [
            54,
            347
          ]
        },
        {
          "content": "We'll conduct all of these steps in sequence using a \"pipeline\".",
          "pos": [
            348,
            412
          ]
        }
      ]
    },
    {
      "pos": [
        16836,
        16881
      ],
      "content": "Evaluate the model on a separate test dataset"
    },
    {
      "pos": [
        16883,
        17327
      ],
      "content": "We can use the model we created earlier to <bpt id=\"p32\">*</bpt>predict<ept id=\"p32\">*</ept><ph id=\"ph34\"/> what the results of new inspections will be, based on the violations that were observed. We trained this model on the dataset <bpt id=\"p33\">**</bpt>Food_Inspections1.csv<ept id=\"p33\">**</ept>. Let us use a second dataset, <bpt id=\"p34\">**</bpt>Food_Inspections2.csv<ept id=\"p34\">**</ept>, to <bpt id=\"p35\">*</bpt>evaluate<ept id=\"p35\">*</ept><ph id=\"ph35\"/> the strength of this model on new data. This second data set (<bpt id=\"p36\">**</bpt>Food_Inspections2.csv<ept id=\"p36\">**</ept>) should already be in the default storage container associated with the cluster.",
      "nodes": [
        {
          "content": "We can use the model we created earlier to <bpt id=\"p32\">*</bpt>predict<ept id=\"p32\">*</ept><ph id=\"ph34\"/> what the results of new inspections will be, based on the violations that were observed.",
          "pos": [
            0,
            196
          ]
        },
        {
          "content": "We trained this model on the dataset <bpt id=\"p33\">**</bpt>Food_Inspections1.csv<ept id=\"p33\">**</ept>.",
          "pos": [
            197,
            300
          ]
        },
        {
          "content": "Let us use a second dataset, <bpt id=\"p34\">**</bpt>Food_Inspections2.csv<ept id=\"p34\">**</ept>, to <bpt id=\"p35\">*</bpt>evaluate<ept id=\"p35\">*</ept><ph id=\"ph35\"/> the strength of this model on new data.",
          "pos": [
            301,
            505
          ]
        },
        {
          "content": "This second data set (<bpt id=\"p36\">**</bpt>Food_Inspections2.csv<ept id=\"p36\">**</ept>) should already be in the default storage container associated with the cluster.",
          "pos": [
            506,
            674
          ]
        }
      ]
    },
    {
      "pos": [
        17329,
        17442
      ],
      "content": "The snippet below creates a new dataframe, <bpt id=\"p37\">**</bpt>predictionsDf<ept id=\"p37\">**</ept><ph id=\"ph36\"/> that contains the prediction generated by the model."
    },
    {
      "pos": [
        17856,
        17900
      ],
      "content": "You should see an output like the following:"
    },
    {
      "pos": [
        18131,
        18180
      ],
      "content": "Look at one of the predictions. Run this snippet:",
      "nodes": [
        {
          "content": "Look at one of the predictions.",
          "pos": [
            0,
            31
          ]
        },
        {
          "content": "Run this snippet:",
          "pos": [
            32,
            49
          ]
        }
      ]
    },
    {
      "pos": [
        18209,
        18278
      ],
      "content": "You will see the prediction for the first entry in the test data set."
    },
    {
      "pos": [
        18280,
        18525
      ],
      "content": "The <ph id=\"ph37\">`model.transform()`</ph><ph id=\"ph38\"/> method will apply the same transformation to any new data with the same schema, and arrive at a prediction of how to classify the data. We can do some simple statistics to get a sense of how accurate our predictions were:",
      "nodes": [
        {
          "content": "The <ph id=\"ph37\">`model.transform()`</ph><ph id=\"ph38\"/> method will apply the same transformation to any new data with the same schema, and arrive at a prediction of how to classify the data.",
          "pos": [
            0,
            193
          ]
        },
        {
          "content": "We can do some simple statistics to get a sense of how accurate our predictions were:",
          "pos": [
            194,
            279
          ]
        }
      ]
    },
    {
      "pos": [
        19064,
        19100
      ],
      "content": "The output looks like the following:"
    },
    {
      "pos": [
        19294,
        19577
      ],
      "content": "Using logistic regression with Spark gives us an accurate model of the relationship between violations descriptions in English and whether a given bussiness would pass or fail a food inspection. We can construct a final visualization to help us reason about the results of this test:",
      "nodes": [
        {
          "content": "Using logistic regression with Spark gives us an accurate model of the relationship between violations descriptions in English and whether a given bussiness would pass or fail a food inspection.",
          "pos": [
            0,
            194
          ]
        },
        {
          "content": "We can construct a final visualization to help us reason about the results of this test:",
          "pos": [
            195,
            283
          ]
        }
      ]
    },
    {
      "pos": [
        20167,
        20203
      ],
      "content": "You should see the following output."
    },
    {
      "pos": [
        20205,
        20304
      ],
      "content": "<ph id=\"ph39\">![</ph>Prediction output<ph id=\"ph40\">](./media/hdinsight-apache-spark-machine-learning-mllib-ipython/output_26_1.png)</ph>"
    },
    {
      "pos": [
        20307,
        20525
      ],
      "content": "In this chart, a \"positive\" result refers to the failed food inspection, while a negative result refers to a passed inspection. This corresponds (roughly) to a 12.6% false negative rate and a 16.0% false positive rate.",
      "nodes": [
        {
          "content": "In this chart, a \"positive\" result refers to the failed food inspection, while a negative result refers to a passed inspection.",
          "pos": [
            0,
            127
          ]
        },
        {
          "content": "This corresponds (roughly) to a 12.6% false negative rate and a 16.0% false positive rate.",
          "pos": [
            128,
            218
          ]
        }
      ]
    },
    {
      "pos": [
        20530,
        20552
      ],
      "content": "Shut down the notebook"
    },
    {
      "pos": [
        20554,
        20780
      ],
      "content": "After you have finished running the application, you should shutdown the notebook to release the resources. To do so, from the <bpt id=\"p38\">**</bpt>File<ept id=\"p38\">**</ept><ph id=\"ph41\"/> menu on the notebook, click <bpt id=\"p39\">**</bpt>Close and Halt<ept id=\"p39\">**</ept>. This will shutdown and close the notebook.",
      "nodes": [
        {
          "content": "After you have finished running the application, you should shutdown the notebook to release the resources.",
          "pos": [
            0,
            107
          ]
        },
        {
          "content": "To do so, from the <bpt id=\"p38\">**</bpt>File<ept id=\"p38\">**</ept><ph id=\"ph41\"/> menu on the notebook, click <bpt id=\"p39\">**</bpt>Close and Halt<ept id=\"p39\">**</ept>.",
          "pos": [
            108,
            278
          ]
        },
        {
          "content": "This will shutdown and close the notebook.",
          "pos": [
            279,
            321
          ]
        }
      ]
    },
    {
      "pos": [
        20808,
        20816
      ],
      "content": "See also"
    },
    {
      "pos": [
        20821,
        20900
      ],
      "content": "<bpt id=\"p40\">[</bpt>Overview: Apache Spark on Azure HDInsight<ept id=\"p40\">](hdinsight-apache-spark-overview.md)</ept>"
    },
    {
      "pos": [
        20906,
        20915
      ],
      "content": "Scenarios"
    },
    {
      "pos": [
        20919,
        21048
      ],
      "content": "<bpt id=\"p41\">[</bpt>Spark with BI: Perform interactive data analysis using Spark in HDInsight with BI tools<ept id=\"p41\">](hdinsight-apache-spark-use-bi-tools.md)</ept>"
    },
    {
      "pos": [
        21052,
        21217
      ],
      "content": "<bpt id=\"p42\">[</bpt>Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data<ept id=\"p42\">](hdinsight-apache-spark-ipython-notebook-machine-learning.md)</ept>"
    },
    {
      "pos": [
        21221,
        21354
      ],
      "content": "<bpt id=\"p43\">[</bpt>Spark Streaming: Use Spark in HDInsight for building real-time streaming applications<ept id=\"p43\">](hdinsight-apache-spark-eventhub-streaming.md)</ept>"
    },
    {
      "pos": [
        21358,
        21468
      ],
      "content": "<bpt id=\"p44\">[</bpt>Website log analysis using Spark in HDInsight<ept id=\"p44\">](hdinsight-apache-spark-custom-library-website-log-analysis.md)</ept>"
    },
    {
      "pos": [
        21474,
        21501
      ],
      "content": "Create and run applications"
    },
    {
      "pos": [
        21505,
        21607
      ],
      "content": "<bpt id=\"p45\">[</bpt>Create a standalone application using Scala<ept id=\"p45\">](hdinsight-apache-spark-create-standalone-application.md)</ept>"
    },
    {
      "pos": [
        21611,
        21707
      ],
      "content": "<bpt id=\"p46\">[</bpt>Run jobs remotely on a Spark cluster using Livy<ept id=\"p46\">](hdinsight-apache-spark-livy-rest-interface.md)</ept>"
    },
    {
      "pos": [
        21713,
        21733
      ],
      "content": "Tools and extensions"
    },
    {
      "pos": [
        21737,
        21876
      ],
      "content": "<bpt id=\"p47\">[</bpt>Use HDInsight Tools Plugin for IntelliJ IDEA to create and submit Spark Scala applicatons<ept id=\"p47\">](hdinsight-apache-spark-intellij-tool-plugin.md)</ept>"
    },
    {
      "pos": [
        21880,
        21987
      ],
      "content": "<bpt id=\"p48\">[</bpt>Use Zeppelin notebooks with a Spark cluster on HDInsight<ept id=\"p48\">](hdinsight-apache-spark-use-zeppelin-notebook.md)</ept>"
    },
    {
      "pos": [
        21991,
        22114
      ],
      "content": "<bpt id=\"p49\">[</bpt>Kernels available for Jupyter notebook in Spark cluster for HDInsight<ept id=\"p49\">](hdinsight-apache-spark-jupyter-notebook-kernels.md)</ept>"
    },
    {
      "pos": [
        22120,
        22136
      ],
      "content": "Manage resources"
    },
    {
      "pos": [
        22140,
        22250
      ],
      "content": "<bpt id=\"p50\">[</bpt>Manage resources for the Apache Spark cluster in Azure HDInsight<ept id=\"p50\">](hdinsight-apache-spark-resource-manager.md)</ept>"
    }
  ],
  "content": "<properties \n    pageTitle=\"Use Apache Spark to build machine learning applications on HDInsight | Microsoft Azure\" \n    description=\"Step-by-step instructions on how to use notebooks with Apache Spark to build machine learning applications\" \n    services=\"hdinsight\" \n    documentationCenter=\"\" \n    authors=\"nitinme\" \n    manager=\"paulettm\" \n    editor=\"cgronlun\"\n    tags=\"azure-portal\"/>\n\n<tags \n    ms.service=\"hdinsight\" \n    ms.workload=\"big-data\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"02/05/2016\" \n    ms.author=\"nitinme\"/>\n\n\n# Machine learning: Predictive analysis on food inspection data using MLlib with Spark on HDInsight (Linux)\n\n> [AZURE.TIP] This tutorial is also available as a Jupyter notebook on a Spark (Linux) cluster that you create in HDInsight. The notebook experience lets you run the Python snippets from the notebook itself. To perform the tutorial from within a notebook, create a Spark cluster, launch a Jupyter notebook (`https://CLUSTERNAME.azurehdinsight.net/jupyter`), and then run the notebook **Spark Machine Learning - Predictive analysis on food inspection data using MLLib.ipynb** under the **Python** folder.\n\n\nThis article demonstrates how to use **MLLib**, Spark's built-in machine learning libraries, to perform a simple predictive analysis on an open dataset. MLLib is a core Spark library that provides a number of utilities that are useful for machine learning tasks, including utilities that are suitable for:\n\n* Classification\n\n* Regression\n\n* Clustering\n\n* Topic modeling\n\n* Singular value decomposition (SVD) and principal component analysis (PCA)\n\n* Hypothesis testing and calculating sample statistics\n\nThis article presents a simple approach to *classification* through logistic regression.\n\n## What are classification and logistic regression?\n\n*Classification*, a very common machine learning task, is the process of sorting input data into categories. It is the job of a classification algorithm to figure out how to assign \"labels\" to input data that you provide. For example, you could think of a machine learning algorithm that accepts stock information as input and divides the stock into two categories: stocks which you should sell and stocks which you should retain.\n\nLogistic regression is the algorithm that you use for classification. Spark's logistic regression API is useful for *binary classification*, or classifying input data into one of two groups. For more information about logistic regressions, see [Wikipedia](https://en.wikipedia.org/wiki/Logistic_regression).\n\nIn summary, the process of logistic regression produces a *logistic function* that can be used to predict the probability that an input vector belongs in one group or the other.  \n\n## What are we trying to accomplish in this article?\n\nYou will use Spark to perform some predictive analysis on food inspection data (**Food_Inspections1.csv**) that was acquired through the [City of Chicago data portal](https://data.cityofchicago.org/). This dataset contains information about food inspections that were conducted in Chicago, including information about each food establishment that was inspected, the violations that were found (if any), and the results of the inspection. \n\nIn the steps below, you develop a model to see what it takes to pass or fail a food inspection. \n\n## Start building a machine learning application using Spark MLlib\n\n1. From the [Azure Preview Portal](https://portal.azure.com/), from the startboard, click the tile for your Spark cluster (if you pinned it to the startboard). You can also navigate to your cluster under **Browse All** > **HDInsight Clusters**.   \n\n2. From the Spark cluster blade, click **Quick Links**, and then from the **Cluster Dashboard** blade, click **Jupyter Notebook**. If prompted, enter the admin credentials for the cluster.\n\n    > [AZURE.NOTE] You may also reach the Jupyter Notebook for your cluster by opening the following URL in your browser. Replace __CLUSTERNAME__ with the name of your cluster:\n    >\n    > `https://CLUSTERNAME.azurehdinsight.net/jupyter`\n\n2. Create a new notebook. Click **New**, and then click **Python 2**.\n\n    ![Create a new Jupyter notebook](./media/hdinsight-apache-spark-machine-learning-mllib-ipython/hdispark.note.jupyter.createnotebook.png \"Create a new Jupyter notebook\")\n\n3. A new notebook is created and opened with the name Untitled.pynb. Click the notebook name at the top, and enter a friendly name.\n\n    ![Provide a name for the notebook](./media/hdinsight-apache-spark-machine-learning-mllib-ipython/hdispark.note.jupyter.notebook.name.png \"Provide a name for the notebook\")\n\n3. Start building your machine learning application. You should start by by setting up the Pyspark environment. To do so, place the cursor in the cell and press **SHIFT + ENTER**.\n\n\n        import pyspark\n        from pyspark import SparkConf\n        from pyspark import SparkContext\n        from pyspark.sql import SQLContext\n        %matplotlib inline\n        import matplotlib.pyplot as plt\n        from pyspark.ml import Pipeline\n        from pyspark.ml.classification import LogisticRegression\n        from pyspark.ml.feature import HashingTF, Tokenizer\n        from pyspark.sql import Row\n        from pyspark.sql.functions import UserDefinedFunction\n        from pyspark.sql.types import *\n        import atexit\n        \n        sc = SparkContext(conf=SparkConf().setMaster('yarn-client'))\n        sqlContext = SQLContext(sc)\n        atexit.register(lambda: sc.stop())\n\n\n## Construct an input dataframe\n\nWe already have a SQLContext that we can use to perform transformations on structured data. The first task is to load the sample data ((**Food_Inspections1.csv**)) into a Spark SQL *dataframe*. The snippets below assume that the data is already uploaded to the default storage container associated with the Spark cluster. \n\n1. Because the raw data is in a CSV format, we need to use the Spark context to pull every line of the file into memory as unstructured text; then, you use Python's CSV library to parse each line individually. \n\n\n        def csvParse(s):\n            import csv\n            from StringIO import StringIO\n            sio = StringIO(s)\n            value = csv.reader(sio).next()\n            sio.close()\n            return value\n        \n        inspections = sc.textFile('wasb:///HdiSamples/HdiSamples/FoodInspectionData/Food_Inspections1.csv')\\\n                        .map(csvParse)\n\n\n2. We now have the CSV file as an RDD. Let us retrieve one row from the RDD to understand the schema of the data.\n\n\n        inspections.take(1)\n\n\n    You should see an output like the following:\n\n        # -----------------\n        # THIS IS AN OUTPUT\n        # -----------------\n\n        [['413707',\n          'LUNA PARK INC',\n          'LUNA PARK  DAY CARE',\n          '2049789',\n          \"Children's Services Facility\",\n          'Risk 1 (High)',\n          '3250 W FOSTER AVE ',\n          'CHICAGO',\n          'IL',\n          '60625',\n          '09/21/2010',\n          'License-Task Force',\n          'Fail',\n          '24. DISH WASHING FACILITIES: PROPERLY DESIGNED, CONSTRUCTED, MAINTAINED, INSTALLED, LOCATED AND OPERATED - Comments: All dishwashing machines must be of a type that complies with all requirements of the plumbing section of the Municipal Code of Chicago and Rules and Regulation of the Board of Health. OBSEVERD THE 3 COMPARTMENT SINK BACKING UP INTO THE 1ST AND 2ND COMPARTMENT WITH CLEAR WATER AND SLOWLY DRAINING OUT. INST NEED HAVE IT REPAIR. CITATION ISSUED, SERIOUS VIOLATION 7-38-030 H000062369-10 COURT DATE 10-28-10 TIME 1 P.M. ROOM 107 400 W. SURPERIOR. | 36. LIGHTING: REQUIRED MINIMUM FOOT-CANDLES OF LIGHT PROVIDED, FIXTURES SHIELDED - Comments: Shielding to protect against broken glass falling into food shall be provided for all artificial lighting sources in preparation, service, and display facilities. LIGHT SHIELD ARE MISSING UNDER HOOD OF  COOKING EQUIPMENT AND NEED TO REPLACE LIGHT UNDER UNIT. 4 LIGHTS ARE OUT IN THE REAR CHILDREN AREA,IN THE KINDERGARDEN CLASS ROOM. 2 LIGHT ARE OUT EAST REAR, LIGHT FRONT WEST ROOM. NEED TO REPLACE ALL LIGHT THAT ARE NOT WORKING. | 35. WALLS, CEILINGS, ATTACHED EQUIPMENT CONSTRUCTED PER CODE: GOOD REPAIR, SURFACES CLEAN AND DUST-LESS CLEANING METHODS - Comments: The walls and ceilings shall be in good repair and easily cleaned. MISSING CEILING TILES WITH STAINS IN WEST,EAST, IN FRONT AREA WEST, AND BY THE 15MOS AREA. NEED TO BE REPLACED. | 32. FOOD AND NON-FOOD CONTACT SURFACES PROPERLY DESIGNED, CONSTRUCTED AND MAINTAINED - Comments: All food and non-food contact equipment and utensils shall be smooth, easily cleanable, and durable, and shall be in good repair. SPLASH GUARDED ARE NEEDED BY THE EXPOSED HAND SINK IN THE KITCHEN AREA | 34. FLOORS: CONSTRUCTED PER CODE, CLEANED, GOOD REPAIR, COVING INSTALLED, DUST-LESS CLEANING METHODS USED - Comments: The floors shall be constructed per code, be smooth and easily cleaned, and be kept clean and in good repair. INST NEED TO ELEVATE ALL FOOD ITEMS 6INCH OFF THE FLOOR 6 INCH AWAY FORM WALL.  ',\n          '41.97583445690982',\n          '-87.7107455232781',\n          '(41.97583445690982, -87.7107455232781)']]\n\n\n3. The above output gives us an idea of the schema of the input file; the file includes the name of every establishment, the type of establishment, the address, the data of the inspections, and the location, among other things. Let's select a few columns that will be useful for our predictive analysis and group the results as a dataframe.\n\n\n        schema = StructType([\n                StructField(\"id\", IntegerType(), False), \n                StructField(\"name\", StringType(), False), \n                StructField(\"results\", StringType(), False), \n                StructField(\"violations\", StringType(), True)])\n        \n        df = sqlContext.createDataFrame(inspections.map(lambda l: (int(l[0]), l[1], l[12], l[13])) , schema)\n\n4. We now have a *dataframe*, `df` on which we can perform our analysis. We've included 4 columns of interest in the dataframe: **id**, **name**, **results**, and **violations**. Let's get a small sample of the data:\n\n\n        df.show(5)\n\n    You should see an output like the following:\n\n        # -----------------\n        # THIS IS AN OUTPUT\n        # -----------------\n\n        +------+--------------------+-------+--------------------+\n        |    id|                name|results|          violations|\n        +------+--------------------+-------+--------------------+\n        |413707|       LUNA PARK INC|   Fail|24. DISH WASHING ...|\n        |391234|       CAFE SELMARIE|   Fail|2. FACILITIES TO ...|\n        |413751|          MANCHU WOK|   Pass|33. FOOD AND NON-...|\n        |413708|BENCHMARK HOSPITA...|   Pass|                    |\n        |413722|           JJ BURGER|   Pass|                    |\n        +------+--------------------+-------+--------------------+\n\n## Understand the data\n\nLet's start to get a sense of what our dataset contains. For example, what are the different values in the **results** column?\n\n\n    df.select('results').distinct().show()\n\n    \nYou should see an output like the following:\n\n    # -----------------\n    # THIS IS AN OUTPUT\n    # -----------------\n\n    +--------------------+\n    |             results|\n    +--------------------+\n    |                Fail|\n    |Business Not Located|\n    |                Pass|\n    |  Pass w/ Conditions|\n    |     Out of Business|\n    +--------------------+\n    \nA quick visualization can help us reason about the distribution of these outcomes.\n\n    countResults = df.groupBy('results').count().collect()\n    labels = [row.results for row in countResults]\n    sizes = [row.count for row in countResults]\n    colors = ['turquoise', 'seagreen', 'mediumslateblue', 'palegreen', 'coral']\n    plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors)\n    plt.axis('equal')\n\n\nYou should see an output like the following:\n\n    \n![Result output](./media/hdinsight-apache-spark-machine-learning-mllib-ipython/output_13_1.png)\n\n\nYou can see that there are 5 distinct results that an inspection can have\n\n* Business not located \n* Fail\n* Pass\n* Pss w/ conditions, and\n* Out of Business \n\nLet us develop a model that can guess the outcome of a food inspection, given the violations. Since logistic regression is a binary classification method, it makes sense to group our data into two categories: **Fail** and **Pass**. A \"Pass w/ Conditions\" is still a Pass, so when we train the model, we will consider the two results equivalent. Data with the other results (\"Business Not Located\", \"Out of Business\") are not useful so we will remove them from our training set. This should be okay since these two categories make up a very small percentage of the results anyway.\n\nLet us go ahead and convert our existing dataframe(`df`) into a new dataframe where each inspection is represented as a label-violations pair. In our case, a label of `0.0` represents a failure, a label of `1.0` represents a success, and a label of `-1.0` represents some results besides those two. We will filter those other results out when computing the new data frame.\n\n\n    def labelForResults(s):\n        if s == 'Fail':\n            return 0.0\n        elif s == 'Pass w/ Conditions' or s == 'Pass':\n            return 1.0\n        else:\n            return -1.0\n    label = UserDefinedFunction(labelForResults, DoubleType())\n    labeledData = df.select(label(df.results).alias('label'), df.violations).where('label >= 0')\n\n\nLet's retrieve one row from the labeled data to see what it looks like.\n\n\n    labeledData.take(1)\n\n\nYou should see an output like the following:\n\n    # -----------------\n    # THIS IS AN OUTPUT\n    # -----------------\n\n    [Row(label=0.0, violations=u\"41. PREMISES MAINTAINED FREE OF LITTER, UNNECESSARY ARTICLES, CLEANING  EQUIPMENT PROPERLY STORED - Comments: All parts of the food establishment and all parts of the property used in connection with the operation of the establishment shall be kept neat and clean and should not produce any offensive odors.  REMOVE MATTRESS FROM SMALL DUMPSTER. | 35. WALLS, CEILINGS, ATTACHED EQUIPMENT CONSTRUCTED PER CODE: GOOD REPAIR, SURFACES CLEAN AND DUST-LESS CLEANING METHODS - Comments: The walls and ceilings shall be in good repair and easily cleaned.  REPAIR MISALIGNED DOORS AND DOOR NEAR ELEVATOR.  DETAIL CLEAN BLACK MOLD LIKE SUBSTANCE FROM WALLS BY BOTH DISH MACHINES.  REPAIR OR REMOVE BASEBOARD UNDER DISH MACHINE (LEFT REAR KITCHEN). SEAL ALL GAPS.  REPLACE MILK CRATES USED IN WALK IN COOLERS AND STORAGE AREAS WITH PROPER SHELVING AT LEAST 6' OFF THE FLOOR.  | 38. VENTILATION: ROOMS AND EQUIPMENT VENTED AS REQUIRED: PLUMBING: INSTALLED AND MAINTAINED - Comments: The flow of air discharged from kitchen fans shall always be through a duct to a point above the roofline.  REPAIR BROKEN VENTILATION IN MEN'S AND WOMEN'S WASHROOMS NEXT TO DINING AREA. | 32. FOOD AND NON-FOOD CONTACT SURFACES PROPERLY DESIGNED, CONSTRUCTED AND MAINTAINED - Comments: All food and non-food contact equipment and utensils shall be smooth, easily cleanable, and durable, and shall be in good repair.  REPAIR DAMAGED PLUG ON LEFT SIDE OF 2 COMPARTMENT SINK.  REPAIR SELF CLOSER ON BOTTOM LEFT DOOR OF 4 DOOR PREP UNIT NEXT TO OFFICE.\")]\n\n\n## Create a logistic regression model from the input dataframe\n\nOur final task is to convert the labeled data into a format that can be analyzed by logistic regression. The input to a logistic regression algorithm should be a set of *label-feature vector pairs*, where the \"feature vector\" is a vector of numbers that represents the input point in some way. So, we need a way to convert the \"violations\" column, which is semi-structured and contains a lot of comments in free-text, to an array of real numbers that a machine could easily understand. \n\nOne standard machine learning approach for processing natural language is to assign each distinct word an \"index\", and then pass a vector to the machine learning algorithm such that each index's value contains the relative frequency of that word in the text string. \n\nMLLib provides an easy way to perform this operation. First, we'll \"tokenize\" each violations string to get the individual words in each string, and then we'll use a `HashingTF` to convert each set of tokens into a feature vector which can then be passed to the logistic regression algorithm to construct a model. We'll conduct all of these steps in sequence using a \"pipeline\".\n\n\n    tokenizer = Tokenizer(inputCol=\"violations\", outputCol=\"words\")\n    hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n    lr = LogisticRegression(maxIter=10, regParam=0.01)\n    pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n    \n    model = pipeline.fit(labeledData)\n\n\n## Evaluate the model on a separate test dataset\n\nWe can use the model we created earlier to *predict* what the results of new inspections will be, based on the violations that were observed. We trained this model on the dataset **Food_Inspections1.csv**. Let us use a second dataset, **Food_Inspections2.csv**, to *evaluate* the strength of this model on new data. This second data set (**Food_Inspections2.csv**) should already be in the default storage container associated with the cluster.\n\nThe snippet below creates a new dataframe, **predictionsDf** that contains the prediction generated by the model.\n\n\n    testData = sc.textFile('wasb:///HdiSamples/HdiSamples/FoodInspectionData/Food_Inspections2.csv')\\\n                 .map(csvParse) \\\n                 .map(lambda l: (int(l[0]), l[1], l[12], l[13]))\n    testDf = sqlContext.createDataFrame(testData, schema).where(\"results = 'Fail' OR results = 'Pass' OR results = 'Pass w/ Conditions'\")\n    predictionsDf = model.transform(testDf)\n    predictionsDf.columns\n\n\nYou should see an output like the following:\n\n    # -----------------\n    # THIS IS AN OUTPUT\n    # -----------------\n    \n    ['id',\n     'name',\n     'results',\n     'violations',\n     'words',\n     'features',\n     'rawPrediction',\n     'probability',\n     'prediction']\n\nLook at one of the predictions. Run this snippet:\n\n    predictionsDf.take(1)\n\nYou will see the prediction for the first entry in the test data set.\n\nThe `model.transform()` method will apply the same transformation to any new data with the same schema, and arrive at a prediction of how to classify the data. We can do some simple statistics to get a sense of how accurate our predictions were:\n\n\n    numSuccesses = predictionsDf.where(\"\"\"(prediction = 0 AND results = 'Fail') OR \n                                          (prediction = 1 AND (results = 'Pass' OR \n                                                               results = 'Pass w/ Conditions'))\"\"\").count()\n    numInspections = predictionsDf.count()\n    \n    print \"There were\", numInspections, \"inspections and there were\", numSuccesses, \"successful predictions\"\n    print \"This is a\", str((float(numSuccesses) / float(numInspections)) * 100) + \"%\", \"success rate\"\n\nThe output looks like the following:\n\n    # -----------------\n    # THIS IS AN OUTPUT\n    # -----------------\n\n    There were 9315 inspections and there were 8087 successful predictions\n    This is a 86.8169618894% success rate\n\n\nUsing logistic regression with Spark gives us an accurate model of the relationship between violations descriptions in English and whether a given bussiness would pass or fail a food inspection. We can construct a final visualization to help us reason about the results of this test:\n\n    \n    failSuccess = predictionsDf.where(\"prediction = 0 AND results = 'Fail'\").count()\n    failFailure = predictionsDf.where(\"prediction = 0 AND results <> 'Fail'\").count()\n    passSuccess = predictionsDf.where(\"prediction = 1 AND results <> 'Fail'\").count()\n    passFailure = predictionsDf.where(\"prediction = 1 AND results = 'Fail'\").count()\n    labels = ['True positive', 'False positive', 'True negative', 'False negative']\n    sizes = [failSuccess, failFailure, passSuccess, passFailure]\n    plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors)\n    plt.axis('equal')\n\n\nYou should see the following output.\n\n![Prediction output](./media/hdinsight-apache-spark-machine-learning-mllib-ipython/output_26_1.png)\n\n\nIn this chart, a \"positive\" result refers to the failed food inspection, while a negative result refers to a passed inspection. This corresponds (roughly) to a 12.6% false negative rate and a 16.0% false positive rate.\n\n## Shut down the notebook\n\nAfter you have finished running the application, you should shutdown the notebook to release the resources. To do so, from the **File** menu on the notebook, click **Close and Halt**. This will shutdown and close the notebook.\n\n\n## <a name=\"seealso\"></a>See also\n\n\n* [Overview: Apache Spark on Azure HDInsight](hdinsight-apache-spark-overview.md)\n\n### Scenarios\n\n* [Spark with BI: Perform interactive data analysis using Spark in HDInsight with BI tools](hdinsight-apache-spark-use-bi-tools.md)\n\n* [Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data](hdinsight-apache-spark-ipython-notebook-machine-learning.md)\n\n* [Spark Streaming: Use Spark in HDInsight for building real-time streaming applications](hdinsight-apache-spark-eventhub-streaming.md)\n\n* [Website log analysis using Spark in HDInsight](hdinsight-apache-spark-custom-library-website-log-analysis.md)\n\n### Create and run applications\n\n* [Create a standalone application using Scala](hdinsight-apache-spark-create-standalone-application.md)\n\n* [Run jobs remotely on a Spark cluster using Livy](hdinsight-apache-spark-livy-rest-interface.md)\n\n### Tools and extensions\n\n* [Use HDInsight Tools Plugin for IntelliJ IDEA to create and submit Spark Scala applicatons](hdinsight-apache-spark-intellij-tool-plugin.md)\n\n* [Use Zeppelin notebooks with a Spark cluster on HDInsight](hdinsight-apache-spark-use-zeppelin-notebook.md)\n\n* [Kernels available for Jupyter notebook in Spark cluster for HDInsight](hdinsight-apache-spark-jupyter-notebook-kernels.md)\n\n### Manage resources\n\n* [Manage resources for the Apache Spark cluster in Azure HDInsight](hdinsight-apache-spark-resource-manager.md)"
}