{
  "nodes": [
    {
      "pos": [
        27,
        92
      ],
      "content": "Use Hue with Hadoop on HDInsight Linux clusters | Microsoft Azure"
    },
    {
      "pos": [
        111,
        184
      ],
      "content": "Learn how to install and use Hue with Hadoop clusters on HDInsight Linux."
    },
    {
      "pos": [
        495,
        543
      ],
      "content": "Install and use Hue on HDInsight Hadoop clusters"
    },
    {
      "pos": [
        545,
        645
      ],
      "content": "Learn how to install Hue on HDInsight Linux clusters and use tunneling to route the requests to Hue."
    },
    {
      "pos": [
        650,
        662
      ],
      "content": "What is Hue?"
    },
    {
      "pos": [
        664,
        979
      ],
      "content": "Hue is a set of Web applications used to interact with a Hadoop cluster. You can use Hue to browse the storage associated with a Hadoop cluster (WASB, in the case of HDInsight clusters), run Hive jobs and Pig scripts, etc. The following components are supported with Hue installation on an HDInsight Hadoop cluster.",
      "nodes": [
        {
          "content": "Hue is a set of Web applications used to interact with a Hadoop cluster.",
          "pos": [
            0,
            72
          ]
        },
        {
          "content": "You can use Hue to browse the storage associated with a Hadoop cluster (WASB, in the case of HDInsight clusters), run Hive jobs and Pig scripts, etc. The following components are supported with Hue installation on an HDInsight Hadoop cluster.",
          "pos": [
            73,
            315
          ]
        }
      ]
    },
    {
      "pos": [
        983,
        1002
      ],
      "content": "Beeswax Hive Editor"
    },
    {
      "pos": [
        1005,
        1008
      ],
      "content": "Pig"
    },
    {
      "pos": [
        1011,
        1028
      ],
      "content": "Metastore manager"
    },
    {
      "pos": [
        1031,
        1036
      ],
      "content": "Oozie"
    },
    {
      "pos": [
        1039,
        1090
      ],
      "content": "FileBrowser (which talks to WASB default container)"
    },
    {
      "pos": [
        1093,
        1104
      ],
      "content": "Job Browser"
    },
    {
      "pos": [
        1110,
        1142
      ],
      "content": "Install Hue using Script Actions"
    },
    {
      "pos": [
        1144,
        1527
      ],
      "content": "The <bpt id=\"p1\">[</bpt>https://hdiconfigactions.blob.core.windows.net/linuxhueconfigactionv02/install-hue-uber-v02.sh<ept id=\"p1\">](https://hdiconfigactions.blob.core.windows.net/linuxhueconfigactionv02/install-hue-uber-v02.sh)</ept><ph id=\"ph2\"/> script action is used to install Hue on an HDInsight cluster. This section provides instructions about how to use the script when provisioning the cluster using the Azure Classic Portal.",
      "nodes": [
        {
          "content": "The <bpt id=\"p1\">[</bpt>https://hdiconfigactions.blob.core.windows.net/linuxhueconfigactionv02/install-hue-uber-v02.sh<ept id=\"p1\">](https://hdiconfigactions.blob.core.windows.net/linuxhueconfigactionv02/install-hue-uber-v02.sh)</ept><ph id=\"ph2\"/> script action is used to install Hue on an HDInsight cluster.",
          "pos": [
            0,
            310
          ]
        },
        {
          "content": "This section provides instructions about how to use the script when provisioning the cluster using the Azure Classic Portal.",
          "pos": [
            311,
            435
          ]
        }
      ]
    },
    {
      "pos": [
        1531,
        1788
      ],
      "content": "<ph id=\"ph3\">[AZURE.NOTE]</ph><ph id=\"ph4\"/> You can also use Azure PowerShell or the HDInsight .NET SDK to create a cluster using this script. For more information on using these methods, see <bpt id=\"p2\">[</bpt>Customize HDInsight clusters with Script Actions<ept id=\"p2\">](hdinsight-hadoop-customize-cluster-linux.md)</ept>.",
      "nodes": [
        {
          "content": "<ph id=\"ph3\">[AZURE.NOTE]</ph><ph id=\"ph4\"/> You can also use Azure PowerShell or the HDInsight .NET SDK to create a cluster using this script.",
          "pos": [
            0,
            143
          ]
        },
        {
          "content": "For more information on using these methods, see <bpt id=\"p2\">[</bpt>Customize HDInsight clusters with Script Actions<ept id=\"p2\">](hdinsight-hadoop-customize-cluster-linux.md)</ept>.",
          "pos": [
            144,
            327
          ]
        }
      ]
    },
    {
      "pos": [
        1793,
        1971
      ],
      "content": "Start provisioning a cluster by using the steps in <bpt id=\"p3\">[</bpt>Provision HDInsight clusters on Linux<ept id=\"p3\">](hdinsight-hadoop-provision-linux-clusters.md#portal)</ept>, but do not complete provisioning."
    },
    {
      "pos": [
        1979,
        2099
      ],
      "content": "<ph id=\"ph5\">[AZURE.NOTE]</ph><ph id=\"ph6\"/> To install Hue on HDInsight clusters, the recommended headnode size is at least A4 (8 cores, 14 GB memory)."
    },
    {
      "pos": [
        2104,
        2215
      ],
      "content": "On the <bpt id=\"p4\">**</bpt>Optional Configuration<ept id=\"p4\">**</ept><ph id=\"ph7\"/> blade, select <bpt id=\"p5\">**</bpt>Script Actions<ept id=\"p5\">**</ept>, and provide the information as shown below:"
    },
    {
      "pos": [
        2221,
        2365
      ],
      "content": "<ph id=\"ph8\">![</ph>Provide script action parameters for Hue<ph id=\"ph9\">](./media/hdinsight-hadoop-hue-linux/hue_script_action.png \"Provide script action parameters for Hue\")</ph>"
    },
    {
      "pos": [
        2373,
        2427
      ],
      "content": "<bpt id=\"p6\">__</bpt>NAME<ept id=\"p6\">__</ept>: Enter a friendly name for the script action."
    },
    {
      "pos": [
        2434,
        2544
      ],
      "content": "<bpt id=\"p7\">__</bpt>SCRIPT URI<ept id=\"p7\">__</ept>: https://hdiconfigactions.blob.core.windows.net/linuxhueconfigactionv02/install-hue-uber-v02.sh"
    },
    {
      "pos": [
        2551,
        2578
      ],
      "content": "<bpt id=\"p8\">__</bpt>HEAD<ept id=\"p8\">__</ept>: Check this option"
    },
    {
      "pos": [
        2585,
        2614
      ],
      "content": "<bpt id=\"p9\">__</bpt>WORKER<ept id=\"p9\">__</ept>: Leave this blank."
    },
    {
      "pos": [
        2621,
        2653
      ],
      "content": "<bpt id=\"p10\">__</bpt>ZOOKEEPER<ept id=\"p10\">__</ept>: Leave this blank."
    },
    {
      "pos": [
        2660,
        2693
      ],
      "content": "<bpt id=\"p11\">__</bpt>PARAMETERS<ept id=\"p11\">__</ept>: Leave this blank."
    },
    {
      "pos": [
        2698,
        2928
      ],
      "content": "At the bottom of the <bpt id=\"p12\">**</bpt>Script Actions<ept id=\"p12\">**</ept>, use the <bpt id=\"p13\">**</bpt>Select<ept id=\"p13\">**</ept><ph id=\"ph10\"/> button to save the configuration. Finally, use the <bpt id=\"p14\">**</bpt>Select<ept id=\"p14\">**</ept><ph id=\"ph11\"/> button at the bottom of the <bpt id=\"p15\">**</bpt>Optional Configuration<ept id=\"p15\">**</ept><ph id=\"ph12\"/> blade to save the optional configuration information.",
      "nodes": [
        {
          "content": "At the bottom of the <bpt id=\"p12\">**</bpt>Script Actions<ept id=\"p12\">**</ept>, use the <bpt id=\"p13\">**</bpt>Select<ept id=\"p13\">**</ept><ph id=\"ph10\"/> button to save the configuration.",
          "pos": [
            0,
            188
          ]
        },
        {
          "content": "Finally, use the <bpt id=\"p14\">**</bpt>Select<ept id=\"p14\">**</ept><ph id=\"ph11\"/> button at the bottom of the <bpt id=\"p15\">**</bpt>Optional Configuration<ept id=\"p15\">**</ept><ph id=\"ph12\"/> blade to save the optional configuration information.",
          "pos": [
            189,
            435
          ]
        }
      ]
    },
    {
      "pos": [
        2933,
        3076
      ],
      "content": "Continue provisioning the cluster as described in <bpt id=\"p16\">[</bpt>Provision HDInsight clusters on Linux<ept id=\"p16\">](hdinsight-hadoop-provision-linux-clusters.md#portal)</ept>."
    },
    {
      "pos": [
        3081,
        3112
      ],
      "content": "Use Hue with HDInsight clusters"
    },
    {
      "pos": [
        3114,
        3408
      ],
      "content": "SSH Tunneling is the only way to access Hue on the cluster once it is running. Tunneling via SSH allows the traffic to go directly to the headnode of the cluster where Hue is running. After the cluster has finished provisioning, use the following steps to use Hue on an HDInsight Linux cluster.",
      "nodes": [
        {
          "content": "SSH Tunneling is the only way to access Hue on the cluster once it is running.",
          "pos": [
            0,
            78
          ]
        },
        {
          "content": "Tunneling via SSH allows the traffic to go directly to the headnode of the cluster where Hue is running.",
          "pos": [
            79,
            183
          ]
        },
        {
          "content": "After the cluster has finished provisioning, use the following steps to use Hue on an HDInsight Linux cluster.",
          "pos": [
            184,
            294
          ]
        }
      ]
    },
    {
      "pos": [
        3413,
        3727
      ],
      "content": "Use the information in <bpt id=\"p17\">[</bpt>Use SSH Tunneling to access Ambari web UI, ResourceManager, JobHistory, NameNode, Oozie, and other web UI's<ept id=\"p17\">](hdinsight-linux-ambari-ssh-tunnel.md)</ept><ph id=\"ph13\"/> to create an SSH tunnel from your client system to the HDInsight cluster, and then configure your Web browser to use the SSH tunnel as a proxy."
    },
    {
      "pos": [
        3732,
        3931
      ],
      "content": "Once you have created an SSH tunnel and configured your browser to proxy traffic through it, you must find the host name of the head node. Use the following steps to get this information from Ambari:",
      "nodes": [
        {
          "content": "Once you have created an SSH tunnel and configured your browser to proxy traffic through it, you must find the host name of the head node.",
          "pos": [
            0,
            138
          ]
        },
        {
          "content": "Use the following steps to get this information from Ambari:",
          "pos": [
            139,
            199
          ]
        }
      ]
    },
    {
      "pos": [
        3940,
        4079
      ],
      "content": "In a browser, go to https://CLUSTERNAME.azurehdinsight.net. When prompted, use the Admin username and password to authenticate to the site.",
      "nodes": [
        {
          "content": "In a browser, go to https://CLUSTERNAME.azurehdinsight.net.",
          "pos": [
            0,
            59
          ]
        },
        {
          "content": "When prompted, use the Admin username and password to authenticate to the site.",
          "pos": [
            60,
            139
          ]
        }
      ]
    },
    {
      "pos": [
        4092,
        4147
      ],
      "content": "From the menu at the top of the page, select <bpt id=\"p18\">__</bpt>Hosts<ept id=\"p18\">__</ept>."
    },
    {
      "pos": [
        4160,
        4420
      ],
      "content": "Select the entry that begins with <bpt id=\"p19\">__</bpt>hn0<ept id=\"p19\">__</ept>. When the page opens, the host name will be displayed at the top. The format of the host name is <bpt id=\"p20\">__</bpt>hn0-CLUSTERNAME.randomcharacters.cx.internal.cloudapp.net<ept id=\"p20\">__</ept>. This is the host name you must use when connecting to Hue.",
      "nodes": [
        {
          "content": "Select the entry that begins with <bpt id=\"p19\">__</bpt>hn0<ept id=\"p19\">__</ept>.",
          "pos": [
            0,
            82
          ]
        },
        {
          "content": "When the page opens, the host name will be displayed at the top.",
          "pos": [
            83,
            147
          ]
        },
        {
          "content": "The format of the host name is <bpt id=\"p20\">__</bpt>hn0-CLUSTERNAME.randomcharacters.cx.internal.cloudapp.net<ept id=\"p20\">__</ept>.",
          "pos": [
            148,
            281
          ]
        },
        {
          "content": "This is the host name you must use when connecting to Hue.",
          "pos": [
            282,
            340
          ]
        }
      ]
    },
    {
      "pos": [
        4425,
        4659
      ],
      "content": "Once you have created an SSH tunnel and configured your browser to proxy traffic through it, use the browser to open the Hue portal at http://HOSTNAME:8888. Replace HOSTNAME with the name you obtained from Ambari in the previous step.",
      "nodes": [
        {
          "content": "Once you have created an SSH tunnel and configured your browser to proxy traffic through it, use the browser to open the Hue portal at http://HOSTNAME:8888.",
          "pos": [
            0,
            156
          ]
        },
        {
          "content": "Replace HOSTNAME with the name you obtained from Ambari in the previous step.",
          "pos": [
            157,
            234
          ]
        }
      ]
    },
    {
      "pos": [
        4667,
        4948
      ],
      "content": "<ph id=\"ph14\">[AZURE.NOTE]</ph><ph id=\"ph15\"/> When you log in for the first time, you will be prompted to create an account to log into the Hue portal. The credentials you specify here will be limited to the portal and are not related to the admin or SSH user credentials you specified while provision the cluster.",
      "nodes": [
        {
          "content": "<ph id=\"ph14\">[AZURE.NOTE]</ph><ph id=\"ph15\"/> When you log in for the first time, you will be prompted to create an account to log into the Hue portal.",
          "pos": [
            0,
            152
          ]
        },
        {
          "content": "The credentials you specify here will be limited to the portal and are not related to the admin or SSH user credentials you specified while provision the cluster.",
          "pos": [
            153,
            315
          ]
        }
      ]
    },
    {
      "pos": [
        4954,
        5078
      ],
      "content": "<ph id=\"ph16\">![</ph>Login to the Hue portal<ph id=\"ph17\">](./media/hdinsight-hadoop-hue-linux/HDI.Hue.Portal.Login.png \"Specify credentials for Hue portal\")</ph>"
    },
    {
      "pos": [
        5084,
        5100
      ],
      "content": "Run a Hive query"
    },
    {
      "pos": [
        5105,
        5199
      ],
      "content": "From the Hue portal, click <bpt id=\"p21\">**</bpt>Query Editors<ept id=\"p21\">**</ept>, and then click <bpt id=\"p22\">**</bpt>Hive<ept id=\"p22\">**</ept><ph id=\"ph18\"/> to open the Hive editor."
    },
    {
      "pos": [
        5205,
        5287
      ],
      "content": "<ph id=\"ph19\">![</ph>Use Hive<ph id=\"ph20\">](./media/hdinsight-hadoop-hue-linux/HDI.Hue.Portal.Hive.png \"Use Hive\")</ph>"
    },
    {
      "pos": [
        5292,
        5580
      ],
      "content": "On the <bpt id=\"p23\">**</bpt>Assist<ept id=\"p23\">**</ept><ph id=\"ph21\"/> tab, under <bpt id=\"p24\">**</bpt>Database<ept id=\"p24\">**</ept>, you should see <bpt id=\"p25\">**</bpt>hivesampletable<ept id=\"p25\">**</ept>. This is a sample table that is shipped with all Hadoop clusters on HDInsight. Enter a sample query in the right pane and see the output on the <bpt id=\"p26\">**</bpt>Results<ept id=\"p26\">**</ept><ph id=\"ph22\"/> tab in the pane below, as shown in the screen capture.",
      "nodes": [
        {
          "content": "On the <bpt id=\"p23\">**</bpt>Assist<ept id=\"p23\">**</ept><ph id=\"ph21\"/> tab, under <bpt id=\"p24\">**</bpt>Database<ept id=\"p24\">**</ept>, you should see <bpt id=\"p25\">**</bpt>hivesampletable<ept id=\"p25\">**</ept>.",
          "pos": [
            0,
            213
          ]
        },
        {
          "content": "This is a sample table that is shipped with all Hadoop clusters on HDInsight.",
          "pos": [
            214,
            291
          ]
        },
        {
          "content": "Enter a sample query in the right pane and see the output on the <bpt id=\"p26\">**</bpt>Results<ept id=\"p26\">**</ept><ph id=\"ph22\"/> tab in the pane below, as shown in the screen capture.",
          "pos": [
            292,
            478
          ]
        }
      ]
    },
    {
      "pos": [
        5586,
        5686
      ],
      "content": "<ph id=\"ph23\">![</ph>Run Hive query<ph id=\"ph24\">](./media/hdinsight-hadoop-hue-linux/HDI.Hue.Portal.Hive.Query.png \"Run Hive query\")</ph>"
    },
    {
      "pos": [
        5692,
        5772
      ],
      "content": "You can also use the <bpt id=\"p27\">**</bpt>Chart<ept id=\"p27\">**</ept><ph id=\"ph25\"/> tab to see a visual representation of the result."
    },
    {
      "pos": [
        5778,
        5804
      ],
      "content": "Browse the cluster storage"
    },
    {
      "pos": [
        5809,
        5893
      ],
      "content": "From the Hue portal, click <bpt id=\"p28\">**</bpt>File Browser<ept id=\"p28\">**</ept><ph id=\"ph26\"/> in the top-right corner of the menu bar."
    },
    {
      "pos": [
        5898,
        6112
      ],
      "content": "By default the file browser opens at the <bpt id=\"p29\">**</bpt>/user/myuser<ept id=\"p29\">**</ept><ph id=\"ph27\"/> directory. Click the forward slash right before the user directory in the path to go to the root of the Azure storage container associated with the cluster.",
      "nodes": [
        {
          "content": "By default the file browser opens at the <bpt id=\"p29\">**</bpt>/user/myuser<ept id=\"p29\">**</ept><ph id=\"ph27\"/> directory.",
          "pos": [
            0,
            123
          ]
        },
        {
          "content": "Click the forward slash right before the user directory in the path to go to the root of the Azure storage container associated with the cluster.",
          "pos": [
            124,
            269
          ]
        }
      ]
    },
    {
      "pos": [
        6118,
        6224
      ],
      "content": "<ph id=\"ph28\">![</ph>Use file browser<ph id=\"ph29\">](./media/hdinsight-hadoop-hue-linux/HDI.Hue.Portal.File.Browser.png \"Use file browser\")</ph>"
    },
    {
      "pos": [
        6229,
        6440
      ],
      "content": "Right-click on a file or folder to see the available operations. Use the <bpt id=\"p30\">**</bpt>Upload<ept id=\"p30\">**</ept><ph id=\"ph30\"/> button in the right corner to upload files to the current directory. Use the <bpt id=\"p31\">**</bpt>New<ept id=\"p31\">**</ept><ph id=\"ph31\"/> button to create new files or directories.",
      "nodes": [
        {
          "content": "Right-click on a file or folder to see the available operations.",
          "pos": [
            0,
            64
          ]
        },
        {
          "content": "Use the <bpt id=\"p30\">**</bpt>Upload<ept id=\"p30\">**</ept><ph id=\"ph30\"/> button in the right corner to upload files to the current directory.",
          "pos": [
            65,
            207
          ]
        },
        {
          "content": "Use the <bpt id=\"p31\">**</bpt>New<ept id=\"p31\">**</ept><ph id=\"ph31\"/> button to create new files or directories.",
          "pos": [
            208,
            321
          ]
        }
      ]
    },
    {
      "pos": [
        6444,
        7081
      ],
      "content": "<ph id=\"ph32\">[AZURE.NOTE]</ph><ph id=\"ph33\"/> The Hue file browser can only show the contents of the default container associated with the HDInsight cluster. Any additional storage accounts/containers that you might have associated with the cluster will not be accessible using the file browser. However, the additional containers associated with the cluster will always be accessible for the Hive jobs. For example, if you enter the command <ph id=\"ph34\">`dfs -ls wasb://newcontainer@mystore.blob.core.windows.net`</ph><ph id=\"ph35\"/> in the Hive editor, you can see the contents of additional containers as well. In this command, <bpt id=\"p32\">**</bpt>newcontainer<ept id=\"p32\">**</ept><ph id=\"ph36\"/> is not the default container associated with a cluster.",
      "nodes": [
        {
          "content": "<ph id=\"ph32\">[AZURE.NOTE]</ph><ph id=\"ph33\"/> The Hue file browser can only show the contents of the default container associated with the HDInsight cluster.",
          "pos": [
            0,
            158
          ]
        },
        {
          "content": "Any additional storage accounts/containers that you might have associated with the cluster will not be accessible using the file browser.",
          "pos": [
            159,
            296
          ]
        },
        {
          "content": "However, the additional containers associated with the cluster will always be accessible for the Hive jobs.",
          "pos": [
            297,
            404
          ]
        },
        {
          "content": "For example, if you enter the command <ph id=\"ph34\">`dfs -ls wasb://newcontainer@mystore.blob.core.windows.net`</ph><ph id=\"ph35\"/> in the Hive editor, you can see the contents of additional containers as well.",
          "pos": [
            405,
            615
          ]
        },
        {
          "content": "In this command, <bpt id=\"p32\">**</bpt>newcontainer<ept id=\"p32\">**</ept><ph id=\"ph36\"/> is not the default container associated with a cluster.",
          "pos": [
            616,
            760
          ]
        }
      ]
    },
    {
      "pos": [
        7086,
        7110
      ],
      "content": "Important considerations"
    },
    {
      "pos": [
        7115,
        7193
      ],
      "content": "The script used to install Hue installs it only on Head node 0 of the cluster."
    },
    {
      "pos": [
        7198,
        7524
      ],
      "content": "During installation, multiple Hadoop services (HDFS, YARN, MR2, Oozie) are restarted for updating the configuration. After the script finishes installing Hue, it might take some time for other Hadoop services to start up. This might affect Hue's performance initially. Once all services start up, Hue will be fully functional.",
      "nodes": [
        {
          "content": "During installation, multiple Hadoop services (HDFS, YARN, MR2, Oozie) are restarted for updating the configuration.",
          "pos": [
            0,
            116
          ]
        },
        {
          "content": "After the script finishes installing Hue, it might take some time for other Hadoop services to start up.",
          "pos": [
            117,
            221
          ]
        },
        {
          "content": "This might affect Hue's performance initially.",
          "pos": [
            222,
            268
          ]
        },
        {
          "content": "Once all services start up, Hue will be fully functional.",
          "pos": [
            269,
            326
          ]
        }
      ]
    },
    {
      "pos": [
        7530,
        7724
      ],
      "content": "Hue does not understand Tez jobs, which is the current default for Hive. If you want to use MapReduce as the Hive execution engine, update the script to use the following command in your script:",
      "nodes": [
        {
          "content": "Hue does not understand Tez jobs, which is the current default for Hive.",
          "pos": [
            0,
            72
          ]
        },
        {
          "content": "If you want to use MapReduce as the Hive execution engine, update the script to use the following command in your script:",
          "pos": [
            73,
            194
          ]
        }
      ]
    },
    {
      "pos": [
        7769,
        8102
      ],
      "content": "With Linux clusters, you can have a scenario where your services are running on head node 0 while the Resource Manager could be running on head node 1. Such a scenario might result in errors (shown below) when using Hue to view details of RUNNING jobs on the cluster. However, you can view the job details when the job has completed.",
      "nodes": [
        {
          "content": "With Linux clusters, you can have a scenario where your services are running on head node 0 while the Resource Manager could be running on head node 1.",
          "pos": [
            0,
            151
          ]
        },
        {
          "content": "Such a scenario might result in errors (shown below) when using Hue to view details of RUNNING jobs on the cluster.",
          "pos": [
            152,
            267
          ]
        },
        {
          "content": "However, you can view the job details when the job has completed.",
          "pos": [
            268,
            333
          ]
        }
      ]
    },
    {
      "pos": [
        8108,
        8207
      ],
      "content": "<ph id=\"ph37\">![</ph>Hue portal error<ph id=\"ph38\">](./media/hdinsight-hadoop-hue-linux/HDI.Hue.Portal.Error.png \"Hue portal error\")</ph>"
    },
    {
      "pos": [
        8213,
        8335
      ],
      "content": "This is due to a known issue. As a workaround, modify Ambari so that the active Resource Manager also runs on head node 0.",
      "nodes": [
        {
          "content": "This is due to a known issue.",
          "pos": [
            0,
            29
          ]
        },
        {
          "content": "As a workaround, modify Ambari so that the active Resource Manager also runs on head node 0.",
          "pos": [
            30,
            122
          ]
        }
      ]
    },
    {
      "pos": [
        8341,
        8691
      ],
      "content": "Hue understands WebHDFS while HDInsight clusters use Azure Storage using <ph id=\"ph39\">`wasb://`</ph>. So, the custom script used with script action installs WebWasb, which is a WebHDFS-compatible service for talking to WASB. So, even though the Hue portal says HDFS in places (like when you move your mouse over the <bpt id=\"p33\">**</bpt>File Browser<ept id=\"p33\">**</ept>), it should be interpreted as WASB.",
      "nodes": [
        {
          "content": "Hue understands WebHDFS while HDInsight clusters use Azure Storage using <ph id=\"ph39\">`wasb://`</ph>.",
          "pos": [
            0,
            102
          ]
        },
        {
          "content": "So, the custom script used with script action installs WebWasb, which is a WebHDFS-compatible service for talking to WASB.",
          "pos": [
            103,
            225
          ]
        },
        {
          "content": "So, even though the Hue portal says HDFS in places (like when you move your mouse over the <bpt id=\"p33\">**</bpt>File Browser<ept id=\"p33\">**</ept>), it should be interpreted as WASB.",
          "pos": [
            226,
            409
          ]
        }
      ]
    },
    {
      "pos": [
        8697,
        8707
      ],
      "content": "Next steps"
    },
    {
      "pos": [
        8711,
        9058
      ],
      "content": "<bpt id=\"p34\">[</bpt>Install and use Spark on HDInsight clusters<ept id=\"p34\">](hdinsight-hadoop-spark-install-linux.md)</ept><ph id=\"ph40\"/> for  instructions about how to use cluster customization to install and use Spark on HDInsight Hadoop clusters. Spark is an open-source parallel processing framework that supports in-memory processing to boost the performance of big data analytic applications.",
      "nodes": [
        {
          "content": "<bpt id=\"p34\">[</bpt>Install and use Spark on HDInsight clusters<ept id=\"p34\">](hdinsight-hadoop-spark-install-linux.md)</ept><ph id=\"ph40\"/> for  instructions about how to use cluster customization to install and use Spark on HDInsight Hadoop clusters.",
          "pos": [
            0,
            253
          ]
        },
        {
          "content": "Spark is an open-source parallel processing framework that supports in-memory processing to boost the performance of big data analytic applications.",
          "pos": [
            254,
            402
          ]
        }
      ]
    },
    {
      "pos": [
        9062,
        9318
      ],
      "content": "<bpt id=\"p35\">[</bpt>Install Giraph on HDInsight clusters<ept id=\"p35\">](hdinsight-hadoop-giraph-install-linux.md)</ept>. Use cluster customization to install Giraph on HDInsight Hadoop clusters. Giraph allows you to perform graph processing using Hadoop, and it can be used with Azure HDInsight.",
      "nodes": [
        {
          "content": "<bpt id=\"p35\">[</bpt>Install Giraph on HDInsight clusters<ept id=\"p35\">](hdinsight-hadoop-giraph-install-linux.md)</ept>.",
          "pos": [
            0,
            121
          ]
        },
        {
          "content": "Use cluster customization to install Giraph on HDInsight Hadoop clusters.",
          "pos": [
            122,
            195
          ]
        },
        {
          "content": "Giraph allows you to perform graph processing using Hadoop, and it can be used with Azure HDInsight.",
          "pos": [
            196,
            296
          ]
        }
      ]
    },
    {
      "pos": [
        9322,
        9541
      ],
      "content": "<bpt id=\"p36\">[</bpt>Install Solr on HDInsight clusters<ept id=\"p36\">](hdinsight-hadoop-solr-install-linux.md)</ept>. Use cluster customization to install Solr on HDInsight Hadoop clusters. Solr allows you to perform powerful search operations on stored data.",
      "nodes": [
        {
          "content": "<bpt id=\"p36\">[</bpt>Install Solr on HDInsight clusters<ept id=\"p36\">](hdinsight-hadoop-solr-install-linux.md)</ept>.",
          "pos": [
            0,
            117
          ]
        },
        {
          "content": "Use cluster customization to install Solr on HDInsight Hadoop clusters.",
          "pos": [
            118,
            189
          ]
        },
        {
          "content": "Solr allows you to perform powerful search operations on stored data.",
          "pos": [
            190,
            259
          ]
        }
      ]
    },
    {
      "pos": [
        9545,
        9965
      ],
      "content": "<bpt id=\"p37\">[</bpt>Install R on HDInsight clusters<ept id=\"p37\">](hdinsight-hadoop-r-scripts-linux.md)</ept>. Use cluster customization to install R on HDInsight Hadoop clusters. R is an open-source language and environment for statistical computing. It provides hundreds of built-in statistical functions and its own programming language that combines aspects of functional and object-oriented programming. It also provides extensive graphical capabilities.",
      "nodes": [
        {
          "content": "<bpt id=\"p37\">[</bpt>Install R on HDInsight clusters<ept id=\"p37\">](hdinsight-hadoop-r-scripts-linux.md)</ept>.",
          "pos": [
            0,
            111
          ]
        },
        {
          "content": "Use cluster customization to install R on HDInsight Hadoop clusters.",
          "pos": [
            112,
            180
          ]
        },
        {
          "content": "R is an open-source language and environment for statistical computing.",
          "pos": [
            181,
            252
          ]
        },
        {
          "content": "It provides hundreds of built-in statistical functions and its own programming language that combines aspects of functional and object-oriented programming.",
          "pos": [
            253,
            409
          ]
        },
        {
          "content": "It also provides extensive graphical capabilities.",
          "pos": [
            410,
            460
          ]
        }
      ]
    }
  ],
  "content": "<properties\n    pageTitle=\"Use Hue with Hadoop on HDInsight Linux clusters | Microsoft Azure\"\n    description=\"Learn how to install and use Hue with Hadoop clusters on HDInsight Linux.\"\n    services=\"hdinsight\"\n    documentationCenter=\"\"\n    authors=\"nitinme\"\n    manager=\"paulettm\"\n    editor=\"cgronlun\"/>\n\n<tags \n    ms.service=\"hdinsight\" \n    ms.workload=\"big-data\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"02/01/2016\" \n    ms.author=\"nitinme\"/>\n\n# Install and use Hue on HDInsight Hadoop clusters\n\nLearn how to install Hue on HDInsight Linux clusters and use tunneling to route the requests to Hue.\n\n## What is Hue?\n\nHue is a set of Web applications used to interact with a Hadoop cluster. You can use Hue to browse the storage associated with a Hadoop cluster (WASB, in the case of HDInsight clusters), run Hive jobs and Pig scripts, etc. The following components are supported with Hue installation on an HDInsight Hadoop cluster.\n\n* Beeswax Hive Editor\n* Pig\n* Metastore manager\n* Oozie\n* FileBrowser (which talks to WASB default container)\n* Job Browser\n\n\n## Install Hue using Script Actions\n\nThe [https://hdiconfigactions.blob.core.windows.net/linuxhueconfigactionv02/install-hue-uber-v02.sh](https://hdiconfigactions.blob.core.windows.net/linuxhueconfigactionv02/install-hue-uber-v02.sh) script action is used to install Hue on an HDInsight cluster. This section provides instructions about how to use the script when provisioning the cluster using the Azure Classic Portal.\n\n> [AZURE.NOTE] You can also use Azure PowerShell or the HDInsight .NET SDK to create a cluster using this script. For more information on using these methods, see [Customize HDInsight clusters with Script Actions](hdinsight-hadoop-customize-cluster-linux.md).\n\n1. Start provisioning a cluster by using the steps in [Provision HDInsight clusters on Linux](hdinsight-hadoop-provision-linux-clusters.md#portal), but do not complete provisioning.\n\n    > [AZURE.NOTE] To install Hue on HDInsight clusters, the recommended headnode size is at least A4 (8 cores, 14 GB memory).\n\n2. On the **Optional Configuration** blade, select **Script Actions**, and provide the information as shown below:\n\n    ![Provide script action parameters for Hue](./media/hdinsight-hadoop-hue-linux/hue_script_action.png \"Provide script action parameters for Hue\")\n\n    * __NAME__: Enter a friendly name for the script action.\n    * __SCRIPT URI__: https://hdiconfigactions.blob.core.windows.net/linuxhueconfigactionv02/install-hue-uber-v02.sh\n    * __HEAD__: Check this option\n    * __WORKER__: Leave this blank.\n    * __ZOOKEEPER__: Leave this blank.\n    * __PARAMETERS__: Leave this blank.\n\n3. At the bottom of the **Script Actions**, use the **Select** button to save the configuration. Finally, use the **Select** button at the bottom of the **Optional Configuration** blade to save the optional configuration information.\n\n4. Continue provisioning the cluster as described in [Provision HDInsight clusters on Linux](hdinsight-hadoop-provision-linux-clusters.md#portal).\n\n## Use Hue with HDInsight clusters\n\nSSH Tunneling is the only way to access Hue on the cluster once it is running. Tunneling via SSH allows the traffic to go directly to the headnode of the cluster where Hue is running. After the cluster has finished provisioning, use the following steps to use Hue on an HDInsight Linux cluster.\n\n1. Use the information in [Use SSH Tunneling to access Ambari web UI, ResourceManager, JobHistory, NameNode, Oozie, and other web UI's](hdinsight-linux-ambari-ssh-tunnel.md) to create an SSH tunnel from your client system to the HDInsight cluster, and then configure your Web browser to use the SSH tunnel as a proxy.\n\n2. Once you have created an SSH tunnel and configured your browser to proxy traffic through it, you must find the host name of the head node. Use the following steps to get this information from Ambari:\n\n    1. In a browser, go to https://CLUSTERNAME.azurehdinsight.net. When prompted, use the Admin username and password to authenticate to the site.\n    \n    2. From the menu at the top of the page, select __Hosts__.\n    \n    3. Select the entry that begins with __hn0__. When the page opens, the host name will be displayed at the top. The format of the host name is __hn0-CLUSTERNAME.randomcharacters.cx.internal.cloudapp.net__. This is the host name you must use when connecting to Hue.\n\n2. Once you have created an SSH tunnel and configured your browser to proxy traffic through it, use the browser to open the Hue portal at http://HOSTNAME:8888. Replace HOSTNAME with the name you obtained from Ambari in the previous step.\n\n    > [AZURE.NOTE] When you log in for the first time, you will be prompted to create an account to log into the Hue portal. The credentials you specify here will be limited to the portal and are not related to the admin or SSH user credentials you specified while provision the cluster.\n\n    ![Login to the Hue portal](./media/hdinsight-hadoop-hue-linux/HDI.Hue.Portal.Login.png \"Specify credentials for Hue portal\")\n\n### Run a Hive query\n\n1. From the Hue portal, click **Query Editors**, and then click **Hive** to open the Hive editor.\n\n    ![Use Hive](./media/hdinsight-hadoop-hue-linux/HDI.Hue.Portal.Hive.png \"Use Hive\")\n\n2. On the **Assist** tab, under **Database**, you should see **hivesampletable**. This is a sample table that is shipped with all Hadoop clusters on HDInsight. Enter a sample query in the right pane and see the output on the **Results** tab in the pane below, as shown in the screen capture.\n\n    ![Run Hive query](./media/hdinsight-hadoop-hue-linux/HDI.Hue.Portal.Hive.Query.png \"Run Hive query\")\n\n    You can also use the **Chart** tab to see a visual representation of the result.\n\n### Browse the cluster storage\n\n1. From the Hue portal, click **File Browser** in the top-right corner of the menu bar.\n\n2. By default the file browser opens at the **/user/myuser** directory. Click the forward slash right before the user directory in the path to go to the root of the Azure storage container associated with the cluster.\n\n    ![Use file browser](./media/hdinsight-hadoop-hue-linux/HDI.Hue.Portal.File.Browser.png \"Use file browser\")\n\n3. Right-click on a file or folder to see the available operations. Use the **Upload** button in the right corner to upload files to the current directory. Use the **New** button to create new files or directories.\n\n> [AZURE.NOTE] The Hue file browser can only show the contents of the default container associated with the HDInsight cluster. Any additional storage accounts/containers that you might have associated with the cluster will not be accessible using the file browser. However, the additional containers associated with the cluster will always be accessible for the Hive jobs. For example, if you enter the command `dfs -ls wasb://newcontainer@mystore.blob.core.windows.net` in the Hive editor, you can see the contents of additional containers as well. In this command, **newcontainer** is not the default container associated with a cluster.\n\n## Important considerations\n\n1. The script used to install Hue installs it only on Head node 0 of the cluster.\n\n2. During installation, multiple Hadoop services (HDFS, YARN, MR2, Oozie) are restarted for updating the configuration. After the script finishes installing Hue, it might take some time for other Hadoop services to start up. This might affect Hue's performance initially. Once all services start up, Hue will be fully functional.\n\n3.  Hue does not understand Tez jobs, which is the current default for Hive. If you want to use MapReduce as the Hive execution engine, update the script to use the following command in your script:\n\n        set hive.execution.engine=mr;\n\n4.  With Linux clusters, you can have a scenario where your services are running on head node 0 while the Resource Manager could be running on head node 1. Such a scenario might result in errors (shown below) when using Hue to view details of RUNNING jobs on the cluster. However, you can view the job details when the job has completed.\n\n    ![Hue portal error](./media/hdinsight-hadoop-hue-linux/HDI.Hue.Portal.Error.png \"Hue portal error\")\n\n    This is due to a known issue. As a workaround, modify Ambari so that the active Resource Manager also runs on head node 0.\n\n5.  Hue understands WebHDFS while HDInsight clusters use Azure Storage using `wasb://`. So, the custom script used with script action installs WebWasb, which is a WebHDFS-compatible service for talking to WASB. So, even though the Hue portal says HDFS in places (like when you move your mouse over the **File Browser**), it should be interpreted as WASB.\n\n\n## Next steps\n\n- [Install and use Spark on HDInsight clusters](hdinsight-hadoop-spark-install-linux.md) for  instructions about how to use cluster customization to install and use Spark on HDInsight Hadoop clusters. Spark is an open-source parallel processing framework that supports in-memory processing to boost the performance of big data analytic applications.\n\n- [Install Giraph on HDInsight clusters](hdinsight-hadoop-giraph-install-linux.md). Use cluster customization to install Giraph on HDInsight Hadoop clusters. Giraph allows you to perform graph processing using Hadoop, and it can be used with Azure HDInsight.\n\n- [Install Solr on HDInsight clusters](hdinsight-hadoop-solr-install-linux.md). Use cluster customization to install Solr on HDInsight Hadoop clusters. Solr allows you to perform powerful search operations on stored data.\n\n- [Install R on HDInsight clusters](hdinsight-hadoop-r-scripts-linux.md). Use cluster customization to install R on HDInsight Hadoop clusters. R is an open-source language and environment for statistical computing. It provides hundreds of built-in statistical functions and its own programming language that combines aspects of functional and object-oriented programming. It also provides extensive graphical capabilities.\n\n[powershell-install-configure]: install-configure-powershell-linux.md\n[hdinsight-provision]: hdinsight-provision-clusters-linux.md\n[hdinsight-cluster-customize]: hdinsight-hadoop-customize-cluster-linux.md\n[hdinsight-install-spark]: hdinsight-hadoop-spark-install-linux.md\n"
}