{
  "nodes": [
    {
      "pos": [
        27,
        97
      ],
      "content": "Use Script Action to install Spark on Hadoop cluster | Microsoft Azure"
    },
    {
      "pos": [
        116,
        191
      ],
      "content": "Learn how to customize an HDInsight cluster with Spark using Script Action."
    },
    {
      "pos": [
        495,
        565
      ],
      "content": "Install and use Spark on HDInsight Hadoop clusters using Script Action"
    },
    {
      "pos": [
        569,
        1032
      ],
      "content": "<ph id=\"ph2\">[AZURE.IMPORTANT]</ph><ph id=\"ph3\"/> This article is now deprecated. HDInsight now provides Spark as a first-class cluster type for Windows-based clusters, which means you can now directly create a Spark cluster without modifying a Hadoop cluster using Script action. Using the Spark cluster type, you get an HDInsight version 3.2 cluster with Spark version 1.3.1.  To install different versions of Spark, you can use Script action. HDInsight provides a sample Script Action script.",
      "nodes": [
        {
          "content": "<ph id=\"ph2\">[AZURE.IMPORTANT]</ph><ph id=\"ph3\"/> This article is now deprecated.",
          "pos": [
            0,
            81
          ]
        },
        {
          "content": "HDInsight now provides Spark as a first-class cluster type for Windows-based clusters, which means you can now directly create a Spark cluster without modifying a Hadoop cluster using Script action.",
          "pos": [
            82,
            280
          ]
        },
        {
          "content": "Using the Spark cluster type, you get an HDInsight version 3.2 cluster with Spark version 1.3.1.",
          "pos": [
            281,
            377
          ]
        },
        {
          "content": "To install different versions of Spark, you can use Script action.",
          "pos": [
            379,
            445
          ]
        },
        {
          "content": "HDInsight provides a sample Script Action script.",
          "pos": [
            446,
            495
          ]
        }
      ]
    },
    {
      "pos": [
        1034,
        1160
      ],
      "content": "Learn how to install Spark on Windows based HDInsight using Script Action, and how to run Spark queries on HDInsight clusters."
    },
    {
      "pos": [
        1163,
        1183
      ],
      "content": "<bpt id=\"p1\">**</bpt>Related articles<ept id=\"p1\">**</ept>"
    },
    {
      "pos": [
        1186,
        1277
      ],
      "content": "<bpt id=\"p2\">[</bpt>Install Spark on Linux-based HDInsight clusters<ept id=\"p2\">](hdinsight-hadoop-spark-install-linux.md)</ept>."
    },
    {
      "pos": [
        1281,
        1404
      ],
      "content": "<bpt id=\"p3\">[</bpt>Create Hadoop clusters in HDInsight<ept id=\"p3\">](hdinsight-provision-clusters.md)</ept>: general information on creating HDInsight clusters."
    },
    {
      "pos": [
        1408,
        1559
      ],
      "content": "<bpt id=\"p4\">[</bpt>Get Started with Apache Spark on HDInsight<ept id=\"p4\">](hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql.md)</ept>: create a Spark type cluster on Windows OS."
    },
    {
      "pos": [
        1563,
        1717
      ],
      "content": "<bpt id=\"p5\">[</bpt>Customize HDInsight cluster using Script Action<ept id=\"p5\">][hdinsight-cluster-customize]</ept>: general information on customizing HDInsight clusters using Script Action."
    },
    {
      "pos": [
        1721,
        1803
      ],
      "content": "<bpt id=\"p6\">[</bpt>Develop Script Action scripts for HDInsight<ept id=\"p6\">](hdinsight-hadoop-script-actions.md)</ept>."
    },
    {
      "pos": [
        1808,
        1822
      ],
      "content": "What is Spark?"
    },
    {
      "pos": [
        1897,
        1909
      ],
      "content": "Apache Spark"
    },
    {
      "pos": [
        1914,
        2190
      ],
      "content": "is an open-source parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications. Spark's in-memory computation capabilities make it a good choice for iterative algorithms in machine learning and graph computations.",
      "nodes": [
        {
          "content": "is an open-source parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications.",
          "pos": [
            0,
            142
          ]
        },
        {
          "content": "Spark's in-memory computation capabilities make it a good choice for iterative algorithms in machine learning and graph computations.",
          "pos": [
            143,
            276
          ]
        }
      ]
    },
    {
      "pos": [
        2192,
        2524
      ],
      "content": "Spark can also be used to perform conventional disk-based data processing. Spark improves the traditional MapReduce framework by avoiding writes to disk in the intermediate stages. Also, Spark is compatible with the Hadoop Distributed File System (HDFS) and Azure Blob storage so the existing data can easily be processed via Spark.",
      "nodes": [
        {
          "content": "Spark can also be used to perform conventional disk-based data processing.",
          "pos": [
            0,
            74
          ]
        },
        {
          "content": "Spark improves the traditional MapReduce framework by avoiding writes to disk in the intermediate stages.",
          "pos": [
            75,
            180
          ]
        },
        {
          "content": "Also, Spark is compatible with the Hadoop Distributed File System (HDFS) and Azure Blob storage so the existing data can easily be processed via Spark.",
          "pos": [
            181,
            332
          ]
        }
      ]
    },
    {
      "pos": [
        2526,
        2617
      ],
      "content": "This topic provides instructions on how to customize an HDInsight cluster to install Spark."
    },
    {
      "pos": [
        2622,
        2658
      ],
      "content": "Install Spark using the Azure Portal"
    },
    {
      "pos": [
        2660,
        3069
      ],
      "content": "A sample script to install Spark on an HDInsight cluster is available from a read-only Azure storage blob at <bpt id=\"p7\">[</bpt>https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv03/spark-installer-v03.ps1<ept id=\"p7\">](https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv03/spark-installer-v03.ps1)</ept>. This script can install Spark 1.2.0 or Spark 1.0.2 depending on the version of the HDInsight cluster you create.",
      "nodes": [
        {
          "content": "A sample script to install Spark on an HDInsight cluster is available from a read-only Azure storage blob at <bpt id=\"p7\">[</bpt>https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv03/spark-installer-v03.ps1<ept id=\"p7\">](https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv03/spark-installer-v03.ps1)</ept>.",
          "pos": [
            0,
            334
          ]
        },
        {
          "content": "This script can install Spark 1.2.0 or Spark 1.0.2 depending on the version of the HDInsight cluster you create.",
          "pos": [
            335,
            447
          ]
        }
      ]
    },
    {
      "pos": [
        3073,
        3168
      ],
      "content": "If you use the script while creating an <bpt id=\"p8\">**</bpt>HDInsight 3.2<ept id=\"p8\">**</ept><ph id=\"ph4\"/> cluster, it installs <bpt id=\"p9\">**</bpt>Spark 1.2.0<ept id=\"p9\">**</ept>."
    },
    {
      "pos": [
        3171,
        3266
      ],
      "content": "If you use the script while creating an <bpt id=\"p10\">**</bpt>HDInsight 3.1<ept id=\"p10\">**</ept><ph id=\"ph5\"/> cluster, it installs <bpt id=\"p11\">**</bpt>Spark 1.0.2<ept id=\"p11\">**</ept>."
    },
    {
      "pos": [
        3268,
        3356
      ],
      "content": "You can modify this script or create your own script to install other versions of Spark."
    },
    {
      "pos": [
        3360,
        3559
      ],
      "content": "<ph id=\"ph6\">[AZURE.NOTE]</ph><ph id=\"ph7\"/> The sample script works only with HDInsight 3.1 and 3.2 clusters. For more information on HDInsight cluster versions, see <bpt id=\"p12\">[</bpt>HDInsight cluster versions<ept id=\"p12\">](hdinsight-component-versioning.md)</ept>.",
      "nodes": [
        {
          "content": "<ph id=\"ph6\">[AZURE.NOTE]</ph><ph id=\"ph7\"/> The sample script works only with HDInsight 3.1 and 3.2 clusters.",
          "pos": [
            0,
            110
          ]
        },
        {
          "content": "For more information on HDInsight cluster versions, see <bpt id=\"p12\">[</bpt>HDInsight cluster versions<ept id=\"p12\">](hdinsight-component-versioning.md)</ept>.",
          "pos": [
            111,
            271
          ]
        }
      ]
    },
    {
      "pos": [
        3564,
        3775
      ],
      "content": "Start creating a cluster by using the <bpt id=\"p13\">**</bpt>CUSTOM CREATE<ept id=\"p13\">**</ept><ph id=\"ph8\"/> option, as described at <bpt id=\"p14\">[</bpt>Create Hadoop clusters in HDInsight<ept id=\"p14\">](hdinsight-provision-clusters.md#portal)</ept>. Pick the cluster version depending on the following:",
      "nodes": [
        {
          "content": "Start creating a cluster by using the <bpt id=\"p13\">**</bpt>CUSTOM CREATE<ept id=\"p13\">**</ept><ph id=\"ph8\"/> option, as described at <bpt id=\"p14\">[</bpt>Create Hadoop clusters in HDInsight<ept id=\"p14\">](hdinsight-provision-clusters.md#portal)</ept>.",
          "pos": [
            0,
            252
          ]
        },
        {
          "content": "Pick the cluster version depending on the following:",
          "pos": [
            253,
            305
          ]
        }
      ]
    },
    {
      "pos": [
        3783,
        3855
      ],
      "content": "If you want to install <bpt id=\"p15\">**</bpt>Spark 1.2.0<ept id=\"p15\">**</ept>, create an HDInsight 3.2 cluster."
    },
    {
      "pos": [
        3862,
        3934
      ],
      "content": "If you want to install <bpt id=\"p16\">**</bpt>Spark 1.0.2<ept id=\"p16\">**</ept>, create an HDInsight 3.1 cluster."
    },
    {
      "pos": [
        3940,
        4073
      ],
      "content": "On the <bpt id=\"p17\">**</bpt>Script Actions<ept id=\"p17\">**</ept><ph id=\"ph9\"/> page of the wizard, click <bpt id=\"p18\">**</bpt>add script action<ept id=\"p18\">**</ept><ph id=\"ph10\"/> to provide details about the script action, as shown below:"
    },
    {
      "pos": [
        4079,
        4235
      ],
      "content": "<ph id=\"ph11\">![</ph>Use Script Action to customize a cluster<ph id=\"ph12\">](./media/hdinsight-hadoop-spark-install/HDI.CustomProvision.Page6.png \"Use Script Action to customize a cluster\")</ph>"
    },
    {
      "pos": [
        4241,
        5109
      ],
      "leadings": [
        "",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   "
      ],
      "content": "<ph id=\"ph13\">&lt;table border='1'&gt;</ph><ph id=\"ph14\">\n     &lt;tr&gt;</ph><ph id=\"ph15\">&lt;th&gt;</ph>Property<ph id=\"ph16\">&lt;/th&gt;</ph><ph id=\"ph17\">&lt;th&gt;</ph>Value<ph id=\"ph18\">&lt;/th&gt;</ph><ph id=\"ph19\">&lt;/tr&gt;</ph><ph id=\"ph20\">\n     &lt;tr&gt;</ph><ph id=\"ph21\">&lt;td&gt;</ph>Name<ph id=\"ph22\">&lt;/td&gt;</ph><ph id=\"ph23\">\n         &lt;td&gt;</ph>Specify a name for the script action. For example, <ph id=\"ph24\">&lt;b&gt;</ph>Install Spark<ph id=\"ph25\">&lt;/b&gt;</ph>.<ph id=\"ph26\">&lt;/td&gt;</ph><ph id=\"ph27\">&lt;/tr&gt;</ph><ph id=\"ph28\">\n     &lt;tr&gt;</ph><ph id=\"ph29\">&lt;td&gt;</ph>Script URI<ph id=\"ph30\">&lt;/td&gt;</ph><ph id=\"ph31\">\n         &lt;td&gt;</ph>Specify the Uniform Resource Identifier (URI) to the script that is invoked to customize the cluster. For example, <ph id=\"ph32\">&lt;i&gt;</ph>https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv03/spark-installer-v03.ps1<ph id=\"ph33\">&lt;/i&gt;</ph><ph id=\"ph34\">&lt;/td&gt;</ph><ph id=\"ph35\">&lt;/tr&gt;</ph><ph id=\"ph36\">\n     &lt;tr&gt;</ph><ph id=\"ph37\">&lt;td&gt;</ph>Node Type<ph id=\"ph38\">&lt;/td&gt;</ph><ph id=\"ph39\">\n         &lt;td&gt;</ph>Specify the nodes on which the customization script is run. You can choose <ph id=\"ph40\">&lt;b&gt;</ph>All nodes<ph id=\"ph41\">&lt;/b&gt;</ph>, <ph id=\"ph42\">&lt;b&gt;</ph>Head nodes only<ph id=\"ph43\">&lt;/b&gt;</ph>, or <ph id=\"ph44\">&lt;b&gt;</ph>Worker nodes only<ph id=\"ph45\">&lt;/b&gt;</ph>.\n     <ph id=\"ph46\">&lt;tr&gt;</ph><ph id=\"ph47\">&lt;td&gt;</ph>Parameters<ph id=\"ph48\">&lt;/td&gt;</ph><ph id=\"ph49\">\n         &lt;td&gt;</ph>Specify the parameters, if required by the script. The script to install Spark does not require any parameters so you can leave this blank.<ph id=\"ph50\">&lt;/td&gt;</ph><ph id=\"ph51\">&lt;/tr&gt;</ph><ph id=\"ph52\">\n &lt;/table&gt;</ph>",
      "nodes": [
        {
          "content": "<ph id=\"ph13\">&lt;table border='1'&gt;</ph><ph id=\"ph14\">\n     &lt;tr&gt;</ph><ph id=\"ph15\">&lt;th&gt;</ph>Property<ph id=\"ph16\">&lt;/th&gt;</ph><ph id=\"ph17\">&lt;th&gt;</ph>Value<ph id=\"ph18\">&lt;/th&gt;</ph><ph id=\"ph19\">&lt;/tr&gt;</ph><ph id=\"ph20\">\n     &lt;tr&gt;</ph><ph id=\"ph21\">&lt;td&gt;</ph>Name<ph id=\"ph22\">&lt;/td&gt;</ph><ph id=\"ph23\">\n         &lt;td&gt;</ph>Specify a name for the script action.",
          "pos": [
            0,
            413
          ]
        },
        {
          "content": "For example, <ph id=\"ph24\">&lt;b&gt;</ph>Install Spark<ph id=\"ph25\">&lt;/b&gt;</ph>.<ph id=\"ph26\">&lt;/td&gt;</ph><ph id=\"ph27\">&lt;/tr&gt;</ph><ph id=\"ph28\">\n     &lt;tr&gt;</ph><ph id=\"ph29\">&lt;td&gt;</ph>Script URI<ph id=\"ph30\">&lt;/td&gt;</ph><ph id=\"ph31\">\n         &lt;td&gt;</ph>Specify the Uniform Resource Identifier (URI) to the script that is invoked to customize the cluster.",
          "pos": [
            414,
            802
          ]
        },
        {
          "content": "For example, <ph id=\"ph32\">&lt;i&gt;</ph>https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv03/spark-installer-v03.ps1<ph id=\"ph33\">&lt;/i&gt;</ph><ph id=\"ph34\">&lt;/td&gt;</ph><ph id=\"ph35\">&lt;/tr&gt;</ph><ph id=\"ph36\">\n     &lt;tr&gt;</ph><ph id=\"ph37\">&lt;td&gt;</ph>Node Type<ph id=\"ph38\">&lt;/td&gt;</ph><ph id=\"ph39\">\n         &lt;td&gt;</ph>Specify the nodes on which the customization script is run.",
          "pos": [
            803,
            1225
          ]
        },
        {
          "content": "You can choose <ph id=\"ph40\">&lt;b&gt;</ph>All nodes<ph id=\"ph41\">&lt;/b&gt;</ph>, <ph id=\"ph42\">&lt;b&gt;</ph>Head nodes only<ph id=\"ph43\">&lt;/b&gt;</ph>, or <ph id=\"ph44\">&lt;b&gt;</ph>Worker nodes only<ph id=\"ph45\">&lt;/b&gt;</ph>.",
          "pos": [
            1226,
            1461
          ]
        },
        {
          "content": "<ph id=\"ph46\">&lt;tr&gt;</ph><ph id=\"ph47\">&lt;td&gt;</ph>Parameters<ph id=\"ph48\">&lt;/td&gt;</ph><ph id=\"ph49\">\n         &lt;td&gt;</ph>Specify the parameters, if required by the script.",
          "pos": [
            1467,
            1654
          ]
        },
        {
          "content": "The script to install Spark does not require any parameters so you can leave this blank.<ph id=\"ph50\">&lt;/td&gt;</ph><ph id=\"ph51\">&lt;/tr&gt;</ph><ph id=\"ph52\">\n &lt;/table&gt;</ph>",
          "pos": [
            1655,
            1838
          ]
        }
      ]
    },
    {
      "pos": [
        5115,
        5286
      ],
      "content": "You can add more than one script action to install multiple components on the cluster. After you have added the scripts, click the checkmark to start creating the cluster.",
      "nodes": [
        {
          "content": "You can add more than one script action to install multiple components on the cluster.",
          "pos": [
            0,
            86
          ]
        },
        {
          "content": "After you have added the scripts, click the checkmark to start creating the cluster.",
          "pos": [
            87,
            171
          ]
        }
      ]
    },
    {
      "pos": [
        5288,
        5466
      ],
      "content": "You can also use the script to install Spark on HDInsight by using Azure PowerShell or the HDInsight .NET SDK. Instructions for these procedures are provided later in this topic.",
      "nodes": [
        {
          "content": "You can also use the script to install Spark on HDInsight by using Azure PowerShell or the HDInsight .NET SDK.",
          "pos": [
            0,
            110
          ]
        },
        {
          "content": "Instructions for these procedures are provided later in this topic.",
          "pos": [
            111,
            178
          ]
        }
      ]
    },
    {
      "pos": [
        5471,
        5493
      ],
      "content": "Use Spark in HDInsight"
    },
    {
      "pos": [
        5494,
        5702
      ],
      "content": "Spark provides APIs in Scala, Python, and Java. You can also use the interactive Spark shell to run Spark queries. This section provides instructions on how to use the different approaches to work with Spark:",
      "nodes": [
        {
          "content": "Spark provides APIs in Scala, Python, and Java.",
          "pos": [
            0,
            47
          ]
        },
        {
          "content": "You can also use the interactive Spark shell to run Spark queries.",
          "pos": [
            48,
            114
          ]
        },
        {
          "content": "This section provides instructions on how to use the different approaches to work with Spark:",
          "pos": [
            115,
            208
          ]
        }
      ]
    },
    {
      "pos": [
        5706,
        5767
      ],
      "content": "<bpt id=\"p19\">[</bpt>Use the Spark shell to run interactive queries<ept id=\"p19\">](#sparkshell)</ept>"
    },
    {
      "pos": [
        5770,
        5827
      ],
      "content": "<bpt id=\"p20\">[</bpt>Use the Spark shell to run Spark SQL queries<ept id=\"p20\">](#sparksql)</ept>"
    },
    {
      "pos": [
        5830,
        5875
      ],
      "content": "<bpt id=\"p21\">[</bpt>Use a standalone Scala program<ept id=\"p21\">](#standalone)</ept>"
    },
    {
      "pos": [
        5877,
        5880
      ],
      "content": "###"
    },
    {
      "pos": [
        5905,
        5951
      ],
      "content": "Use the Spark shell to run interactive queries"
    },
    {
      "pos": [
        5952,
        6183
      ],
      "content": "Perform the following steps to run Spark queries from an interactive Spark shell. In this section, we run a Spark query on a sample data file (/example/data/gutenberg/davinci.txt) that is available on HDInsight clusters by default.",
      "nodes": [
        {
          "content": "Perform the following steps to run Spark queries from an interactive Spark shell.",
          "pos": [
            0,
            81
          ]
        },
        {
          "content": "In this section, we run a Spark query on a sample data file (/example/data/gutenberg/davinci.txt) that is available on HDInsight clusters by default.",
          "pos": [
            82,
            231
          ]
        }
      ]
    },
    {
      "pos": [
        6188,
        6432
      ],
      "content": "From the Azure portal, enable Remote Desktop for the cluster you created with Spark installed, and then remote into the cluster. For instructions, see <bpt id=\"p22\">[</bpt>Connect to HDInsight clusters using RDP<ept id=\"p22\">](hdinsight-administer-use-management-portal.md#rdp)</ept>.",
      "nodes": [
        {
          "content": "From the Azure portal, enable Remote Desktop for the cluster you created with Spark installed, and then remote into the cluster.",
          "pos": [
            0,
            128
          ]
        },
        {
          "content": "For instructions, see <bpt id=\"p22\">[</bpt>Connect to HDInsight clusters using RDP<ept id=\"p22\">](hdinsight-administer-use-management-portal.md#rdp)</ept>.",
          "pos": [
            129,
            284
          ]
        }
      ]
    },
    {
      "pos": [
        6437,
        6654
      ],
      "content": "In the Remote Desktop Protocol (RDP) session, from the desktop, open the Hadoop command line (from a desktop shortcut), and navigate to the location where Spark is installed; for example, <bpt id=\"p23\">**</bpt>C:\\apps\\dist\\spark-1.2.0<ept id=\"p23\">**</ept>."
    },
    {
      "pos": [
        6660,
        6711
      ],
      "content": "Run the following command to start the Spark shell:"
    },
    {
      "pos": [
        6759,
        6825
      ],
      "content": "After the command finishes running, you should get a Scala prompt:"
    },
    {
      "pos": [
        6847,
        7086
      ],
      "content": "On the Scala prompt, enter the Spark query shown below. This query counts the occurrence of each word in the davinci.txt file that is available at the /example/data/gutenberg/ location on the Azure Blob storage associated with the cluster.",
      "nodes": [
        {
          "content": "On the Scala prompt, enter the Spark query shown below.",
          "pos": [
            0,
            55
          ]
        },
        {
          "content": "This query counts the occurrence of each word in the davinci.txt file that is available at the /example/data/gutenberg/ location on the Azure Blob storage associated with the cluster.",
          "pos": [
            56,
            239
          ]
        }
      ]
    },
    {
      "pos": [
        7305,
        7346
      ],
      "content": "The output should resemble the following:"
    },
    {
      "pos": [
        7352,
        7488
      ],
      "content": "<ph id=\"ph53\">![</ph>Output from running Scala interactive shell in an HDInsight cluster<ph id=\"ph54\">](./media/hdinsight-hadoop-spark-install/hdi-scala-interactive.png)</ph>"
    },
    {
      "pos": [
        7494,
        7528
      ],
      "content": "Enter :q to exit the Scala prompt."
    },
    {
      "pos": [
        7542,
        7545
      ],
      "content": "###"
    },
    {
      "pos": [
        7568,
        7612
      ],
      "content": "Use the Spark shell to run Spark SQL queries"
    },
    {
      "pos": [
        7614,
        7937
      ],
      "content": "Spark SQL allows you to use Spark to run relational queries expressed in Structured Query Language (SQL), HiveQL, or Scala. In this section, we look at using Spark to run a Hive query on a sample Hive table. The Hive table used in this section (called <bpt id=\"p24\">**</bpt>hivesampletable<ept id=\"p24\">**</ept>) is available by default when you create a cluster.",
      "nodes": [
        {
          "content": "Spark SQL allows you to use Spark to run relational queries expressed in Structured Query Language (SQL), HiveQL, or Scala.",
          "pos": [
            0,
            123
          ]
        },
        {
          "content": "In this section, we look at using Spark to run a Hive query on a sample Hive table.",
          "pos": [
            124,
            207
          ]
        },
        {
          "content": "The Hive table used in this section (called <bpt id=\"p24\">**</bpt>hivesampletable<ept id=\"p24\">**</ept>) is available by default when you create a cluster.",
          "pos": [
            208,
            363
          ]
        }
      ]
    },
    {
      "pos": [
        7940,
        8092
      ],
      "content": "<ph id=\"ph55\">[AZURE.NOTE]</ph><ph id=\"ph56\"/> The sample below was created against <bpt id=\"p25\">**</bpt>Spark 1.2.0<ept id=\"p25\">**</ept>, which is installed if you run the script action while creating HDInsight 3.2 cluster."
    },
    {
      "pos": [
        8097,
        8341
      ],
      "content": "From the Azure portal, enable Remote Desktop for the cluster you created with Spark installed, and then remote into the cluster. For instructions, see <bpt id=\"p26\">[</bpt>Connect to HDInsight clusters using RDP<ept id=\"p26\">](hdinsight-administer-use-management-portal.md#rdp)</ept>.",
      "nodes": [
        {
          "content": "From the Azure portal, enable Remote Desktop for the cluster you created with Spark installed, and then remote into the cluster.",
          "pos": [
            0,
            128
          ]
        },
        {
          "content": "For instructions, see <bpt id=\"p26\">[</bpt>Connect to HDInsight clusters using RDP<ept id=\"p26\">](hdinsight-administer-use-management-portal.md#rdp)</ept>.",
          "pos": [
            129,
            284
          ]
        }
      ]
    },
    {
      "pos": [
        8346,
        8537
      ],
      "content": "In the RDP session, from the desktop, open the Hadoop command line (from a desktop shortcut), and navigate to the location where Spark is installed; for example, <bpt id=\"p27\">**</bpt>C:\\apps\\dist\\spark-1.2.0<ept id=\"p27\">**</ept>."
    },
    {
      "pos": [
        8543,
        8594
      ],
      "content": "Run the following command to start the Spark shell:"
    },
    {
      "pos": [
        8642,
        8708
      ],
      "content": "After the command finishes running, you should get a Scala prompt:"
    },
    {
      "pos": [
        8730,
        8831
      ],
      "content": "On the Scala prompt, set the Hive context. This is required to work with Hive queries by using Spark.",
      "nodes": [
        {
          "content": "On the Scala prompt, set the Hive context.",
          "pos": [
            0,
            42
          ]
        },
        {
          "content": "This is required to work with Hive queries by using Spark.",
          "pos": [
            43,
            101
          ]
        }
      ]
    },
    {
      "pos": [
        8910,
        8995
      ],
      "content": "Note that <bpt id=\"p28\">**</bpt>sc<ept id=\"p28\">**</ept><ph id=\"ph57\"/> is default Spark context that is set when you start the Spark shell."
    },
    {
      "pos": [
        9000,
        9184
      ],
      "content": "Run a Hive query by using the Hive context and print the output to the console. The query retrieves data on devices of a specific make and limits the number of records retrieved to 20.",
      "nodes": [
        {
          "content": "Run a Hive query by using the Hive context and print the output to the console.",
          "pos": [
            0,
            79
          ]
        },
        {
          "content": "The query retrieves data on devices of a specific make and limits the number of records retrieved to 20.",
          "pos": [
            80,
            184
          ]
        }
      ]
    },
    {
      "pos": [
        9316,
        9360
      ],
      "content": "You should see an output like the following:"
    },
    {
      "pos": [
        9366,
        9480
      ],
      "content": "<ph id=\"ph58\">![</ph>Output from running Spark SQL on an HDInsight cluster<ph id=\"ph59\">](./media/hdinsight-hadoop-spark-install/hdi-spark-sql.png)</ph>"
    },
    {
      "pos": [
        9485,
        9519
      ],
      "content": "Enter :q to exit the Scala prompt."
    },
    {
      "pos": [
        9562,
        9592
      ],
      "content": "Use a standalone Scala program"
    },
    {
      "pos": [
        9594,
        9951
      ],
      "content": "In this section, we write a Scala application that counts the number of lines containing the letters 'a' and 'b' in a sample data file (/example/data/gutenberg/davinci.txt) that is available on HDInsight clusters by default. To write and use a standalone Scala program with a cluster customized with Spark installation, you must perform the following steps:",
      "nodes": [
        {
          "content": "In this section, we write a Scala application that counts the number of lines containing the letters 'a' and 'b' in a sample data file (/example/data/gutenberg/davinci.txt) that is available on HDInsight clusters by default.",
          "pos": [
            0,
            224
          ]
        },
        {
          "content": "To write and use a standalone Scala program with a cluster customized with Spark installation, you must perform the following steps:",
          "pos": [
            225,
            357
          ]
        }
      ]
    },
    {
      "pos": [
        9955,
        9976
      ],
      "content": "Write a Scala program"
    },
    {
      "pos": [
        9979,
        10023
      ],
      "content": "Build the Scala program to get the .jar file"
    },
    {
      "pos": [
        10026,
        10052
      ],
      "content": "Run the job on the cluster"
    },
    {
      "pos": [
        10059,
        10080
      ],
      "content": "Write a Scala program"
    },
    {
      "pos": [
        10081,
        10203
      ],
      "content": "In this section, you write a Scala program that counts the number of lines containing 'a' and 'b' in the sample data file."
    },
    {
      "pos": [
        10208,
        10256
      ],
      "content": "Open a text editor and paste the following code:"
    },
    {
      "pos": [
        11050,
        11098
      ],
      "content": "Save the file with the name <bpt id=\"p29\">**</bpt>SimpleApp.scala<ept id=\"p29\">**</ept>."
    },
    {
      "pos": [
        11105,
        11128
      ],
      "content": "Build the Scala program"
    },
    {
      "pos": [
        11129,
        11157
      ],
      "content": "In this section, you use the"
    },
    {
      "pos": [
        11230,
        11247
      ],
      "content": "Simple Build Tool"
    },
    {
      "pos": [
        11252,
        11415
      ],
      "content": "(or sbt) to build the Scala program. sbt requires Java 1.6 or later, so make sure you have the right version of Java installed before continuing with this section.",
      "nodes": [
        {
          "content": "(or sbt) to build the Scala program.",
          "pos": [
            0,
            36
          ]
        },
        {
          "content": "sbt requires Java 1.6 or later, so make sure you have the right version of Java installed before continuing with this section.",
          "pos": [
            37,
            163
          ]
        }
      ]
    },
    {
      "pos": [
        11420,
        11507
      ],
      "content": "Install sbt from http://www.scala-sbt.org/0.13/tutorial/Installing-sbt-on-Windows.html."
    },
    {
      "pos": [
        11511,
        11778
      ],
      "content": "Create a folder called <bpt id=\"p30\">**</bpt>SimpleScalaApp<ept id=\"p30\">**</ept>, and within this folder create a file called <bpt id=\"p31\">**</bpt>simple.sbt<ept id=\"p31\">**</ept>. This is a configuration file that contains information about the Scala version, library dependencies, etc. Paste the following into the simple.sbt file and save it:",
      "nodes": [
        {
          "content": "Create a folder called <bpt id=\"p30\">**</bpt>SimpleScalaApp<ept id=\"p30\">**</ept>, and within this folder create a file called <bpt id=\"p31\">**</bpt>simple.sbt<ept id=\"p31\">**</ept>.",
          "pos": [
            0,
            182
          ]
        },
        {
          "content": "This is a configuration file that contains information about the Scala version, library dependencies, etc. Paste the following into the simple.sbt file and save it:",
          "pos": [
            183,
            347
          ]
        }
      ]
    },
    {
      "pos": [
        12022,
        12211
      ],
      "content": "Under the <bpt id=\"p32\">**</bpt>SimpleScalaApp<ept id=\"p32\">**</ept><ph id=\"ph60\"/> folder, create a directory structure <bpt id=\"p33\">**</bpt>\\src\\main\\scala<ept id=\"p33\">**</ept><ph id=\"ph61\"/> and paste the Scala program (<bpt id=\"p34\">**</bpt>SimpleApp.scala<ept id=\"p34\">**</ept>) you created earlier under the \\src\\main\\scala folder."
    },
    {
      "pos": [
        12215,
        12312
      ],
      "content": "Open a command prompt, navigate to the SimpleScalaApp directory, and enter the following command:"
    },
    {
      "pos": [
        12519,
        12545
      ],
      "content": "Run the job on the cluster"
    },
    {
      "pos": [
        12546,
        12749
      ],
      "content": "In this section, you remote into the cluster that has Spark installed and then copy the SimpleScalaApp project's target folder. You then use the <bpt id=\"p35\">**</bpt>spark-submit<ept id=\"p35\">**</ept><ph id=\"ph62\"/> command to submit the job on the cluster.",
      "nodes": [
        {
          "content": "In this section, you remote into the cluster that has Spark installed and then copy the SimpleScalaApp project's target folder.",
          "pos": [
            0,
            127
          ]
        },
        {
          "content": "You then use the <bpt id=\"p35\">**</bpt>spark-submit<ept id=\"p35\">**</ept><ph id=\"ph62\"/> command to submit the job on the cluster.",
          "pos": [
            128,
            258
          ]
        }
      ]
    },
    {
      "pos": [
        12754,
        12961
      ],
      "content": "Remote into the cluster that has Spark installed. From the computer where you wrote and built the SimpleApp.scala program, copy the <bpt id=\"p36\">**</bpt>SimpleScalaApp\\target<ept id=\"p36\">**</ept><ph id=\"ph63\"/> folder and paste it to a location on the cluster.",
      "nodes": [
        {
          "content": "Remote into the cluster that has Spark installed.",
          "pos": [
            0,
            49
          ]
        },
        {
          "content": "From the computer where you wrote and built the SimpleApp.scala program, copy the <bpt id=\"p36\">**</bpt>SimpleScalaApp\\target<ept id=\"p36\">**</ept><ph id=\"ph63\"/> folder and paste it to a location on the cluster.",
          "pos": [
            50,
            262
          ]
        }
      ]
    },
    {
      "pos": [
        12965,
        13101
      ],
      "content": "In the RDP session, from the desktop, open the Hadoop command line, and navigate to the location where you pasted the <bpt id=\"p37\">**</bpt>target<ept id=\"p37\">**</ept><ph id=\"ph64\"/> folder."
    },
    {
      "pos": [
        13105,
        13168
      ],
      "content": "Enter the following command to run the SimpleApp.scala program:"
    },
    {
      "pos": [
        13301,
        13375
      ],
      "content": "When the program finishes running, the output is displayed on the console."
    },
    {
      "pos": [
        13431,
        13467
      ],
      "content": "Install Spark using Azure PowerShell"
    },
    {
      "pos": [
        13469,
        13932
      ],
      "content": "In this section, we use the <bpt id=\"p38\">**</bpt><ph id=\"ph65\">&lt;a href = \"http://msdn.microsoft.com/library/dn858088.aspx\" target=\"_blank\"&gt;</ph>Add-AzureHDInsightScriptAction<ph id=\"ph66\">&lt;/a&gt;</ph><ept id=\"p38\">**</ept><ph id=\"ph67\"/> cmdlet to invoke scripts by using Script Action to customize a cluster. Before proceeding, make sure you have installed and configured Azure PowerShell. For information on configuring a workstation to run Azure PowerShell cmdlets for HDInsight, see <bpt id=\"p39\">[</bpt>Install and configure Azure PowerShell<ept id=\"p39\">][powershell-install-configure]</ept>.",
      "nodes": [
        {
          "content": "In this section, we use the <bpt id=\"p38\">**</bpt><ph id=\"ph65\">&lt;a href = \"http://msdn.microsoft.com/library/dn858088.aspx\" target=\"_blank\"&gt;</ph>Add-AzureHDInsightScriptAction<ph id=\"ph66\">&lt;/a&gt;</ph><ept id=\"p38\">**</ept><ph id=\"ph67\"/> cmdlet to invoke scripts by using Script Action to customize a cluster.",
          "pos": [
            0,
            319
          ]
        },
        {
          "content": "Before proceeding, make sure you have installed and configured Azure PowerShell.",
          "pos": [
            320,
            400
          ]
        },
        {
          "content": "For information on configuring a workstation to run Azure PowerShell cmdlets for HDInsight, see <bpt id=\"p39\">[</bpt>Install and configure Azure PowerShell<ept id=\"p39\">][powershell-install-configure]</ept>.",
          "pos": [
            401,
            608
          ]
        }
      ]
    },
    {
      "pos": [
        13934,
        13962
      ],
      "content": "Perform the following steps:"
    },
    {
      "pos": [
        13967,
        14035
      ],
      "content": "Open an Azure PowerShell window and declare the following variables:"
    },
    {
      "pos": [
        14821,
        14918
      ],
      "content": "Specify the configuration values such as nodes in the cluster and the default storage to be used."
    },
    {
      "pos": [
        15353,
        15526
      ],
      "content": "Use the <bpt id=\"p40\">**</bpt>Add-AzureHDInsightScriptAction<ept id=\"p40\">**</ept><ph id=\"ph68\"/> cmdlet to add a script action to cluster configuration. Later, when the cluster is being created, the script action gets executed.",
      "nodes": [
        {
          "content": "Use the <bpt id=\"p40\">**</bpt>Add-AzureHDInsightScriptAction<ept id=\"p40\">**</ept><ph id=\"ph68\"/> cmdlet to add a script action to cluster configuration.",
          "pos": [
            0,
            153
          ]
        },
        {
          "content": "Later, when the cluster is being created, the script action gets executed.",
          "pos": [
            154,
            228
          ]
        }
      ]
    },
    {
      "pos": [
        15808,
        15881
      ],
      "content": "<bpt id=\"p41\">**</bpt>Add-AzureHDInsightScriptAction<ept id=\"p41\">**</ept><ph id=\"ph69\"/> cmdlet takes the following parameters:"
    },
    {
      "pos": [
        15887,
        18313
      ],
      "leadings": [
        "",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   "
      ],
      "content": "<ph id=\"ph70\">&lt;table style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse;\"&gt;</ph><ph id=\"ph71\">\n &lt;tr&gt;</ph><ph id=\"ph72\">\n &lt;th style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; width:90px; padding-left:5px; padding-right:5px;\"&gt;</ph>Parameter<ph id=\"ph73\">&lt;/th&gt;</ph><ph id=\"ph74\">\n &lt;th style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; width:550px; padding-left:5px; padding-right:5px;\"&gt;</ph>Definition<ph id=\"ph75\">&lt;/th&gt;</ph><ph id=\"ph76\">&lt;/tr&gt;</ph><ph id=\"ph77\">\n &lt;tr&gt;</ph><ph id=\"ph78\">\n &lt;td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\"&gt;</ph>Config<ph id=\"ph79\">&lt;/td&gt;</ph><ph id=\"ph80\">\n &lt;td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px; padding-right:5px;\"&gt;</ph>The configuration object to which script action information is added.<ph id=\"ph81\">&lt;/td&gt;</ph><ph id=\"ph82\">&lt;/tr&gt;</ph><ph id=\"ph83\">\n &lt;tr&gt;</ph><ph id=\"ph84\">\n &lt;td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\"&gt;</ph>Name<ph id=\"ph85\">&lt;/td&gt;</ph><ph id=\"ph86\">\n &lt;td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\"&gt;</ph>Name of the script action.<ph id=\"ph87\">&lt;/td&gt;</ph><ph id=\"ph88\">&lt;/tr&gt;</ph><ph id=\"ph89\">\n &lt;tr&gt;</ph><ph id=\"ph90\">\n &lt;td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\"&gt;</ph>ClusterRoleCollection<ph id=\"ph91\">&lt;/td&gt;</ph><ph id=\"ph92\">\n &lt;td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\"&gt;</ph>Specifies the nodes on which the customization script is run. The valid values are HeadNode (to install on the head node) or DataNode (to install on all the data nodes). You can use either or both values.<ph id=\"ph93\">&lt;/td&gt;</ph><ph id=\"ph94\">&lt;/tr&gt;</ph><ph id=\"ph95\">\n &lt;tr&gt;</ph><ph id=\"ph96\">\n &lt;td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\"&gt;</ph>Uri<ph id=\"ph97\">&lt;/td&gt;</ph><ph id=\"ph98\">\n &lt;td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\"&gt;</ph>Specifies the URI to the script that is executed.<ph id=\"ph99\">&lt;/td&gt;</ph><ph id=\"ph100\">&lt;/tr&gt;</ph><ph id=\"ph101\">\n &lt;tr&gt;</ph><ph id=\"ph102\">\n &lt;td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\"&gt;</ph>Parameters<ph id=\"ph103\">&lt;/td&gt;</ph><ph id=\"ph104\">\n &lt;td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\"&gt;</ph>Parameters required by the script. The sample script used in this topic does not require any parameters, and hence you do not see this parameter in the snippet above.\n <ph id=\"ph105\">&lt;/td&gt;</ph><ph id=\"ph106\">&lt;/tr&gt;</ph><ph id=\"ph107\">\n &lt;/table&gt;</ph>",
      "nodes": [
        {
          "content": "<ph id=\"ph70\">&lt;table style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse;\"&gt;</ph><ph id=\"ph71\">\n &lt;tr&gt;</ph><ph id=\"ph72\">\n &lt;th style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; width:90px; padding-left:5px; padding-right:5px;\"&gt;</ph>Parameter<ph id=\"ph73\">&lt;/th&gt;</ph><ph id=\"ph74\">\n &lt;th style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; width:550px; padding-left:5px; padding-right:5px;\"&gt;</ph>Definition<ph id=\"ph75\">&lt;/th&gt;</ph><ph id=\"ph76\">&lt;/tr&gt;</ph><ph id=\"ph77\">\n &lt;tr&gt;</ph><ph id=\"ph78\">\n &lt;td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\"&gt;</ph>Config<ph id=\"ph79\">&lt;/td&gt;</ph><ph id=\"ph80\">\n &lt;td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px; padding-right:5px;\"&gt;</ph>The configuration object to which script action information is added.<ph id=\"ph81\">&lt;/td&gt;</ph><ph id=\"ph82\">&lt;/tr&gt;</ph><ph id=\"ph83\">\n &lt;tr&gt;</ph><ph id=\"ph84\">\n &lt;td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\"&gt;</ph>Name<ph id=\"ph85\">&lt;/td&gt;</ph><ph id=\"ph86\">\n &lt;td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\"&gt;</ph>Name of the script action.<ph id=\"ph87\">&lt;/td&gt;</ph><ph id=\"ph88\">&lt;/tr&gt;</ph><ph id=\"ph89\">\n &lt;tr&gt;</ph><ph id=\"ph90\">\n &lt;td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\"&gt;</ph>ClusterRoleCollection<ph id=\"ph91\">&lt;/td&gt;</ph><ph id=\"ph92\">\n &lt;td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\"&gt;</ph>Specifies the nodes on which the customization script is run.",
          "pos": [
            0,
            2018
          ]
        },
        {
          "content": "The valid values are HeadNode (to install on the head node) or DataNode (to install on all the data nodes).",
          "pos": [
            2019,
            2126
          ]
        },
        {
          "content": "You can use either or both values.<ph id=\"ph93\">&lt;/td&gt;</ph><ph id=\"ph94\">&lt;/tr&gt;</ph><ph id=\"ph95\">\n &lt;tr&gt;</ph><ph id=\"ph96\">\n &lt;td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\"&gt;</ph>Uri<ph id=\"ph97\">&lt;/td&gt;</ph><ph id=\"ph98\">\n &lt;td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\"&gt;</ph>Specifies the URI to the script that is executed.<ph id=\"ph99\">&lt;/td&gt;</ph><ph id=\"ph100\">&lt;/tr&gt;</ph><ph id=\"ph101\">\n &lt;tr&gt;</ph><ph id=\"ph102\">\n &lt;td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\"&gt;</ph>Parameters<ph id=\"ph103\">&lt;/td&gt;</ph><ph id=\"ph104\">\n &lt;td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\"&gt;</ph>Parameters required by the script.",
          "pos": [
            2127,
            3092
          ]
        },
        {
          "content": "The sample script used in this topic does not require any parameters, and hence you do not see this parameter in the snippet above.",
          "pos": [
            3093,
            3224
          ]
        },
        {
          "content": "<ph id=\"ph105\">&lt;/td&gt;</ph><ph id=\"ph106\">&lt;/tr&gt;</ph><ph id=\"ph107\">\n &lt;/table&gt;</ph>",
          "pos": [
            3226,
            3324
          ]
        }
      ]
    },
    {
      "pos": [
        18318,
        18384
      ],
      "content": "Finally, start creating a customized cluster with Spark installed."
    },
    {
      "pos": [
        18552,
        18664
      ],
      "content": "When prompted, enter the credentials for the cluster. It can take several minutes before the cluster is created.",
      "nodes": [
        {
          "content": "When prompted, enter the credentials for the cluster.",
          "pos": [
            0,
            53
          ]
        },
        {
          "content": "It can take several minutes before the cluster is created.",
          "pos": [
            54,
            112
          ]
        }
      ]
    },
    {
      "pos": [
        18669,
        18699
      ],
      "content": "Install Spark using PowerShell"
    },
    {
      "pos": [
        18701,
        18825
      ],
      "content": "See <bpt id=\"p42\">[</bpt>Customize HDInsight clusters using Script Action<ept id=\"p42\">](hdinsight-hadoop-customize-cluster.md#call_scripts_using_powershell)</ept>."
    },
    {
      "pos": [
        18830,
        18858
      ],
      "content": "Install Spark using .NET SDK"
    },
    {
      "pos": [
        18860,
        18990
      ],
      "content": "See <bpt id=\"p43\">[</bpt>Customize HDInsight clusters using Script Action<ept id=\"p43\">](hdinsight-hadoop-customize-cluster.md#call_scripts_using_azure_powershell)</ept>."
    },
    {
      "pos": [
        18996,
        19004
      ],
      "content": "See also"
    },
    {
      "pos": [
        19008,
        19148
      ],
      "content": "<bpt id=\"p44\">[</bpt>Install Spark on Linux-based HDInsight clusters<ept id=\"p44\">](hdinsight-hadoop-spark-install-linux.md)</ept>: install Spark on Linux based HDInsight clusters."
    },
    {
      "pos": [
        19151,
        19249
      ],
      "content": "<bpt id=\"p45\">[</bpt>Create Hadoop clusters in HDInsight<ept id=\"p45\">](hdinsight-provision-clusters.md)</ept>: create HDInsight clusters."
    },
    {
      "pos": [
        19252,
        19397
      ],
      "content": "<bpt id=\"p46\">[</bpt>Get Started with Apache Spark on HDInsight<ept id=\"p46\">](hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql.md)</ept>: get started with Spark on HDInsight."
    },
    {
      "pos": [
        19400,
        19529
      ],
      "content": "<bpt id=\"p47\">[</bpt>Customize HDInsight cluster using Script Action<ept id=\"p47\">][hdinsight-cluster-customize]</ept>: customize HDInsight clusters using Script Action."
    },
    {
      "pos": [
        19532,
        19645
      ],
      "content": "<bpt id=\"p48\">[</bpt>Develop Script Action scripts for HDInsight<ept id=\"p48\">](hdinsight-hadoop-script-actions.md)</ept>: develop Script Action scripts."
    },
    {
      "pos": [
        19648,
        20091
      ],
      "content": "<bpt id=\"p49\">[</bpt>Install R on HDInsight clusters<ept id=\"p49\">][hdinsight-install-r]</ept><ph id=\"ph108\"/> provides instructions on how to use cluster customization to install and use R on HDInsight Hadoop clusters. R is an open-source language and environment for statistical computing. It provides hundreds of built-in statistical functions and its own programming language that combines aspects of functional and object-oriented programming. It also provides extensive graphical capabilities.",
      "nodes": [
        {
          "content": "<bpt id=\"p49\">[</bpt>Install R on HDInsight clusters<ept id=\"p49\">][hdinsight-install-r]</ept><ph id=\"ph108\"/> provides instructions on how to use cluster customization to install and use R on HDInsight Hadoop clusters.",
          "pos": [
            0,
            219
          ]
        },
        {
          "content": "R is an open-source language and environment for statistical computing.",
          "pos": [
            220,
            291
          ]
        },
        {
          "content": "It provides hundreds of built-in statistical functions and its own programming language that combines aspects of functional and object-oriented programming.",
          "pos": [
            292,
            448
          ]
        },
        {
          "content": "It also provides extensive graphical capabilities.",
          "pos": [
            449,
            499
          ]
        }
      ]
    },
    {
      "pos": [
        20094,
        20344
      ],
      "content": "<bpt id=\"p50\">[</bpt>Install Giraph on HDInsight clusters<ept id=\"p50\">](hdinsight-hadoop-giraph-install.md)</ept>. Use cluster customization to install Giraph on HDInsight Hadoop clusters. Giraph allows you to perform graph processing by using Hadoop, and can be used with Azure HDInsight.",
      "nodes": [
        {
          "content": "<bpt id=\"p50\">[</bpt>Install Giraph on HDInsight clusters<ept id=\"p50\">](hdinsight-hadoop-giraph-install.md)</ept>.",
          "pos": [
            0,
            115
          ]
        },
        {
          "content": "Use cluster customization to install Giraph on HDInsight Hadoop clusters.",
          "pos": [
            116,
            189
          ]
        },
        {
          "content": "Giraph allows you to perform graph processing by using Hadoop, and can be used with Azure HDInsight.",
          "pos": [
            190,
            290
          ]
        }
      ]
    },
    {
      "pos": [
        20347,
        20560
      ],
      "content": "<bpt id=\"p51\">[</bpt>Install Solr on HDInsight clusters<ept id=\"p51\">](hdinsight-hadoop-solr-install.md)</ept>. Use cluster customization to install Solr on HDInsight Hadoop clusters. Solr allows you to perform powerful search operations on data stored.",
      "nodes": [
        {
          "content": "<bpt id=\"p51\">[</bpt>Install Solr on HDInsight clusters<ept id=\"p51\">](hdinsight-hadoop-solr-install.md)</ept>.",
          "pos": [
            0,
            111
          ]
        },
        {
          "content": "Use cluster customization to install Solr on HDInsight Hadoop clusters.",
          "pos": [
            112,
            183
          ]
        },
        {
          "content": "Solr allows you to perform powerful search operations on data stored.",
          "pos": [
            184,
            253
          ]
        }
      ]
    }
  ],
  "content": "<properties\n    pageTitle=\"Use Script Action to install Spark on Hadoop cluster | Microsoft Azure\"\n    description=\"Learn how to customize an HDInsight cluster with Spark using Script Action.\"\n    services=\"hdinsight\"\n    documentationCenter=\"\"\n    authors=\"nitinme\"\n    manager=\"paulettm\"\n    editor=\"cgronlun\"/>\n\n<tags\n    ms.service=\"hdinsight\"\n    ms.workload=\"big-data\"\n    ms.tgt_pltfrm=\"na\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\"\n    ms.date=\"02/05/2016\"\n    ms.author=\"nitinme\"/>\n\n# Install and use Spark on HDInsight Hadoop clusters using Script Action\n\n> [AZURE.IMPORTANT] This article is now deprecated. HDInsight now provides Spark as a first-class cluster type for Windows-based clusters, which means you can now directly create a Spark cluster without modifying a Hadoop cluster using Script action. Using the Spark cluster type, you get an HDInsight version 3.2 cluster with Spark version 1.3.1.  To install different versions of Spark, you can use Script action. HDInsight provides a sample Script Action script.\n\nLearn how to install Spark on Windows based HDInsight using Script Action, and how to run Spark queries on HDInsight clusters.\n\n\n**Related articles**\n- [Install Spark on Linux-based HDInsight clusters](hdinsight-hadoop-spark-install-linux.md).\n\n- [Create Hadoop clusters in HDInsight](hdinsight-provision-clusters.md): general information on creating HDInsight clusters.\n\n- [Get Started with Apache Spark on HDInsight](hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql.md): create a Spark type cluster on Windows OS.\n\n- [Customize HDInsight cluster using Script Action][hdinsight-cluster-customize]: general information on customizing HDInsight clusters using Script Action.\n\n- [Develop Script Action scripts for HDInsight](hdinsight-hadoop-script-actions.md).\n\n## What is Spark?\n\n<a href=\"http://spark.apache.org/docs/latest/index.html\" target=\"_blank\">Apache Spark</a> is an open-source parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications. Spark's in-memory computation capabilities make it a good choice for iterative algorithms in machine learning and graph computations.\n\nSpark can also be used to perform conventional disk-based data processing. Spark improves the traditional MapReduce framework by avoiding writes to disk in the intermediate stages. Also, Spark is compatible with the Hadoop Distributed File System (HDFS) and Azure Blob storage so the existing data can easily be processed via Spark.\n\nThis topic provides instructions on how to customize an HDInsight cluster to install Spark.\n\n## Install Spark using the Azure Portal\n\nA sample script to install Spark on an HDInsight cluster is available from a read-only Azure storage blob at [https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv03/spark-installer-v03.ps1](https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv03/spark-installer-v03.ps1). This script can install Spark 1.2.0 or Spark 1.0.2 depending on the version of the HDInsight cluster you create.\n\n- If you use the script while creating an **HDInsight 3.2** cluster, it installs **Spark 1.2.0**.\n- If you use the script while creating an **HDInsight 3.1** cluster, it installs **Spark 1.0.2**.\n\nYou can modify this script or create your own script to install other versions of Spark.\n\n> [AZURE.NOTE] The sample script works only with HDInsight 3.1 and 3.2 clusters. For more information on HDInsight cluster versions, see [HDInsight cluster versions](hdinsight-component-versioning.md).\n\n1. Start creating a cluster by using the **CUSTOM CREATE** option, as described at [Create Hadoop clusters in HDInsight](hdinsight-provision-clusters.md#portal). Pick the cluster version depending on the following:\n\n    - If you want to install **Spark 1.2.0**, create an HDInsight 3.2 cluster.\n    - If you want to install **Spark 1.0.2**, create an HDInsight 3.1 cluster.\n\n\n2. On the **Script Actions** page of the wizard, click **add script action** to provide details about the script action, as shown below:\n\n    ![Use Script Action to customize a cluster](./media/hdinsight-hadoop-spark-install/HDI.CustomProvision.Page6.png \"Use Script Action to customize a cluster\")\n\n    <table border='1'>\n        <tr><th>Property</th><th>Value</th></tr>\n        <tr><td>Name</td>\n            <td>Specify a name for the script action. For example, <b>Install Spark</b>.</td></tr>\n        <tr><td>Script URI</td>\n            <td>Specify the Uniform Resource Identifier (URI) to the script that is invoked to customize the cluster. For example, <i>https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv03/spark-installer-v03.ps1</i></td></tr>\n        <tr><td>Node Type</td>\n            <td>Specify the nodes on which the customization script is run. You can choose <b>All nodes</b>, <b>Head nodes only</b>, or <b>Worker nodes only</b>.\n        <tr><td>Parameters</td>\n            <td>Specify the parameters, if required by the script. The script to install Spark does not require any parameters so you can leave this blank.</td></tr>\n    </table>\n\n    You can add more than one script action to install multiple components on the cluster. After you have added the scripts, click the checkmark to start creating the cluster.\n\nYou can also use the script to install Spark on HDInsight by using Azure PowerShell or the HDInsight .NET SDK. Instructions for these procedures are provided later in this topic.\n\n## Use Spark in HDInsight\nSpark provides APIs in Scala, Python, and Java. You can also use the interactive Spark shell to run Spark queries. This section provides instructions on how to use the different approaches to work with Spark:\n\n- [Use the Spark shell to run interactive queries](#sparkshell)\n- [Use the Spark shell to run Spark SQL queries](#sparksql)\n- [Use a standalone Scala program](#standalone)\n\n###<a name=\"sparkshell\"></a>Use the Spark shell to run interactive queries\nPerform the following steps to run Spark queries from an interactive Spark shell. In this section, we run a Spark query on a sample data file (/example/data/gutenberg/davinci.txt) that is available on HDInsight clusters by default.\n\n1. From the Azure portal, enable Remote Desktop for the cluster you created with Spark installed, and then remote into the cluster. For instructions, see [Connect to HDInsight clusters using RDP](hdinsight-administer-use-management-portal.md#rdp).\n\n2. In the Remote Desktop Protocol (RDP) session, from the desktop, open the Hadoop command line (from a desktop shortcut), and navigate to the location where Spark is installed; for example, **C:\\apps\\dist\\spark-1.2.0**.\n\n\n3. Run the following command to start the Spark shell:\n\n         .\\bin\\spark-shell --master yarn\n\n    After the command finishes running, you should get a Scala prompt:\n\n         scala>\n\n5. On the Scala prompt, enter the Spark query shown below. This query counts the occurrence of each word in the davinci.txt file that is available at the /example/data/gutenberg/ location on the Azure Blob storage associated with the cluster.\n\n        val file = sc.textFile(\"/example/data/gutenberg/davinci.txt\")\n        val counts = file.flatMap(line => line.split(\" \")).map(word => (word, 1)).reduceByKey(_ + _)\n        counts.toArray().foreach(println)\n\n6. The output should resemble the following:\n\n    ![Output from running Scala interactive shell in an HDInsight cluster](./media/hdinsight-hadoop-spark-install/hdi-scala-interactive.png)\n\n\n7. Enter :q to exit the Scala prompt.\n\n        :q\n\n###<a name=\"sparksql\"></a>Use the Spark shell to run Spark SQL queries\n\nSpark SQL allows you to use Spark to run relational queries expressed in Structured Query Language (SQL), HiveQL, or Scala. In this section, we look at using Spark to run a Hive query on a sample Hive table. The Hive table used in this section (called **hivesampletable**) is available by default when you create a cluster.\n\n>[AZURE.NOTE] The sample below was created against **Spark 1.2.0**, which is installed if you run the script action while creating HDInsight 3.2 cluster.\n\n1. From the Azure portal, enable Remote Desktop for the cluster you created with Spark installed, and then remote into the cluster. For instructions, see [Connect to HDInsight clusters using RDP](hdinsight-administer-use-management-portal.md#rdp).\n\n2. In the RDP session, from the desktop, open the Hadoop command line (from a desktop shortcut), and navigate to the location where Spark is installed; for example, **C:\\apps\\dist\\spark-1.2.0**.\n\n\n3. Run the following command to start the Spark shell:\n\n         .\\bin\\spark-shell --master yarn\n\n    After the command finishes running, you should get a Scala prompt:\n\n         scala>\n\n4. On the Scala prompt, set the Hive context. This is required to work with Hive queries by using Spark.\n\n        val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)\n\n    Note that **sc** is default Spark context that is set when you start the Spark shell.\n\n5. Run a Hive query by using the Hive context and print the output to the console. The query retrieves data on devices of a specific make and limits the number of records retrieved to 20.\n\n        hiveContext.sql(\"\"\"SELECT * FROM hivesampletable WHERE devicemake LIKE \"HTC%\" LIMIT 20\"\"\").collect().foreach(println)\n\n6. You should see an output like the following:\n\n    ![Output from running Spark SQL on an HDInsight cluster](./media/hdinsight-hadoop-spark-install/hdi-spark-sql.png)\n\n7. Enter :q to exit the Scala prompt.\n\n        :q\n\n### <a name=\"standalone\"></a>Use a standalone Scala program\n\nIn this section, we write a Scala application that counts the number of lines containing the letters 'a' and 'b' in a sample data file (/example/data/gutenberg/davinci.txt) that is available on HDInsight clusters by default. To write and use a standalone Scala program with a cluster customized with Spark installation, you must perform the following steps:\n\n- Write a Scala program\n- Build the Scala program to get the .jar file\n- Run the job on the cluster\n\n#### Write a Scala program\nIn this section, you write a Scala program that counts the number of lines containing 'a' and 'b' in the sample data file.\n\n1. Open a text editor and paste the following code:\n\n\n        /* SimpleApp.scala */\n        import org.apache.spark.SparkContext\n        import org.apache.spark.SparkContext._\n        import org.apache.spark.SparkConf\n\n        object SimpleApp {\n          def main(args: Array[String]) {\n            val logFile = \"/example/data/gutenberg/davinci.txt\"         //Location of the sample data file on Azure Blob storage\n            val conf = new SparkConf().setAppName(\"SimpleApplication\")\n            val sc = new SparkContext(conf)\n            val logData = sc.textFile(logFile, 2).cache()\n            val numAs = logData.filter(line => line.contains(\"a\")).count()\n            val numBs = logData.filter(line => line.contains(\"b\")).count()\n            println(\"Lines with a: %s, Lines with b: %s\".format(numAs, numBs))\n          }\n        }\n\n2. Save the file with the name **SimpleApp.scala**.\n\n#### Build the Scala program\nIn this section, you use the <a href=\"http://www.scala-sbt.org/0.13/docs/index.html\" target=\"_blank\">Simple Build Tool</a> (or sbt) to build the Scala program. sbt requires Java 1.6 or later, so make sure you have the right version of Java installed before continuing with this section.\n\n1. Install sbt from http://www.scala-sbt.org/0.13/tutorial/Installing-sbt-on-Windows.html.\n2. Create a folder called **SimpleScalaApp**, and within this folder create a file called **simple.sbt**. This is a configuration file that contains information about the Scala version, library dependencies, etc. Paste the following into the simple.sbt file and save it:\n\n\n        name := \"SimpleApp\"\n\n        version := \"1.0\"\n\n        scalaVersion := \"2.10.4\"\n\n        libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"1.2.0\"\n\n\n\n    >[AZURE.NOTE] Make sure you retain the empty lines in the file.\n\n\n3. Under the **SimpleScalaApp** folder, create a directory structure **\\src\\main\\scala** and paste the Scala program (**SimpleApp.scala**) you created earlier under the \\src\\main\\scala folder.\n4. Open a command prompt, navigate to the SimpleScalaApp directory, and enter the following command:\n\n\n        sbt package\n\n\n    Once the application is compiled, you will see a **simpleapp_2.10-1.0.jar** file created under the **\\target\\scala-2.10** directory within the root SimpleScalaApp folder.\n\n\n#### Run the job on the cluster\nIn this section, you remote into the cluster that has Spark installed and then copy the SimpleScalaApp project's target folder. You then use the **spark-submit** command to submit the job on the cluster.\n\n1. Remote into the cluster that has Spark installed. From the computer where you wrote and built the SimpleApp.scala program, copy the **SimpleScalaApp\\target** folder and paste it to a location on the cluster.\n2. In the RDP session, from the desktop, open the Hadoop command line, and navigate to the location where you pasted the **target** folder.\n3. Enter the following command to run the SimpleApp.scala program:\n\n\n        C:\\apps\\dist\\spark-1.2.0\\bin\\spark-submit --class \"SimpleApp\" --master local target/scala-2.10/simpleapp_2.10-1.0.jar\n\n4. When the program finishes running, the output is displayed on the console.\n\n\n        Lines with a: 21374, Lines with b: 11430\n\n## Install Spark using Azure PowerShell\n\nIn this section, we use the **<a href = \"http://msdn.microsoft.com/library/dn858088.aspx\" target=\"_blank\">Add-AzureHDInsightScriptAction</a>** cmdlet to invoke scripts by using Script Action to customize a cluster. Before proceeding, make sure you have installed and configured Azure PowerShell. For information on configuring a workstation to run Azure PowerShell cmdlets for HDInsight, see [Install and configure Azure PowerShell][powershell-install-configure].\n\nPerform the following steps:\n\n1. Open an Azure PowerShell window and declare the following variables:\n\n        # Provide values for these variables\n        $subscriptionName = \"<SubscriptionName>\"        # Name of the Azure subscription\n        $clusterName = \"<HDInsightClusterName>\"         # HDInsight cluster name\n        $storageAccountName = \"<StorageAccountName>\"    # Azure Storage account that hosts the default container\n        $storageAccountKey = \"<StorageAccountKey>\"      # Key for the Storage account\n        $containerName = $clusterName\n        $location = \"<MicrosoftDataCenter>\"             # Location of the HDInsight cluster. It must be in the same data center as the Storage account.\n        $clusterNodes = <ClusterSizeInNumbers>          # Number of nodes in the HDInsight cluster\n        $version = \"<HDInsightClusterVersion>\"          # For example, \"3.2\"\n\n2. Specify the configuration values such as nodes in the cluster and the default storage to be used.\n\n        # Specify the configuration options\n        Select-AzureSubscription $subscriptionName\n        $config = New-AzureHDInsightClusterConfig -ClusterSizeInNodes $clusterNodes\n        $config.DefaultStorageAccount.StorageAccountName=\"$storageAccountName.blob.core.windows.net\"\n        $config.DefaultStorageAccount.StorageAccountKey=$storageAccountKey\n        $config.DefaultStorageAccount.StorageContainerName=$containerName\n\n3. Use the **Add-AzureHDInsightScriptAction** cmdlet to add a script action to cluster configuration. Later, when the cluster is being created, the script action gets executed.\n\n        # Add a script action to the cluster configuration\n        $config = Add-AzureHDInsightScriptAction -Config $config -Name \"Install Spark\" -ClusterRoleCollection HeadNode -Uri https://hdiconfigactions.blob.core.windows.net/sparkconfigactionv03/spark-installer-v03.ps1\n\n    **Add-AzureHDInsightScriptAction** cmdlet takes the following parameters:\n\n    <table style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse;\">\n    <tr>\n    <th style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; width:90px; padding-left:5px; padding-right:5px;\">Parameter</th>\n    <th style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; width:550px; padding-left:5px; padding-right:5px;\">Definition</th></tr>\n    <tr>\n    <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Config</td>\n    <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px; padding-right:5px;\">The configuration object to which script action information is added.</td></tr>\n    <tr>\n    <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Name</td>\n    <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Name of the script action.</td></tr>\n    <tr>\n    <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">ClusterRoleCollection</td>\n    <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Specifies the nodes on which the customization script is run. The valid values are HeadNode (to install on the head node) or DataNode (to install on all the data nodes). You can use either or both values.</td></tr>\n    <tr>\n    <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Uri</td>\n    <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Specifies the URI to the script that is executed.</td></tr>\n    <tr>\n    <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Parameters</td>\n    <td style=\"border-color: #c6c6c6; border-width: 2px; border-style: solid; border-collapse: collapse; padding-left:5px;\">Parameters required by the script. The sample script used in this topic does not require any parameters, and hence you do not see this parameter in the snippet above.\n    </td></tr>\n    </table>\n\n4. Finally, start creating a customized cluster with Spark installed.  \n\n        # Start creating a cluster with Spark installed\n        New-AzureHDInsightCluster -Config $config -Name $clusterName -Location $location -Version $version\n\nWhen prompted, enter the credentials for the cluster. It can take several minutes before the cluster is created.\n\n## Install Spark using PowerShell\n\nSee [Customize HDInsight clusters using Script Action](hdinsight-hadoop-customize-cluster.md#call_scripts_using_powershell).\n\n## Install Spark using .NET SDK\n\nSee [Customize HDInsight clusters using Script Action](hdinsight-hadoop-customize-cluster.md#call_scripts_using_azure_powershell).\n\n\n## See also\n\n- [Install Spark on Linux-based HDInsight clusters](hdinsight-hadoop-spark-install-linux.md): install Spark on Linux based HDInsight clusters.\n- [Create Hadoop clusters in HDInsight](hdinsight-provision-clusters.md): create HDInsight clusters.\n- [Get Started with Apache Spark on HDInsight](hdinsight-apache-spark-zeppelin-notebook-jupyter-spark-sql.md): get started with Spark on HDInsight.\n- [Customize HDInsight cluster using Script Action][hdinsight-cluster-customize]: customize HDInsight clusters using Script Action.\n- [Develop Script Action scripts for HDInsight](hdinsight-hadoop-script-actions.md): develop Script Action scripts.\n- [Install R on HDInsight clusters][hdinsight-install-r] provides instructions on how to use cluster customization to install and use R on HDInsight Hadoop clusters. R is an open-source language and environment for statistical computing. It provides hundreds of built-in statistical functions and its own programming language that combines aspects of functional and object-oriented programming. It also provides extensive graphical capabilities.\n- [Install Giraph on HDInsight clusters](hdinsight-hadoop-giraph-install.md). Use cluster customization to install Giraph on HDInsight Hadoop clusters. Giraph allows you to perform graph processing by using Hadoop, and can be used with Azure HDInsight.\n- [Install Solr on HDInsight clusters](hdinsight-hadoop-solr-install.md). Use cluster customization to install Solr on HDInsight Hadoop clusters. Solr allows you to perform powerful search operations on data stored.\n\n[hdinsight-provision]: hdinsight-provision-clusters.md\n[hdinsight-install-r]: hdinsight-hadoop-r-scripts.md\n[hdinsight-cluster-customize]: hdinsight-hadoop-customize-cluster.md\n[powershell-install-configure]: powershell-install-configure.md\n"
}