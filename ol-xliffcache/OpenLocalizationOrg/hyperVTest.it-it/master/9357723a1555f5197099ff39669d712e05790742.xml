{
  "nodes": [
    {
      "pos": [
        27,
        86
      ],
      "content": "How to choose machine learning algorithms | Microsoft Azure"
    },
    {
      "pos": [
        105,
        251
      ],
      "content": "How to choose Azure Machine Learning algorithms for supervised and unsupervised learning in clustering, classification, or regression experiments."
    },
    {
      "pos": [
        597,
        658
      ],
      "content": "How to choose algorithms for Microsoft Azure Machine Learning"
    },
    {
      "pos": [
        660,
        1121
      ],
      "content": "The answer to the question \"What machine learning algorithm should I use?\" is always \"It depends.\" It depends on the size, quality, and nature of the data. It depends what you want to do with the answer. It depends on how the math of the algorithm was translated into instructions for the computer you are using. And it depends on how much time you have. Even the most experienced data scientists can't tell which algorithm will perform best before trying them.",
      "nodes": [
        {
          "content": "The answer to the question \"What machine learning algorithm should I use?\"",
          "pos": [
            0,
            74
          ]
        },
        {
          "content": "is always \"It depends.\"",
          "pos": [
            75,
            98
          ]
        },
        {
          "content": "It depends on the size, quality, and nature of the data.",
          "pos": [
            99,
            155
          ]
        },
        {
          "content": "It depends what you want to do with the answer.",
          "pos": [
            156,
            203
          ]
        },
        {
          "content": "It depends on how the math of the algorithm was translated into instructions for the computer you are using.",
          "pos": [
            204,
            312
          ]
        },
        {
          "content": "And it depends on how much time you have.",
          "pos": [
            313,
            354
          ]
        },
        {
          "content": "Even the most experienced data scientists can't tell which algorithm will perform best before trying them.",
          "pos": [
            355,
            461
          ]
        }
      ]
    },
    {
      "pos": [
        1126,
        1168
      ],
      "content": "The Machine Learning Algorithm Cheat Sheet"
    },
    {
      "pos": [
        1170,
        1437
      ],
      "content": "The <bpt id=\"p1\">**</bpt>Microsoft Azure Machine Learning Algorithm Cheat Sheet<ept id=\"p1\">**</ept><ph id=\"ph2\"/> helps you choose the right machine learning algorithm for your predictive analytics solutions from the Microsoft Azure Machine Learning library of algorithms.\nThis article walks you through how to use it.",
      "nodes": [
        {
          "content": "The <bpt id=\"p1\">**</bpt>Microsoft Azure Machine Learning Algorithm Cheat Sheet<ept id=\"p1\">**</ept><ph id=\"ph2\"/> helps you choose the right machine learning algorithm for your predictive analytics solutions from the Microsoft Azure Machine Learning library of algorithms.",
          "pos": [
            0,
            273
          ]
        },
        {
          "content": "This article walks you through how to use it.",
          "pos": [
            274,
            319
          ]
        }
      ]
    },
    {
      "pos": [
        1441,
        1652
      ],
      "content": "<ph id=\"ph3\">[AZURE.NOTE]</ph><ph id=\"ph4\"/> To download the cheat sheet and follow along with this article, go to <bpt id=\"p2\">[</bpt>Machine learning algorithm cheat sheet for Microsoft Azure Machine Learning Studio<ept id=\"p2\">](machine-learning-algorithm-cheat-sheet.md)</ept>."
    },
    {
      "pos": [
        1654,
        2138
      ],
      "content": "This cheat sheet has a very specific audience in mind: a beginning data scientist with undergraduate-level machine learning, trying to choose an algorithm to start with in Azure Machine Learning Studio. That means that it makes some generalizations and oversimplifications, but it will point you in a safe direction. It also means that there are lots of algorithms not listed here. As Azure Machine Learning grows to encompass a more complete set of available methods, we'll add them.",
      "nodes": [
        {
          "content": "This cheat sheet has a very specific audience in mind: a beginning data scientist with undergraduate-level machine learning, trying to choose an algorithm to start with in Azure Machine Learning Studio.",
          "pos": [
            0,
            202
          ]
        },
        {
          "content": "That means that it makes some generalizations and oversimplifications, but it will point you in a safe direction.",
          "pos": [
            203,
            316
          ]
        },
        {
          "content": "It also means that there are lots of algorithms not listed here.",
          "pos": [
            317,
            381
          ]
        },
        {
          "content": "As Azure Machine Learning grows to encompass a more complete set of available methods, we'll add them.",
          "pos": [
            382,
            484
          ]
        }
      ]
    },
    {
      "pos": [
        2140,
        2412
      ],
      "content": "These recommendations are compiled feedback and tips from a lot of data scientists and machine learning experts. We didn't agree on everything, but I've tried to harmonize our opinions into a rough consensus. Most of the statements of disagreement begin with \"It depends…\"",
      "nodes": [
        {
          "content": "These recommendations are compiled feedback and tips from a lot of data scientists and machine learning experts.",
          "pos": [
            0,
            112
          ]
        },
        {
          "content": "We didn't agree on everything, but I've tried to harmonize our opinions into a rough consensus.",
          "pos": [
            113,
            208
          ]
        },
        {
          "content": "Most of the statements of disagreement begin with \"It depends…\"",
          "pos": [
            209,
            272
          ]
        }
      ]
    },
    {
      "pos": [
        2418,
        2444
      ],
      "content": "How to use the cheat sheet"
    },
    {
      "pos": [
        2446,
        2913
      ],
      "content": "Read the path and algorithm labels on the chart as \"For <bpt id=\"p3\">*</bpt>&amp;lt;path\nlabel&amp;gt;<ept id=\"p3\">*</ept><ph id=\"ph5\"/> use <bpt id=\"p4\">*</bpt>&amp;lt;algorithm&amp;gt;<ept id=\"p4\">*</ept>.\" For example, \"For <bpt id=\"p5\">*</bpt>speed<ept id=\"p5\">*</ept><ph id=\"ph6\"/> use <bpt id=\"p6\">*</bpt>two\nclass logistic regression<ept id=\"p6\">*</ept>.\" Sometimes more than one branch will apply.\nSometimes none of them will be a perfect fit. They're intended to be\nrule-of-thumb recommendations, so don't worry about it being exact.\nSeveral data scientists I talked with said that the only sure way to\nfind the very best algorithm is to try all of them.",
      "nodes": [
        {
          "content": "Read the path and algorithm labels on the chart as \"For <bpt id=\"p3\">*</bpt>&amp;lt;path\nlabel&amp;gt;<ept id=\"p3\">*</ept><ph id=\"ph5\"/> use <bpt id=\"p4\">*</bpt>&amp;lt;algorithm&amp;gt;<ept id=\"p4\">*</ept>.\"",
          "pos": [
            0,
            208
          ]
        },
        {
          "content": "For example, \"For <bpt id=\"p5\">*</bpt>speed<ept id=\"p5\">*</ept><ph id=\"ph6\"/> use <bpt id=\"p6\">*</bpt>two\nclass logistic regression<ept id=\"p6\">*</ept>.\"",
          "pos": [
            209,
            362
          ]
        },
        {
          "content": "Sometimes more than one branch will apply.",
          "pos": [
            363,
            405
          ]
        },
        {
          "content": "Sometimes none of them will be a perfect fit.",
          "pos": [
            406,
            451
          ]
        },
        {
          "content": "They're intended to be\nrule-of-thumb recommendations, so don't worry about it being exact.",
          "pos": [
            452,
            542
          ]
        },
        {
          "content": "Several data scientists I talked with said that the only sure way to\nfind the very best algorithm is to try all of them.",
          "pos": [
            543,
            663
          ]
        }
      ]
    },
    {
      "pos": [
        2915,
        3217
      ],
      "content": "Here's an example from the <bpt id=\"p7\">[</bpt>Cortana Analytics Gallery<ept id=\"p7\">](http://gallery.azureml.net/)</ept><ph id=\"ph7\"/> of an experiment that tries\nseveral algorithms against the same data and compares the results:\n<bpt id=\"p8\">[</bpt>Compare Multi-class Classifiers: Letter\nrecognition<ept id=\"p8\">](http://gallery.azureml.net/Details/a635502fc98b402a890efe21cec65b92)</ept>."
    },
    {
      "pos": [
        3220,
        3449
      ],
      "content": "<ph id=\"ph8\">[AZURE.TIP]</ph><ph id=\"ph9\"/> To download and print a diagram that gives an overview of the capabilities of Machine Learning Studio, see <bpt id=\"p9\">[</bpt>Overview diagram of Azure Machine Learning Studio capabilities<ept id=\"p9\">](machine-learning-studio-overview-diagram.md)</ept>."
    },
    {
      "pos": [
        3454,
        3481
      ],
      "content": "Flavors of machine learning"
    },
    {
      "pos": [
        3487,
        3497
      ],
      "content": "Supervised"
    },
    {
      "pos": [
        3499,
        3564
      ],
      "content": "Supervised learning algorithms make predictions based on a set of"
    },
    {
      "pos": [
        3565,
        3635
      ],
      "content": "examples. For instance, historical stock prices can be used to hazard\n",
      "nodes": [
        {
          "content": "examples.",
          "pos": [
            0,
            9
          ]
        },
        {
          "content": "For instance, historical stock prices can be used to hazard",
          "pos": [
            10,
            69
          ]
        }
      ]
    },
    {
      "pos": [
        3635,
        3708
      ],
      "content": "guesses at future prices. Each example used for training is labeled with\n",
      "nodes": [
        {
          "content": "guesses at future prices.",
          "pos": [
            0,
            25
          ]
        },
        {
          "content": "Each example used for training is labeled with",
          "pos": [
            26,
            72
          ]
        }
      ]
    },
    {
      "pos": [
        3708,
        3773
      ],
      "content": "the value of interest—in this case the stock price. A supervised\n",
      "nodes": [
        {
          "content": "the value of interest—in this case the stock price.",
          "pos": [
            0,
            51
          ]
        },
        {
          "content": "A supervised",
          "pos": [
            52,
            64
          ]
        }
      ]
    },
    {
      "pos": [
        3773,
        3845
      ],
      "content": "learning algorithm looks for patterns in those value labels. It can use\n",
      "nodes": [
        {
          "content": "learning algorithm looks for patterns in those value labels.",
          "pos": [
            0,
            60
          ]
        },
        {
          "content": "It can use",
          "pos": [
            61,
            71
          ]
        }
      ]
    },
    {
      "pos": [
        3845,
        3916
      ],
      "content": "any information that might be relevant—the day of the week, the season,"
    },
    {
      "pos": [
        3917,
        3984
      ],
      "content": "the company's financial data, the type of industry, the presence of"
    },
    {
      "pos": [
        3985,
        4056
      ],
      "content": "disruptive geopolicitical events—and each algorithm looks for different"
    },
    {
      "pos": [
        4057,
        4126
      ],
      "content": "types of patterns. After the algorithm has found the best pattern it\n",
      "nodes": [
        {
          "content": "types of patterns.",
          "pos": [
            0,
            18
          ]
        },
        {
          "content": "After the algorithm has found the best pattern it",
          "pos": [
            19,
            68
          ]
        }
      ]
    },
    {
      "pos": [
        4126,
        4193
      ],
      "content": "can, it uses that pattern to make predictions for unlabeled testing"
    },
    {
      "pos": [
        4194,
        4217
      ],
      "content": "data—tomorrow's prices."
    },
    {
      "pos": [
        4219,
        4283
      ],
      "content": "This is a popular and useful type of machine learning. With one\n",
      "nodes": [
        {
          "content": "This is a popular and useful type of machine learning.",
          "pos": [
            0,
            54
          ]
        },
        {
          "content": "With one",
          "pos": [
            55,
            63
          ]
        }
      ]
    },
    {
      "pos": [
        4283,
        4362
      ],
      "content": "exception, all of the modules in Azure Machine Learning are supervised learning"
    },
    {
      "pos": [
        4363,
        4436
      ],
      "content": "algorithms. There are several specific types of supervised learning that\n",
      "nodes": [
        {
          "content": "algorithms.",
          "pos": [
            0,
            11
          ]
        },
        {
          "content": "There are several specific types of supervised learning that",
          "pos": [
            12,
            72
          ]
        }
      ]
    },
    {
      "pos": [
        4436,
        4522
      ],
      "content": "are represented within Azure Machine Learning: classification, regression, and anomaly"
    },
    {
      "pos": [
        4523,
        4533
      ],
      "content": "detection."
    },
    {
      "pos": [
        4537,
        4994
      ],
      "content": "<bpt id=\"p10\">**</bpt>Classification<ept id=\"p10\">**</ept>. When the data are being used to predict a\ncategory, supervised learning is also called classification. This is\nthe case when assigning an image as a picture of either a 'cat' or a\n'dog'. When there are only two choices, this is called <bpt id=\"p11\">**</bpt>two-class<ept id=\"p11\">**</ept>\nor <bpt id=\"p12\">**</bpt>binomial classification<ept id=\"p12\">**</ept>. When there are more categories, as\nwhen predicting the winner of the NCAA March Madness tournament, this\nproblem is known as <bpt id=\"p13\">**</bpt>multi-class classification<ept id=\"p13\">**</ept>.",
      "nodes": [
        {
          "content": "<bpt id=\"p10\">**</bpt>Classification<ept id=\"p10\">**</ept>.",
          "pos": [
            0,
            59
          ]
        },
        {
          "content": "When the data are being used to predict a\ncategory, supervised learning is also called classification.",
          "pos": [
            60,
            162
          ]
        },
        {
          "content": "This is\nthe case when assigning an image as a picture of either a 'cat' or a\n'dog'.",
          "pos": [
            163,
            246
          ]
        },
        {
          "content": "When there are only two choices, this is called <bpt id=\"p11\">**</bpt>two-class<ept id=\"p11\">**</ept>\nor <bpt id=\"p12\">**</bpt>binomial classification<ept id=\"p12\">**</ept>.",
          "pos": [
            247,
            420
          ]
        },
        {
          "content": "When there are more categories, as\nwhen predicting the winner of the NCAA March Madness tournament, this\nproblem is known as <bpt id=\"p13\">**</bpt>multi-class classification<ept id=\"p13\">**</ept>.",
          "pos": [
            421,
            617
          ]
        }
      ]
    },
    {
      "pos": [
        4998,
        5110
      ],
      "content": "<bpt id=\"p14\">**</bpt>Regression<ept id=\"p14\">**</ept>. When a value is being predicted, as with stock prices,\nsupervised learning is called regression.",
      "nodes": [
        {
          "content": "<bpt id=\"p14\">**</bpt>Regression<ept id=\"p14\">**</ept>.",
          "pos": [
            0,
            55
          ]
        },
        {
          "content": "When a value is being predicted, as with stock prices,\nsupervised learning is called regression.",
          "pos": [
            56,
            152
          ]
        }
      ]
    },
    {
      "pos": [
        5114,
        5639
      ],
      "content": "<bpt id=\"p15\">**</bpt>Anomaly detection<ept id=\"p15\">**</ept>. Sometimes the goal is to identify data points\nthat are simply unusual. In fraud detection, for example, any highly\nunusual credit card spending patterns are suspect. The possible\nvariations are so numerous and the training examples so few, that it's\nnot feasible to learn what fraudulent activity looks like. The\napproach that anomaly detection takes is to simply learn what normal\nactivity looks like (using a history non-fraudulent transactions) and\nidentify anything that is significantly different.",
      "nodes": [
        {
          "content": "<bpt id=\"p15\">**</bpt>Anomaly detection<ept id=\"p15\">**</ept>.",
          "pos": [
            0,
            62
          ]
        },
        {
          "content": "Sometimes the goal is to identify data points\nthat are simply unusual.",
          "pos": [
            63,
            133
          ]
        },
        {
          "content": "In fraud detection, for example, any highly\nunusual credit card spending patterns are suspect.",
          "pos": [
            134,
            228
          ]
        },
        {
          "content": "The possible\nvariations are so numerous and the training examples so few, that it's\nnot feasible to learn what fraudulent activity looks like.",
          "pos": [
            229,
            371
          ]
        },
        {
          "content": "The\napproach that anomaly detection takes is to simply learn what normal\nactivity looks like (using a history non-fraudulent transactions) and\nidentify anything that is significantly different.",
          "pos": [
            372,
            565
          ]
        }
      ]
    },
    {
      "pos": [
        5645,
        5657
      ],
      "content": "Unsupervised"
    },
    {
      "pos": [
        5659,
        5727
      ],
      "content": "In unsupervised learning, data points have no labels associated with"
    },
    {
      "pos": [
        5728,
        5796
      ],
      "content": "them. Instead, the goal of an unsupervised learning algorithm is to\n",
      "nodes": [
        {
          "content": "them.",
          "pos": [
            0,
            5
          ]
        },
        {
          "content": "Instead, the goal of an unsupervised learning algorithm is to",
          "pos": [
            6,
            67
          ]
        }
      ]
    },
    {
      "pos": [
        5796,
        5865
      ],
      "content": "organize the data in some way or to describe its structure. This can\n",
      "nodes": [
        {
          "content": "organize the data in some way or to describe its structure.",
          "pos": [
            0,
            59
          ]
        },
        {
          "content": "This can",
          "pos": [
            60,
            68
          ]
        }
      ]
    },
    {
      "pos": [
        5865,
        5935
      ],
      "content": "mean grouping it into clusters or finding different ways of looking at"
    },
    {
      "pos": [
        5936,
        5994
      ],
      "content": "complex data so that it appears simpler or more organized."
    },
    {
      "pos": [
        6000,
        6022
      ],
      "content": "Reinforcement learning"
    },
    {
      "pos": [
        6024,
        6092
      ],
      "content": "In reinforcement learning, the algorithm gets to choose an action in"
    },
    {
      "pos": [
        6093,
        6161
      ],
      "content": "response to each data point. The learning algorithm also receives a\n",
      "nodes": [
        {
          "content": "response to each data point.",
          "pos": [
            0,
            28
          ]
        },
        {
          "content": "The learning algorithm also receives a",
          "pos": [
            29,
            67
          ]
        }
      ]
    },
    {
      "pos": [
        6161,
        6233
      ],
      "content": "reward signal a short time later, indicating how good the decision was.\n",
      "nodes": [
        {
          "content": "reward signal a short time later, indicating how good the decision was.",
          "pos": [
            0,
            71
          ]
        },
        {
          "content": "\n",
          "pos": [
            71,
            72
          ]
        }
      ]
    },
    {
      "pos": [
        6233,
        6303
      ],
      "content": "Based on this, the algorithm modifies its strategy in order to achieve"
    },
    {
      "pos": [
        6304,
        6370
      ],
      "content": "the highest reward. Currently there are no reinforcement learning\n",
      "nodes": [
        {
          "content": "the highest reward.",
          "pos": [
            0,
            19
          ]
        },
        {
          "content": "Currently there are no reinforcement learning",
          "pos": [
            20,
            65
          ]
        }
      ]
    },
    {
      "pos": [
        6370,
        6451
      ],
      "content": "algorithm modules in Azure Machine Learning. Reinforcement learning is common in\n",
      "nodes": [
        {
          "content": "algorithm modules in Azure Machine Learning.",
          "pos": [
            0,
            44
          ]
        },
        {
          "content": "Reinforcement learning is common in",
          "pos": [
            45,
            80
          ]
        }
      ]
    },
    {
      "pos": [
        6451,
        6519
      ],
      "content": "robotics, where the set of sensor readings at one point in time is a"
    },
    {
      "pos": [
        6520,
        6593
      ],
      "content": "data point, and the algorithm must choose the robot's next action. It is\n",
      "nodes": [
        {
          "content": "data point, and the algorithm must choose the robot's next action.",
          "pos": [
            0,
            66
          ]
        },
        {
          "content": "It is",
          "pos": [
            67,
            72
          ]
        }
      ]
    },
    {
      "pos": [
        6593,
        6648
      ],
      "content": "also a natural fit for Internet of Things applications."
    },
    {
      "pos": [
        6653,
        6694
      ],
      "content": "Considerations when choosing an algorithm"
    },
    {
      "pos": [
        6700,
        6708
      ],
      "content": "Accuracy"
    },
    {
      "pos": [
        6710,
        7105
      ],
      "content": "Getting the most accurate answer possible isn't always necessary.\nSometimes an approximation is adequate, depending on what you want to\nuse it for. If that's the case, you may be able to cut your processing\ntime dramatically by sticking with more approximate methods. Another\nadvantage of more approximate methods is that they naturally tend to\navoid <bpt id=\"p16\">[</bpt>overfitting<ept id=\"p16\">](https://youtu.be/DQWI1kvmwRg)</ept>.",
      "nodes": [
        {
          "content": "Getting the most accurate answer possible isn't always necessary.",
          "pos": [
            0,
            65
          ]
        },
        {
          "content": "Sometimes an approximation is adequate, depending on what you want to\nuse it for.",
          "pos": [
            66,
            147
          ]
        },
        {
          "content": "If that's the case, you may be able to cut your processing\ntime dramatically by sticking with more approximate methods.",
          "pos": [
            148,
            267
          ]
        },
        {
          "content": "Another\nadvantage of more approximate methods is that they naturally tend to\navoid <bpt id=\"p16\">[</bpt>overfitting<ept id=\"p16\">](https://youtu.be/DQWI1kvmwRg)</ept>.",
          "pos": [
            268,
            435
          ]
        }
      ]
    },
    {
      "pos": [
        7111,
        7124
      ],
      "content": "Training time"
    },
    {
      "pos": [
        7126,
        7198
      ],
      "content": "The number of minutes or hours necessary to train a model varies a great"
    },
    {
      "pos": [
        7199,
        7263
      ],
      "content": "deal between algorithms. Training time is often closely tied to\n",
      "nodes": [
        {
          "content": "deal between algorithms.",
          "pos": [
            0,
            24
          ]
        },
        {
          "content": "Training time is often closely tied to",
          "pos": [
            25,
            63
          ]
        }
      ]
    },
    {
      "pos": [
        7263,
        7327
      ],
      "content": "accuracy—one typically accompanies the other. In addition, some\n",
      "nodes": [
        {
          "content": "accuracy—one typically accompanies the other.",
          "pos": [
            0,
            45
          ]
        },
        {
          "content": "In addition, some",
          "pos": [
            46,
            63
          ]
        }
      ]
    },
    {
      "pos": [
        7327,
        7399
      ],
      "content": "algorithms are more sensitive to the number of data points than others.\n",
      "nodes": [
        {
          "content": "algorithms are more sensitive to the number of data points than others.",
          "pos": [
            0,
            71
          ]
        },
        {
          "content": "\n",
          "pos": [
            71,
            72
          ]
        }
      ]
    },
    {
      "pos": [
        7399,
        7468
      ],
      "content": "When time is limited it can drive the choice of algorithm, especially"
    },
    {
      "pos": [
        7469,
        7496
      ],
      "content": "when the data set is large."
    },
    {
      "pos": [
        7502,
        7511
      ],
      "content": "Linearity"
    },
    {
      "pos": [
        7513,
        7579
      ],
      "content": "Lots of machine learning algorithms make use of linearity. Linear\n",
      "nodes": [
        {
          "content": "Lots of machine learning algorithms make use of linearity.",
          "pos": [
            0,
            58
          ]
        },
        {
          "content": "Linear",
          "pos": [
            59,
            65
          ]
        }
      ]
    },
    {
      "pos": [
        7579,
        7646
      ],
      "content": "classification algorithms assume that classes can be separated by a"
    },
    {
      "pos": [
        7647,
        7720
      ],
      "content": "straight line (or its higher-dimensional analog). These include logistic\n",
      "nodes": [
        {
          "content": "straight line (or its higher-dimensional analog).",
          "pos": [
            0,
            49
          ]
        },
        {
          "content": "These include logistic",
          "pos": [
            50,
            72
          ]
        }
      ]
    },
    {
      "pos": [
        7720,
        7803
      ],
      "content": "regression and support vector machines (as implemented in Azure Machine Learning).\n",
      "nodes": [
        {
          "content": "regression and support vector machines (as implemented in Azure Machine Learning).",
          "pos": [
            0,
            82
          ]
        },
        {
          "content": "\n",
          "pos": [
            82,
            83
          ]
        }
      ]
    },
    {
      "pos": [
        7803,
        7873
      ],
      "content": "Linear regression algorithms assume that data trends follow a straight"
    },
    {
      "pos": [
        7874,
        7947
      ],
      "content": "line. These assumptions aren't bad for some problems, but on others they\n",
      "nodes": [
        {
          "content": "line.",
          "pos": [
            0,
            5
          ]
        },
        {
          "content": "These assumptions aren't bad for some problems, but on others they",
          "pos": [
            6,
            72
          ]
        }
      ]
    },
    {
      "pos": [
        7947,
        7967
      ],
      "content": "bring accuracy down."
    },
    {
      "pos": [
        7969,
        7999
      ],
      "content": "<ph id=\"ph10\">![</ph>Non-linear class bounday<ph id=\"ph11\">][1]</ph>"
    },
    {
      "pos": [
        8001,
        8110
      ],
      "content": "<bpt id=\"p17\">***</bpt><bpt id=\"p18\"/>Non-linear class boundary<ept id=\"p18\">***</ept><ept id=\"p17\"/> <bpt id=\"p19\">*</bpt>- relying on a linear classification\nalgorithm would result in low accuracy<ept id=\"p19\">*</ept>"
    },
    {
      "pos": [
        8112,
        8145
      ],
      "content": "<ph id=\"ph12\">![</ph>Data with a nonlinear trend<ph id=\"ph13\">][2]</ph>"
    },
    {
      "pos": [
        8147,
        8266
      ],
      "content": "<bpt id=\"p20\">***</bpt><bpt id=\"p21\"/>Data with a nonlinear trend<ept id=\"p21\">***</ept><ept id=\"p20\"/> <bpt id=\"p22\">*</bpt>- using a linear regression method would\ngenerate much larger errors than necessary<ept id=\"p22\">*</ept>"
    },
    {
      "pos": [
        8268,
        8336
      ],
      "content": "Despite their dangers, linear algorithms are very popular as a first"
    },
    {
      "pos": [
        8337,
        8404
      ],
      "content": "line of attack. They tend to be algorithmically simple and fast to\n",
      "nodes": [
        {
          "content": "line of attack.",
          "pos": [
            0,
            15
          ]
        },
        {
          "content": "They tend to be algorithmically simple and fast to",
          "pos": [
            16,
            66
          ]
        }
      ]
    },
    {
      "pos": [
        8404,
        8410
      ],
      "content": "train."
    },
    {
      "pos": [
        8416,
        8436
      ],
      "content": "Number of parameters"
    },
    {
      "pos": [
        8438,
        8508
      ],
      "content": "Parameters are the knobs a data scientist gets to turn when setting up"
    },
    {
      "pos": [
        8509,
        8578
      ],
      "content": "an algorithm. They are numbers that affect the algorithm's behavior,\n",
      "nodes": [
        {
          "content": "an algorithm.",
          "pos": [
            0,
            13
          ]
        },
        {
          "content": "They are numbers that affect the algorithm's behavior,",
          "pos": [
            14,
            68
          ]
        }
      ]
    },
    {
      "pos": [
        8578,
        8645
      ],
      "content": "such as error tolerance or number of iterations, or options between"
    },
    {
      "pos": [
        8646,
        8719
      ],
      "content": "variants of how the algorithm behaves. The training time and accuracy of\n",
      "nodes": [
        {
          "content": "variants of how the algorithm behaves.",
          "pos": [
            0,
            38
          ]
        },
        {
          "content": "The training time and accuracy of",
          "pos": [
            39,
            72
          ]
        }
      ]
    },
    {
      "pos": [
        8719,
        8791
      ],
      "content": "the algorithm can sometimes be quite sensitive to getting just the right"
    },
    {
      "pos": [
        8792,
        8862
      ],
      "content": "settings. Typically, algorithms with large numbers parameters require\n",
      "nodes": [
        {
          "content": "settings.",
          "pos": [
            0,
            9
          ]
        },
        {
          "content": "Typically, algorithms with large numbers parameters require",
          "pos": [
            10,
            69
          ]
        }
      ]
    },
    {
      "pos": [
        8862,
        8914
      ],
      "content": "the most trial and error to find a good combination."
    },
    {
      "pos": [
        8916,
        9305
      ],
      "content": "Alternatively, there is a <bpt id=\"p23\">[</bpt>parameter\nsweeping<ept id=\"p23\">](machine-learning-algorithm-parameters-optimize.md)</ept>\nmodule block in Azure Machine Learning that automatically tries all parameter\ncombinations at whatever granularity you choose. While this is a great\nway to make sure you've spanned the parameter space, the time required\nto train a model increases exponentially with the number of parameters.",
      "nodes": [
        {
          "content": "Alternatively, there is a <bpt id=\"p23\">[</bpt>parameter\nsweeping<ept id=\"p23\">](machine-learning-algorithm-parameters-optimize.md)</ept>\nmodule block in Azure Machine Learning that automatically tries all parameter\ncombinations at whatever granularity you choose.",
          "pos": [
            0,
            264
          ]
        },
        {
          "content": "While this is a great\nway to make sure you've spanned the parameter space, the time required\nto train a model increases exponentially with the number of parameters.",
          "pos": [
            265,
            429
          ]
        }
      ]
    },
    {
      "pos": [
        9307,
        9376
      ],
      "content": "The upside is that having many parameters typically indicates that an"
    },
    {
      "pos": [
        9377,
        9443
      ],
      "content": "algorithm has greater flexibility. It can often achieve very good\n",
      "nodes": [
        {
          "content": "algorithm has greater flexibility.",
          "pos": [
            0,
            34
          ]
        },
        {
          "content": "It can often achieve very good",
          "pos": [
            35,
            65
          ]
        }
      ]
    },
    {
      "pos": [
        9443,
        9510
      ],
      "content": "accuracy. Provided you can find the right combination of parameter\n",
      "nodes": [
        {
          "content": "accuracy.",
          "pos": [
            0,
            9
          ]
        },
        {
          "content": "Provided you can find the right combination of parameter",
          "pos": [
            10,
            66
          ]
        }
      ]
    },
    {
      "pos": [
        9510,
        9519
      ],
      "content": "settings."
    },
    {
      "pos": [
        9525,
        9543
      ],
      "content": "Number of features"
    },
    {
      "pos": [
        9545,
        9612
      ],
      "content": "For certain types of data, the number of features can be very large"
    },
    {
      "pos": [
        9613,
        9680
      ],
      "content": "compared to the number of data points. This is often the case with\n",
      "nodes": [
        {
          "content": "compared to the number of data points.",
          "pos": [
            0,
            38
          ]
        },
        {
          "content": "This is often the case with",
          "pos": [
            39,
            66
          ]
        }
      ]
    },
    {
      "pos": [
        9680,
        9753
      ],
      "content": "genetics or textual data. The large number of features can bog down some\n",
      "nodes": [
        {
          "content": "genetics or textual data.",
          "pos": [
            0,
            25
          ]
        },
        {
          "content": "The large number of features can bog down some",
          "pos": [
            26,
            72
          ]
        }
      ]
    },
    {
      "pos": [
        9753,
        9820
      ],
      "content": "learning algorithms, making training time unfeasibly long. Support\n",
      "nodes": [
        {
          "content": "learning algorithms, making training time unfeasibly long.",
          "pos": [
            0,
            58
          ]
        },
        {
          "content": "Support",
          "pos": [
            59,
            66
          ]
        }
      ]
    },
    {
      "pos": [
        9820,
        9890
      ],
      "content": "Vector Machines are particularly well suited to this case (see below)."
    },
    {
      "pos": [
        9896,
        9909
      ],
      "content": "Special cases"
    },
    {
      "pos": [
        9911,
        9983
      ],
      "content": "Some learning algorithms make particular assumptions about the structure"
    },
    {
      "pos": [
        9984,
        10055
      ],
      "content": "of the data or the desired results. If you can find one that fits your\n",
      "nodes": [
        {
          "content": "of the data or the desired results.",
          "pos": [
            0,
            35
          ]
        },
        {
          "content": "If you can find one that fits your",
          "pos": [
            36,
            70
          ]
        }
      ]
    },
    {
      "pos": [
        10055,
        10125
      ],
      "content": "needs, it can give you more useful results, more accurate predictions,"
    },
    {
      "pos": [
        10126,
        10151
      ],
      "content": "or faster training times."
    },
    {
      "pos": [
        10154,
        10167
      ],
      "content": "<bpt id=\"p24\">**</bpt>Algorithm<ept id=\"p24\">**</ept>"
    },
    {
      "pos": [
        10168,
        10180
      ],
      "content": "<bpt id=\"p25\">**</bpt>Accuracy<ept id=\"p25\">**</ept>"
    },
    {
      "pos": [
        10181,
        10198
      ],
      "content": "<bpt id=\"p26\">**</bpt>Training time<ept id=\"p26\">**</ept>"
    },
    {
      "pos": [
        10199,
        10212
      ],
      "content": "<bpt id=\"p27\">**</bpt>Linearity<ept id=\"p27\">**</ept>"
    },
    {
      "pos": [
        10213,
        10227
      ],
      "content": "<bpt id=\"p28\">**</bpt>Parameters<ept id=\"p28\">**</ept>"
    },
    {
      "pos": [
        10228,
        10237
      ],
      "content": "<bpt id=\"p29\">**</bpt>Notes<ept id=\"p29\">**</ept>"
    },
    {
      "pos": [
        10274,
        10302
      ],
      "content": "<bpt id=\"p30\">**</bpt>Two-class classification<ept id=\"p30\">**</ept>"
    },
    {
      "pos": [
        10315,
        10392
      ],
      "content": "<bpt id=\"p31\">[</bpt>logistic regression<ept id=\"p31\">](https://msdn.microsoft.com/library/azure/dn905994.aspx)</ept>"
    },
    {
      "pos": [
        10415,
        10416
      ],
      "content": "●"
    },
    {
      "pos": [
        10417,
        10418
      ],
      "content": "●"
    },
    {
      "pos": [
        10419,
        10420
      ],
      "content": "5"
    },
    {
      "pos": [
        10425,
        10498
      ],
      "content": "<bpt id=\"p32\">[</bpt>decision forest<ept id=\"p32\">](https://msdn.microsoft.com/library/azure/dn906008.aspx)</ept>"
    },
    {
      "pos": [
        10499,
        10500
      ],
      "content": "●"
    },
    {
      "pos": [
        10501,
        10502
      ],
      "content": "○"
    },
    {
      "pos": [
        10505,
        10506
      ],
      "content": "6"
    },
    {
      "pos": [
        10511,
        10584
      ],
      "content": "<bpt id=\"p33\">[</bpt>decision jungle<ept id=\"p33\">](https://msdn.microsoft.com/library/azure/dn905976.aspx)</ept>"
    },
    {
      "pos": [
        10585,
        10586
      ],
      "content": "●"
    },
    {
      "pos": [
        10587,
        10588
      ],
      "content": "○"
    },
    {
      "pos": [
        10591,
        10592
      ],
      "content": "6"
    },
    {
      "pos": [
        10593,
        10613
      ],
      "content": "Low memory footprint"
    },
    {
      "pos": [
        10616,
        10695
      ],
      "content": "<bpt id=\"p34\">[</bpt>boosted decision tree<ept id=\"p34\">](https://msdn.microsoft.com/library/azure/dn906025.aspx)</ept>"
    },
    {
      "pos": [
        10696,
        10697
      ],
      "content": "●"
    },
    {
      "pos": [
        10698,
        10699
      ],
      "content": "○"
    },
    {
      "pos": [
        10702,
        10703
      ],
      "content": "6"
    },
    {
      "pos": [
        10704,
        10726
      ],
      "content": "Large memory footprint"
    },
    {
      "pos": [
        10729,
        10801
      ],
      "content": "<bpt id=\"p35\">[</bpt>neural network<ept id=\"p35\">](https://msdn.microsoft.com/library/azure/dn905947.aspx)</ept>"
    },
    {
      "pos": [
        10802,
        10803
      ],
      "content": "●"
    },
    {
      "pos": [
        10808,
        10809
      ],
      "content": "9"
    },
    {
      "pos": [
        10810,
        10895
      ],
      "content": "<bpt id=\"p36\">[</bpt>Additional customization is possible<ept id=\"p36\">](http://go.microsoft.com/fwlink/?LinkId=402867)</ept>"
    },
    {
      "pos": [
        10898,
        10975
      ],
      "content": "<bpt id=\"p37\">[</bpt>averaged perceptron<ept id=\"p37\">](https://msdn.microsoft.com/library/azure/dn906036.aspx)</ept>"
    },
    {
      "pos": [
        10976,
        10977
      ],
      "content": "○"
    },
    {
      "pos": [
        10978,
        10979
      ],
      "content": "○"
    },
    {
      "pos": [
        10980,
        10981
      ],
      "content": "●"
    },
    {
      "pos": [
        10982,
        10983
      ],
      "content": "4"
    },
    {
      "pos": [
        10988,
        11068
      ],
      "content": "<bpt id=\"p38\">[</bpt>support vector machine<ept id=\"p38\">](https://msdn.microsoft.com/library/azure/dn905835.aspx)</ept>"
    },
    {
      "pos": [
        11071,
        11072
      ],
      "content": "○"
    },
    {
      "pos": [
        11073,
        11074
      ],
      "content": "●"
    },
    {
      "pos": [
        11075,
        11076
      ],
      "content": "5"
    },
    {
      "pos": [
        11077,
        11104
      ],
      "content": "Good for large feature sets"
    },
    {
      "pos": [
        11107,
        11200
      ],
      "content": "<bpt id=\"p39\">[</bpt>locally deep support vector machine<ept id=\"p39\">](https://msdn.microsoft.com/library/azure/dn913070.aspx)</ept>"
    },
    {
      "pos": [
        11201,
        11202
      ],
      "content": "○"
    },
    {
      "pos": [
        11207,
        11208
      ],
      "content": "8"
    },
    {
      "pos": [
        11209,
        11236
      ],
      "content": "Good for large feature sets"
    },
    {
      "pos": [
        11239,
        11317
      ],
      "content": "<bpt id=\"p40\">[</bpt>Bayes’ point machine<ept id=\"p40\">](https://msdn.microsoft.com/library/azure/dn905930.aspx)</ept>"
    },
    {
      "pos": [
        11320,
        11321
      ],
      "content": "○"
    },
    {
      "pos": [
        11322,
        11323
      ],
      "content": "●"
    },
    {
      "pos": [
        11324,
        11325
      ],
      "content": "3"
    },
    {
      "pos": [
        11330,
        11360
      ],
      "content": "<bpt id=\"p41\">**</bpt>Multi-class classification<ept id=\"p41\">**</ept>"
    },
    {
      "pos": [
        11373,
        11450
      ],
      "content": "<bpt id=\"p42\">[</bpt>logistic regression<ept id=\"p42\">](https://msdn.microsoft.com/library/azure/dn905853.aspx)</ept>"
    },
    {
      "pos": [
        11453,
        11454
      ],
      "content": "●"
    },
    {
      "pos": [
        11455,
        11456
      ],
      "content": "●"
    },
    {
      "pos": [
        11457,
        11458
      ],
      "content": "5"
    },
    {
      "pos": [
        11463,
        11536
      ],
      "content": "<bpt id=\"p43\">[</bpt>decision forest<ept id=\"p43\">](https://msdn.microsoft.com/library/azure/dn906015.aspx)</ept>"
    },
    {
      "pos": [
        11537,
        11538
      ],
      "content": "●"
    },
    {
      "pos": [
        11539,
        11540
      ],
      "content": "○"
    },
    {
      "pos": [
        11543,
        11544
      ],
      "content": "6"
    },
    {
      "pos": [
        11549,
        11623
      ],
      "content": "<bpt id=\"p44\">[</bpt>decision jungle <ept id=\"p44\">](https://msdn.microsoft.com/library/azure/dn905963.aspx)</ept>"
    },
    {
      "pos": [
        11624,
        11625
      ],
      "content": "●"
    },
    {
      "pos": [
        11626,
        11627
      ],
      "content": "○"
    },
    {
      "pos": [
        11630,
        11631
      ],
      "content": "6"
    },
    {
      "pos": [
        11632,
        11652
      ],
      "content": "Low memory footprint"
    },
    {
      "pos": [
        11655,
        11727
      ],
      "content": "<bpt id=\"p45\">[</bpt>neural network<ept id=\"p45\">](https://msdn.microsoft.com/library/azure/dn906030.aspx)</ept>"
    },
    {
      "pos": [
        11728,
        11729
      ],
      "content": "●"
    },
    {
      "pos": [
        11734,
        11735
      ],
      "content": "9"
    },
    {
      "pos": [
        11736,
        11821
      ],
      "content": "<bpt id=\"p46\">[</bpt>Additional customization is possible<ept id=\"p46\">](http://go.microsoft.com/fwlink/?LinkId=402867)</ept>"
    },
    {
      "pos": [
        11824,
        11891
      ],
      "content": "<bpt id=\"p47\">[</bpt>one-v-all<ept id=\"p47\">](https://msdn.microsoft.com/library/azure/dn905887.aspx)</ept>"
    },
    {
      "pos": [
        11892,
        11893
      ],
      "content": "-"
    },
    {
      "pos": [
        11894,
        11895
      ],
      "content": "-"
    },
    {
      "pos": [
        11896,
        11897
      ],
      "content": "-"
    },
    {
      "pos": [
        11898,
        11899
      ],
      "content": "-"
    },
    {
      "pos": [
        11900,
        11947
      ],
      "content": "See properties of the two-class method selected"
    },
    {
      "pos": [
        11950,
        11964
      ],
      "content": "<bpt id=\"p48\">**</bpt>Regression<ept id=\"p48\">**</ept>"
    },
    {
      "pos": [
        11977,
        12042
      ],
      "content": "<bpt id=\"p49\">[</bpt>linear <ept id=\"p49\">](https://msdn.microsoft.com/library/azure/dn905978.aspx)</ept>"
    },
    {
      "pos": [
        12045,
        12046
      ],
      "content": "●"
    },
    {
      "pos": [
        12047,
        12048
      ],
      "content": "●"
    },
    {
      "pos": [
        12049,
        12050
      ],
      "content": "4"
    },
    {
      "pos": [
        12055,
        12128
      ],
      "content": "<bpt id=\"p50\">[</bpt>Bayesian linear<ept id=\"p50\">](https://msdn.microsoft.com/library/azure/dn906022.aspx)</ept>"
    },
    {
      "pos": [
        12131,
        12132
      ],
      "content": "○"
    },
    {
      "pos": [
        12133,
        12134
      ],
      "content": "●"
    },
    {
      "pos": [
        12135,
        12136
      ],
      "content": "2"
    },
    {
      "pos": [
        12141,
        12214
      ],
      "content": "<bpt id=\"p51\">[</bpt>decision forest<ept id=\"p51\">](https://msdn.microsoft.com/library/azure/dn905862.aspx)</ept>"
    },
    {
      "pos": [
        12215,
        12216
      ],
      "content": "●"
    },
    {
      "pos": [
        12217,
        12218
      ],
      "content": "○"
    },
    {
      "pos": [
        12221,
        12222
      ],
      "content": "6"
    },
    {
      "pos": [
        12227,
        12306
      ],
      "content": "<bpt id=\"p52\">[</bpt>boosted decision tree<ept id=\"p52\">](https://msdn.microsoft.com/library/azure/dn905801.aspx)</ept>"
    },
    {
      "pos": [
        12307,
        12308
      ],
      "content": "●"
    },
    {
      "pos": [
        12309,
        12310
      ],
      "content": "○"
    },
    {
      "pos": [
        12313,
        12314
      ],
      "content": "5"
    },
    {
      "pos": [
        12315,
        12337
      ],
      "content": "Large memory footprint"
    },
    {
      "pos": [
        12340,
        12418
      ],
      "content": "<bpt id=\"p53\">[</bpt>fast forest quantile<ept id=\"p53\">](https://msdn.microsoft.com/library/azure/dn913093.aspx)</ept>"
    },
    {
      "pos": [
        12419,
        12420
      ],
      "content": "●"
    },
    {
      "pos": [
        12421,
        12422
      ],
      "content": "○"
    },
    {
      "pos": [
        12425,
        12426
      ],
      "content": "9"
    },
    {
      "pos": [
        12427,
        12470
      ],
      "content": "Distributions rather than point predictions"
    },
    {
      "pos": [
        12473,
        12545
      ],
      "content": "<bpt id=\"p54\">[</bpt>neural network<ept id=\"p54\">](https://msdn.microsoft.com/library/azure/dn905924.aspx)</ept>"
    },
    {
      "pos": [
        12546,
        12547
      ],
      "content": "●"
    },
    {
      "pos": [
        12552,
        12553
      ],
      "content": "9"
    },
    {
      "pos": [
        12554,
        12639
      ],
      "content": "<bpt id=\"p55\">[</bpt>Additional customization is possible<ept id=\"p55\">](http://go.microsoft.com/fwlink/?LinkId=402867)</ept>"
    },
    {
      "pos": [
        12642,
        12708
      ],
      "content": "<bpt id=\"p56\">[</bpt>Poisson <ept id=\"p56\">](https://msdn.microsoft.com/library/azure/dn905988.aspx)</ept>"
    },
    {
      "pos": [
        12713,
        12714
      ],
      "content": "●"
    },
    {
      "pos": [
        12715,
        12716
      ],
      "content": "5"
    },
    {
      "pos": [
        12717,
        12762
      ],
      "content": "Technically log-linear. For predicting counts",
      "nodes": [
        {
          "content": "Technically log-linear.",
          "pos": [
            0,
            23
          ]
        },
        {
          "content": "For predicting counts",
          "pos": [
            24,
            45
          ]
        }
      ]
    },
    {
      "pos": [
        12765,
        12830
      ],
      "content": "<bpt id=\"p57\">[</bpt>ordinal<ept id=\"p57\">](https://msdn.microsoft.com/library/azure/dn906029.aspx)</ept>"
    },
    {
      "pos": [
        12837,
        12838
      ],
      "content": "0"
    },
    {
      "pos": [
        12839,
        12867
      ],
      "content": "For predicting rank-ordering"
    },
    {
      "pos": [
        12870,
        12891
      ],
      "content": "<bpt id=\"p58\">**</bpt>Anomaly detection<ept id=\"p58\">**</ept>"
    },
    {
      "pos": [
        12904,
        12984
      ],
      "content": "<bpt id=\"p59\">[</bpt>support vector machine<ept id=\"p59\">](https://msdn.microsoft.com/library/azure/dn913103.aspx)</ept>"
    },
    {
      "pos": [
        12985,
        12986
      ],
      "content": "○"
    },
    {
      "pos": [
        12987,
        12988
      ],
      "content": "○"
    },
    {
      "pos": [
        12991,
        12992
      ],
      "content": "2"
    },
    {
      "pos": [
        12993,
        13031
      ],
      "content": "Especially good for large feature sets"
    },
    {
      "pos": [
        13034,
        13120
      ],
      "content": "<bpt id=\"p60\">[</bpt>PCA-based anomaly detection <ept id=\"p60\">](https://msdn.microsoft.com/library/azure/dn913102.aspx)</ept>"
    },
    {
      "pos": [
        13123,
        13124
      ],
      "content": "○"
    },
    {
      "pos": [
        13125,
        13126
      ],
      "content": "●"
    },
    {
      "pos": [
        13127,
        13128
      ],
      "content": "3"
    },
    {
      "pos": [
        13133,
        13222
      ],
      "content": "<bpt id=\"p61\">[</bpt>K-means<ept id=\"p61\">](https://msdn.microsoft.com/library/azure/5049a09b-bd90-4c4e-9b46-7c87e3a36810/)</ept>"
    },
    {
      "pos": [
        13225,
        13226
      ],
      "content": "○"
    },
    {
      "pos": [
        13227,
        13228
      ],
      "content": "●"
    },
    {
      "pos": [
        13229,
        13230
      ],
      "content": "4"
    },
    {
      "pos": [
        13231,
        13253
      ],
      "content": "A clustering algorithm"
    },
    {
      "pos": [
        13257,
        13282
      ],
      "content": "<bpt id=\"p62\">**</bpt>Algorithm properties:<ept id=\"p62\">**</ept>"
    },
    {
      "pos": [
        13284,
        13363
      ],
      "content": "<bpt id=\"p63\">**</bpt>●<ept id=\"p63\">**</ept><ph id=\"ph14\"/> - shows excellent accuracy, fast training times, and the use of linearity"
    },
    {
      "pos": [
        13365,
        13420
      ],
      "content": "<bpt id=\"p64\">**</bpt>○<ept id=\"p64\">**</ept><ph id=\"ph15\"/> - shows good accuracy and moderate training times"
    },
    {
      "pos": [
        13425,
        13440
      ],
      "content": "Algorithm notes"
    },
    {
      "pos": [
        13446,
        13463
      ],
      "content": "Linear regression"
    },
    {
      "pos": [
        13465,
        13801
      ],
      "content": "As mentioned previously, <bpt id=\"p65\">[</bpt>linear regression<ept id=\"p65\">](https://msdn.microsoft.com/library/azure/dn905978.aspx)</ept>\nfits a line (or plane, or hyperplane) to the data set. It's a workhorse,\nsimple and fast, but it may be overly simplistic for some problems.\nCheck here for a <bpt id=\"p66\">[</bpt>linear regression\ntutorial<ept id=\"p66\">](machine-learning-linear-regression-in-azure.md)</ept>.",
      "nodes": [
        {
          "content": "As mentioned previously, <bpt id=\"p65\">[</bpt>linear regression<ept id=\"p65\">](https://msdn.microsoft.com/library/azure/dn905978.aspx)</ept>\nfits a line (or plane, or hyperplane) to the data set.",
          "pos": [
            0,
            195
          ]
        },
        {
          "content": "It's a workhorse,\nsimple and fast, but it may be overly simplistic for some problems.\nCheck here for a <bpt id=\"p66\">[</bpt>linear regression\ntutorial<ept id=\"p66\">](machine-learning-linear-regression-in-azure.md)</ept>.",
          "pos": [
            196,
            416
          ]
        }
      ]
    },
    {
      "pos": [
        13803,
        13833
      ],
      "content": "<ph id=\"ph16\">![</ph>Data with a linear trend<ph id=\"ph17\">][3]</ph>"
    },
    {
      "pos": [
        13835,
        13865
      ],
      "content": "<bpt id=\"p67\">***</bpt><bpt id=\"p68\"/>Data with a linear trend<ept id=\"p68\">***</ept><ept id=\"p67\"/>"
    },
    {
      "pos": [
        13871,
        13890
      ],
      "content": "Logistic regression"
    },
    {
      "pos": [
        13892,
        14440
      ],
      "content": "Although it confusingly includes 'regression' in the name, logistic\nregression is actually a powerful tool for\n<bpt id=\"p69\">[</bpt>two-class<ept id=\"p69\">](https://msdn.microsoft.com/library/azure/dn905994.aspx)</ept>\nand\n<bpt id=\"p70\">[</bpt>multiclass<ept id=\"p70\">](https://msdn.microsoft.com/library/azure/dn905853.aspx)</ept>\nclassification. It's fast and simple. The fact that it uses an\n'S'-shaped curve instead of a straight line makes it a natural fit for\ndividing data into groups. Logistic regression gives linear class\nboundaries, so when you use it, make sure a linear approximation is\nsomething you can live with.",
      "nodes": [
        {
          "content": "Although it confusingly includes 'regression' in the name, logistic\nregression is actually a powerful tool for\n<bpt id=\"p69\">[</bpt>two-class<ept id=\"p69\">](https://msdn.microsoft.com/library/azure/dn905994.aspx)</ept>\nand\n<bpt id=\"p70\">[</bpt>multiclass<ept id=\"p70\">](https://msdn.microsoft.com/library/azure/dn905853.aspx)</ept>\nclassification.",
          "pos": [
            0,
            347
          ]
        },
        {
          "content": "It's fast and simple.",
          "pos": [
            348,
            369
          ]
        },
        {
          "content": "The fact that it uses an\n'S'-shaped curve instead of a straight line makes it a natural fit for\ndividing data into groups.",
          "pos": [
            370,
            492
          ]
        },
        {
          "content": "Logistic regression gives linear class\nboundaries, so when you use it, make sure a linear approximation is\nsomething you can live with.",
          "pos": [
            493,
            628
          ]
        }
      ]
    },
    {
      "pos": [
        14442,
        14507
      ],
      "content": "<ph id=\"ph18\">![</ph>Logistic regression to two-class data with just one feature<ph id=\"ph19\">][4]</ph>"
    },
    {
      "pos": [
        14509,
        14673
      ],
      "content": "<bpt id=\"p71\">***</bpt><bpt id=\"p72\"/>A logistic regression to two-class data with just one feature<ept id=\"p72\">***</ept><ept id=\"p71\"/> <bpt id=\"p73\">*</bpt>- the\nclass boundary is the point at which the logistic curve is just as close to both classes<ept id=\"p73\">*</ept>"
    },
    {
      "pos": [
        14679,
        14706
      ],
      "content": "Trees, forests, and jungles"
    },
    {
      "pos": [
        14708,
        15618
      ],
      "content": "Decision forests\n(<bpt id=\"p74\">[</bpt>regression<ept id=\"p74\">](https://msdn.microsoft.com/library/azure/dn905862.aspx)</ept>,\n<bpt id=\"p75\">[</bpt>two-class<ept id=\"p75\">](https://msdn.microsoft.com/library/azure/dn906008.aspx)</ept>,\nand\n<bpt id=\"p76\">[</bpt>multiclass<ept id=\"p76\">](https://msdn.microsoft.com/library/azure/dn906015.aspx)</ept>),\ndecision jungles\n(<bpt id=\"p77\">[</bpt>two-class<ept id=\"p77\">](https://msdn.microsoft.com/library/azure/dn905976.aspx)</ept>\nand\n<bpt id=\"p78\">[</bpt>multiclass<ept id=\"p78\">](https://msdn.microsoft.com/library/azure/dn905963.aspx)</ept>),\nand boosted decision trees\n(<bpt id=\"p79\">[</bpt>regression<ept id=\"p79\">](https://msdn.microsoft.com/library/azure/dn905801.aspx)</ept>\nand\n<bpt id=\"p80\">[</bpt>two-class<ept id=\"p80\">](https://msdn.microsoft.com/library/azure/dn906025.aspx)</ept>)\nare all based on decision trees, a foundational machine learning\nconcept. There are many variants of decision trees, but they all do the\nsame thing—subdivide the feature space into regions with mostly the same\nlabel. These can be regions of consistent category or of constant value,\ndepending on whether you are doing classification or regression.",
      "nodes": [
        {
          "content": "Decision forests\n(<bpt id=\"p74\">[</bpt>regression<ept id=\"p74\">](https://msdn.microsoft.com/library/azure/dn905862.aspx)</ept>,\n<bpt id=\"p75\">[</bpt>two-class<ept id=\"p75\">](https://msdn.microsoft.com/library/azure/dn906008.aspx)</ept>,\nand\n<bpt id=\"p76\">[</bpt>multiclass<ept id=\"p76\">](https://msdn.microsoft.com/library/azure/dn906015.aspx)</ept>),\ndecision jungles\n(<bpt id=\"p77\">[</bpt>two-class<ept id=\"p77\">](https://msdn.microsoft.com/library/azure/dn905976.aspx)</ept>\nand\n<bpt id=\"p78\">[</bpt>multiclass<ept id=\"p78\">](https://msdn.microsoft.com/library/azure/dn905963.aspx)</ept>),\nand boosted decision trees\n(<bpt id=\"p79\">[</bpt>regression<ept id=\"p79\">](https://msdn.microsoft.com/library/azure/dn905801.aspx)</ept>\nand\n<bpt id=\"p80\">[</bpt>two-class<ept id=\"p80\">](https://msdn.microsoft.com/library/azure/dn906025.aspx)</ept>)\nare all based on decision trees, a foundational machine learning\nconcept.",
          "pos": [
            0,
            916
          ]
        },
        {
          "content": "There are many variants of decision trees, but they all do the\nsame thing—subdivide the feature space into regions with mostly the same\nlabel.",
          "pos": [
            917,
            1059
          ]
        },
        {
          "content": "These can be regions of consistent category or of constant value,\ndepending on whether you are doing classification or regression.",
          "pos": [
            1060,
            1190
          ]
        }
      ]
    },
    {
      "pos": [
        15620,
        15666
      ],
      "content": "<ph id=\"ph20\">![</ph>Decision tree subdivides a feature space<ph id=\"ph21\">][5]</ph>"
    },
    {
      "pos": [
        15668,
        15755
      ],
      "content": "<bpt id=\"p81\">***</bpt><bpt id=\"p82\"/>A decision tree subdivides a feature space into regions of roughly\nuniform values<ept id=\"p82\">***</ept><ept id=\"p81\"/>"
    },
    {
      "pos": [
        15757,
        15821
      ],
      "content": "Because a feature space can be subdivided into arbitrarily small"
    },
    {
      "pos": [
        15822,
        15894
      ],
      "content": "regions, it's easy to imagine dividing it finely enough to have one data"
    },
    {
      "pos": [
        15895,
        15965
      ],
      "content": "point per region—an extreme example of overfitting. In order to avoid\n",
      "nodes": [
        {
          "content": "point per region—an extreme example of overfitting.",
          "pos": [
            0,
            51
          ]
        },
        {
          "content": "In order to avoid",
          "pos": [
            52,
            69
          ]
        }
      ]
    },
    {
      "pos": [
        15965,
        16033
      ],
      "content": "this, a large set of trees are constructed with special mathematical"
    },
    {
      "pos": [
        16034,
        16100
      ],
      "content": "care taken that the trees are not correlated. The average of this\n",
      "nodes": [
        {
          "content": "care taken that the trees are not correlated.",
          "pos": [
            0,
            45
          ]
        },
        {
          "content": "The average of this",
          "pos": [
            46,
            65
          ]
        }
      ]
    },
    {
      "pos": [
        16100,
        16170
      ],
      "content": "\"decision forest\" is a tree that avoids overfitting. Decision forests\n",
      "nodes": [
        {
          "content": "\"decision forest\" is a tree that avoids overfitting.",
          "pos": [
            0,
            52
          ]
        },
        {
          "content": "Decision forests",
          "pos": [
            53,
            69
          ]
        }
      ]
    },
    {
      "pos": [
        16170,
        16240
      ],
      "content": "can use a lot of memory. Decision jungles are a variant that consumes\n",
      "nodes": [
        {
          "content": "can use a lot of memory.",
          "pos": [
            0,
            24
          ]
        },
        {
          "content": "Decision jungles are a variant that consumes",
          "pos": [
            25,
            69
          ]
        }
      ]
    },
    {
      "pos": [
        16240,
        16302
      ],
      "content": "less memory at the expense of a slightly longer training time."
    },
    {
      "pos": [
        16304,
        16760
      ],
      "content": "Boosted decision trees avoid overfitting by limiting how many times they\ncan subdivide and how few data points are allowed in each region. The\nalgorithm constructs a sequence of trees, each of which learns to\ncompensate for the error left by the tree before. The result is a very\naccurate learner that tends to use a lot of memory. For the full\ntechnical description, check out <bpt id=\"p83\">[</bpt>Friedman's original\npaper<ept id=\"p83\">](http://www-stat.stanford.edu/~jhf/ftp/trebst.pdf)</ept>.",
      "nodes": [
        {
          "content": "Boosted decision trees avoid overfitting by limiting how many times they\ncan subdivide and how few data points are allowed in each region.",
          "pos": [
            0,
            138
          ]
        },
        {
          "content": "The\nalgorithm constructs a sequence of trees, each of which learns to\ncompensate for the error left by the tree before.",
          "pos": [
            139,
            258
          ]
        },
        {
          "content": "The result is a very\naccurate learner that tends to use a lot of memory.",
          "pos": [
            259,
            331
          ]
        },
        {
          "content": "For the full\ntechnical description, check out <bpt id=\"p83\">[</bpt>Friedman's original\npaper<ept id=\"p83\">](http://www-stat.stanford.edu/~jhf/ftp/trebst.pdf)</ept>.",
          "pos": [
            332,
            496
          ]
        }
      ]
    },
    {
      "pos": [
        16762,
        17045
      ],
      "content": "<bpt id=\"p84\">[</bpt>Fast forest quantile\nregression<ept id=\"p84\">](https://msdn.microsoft.com/library/azure/dn913093.aspx)</ept>\nis a variation of decision trees for the special case where you want to\nknow not only the typical (median) value of the data within a region,\nbut also its distribution in the form of quantiles."
    },
    {
      "pos": [
        17051,
        17082
      ],
      "content": "Neural networks and perceptrons"
    },
    {
      "pos": [
        17084,
        17994
      ],
      "content": "Neural networks are brain-inspired learning algorithms covering\n<bpt id=\"p85\">[</bpt>multiclass<ept id=\"p85\">](https://msdn.microsoft.com/library/azure/dn906030.aspx)</ept>,\n<bpt id=\"p86\">[</bpt>two-class<ept id=\"p86\">](https://msdn.microsoft.com/library/azure/dn905947.aspx)</ept>,\nand\n<bpt id=\"p87\">[</bpt>regression<ept id=\"p87\">](https://msdn.microsoft.com/library/azure/dn905924.aspx)</ept>\nproblems. They come in an infinite variety, but the neural networks\nwithin Azure Machine Learning are all of the form of directed acyclic graphs. That\nmeans that input features are passed forward (never backward) through a\nsequence of layers before being turned into outputs. In each layer,\ninputs are weighted in various combinations, summed, and passed on to\nthe next layer. This combination of simple calculations results in the\nability to learn sophisticated class boundaries and data trends,\nseemingly by magic. Many-layered networks of this sort perform the \"deep\nlearning\" that fuels so much tech reporting and science fiction.",
      "nodes": [
        {
          "content": "Neural networks are brain-inspired learning algorithms covering\n<bpt id=\"p85\">[</bpt>multiclass<ept id=\"p85\">](https://msdn.microsoft.com/library/azure/dn906030.aspx)</ept>,\n<bpt id=\"p86\">[</bpt>two-class<ept id=\"p86\">](https://msdn.microsoft.com/library/azure/dn905947.aspx)</ept>,\nand\n<bpt id=\"p87\">[</bpt>regression<ept id=\"p87\">](https://msdn.microsoft.com/library/azure/dn905924.aspx)</ept>\nproblems.",
          "pos": [
            0,
            405
          ]
        },
        {
          "content": "They come in an infinite variety, but the neural networks\nwithin Azure Machine Learning are all of the form of directed acyclic graphs.",
          "pos": [
            406,
            541
          ]
        },
        {
          "content": "That\nmeans that input features are passed forward (never backward) through a\nsequence of layers before being turned into outputs.",
          "pos": [
            542,
            671
          ]
        },
        {
          "content": "In each layer,\ninputs are weighted in various combinations, summed, and passed on to\nthe next layer.",
          "pos": [
            672,
            772
          ]
        },
        {
          "content": "This combination of simple calculations results in the\nability to learn sophisticated class boundaries and data trends,\nseemingly by magic.",
          "pos": [
            773,
            912
          ]
        },
        {
          "content": "Many-layered networks of this sort perform the \"deep\nlearning\" that fuels so much tech reporting and science fiction.",
          "pos": [
            913,
            1030
          ]
        }
      ]
    },
    {
      "pos": [
        17996,
        18443
      ],
      "content": "This high performance doesn't come for free, though. Neural networks can\ntake a long time to train, particularly for large data sets with lots of\nfeatures. They also have more parameters than most algorithms, which\nmeans that parameter sweeping expands the training time a great deal.\nAnd for those overachievers who wish to <bpt id=\"p88\">[</bpt>specify their own network\nstructure<ept id=\"p88\">](http://go.microsoft.com/fwlink/?LinkId=402867)</ept>, the\npossibilities are inexhaustible.",
      "nodes": [
        {
          "content": "This high performance doesn't come for free, though.",
          "pos": [
            0,
            52
          ]
        },
        {
          "content": "Neural networks can\ntake a long time to train, particularly for large data sets with lots of\nfeatures.",
          "pos": [
            53,
            155
          ]
        },
        {
          "content": "They also have more parameters than most algorithms, which\nmeans that parameter sweeping expands the training time a great deal.",
          "pos": [
            156,
            284
          ]
        },
        {
          "content": "And for those overachievers who wish to <bpt id=\"p88\">[</bpt>specify their own network\nstructure<ept id=\"p88\">](http://go.microsoft.com/fwlink/?LinkId=402867)</ept>, the\npossibilities are inexhaustible.",
          "pos": [
            285,
            487
          ]
        }
      ]
    },
    {
      "pos": [
        18445,
        18488
      ],
      "content": "<ph id=\"ph22\">![</ph>Boundaries learned by neural networks<ph id=\"ph23\">][6]</ph>"
    },
    {
      "pos": [
        18518,
        18594
      ],
      "content": "<bpt id=\"p89\">***</bpt><bpt id=\"p90\"/>The boundaries learned by neural networks can be complex and\nirregular<ept id=\"p90\">***</ept><ept id=\"p89\"/>"
    },
    {
      "pos": [
        18596,
        18939
      ],
      "content": "The <bpt id=\"p91\">[</bpt>two-class averaged\nperceptron<ept id=\"p91\">](https://msdn.microsoft.com/library/azure/dn906036.aspx)</ept>\nis neural networks' answer to skyrocketing training times. It uses a\nnetwork structure that gives linear class boundaries. It is almost\nprimitive by today's standards, but it has a long history of working\nrobustly and is small enough to learn quickly.",
      "nodes": [
        {
          "content": "The <bpt id=\"p91\">[</bpt>two-class averaged\nperceptron<ept id=\"p91\">](https://msdn.microsoft.com/library/azure/dn906036.aspx)</ept>\nis neural networks' answer to skyrocketing training times.",
          "pos": [
            0,
            190
          ]
        },
        {
          "content": "It uses a\nnetwork structure that gives linear class boundaries.",
          "pos": [
            191,
            254
          ]
        },
        {
          "content": "It is almost\nprimitive by today's standards, but it has a long history of working\nrobustly and is small enough to learn quickly.",
          "pos": [
            255,
            383
          ]
        }
      ]
    },
    {
      "pos": [
        18945,
        18949
      ],
      "content": "SVMs"
    },
    {
      "pos": [
        18951,
        19670
      ],
      "content": "Support vector machines (SVMs) find the boundary that separates classes\nby as wide a margin as possible. When the two classes can't be clearly\nseparated, the algorithms find the best boundary they can. As written in\nAzure Machine Learning, the <bpt id=\"p92\">[</bpt>two-class\nSVM<ept id=\"p92\">](https://msdn.microsoft.com/library/azure/dn905835.aspx)</ept><ph id=\"ph24\"/> does\nthis with a straight line only. (In SVM-speak, it uses a linear kernel.)\nBecause it makes this linear approximation, it is able to run fairly\nquickly. Where it really shines is with feature-intense data, like text\nor genomic. In these cases SVMs are able to separate classes more\nquickly and with less overfitting than most other algorithms, in\naddition to requiring only a modest amount of memory.",
      "nodes": [
        {
          "content": "Support vector machines (SVMs) find the boundary that separates classes\nby as wide a margin as possible.",
          "pos": [
            0,
            104
          ]
        },
        {
          "content": "When the two classes can't be clearly\nseparated, the algorithms find the best boundary they can.",
          "pos": [
            105,
            201
          ]
        },
        {
          "content": "As written in\nAzure Machine Learning, the <bpt id=\"p92\">[</bpt>two-class\nSVM<ept id=\"p92\">](https://msdn.microsoft.com/library/azure/dn905835.aspx)</ept><ph id=\"ph24\"/> does\nthis with a straight line only.",
          "pos": [
            202,
            407
          ]
        },
        {
          "content": "(In SVM-speak, it uses a linear kernel.)\nBecause it makes this linear approximation, it is able to run fairly\nquickly.",
          "pos": [
            408,
            526
          ]
        },
        {
          "content": "Where it really shines is with feature-intense data, like text\nor genomic.",
          "pos": [
            527,
            601
          ]
        },
        {
          "content": "In these cases SVMs are able to separate classes more\nquickly and with less overfitting than most other algorithms, in\naddition to requiring only a modest amount of memory.",
          "pos": [
            602,
            774
          ]
        }
      ]
    },
    {
      "pos": [
        19672,
        19715
      ],
      "content": "<ph id=\"ph25\">![</ph>Support vector machine class boundary<ph id=\"ph26\">][7]</ph>"
    },
    {
      "pos": [
        19717,
        19814
      ],
      "content": "<bpt id=\"p93\">***</bpt><bpt id=\"p94\"/>A typical support vector machine class boundary maximizes the margin\nseparating two classes<ept id=\"p94\">***</ept><ept id=\"p93\"/>"
    },
    {
      "pos": [
        19816,
        20378
      ],
      "content": "Another product of Microsoft Research, the <bpt id=\"p95\">[</bpt>two-class locally deep\nSVM<ept id=\"p95\">](https://msdn.microsoft.com/library/azure/dn913070.aspx)</ept><ph id=\"ph27\"/> is a\nnon-linear variant of SVM that retains most of the speed and memory\nefficiency of the linear version. It is ideal for cases where the linear\napproach doesn't give accurate enough answers. The developers kept it\nfast by breaking the problem down into a bunch of small linear SVM\nproblems. Read the <bpt id=\"p96\">[</bpt>full\ndescription<ept id=\"p96\">](http://research.microsoft.com/um/people/manik/pubs/Jose13.pdf)</ept>\nfor the details on how they pulled off this trick.",
      "nodes": [
        {
          "content": "Another product of Microsoft Research, the <bpt id=\"p95\">[</bpt>two-class locally deep\nSVM<ept id=\"p95\">](https://msdn.microsoft.com/library/azure/dn913070.aspx)</ept><ph id=\"ph27\"/> is a\nnon-linear variant of SVM that retains most of the speed and memory\nefficiency of the linear version.",
          "pos": [
            0,
            289
          ]
        },
        {
          "content": "It is ideal for cases where the linear\napproach doesn't give accurate enough answers.",
          "pos": [
            290,
            375
          ]
        },
        {
          "content": "The developers kept it\nfast by breaking the problem down into a bunch of small linear SVM\nproblems.",
          "pos": [
            376,
            475
          ]
        },
        {
          "content": "Read the <bpt id=\"p96\">[</bpt>full\ndescription<ept id=\"p96\">](http://research.microsoft.com/um/people/manik/pubs/Jose13.pdf)</ept>\nfor the details on how they pulled off this trick.",
          "pos": [
            476,
            657
          ]
        }
      ]
    },
    {
      "pos": [
        20380,
        20688
      ],
      "content": "Using a clever extension of nonlinear SVMs, the <bpt id=\"p97\">[</bpt>one-class\nSVM<ept id=\"p97\">](https://msdn.microsoft.com/library/azure/dn913103.aspx)</ept><ph id=\"ph28\"/> draws\na boundary that tightly outlines the entire data set. It is useful for\nanomaly detection. Any new data points that fall far outside that\nboundary are unusual enough to be noteworthy.",
      "nodes": [
        {
          "content": "Using a clever extension of nonlinear SVMs, the <bpt id=\"p97\">[</bpt>one-class\nSVM<ept id=\"p97\">](https://msdn.microsoft.com/library/azure/dn913103.aspx)</ept><ph id=\"ph28\"/> draws\na boundary that tightly outlines the entire data set.",
          "pos": [
            0,
            234
          ]
        },
        {
          "content": "It is useful for\nanomaly detection.",
          "pos": [
            235,
            270
          ]
        },
        {
          "content": "Any new data points that fall far outside that\nboundary are unusual enough to be noteworthy.",
          "pos": [
            271,
            363
          ]
        }
      ]
    },
    {
      "pos": [
        20694,
        20710
      ],
      "content": "Bayesian methods"
    },
    {
      "pos": [
        20712,
        21304
      ],
      "content": "Bayesian methods have a highly desirable quality: they avoid\noverfitting. They do this by making some assumptions beforehand about\nthe likely distribution of the answer. Another byproduct of this\napproach is that they have very few parameters. Azure Machine Learning has both\nBayesian algorithms for both classification (<bpt id=\"p98\">[</bpt>Two-class Bayes' point\nmachine<ept id=\"p98\">](https://msdn.microsoft.com/library/azure/dn905930.aspx)</ept>)\nand regression (<bpt id=\"p99\">[</bpt>Bayesian linear\nregression<ept id=\"p99\">](https://msdn.microsoft.com/library/azure/dn906022.aspx)</ept>).\nNote that these assume that the data can be split or fit with a straight\nline.",
      "nodes": [
        {
          "content": "Bayesian methods have a highly desirable quality: they avoid\noverfitting.",
          "pos": [
            0,
            73
          ]
        },
        {
          "content": "They do this by making some assumptions beforehand about\nthe likely distribution of the answer.",
          "pos": [
            74,
            169
          ]
        },
        {
          "content": "Another byproduct of this\napproach is that they have very few parameters.",
          "pos": [
            170,
            243
          ]
        },
        {
          "content": "Azure Machine Learning has both\nBayesian algorithms for both classification (<bpt id=\"p98\">[</bpt>Two-class Bayes' point\nmachine<ept id=\"p98\">](https://msdn.microsoft.com/library/azure/dn905930.aspx)</ept>)\nand regression (<bpt id=\"p99\">[</bpt>Bayesian linear\nregression<ept id=\"p99\">](https://msdn.microsoft.com/library/azure/dn906022.aspx)</ept>).",
          "pos": [
            244,
            593
          ]
        },
        {
          "content": "Note that these assume that the data can be split or fit with a straight\nline.",
          "pos": [
            594,
            672
          ]
        }
      ]
    },
    {
      "pos": [
        21306,
        21742
      ],
      "content": "On an historical note, Bayes' point machines were developed at Microsoft\nResearch. They have some exceptionally beautiful theoretical work behind\nthem. The interested student is directed to the <bpt id=\"p100\">[</bpt>original article in\nJMLR<ept id=\"p100\">](http://jmlr.org/papers/volume1/herbrich01a/herbrich01a.pdf)</ept><ph id=\"ph29\"/> and an\n<bpt id=\"p101\">[</bpt>insightful blog by Chris\nBishop<ept id=\"p101\">](http://blogs.technet.com/b/machinelearning/archive/2014/10/30/embracing-uncertainty-probabilistic-inference.aspx)</ept>.",
      "nodes": [
        {
          "content": "On an historical note, Bayes' point machines were developed at Microsoft\nResearch.",
          "pos": [
            0,
            82
          ]
        },
        {
          "content": "They have some exceptionally beautiful theoretical work behind\nthem.",
          "pos": [
            83,
            151
          ]
        },
        {
          "content": "The interested student is directed to the <bpt id=\"p100\">[</bpt>original article in\nJMLR<ept id=\"p100\">](http://jmlr.org/papers/volume1/herbrich01a/herbrich01a.pdf)</ept><ph id=\"ph29\"/> and an\n<bpt id=\"p101\">[</bpt>insightful blog by Chris\nBishop<ept id=\"p101\">](http://blogs.technet.com/b/machinelearning/archive/2014/10/30/embracing-uncertainty-probabilistic-inference.aspx)</ept>.",
          "pos": [
            152,
            535
          ]
        }
      ]
    },
    {
      "pos": [
        21748,
        21770
      ],
      "content": "Specialized algorithms"
    },
    {
      "pos": [
        21772,
        22469
      ],
      "content": "If you have a very specific goal you may be in luck. Within the Azure Machine Learning\ncollection there are algorithms that specialize in rank prediction\n(<bpt id=\"p102\">[</bpt>ordinal\nregression<ept id=\"p102\">](https://msdn.microsoft.com/library/azure/dn906029.aspx)</ept>),\ncount prediction (<bpt id=\"p103\">[</bpt>Poisson\nregression<ept id=\"p103\">](https://msdn.microsoft.com/library/azure/dn905988.aspx)</ept>),\nand anomaly detection (one based on <bpt id=\"p104\">[</bpt>principal components\nanalysis<ept id=\"p104\">](https://msdn.microsoft.com/library/azure/dn913102.aspx)</ept>\nand one based on <bpt id=\"p105\">[</bpt>support vector\nmachine<ept id=\"p105\">](https://msdn.microsoft.com/library/azure/dn913103.aspx)</ept>s).\nAnd there is a lone clustering algorithm as well\n(<bpt id=\"p106\">[</bpt>K-means<ept id=\"p106\">](https://msdn.microsoft.com/library/azure/5049a09b-bd90-4c4e-9b46-7c87e3a36810/)</ept>).",
      "nodes": [
        {
          "content": "If you have a very specific goal you may be in luck.",
          "pos": [
            0,
            52
          ]
        },
        {
          "content": "Within the Azure Machine Learning\ncollection there are algorithms that specialize in rank prediction\n(<bpt id=\"p102\">[</bpt>ordinal\nregression<ept id=\"p102\">](https://msdn.microsoft.com/library/azure/dn906029.aspx)</ept>),\ncount prediction (<bpt id=\"p103\">[</bpt>Poisson\nregression<ept id=\"p103\">](https://msdn.microsoft.com/library/azure/dn905988.aspx)</ept>),\nand anomaly detection (one based on <bpt id=\"p104\">[</bpt>principal components\nanalysis<ept id=\"p104\">](https://msdn.microsoft.com/library/azure/dn913102.aspx)</ept>\nand one based on <bpt id=\"p105\">[</bpt>support vector\nmachine<ept id=\"p105\">](https://msdn.microsoft.com/library/azure/dn913103.aspx)</ept>s).",
          "pos": [
            53,
            723
          ]
        },
        {
          "content": "And there is a lone clustering algorithm as well\n(<bpt id=\"p106\">[</bpt>K-means<ept id=\"p106\">](https://msdn.microsoft.com/library/azure/5049a09b-bd90-4c4e-9b46-7c87e3a36810/)</ept>).",
          "pos": [
            724,
            907
          ]
        }
      ]
    },
    {
      "pos": [
        22471,
        22504
      ],
      "content": "<ph id=\"ph30\">![</ph>PCA-based anomaly detection<ph id=\"ph31\">][8]</ph>"
    },
    {
      "pos": [
        22506,
        22679
      ],
      "content": "<bpt id=\"p107\">***</bpt><bpt id=\"p108\"/>PCA-based anomaly detection<ept id=\"p108\">***</ept><ept id=\"p107\"/> <bpt id=\"p109\">*</bpt>- the vast majority of the data falls\ninto a stereotypical distribution; points deviating dramatically from\nthat distribution are suspect<ept id=\"p109\">*</ept>"
    },
    {
      "pos": [
        22681,
        22717
      ],
      "content": "<ph id=\"ph32\">![</ph>Data set grouped using K-means<ph id=\"ph33\">][9]</ph>"
    },
    {
      "pos": [
        22719,
        22776
      ],
      "content": "<bpt id=\"p110\">***</bpt><bpt id=\"p111\"/>A data set is grouped into 5 clusters using K-means<ept id=\"p111\">***</ept><ept id=\"p110\"/>"
    },
    {
      "pos": [
        22778,
        23090
      ],
      "content": "There is also an ensemble <bpt id=\"p112\">[</bpt>one-v-all multiclass\nclassifier<ept id=\"p112\">](https://msdn.microsoft.com/library/azure/dn905887.aspx)</ept>,\nwhich breaks the N-class classification problem into N-1 two-class\nclassification problems. The accuracy, training time, and linearity\nproperties are determined by the two-class classifiers used.",
      "nodes": [
        {
          "content": "There is also an ensemble <bpt id=\"p112\">[</bpt>one-v-all multiclass\nclassifier<ept id=\"p112\">](https://msdn.microsoft.com/library/azure/dn905887.aspx)</ept>,\nwhich breaks the N-class classification problem into N-1 two-class\nclassification problems.",
          "pos": [
            0,
            250
          ]
        },
        {
          "content": "The accuracy, training time, and linearity\nproperties are determined by the two-class classifiers used.",
          "pos": [
            251,
            354
          ]
        }
      ]
    },
    {
      "pos": [
        23092,
        23162
      ],
      "content": "<ph id=\"ph34\">![</ph>Two-class classifiers combined to form a three-class classifier<ph id=\"ph35\">][10]</ph>"
    },
    {
      "pos": [
        23164,
        23242
      ],
      "content": "<bpt id=\"p113\">***</bpt><bpt id=\"p114\"/>A pair of two-class classifiers combine to form a three-class\nclassifier<ept id=\"p114\">***</ept><ept id=\"p113\"/>"
    },
    {
      "pos": [
        23244,
        24221
      ],
      "content": "Azure Machine Learning also includes access to a powerful machine learning framework\nunder the title of <bpt id=\"p115\">[</bpt>Vowpal\nWabbit<ept id=\"p115\">](https://msdn.microsoft.com/library/azure/8383eb49-c0a3-45db-95c8-eb56a1fef5bf)</ept>.\nVW defies categorization here, since it can learn both classification\nand regression problems and can even learn from partially unlabeled\ndata. You can configure it to use any one of a number of learning\nalgorithms, loss functions, and optimization algorithms. It was designed\nfrom the ground up to be efficient, parallel, and extremely fast. It\nhandles ridiculously large feature sets with little apparent effort.\nStarted and led by Microsoft Research's own John Langford, VW is a\nFormula One entry in a field of stock car algorithms. Not every problem\nfits VW, but if yours does, it may be worth your while to climb the\nlearning curve on its interface. It's also available as <bpt id=\"p116\">[</bpt>stand-alone\nopen source code<ept id=\"p116\">](https://github.com/JohnLangford/vowpal_wabbit)</ept><ph id=\"ph36\"/> in\nseveral languages.",
      "nodes": [
        {
          "content": "Azure Machine Learning also includes access to a powerful machine learning framework\nunder the title of <bpt id=\"p115\">[</bpt>Vowpal\nWabbit<ept id=\"p115\">](https://msdn.microsoft.com/library/azure/8383eb49-c0a3-45db-95c8-eb56a1fef5bf)</ept>.",
          "pos": [
            0,
            241
          ]
        },
        {
          "content": "VW defies categorization here, since it can learn both classification\nand regression problems and can even learn from partially unlabeled\ndata.",
          "pos": [
            242,
            385
          ]
        },
        {
          "content": "You can configure it to use any one of a number of learning\nalgorithms, loss functions, and optimization algorithms.",
          "pos": [
            386,
            502
          ]
        },
        {
          "content": "It was designed\nfrom the ground up to be efficient, parallel, and extremely fast.",
          "pos": [
            503,
            584
          ]
        },
        {
          "content": "It\nhandles ridiculously large feature sets with little apparent effort.",
          "pos": [
            585,
            656
          ]
        },
        {
          "content": "Started and led by Microsoft Research's own John Langford, VW is a\nFormula One entry in a field of stock car algorithms.",
          "pos": [
            657,
            777
          ]
        },
        {
          "content": "Not every problem\nfits VW, but if yours does, it may be worth your while to climb the\nlearning curve on its interface.",
          "pos": [
            778,
            896
          ]
        },
        {
          "content": "It's also available as <bpt id=\"p116\">[</bpt>stand-alone\nopen source code<ept id=\"p116\">](https://github.com/JohnLangford/vowpal_wabbit)</ept><ph id=\"ph36\"/> in\nseveral languages.",
          "pos": [
            897,
            1076
          ]
        }
      ]
    }
  ],
  "content": "<properties\n    pageTitle=\"How to choose machine learning algorithms | Microsoft Azure\"\n    description=\"How to choose Azure Machine Learning algorithms for supervised and unsupervised learning in clustering, classification, or regression experiments.\"\n    services=\"machine-learning\"\n    documentationCenter=\"\"\n    authors=\"brohrer\"\n    manager=\"paulettm\"\n    editor=\"cgronlun\"\n    tags=\"\"/>\n    \n<tags\n    ms.service=\"machine-learning\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\"\n    ms.tgt_pltfrm=\"na\"\n    ms.workload=\"data-services\"\n    ms.date=\"02/10/2016\"\n    ms.author=\"brohrer;garye\" />\n\n# How to choose algorithms for Microsoft Azure Machine Learning\n\nThe answer to the question \"What machine learning algorithm should I use?\" is always \"It depends.\" It depends on the size, quality, and nature of the data. It depends what you want to do with the answer. It depends on how the math of the algorithm was translated into instructions for the computer you are using. And it depends on how much time you have. Even the most experienced data scientists can't tell which algorithm will perform best before trying them.\n\n## The Machine Learning Algorithm Cheat Sheet\n\nThe **Microsoft Azure Machine Learning Algorithm Cheat Sheet** helps you choose the right machine learning algorithm for your predictive analytics solutions from the Microsoft Azure Machine Learning library of algorithms.\nThis article walks you through how to use it.\n\n> [AZURE.NOTE] To download the cheat sheet and follow along with this article, go to [Machine learning algorithm cheat sheet for Microsoft Azure Machine Learning Studio](machine-learning-algorithm-cheat-sheet.md).\n\nThis cheat sheet has a very specific audience in mind: a beginning data scientist with undergraduate-level machine learning, trying to choose an algorithm to start with in Azure Machine Learning Studio. That means that it makes some generalizations and oversimplifications, but it will point you in a safe direction. It also means that there are lots of algorithms not listed here. As Azure Machine Learning grows to encompass a more complete set of available methods, we'll add them.\n\nThese recommendations are compiled feedback and tips from a lot of data scientists and machine learning experts. We didn't agree on everything, but I've tried to harmonize our opinions into a rough consensus. Most of the statements of disagreement begin with \"It depends…\"\n\n### How to use the cheat sheet\n\nRead the path and algorithm labels on the chart as \"For *&lt;path\nlabel&gt;* use *&lt;algorithm&gt;*.\" For example, \"For *speed* use *two\nclass logistic regression*.\" Sometimes more than one branch will apply.\nSometimes none of them will be a perfect fit. They're intended to be\nrule-of-thumb recommendations, so don't worry about it being exact.\nSeveral data scientists I talked with said that the only sure way to\nfind the very best algorithm is to try all of them.\n\nHere's an example from the [Cortana Analytics Gallery](http://gallery.azureml.net/) of an experiment that tries\nseveral algorithms against the same data and compares the results:\n[Compare Multi-class Classifiers: Letter\nrecognition](http://gallery.azureml.net/Details/a635502fc98b402a890efe21cec65b92).\n\n>[AZURE.TIP] To download and print a diagram that gives an overview of the capabilities of Machine Learning Studio, see [Overview diagram of Azure Machine Learning Studio capabilities](machine-learning-studio-overview-diagram.md).\n\n## Flavors of machine learning\n\n### Supervised\n\nSupervised learning algorithms make predictions based on a set of\nexamples. For instance, historical stock prices can be used to hazard\nguesses at future prices. Each example used for training is labeled with\nthe value of interest—in this case the stock price. A supervised\nlearning algorithm looks for patterns in those value labels. It can use\nany information that might be relevant—the day of the week, the season,\nthe company's financial data, the type of industry, the presence of\ndisruptive geopolicitical events—and each algorithm looks for different\ntypes of patterns. After the algorithm has found the best pattern it\ncan, it uses that pattern to make predictions for unlabeled testing\ndata—tomorrow's prices.\n\nThis is a popular and useful type of machine learning. With one\nexception, all of the modules in Azure Machine Learning are supervised learning\nalgorithms. There are several specific types of supervised learning that\nare represented within Azure Machine Learning: classification, regression, and anomaly\ndetection.\n\n* **Classification**. When the data are being used to predict a\ncategory, supervised learning is also called classification. This is\nthe case when assigning an image as a picture of either a 'cat' or a\n'dog'. When there are only two choices, this is called **two-class**\nor **binomial classification**. When there are more categories, as\nwhen predicting the winner of the NCAA March Madness tournament, this\nproblem is known as **multi-class classification**.\n\n* **Regression**. When a value is being predicted, as with stock prices,\nsupervised learning is called regression.\n\n* **Anomaly detection**. Sometimes the goal is to identify data points\nthat are simply unusual. In fraud detection, for example, any highly\nunusual credit card spending patterns are suspect. The possible\nvariations are so numerous and the training examples so few, that it's\nnot feasible to learn what fraudulent activity looks like. The\napproach that anomaly detection takes is to simply learn what normal\nactivity looks like (using a history non-fraudulent transactions) and\nidentify anything that is significantly different.\n\n### Unsupervised\n\nIn unsupervised learning, data points have no labels associated with\nthem. Instead, the goal of an unsupervised learning algorithm is to\norganize the data in some way or to describe its structure. This can\nmean grouping it into clusters or finding different ways of looking at\ncomplex data so that it appears simpler or more organized.\n\n### Reinforcement learning\n\nIn reinforcement learning, the algorithm gets to choose an action in\nresponse to each data point. The learning algorithm also receives a\nreward signal a short time later, indicating how good the decision was.\nBased on this, the algorithm modifies its strategy in order to achieve\nthe highest reward. Currently there are no reinforcement learning\nalgorithm modules in Azure Machine Learning. Reinforcement learning is common in\nrobotics, where the set of sensor readings at one point in time is a\ndata point, and the algorithm must choose the robot's next action. It is\nalso a natural fit for Internet of Things applications.\n\n## Considerations when choosing an algorithm\n\n### Accuracy\n\nGetting the most accurate answer possible isn't always necessary.\nSometimes an approximation is adequate, depending on what you want to\nuse it for. If that's the case, you may be able to cut your processing\ntime dramatically by sticking with more approximate methods. Another\nadvantage of more approximate methods is that they naturally tend to\navoid [overfitting](https://youtu.be/DQWI1kvmwRg).\n\n### Training time\n\nThe number of minutes or hours necessary to train a model varies a great\ndeal between algorithms. Training time is often closely tied to\naccuracy—one typically accompanies the other. In addition, some\nalgorithms are more sensitive to the number of data points than others.\nWhen time is limited it can drive the choice of algorithm, especially\nwhen the data set is large.\n\n### Linearity\n\nLots of machine learning algorithms make use of linearity. Linear\nclassification algorithms assume that classes can be separated by a\nstraight line (or its higher-dimensional analog). These include logistic\nregression and support vector machines (as implemented in Azure Machine Learning).\nLinear regression algorithms assume that data trends follow a straight\nline. These assumptions aren't bad for some problems, but on others they\nbring accuracy down.\n\n![Non-linear class bounday][1]\n\n***Non-linear class boundary*** *- relying on a linear classification\nalgorithm would result in low accuracy*\n\n![Data with a nonlinear trend][2]\n\n***Data with a nonlinear trend*** *- using a linear regression method would\ngenerate much larger errors than necessary*\n\nDespite their dangers, linear algorithms are very popular as a first\nline of attack. They tend to be algorithmically simple and fast to\ntrain.\n\n### Number of parameters\n\nParameters are the knobs a data scientist gets to turn when setting up\nan algorithm. They are numbers that affect the algorithm's behavior,\nsuch as error tolerance or number of iterations, or options between\nvariants of how the algorithm behaves. The training time and accuracy of\nthe algorithm can sometimes be quite sensitive to getting just the right\nsettings. Typically, algorithms with large numbers parameters require\nthe most trial and error to find a good combination.\n\nAlternatively, there is a [parameter\nsweeping](machine-learning-algorithm-parameters-optimize.md)\nmodule block in Azure Machine Learning that automatically tries all parameter\ncombinations at whatever granularity you choose. While this is a great\nway to make sure you've spanned the parameter space, the time required\nto train a model increases exponentially with the number of parameters.\n\nThe upside is that having many parameters typically indicates that an\nalgorithm has greater flexibility. It can often achieve very good\naccuracy. Provided you can find the right combination of parameter\nsettings.\n\n### Number of features\n\nFor certain types of data, the number of features can be very large\ncompared to the number of data points. This is often the case with\ngenetics or textual data. The large number of features can bog down some\nlearning algorithms, making training time unfeasibly long. Support\nVector Machines are particularly well suited to this case (see below).\n\n### Special cases\n\nSome learning algorithms make particular assumptions about the structure\nof the data or the desired results. If you can find one that fits your\nneeds, it can give you more useful results, more accurate predictions,\nor faster training times.\n\n|**Algorithm**|**Accuracy**|**Training time**|**Linearity**|**Parameters**|**Notes**|\n|---|:---:|:---:|:---:|:---:|---|\n|**Two-class classification**| | | | | |\n|[logistic regression](https://msdn.microsoft.com/library/azure/dn905994.aspx)                    | |●|●|5| |\n|[decision forest](https://msdn.microsoft.com/library/azure/dn906008.aspx)|●|○| |6| |\n|[decision jungle](https://msdn.microsoft.com/library/azure/dn905976.aspx)|●|○| |6|Low memory footprint|\n|[boosted decision tree](https://msdn.microsoft.com/library/azure/dn906025.aspx)|●|○| |6|Large memory footprint|\n|[neural network](https://msdn.microsoft.com/library/azure/dn905947.aspx)|●| | |9|[Additional customization is possible](http://go.microsoft.com/fwlink/?LinkId=402867)|\n|[averaged perceptron](https://msdn.microsoft.com/library/azure/dn906036.aspx)|○|○|●|4| |\n|[support vector machine](https://msdn.microsoft.com/library/azure/dn905835.aspx)| |○|●|5|Good for large feature sets|\n|[locally deep support vector machine](https://msdn.microsoft.com/library/azure/dn913070.aspx)|○| | |8|Good for large feature sets|\n|[Bayes’ point machine](https://msdn.microsoft.com/library/azure/dn905930.aspx)| |○|●|3| |\n|**Multi-class classification**| | | | | |\n|[logistic regression](https://msdn.microsoft.com/library/azure/dn905853.aspx)| |●|●|5| |\n|[decision forest](https://msdn.microsoft.com/library/azure/dn906015.aspx)|●|○| |6| |\n|[decision jungle ](https://msdn.microsoft.com/library/azure/dn905963.aspx)|●|○| |6|Low memory footprint|\n|[neural network](https://msdn.microsoft.com/library/azure/dn906030.aspx)|●| | |9|[Additional customization is possible](http://go.microsoft.com/fwlink/?LinkId=402867)|\n|[one-v-all](https://msdn.microsoft.com/library/azure/dn905887.aspx)|-|-|-|-|See properties of the two-class method selected|\n|**Regression**| | | | | |\n|[linear ](https://msdn.microsoft.com/library/azure/dn905978.aspx)| |●|●|4| |\n|[Bayesian linear](https://msdn.microsoft.com/library/azure/dn906022.aspx)| |○|●|2| |\n|[decision forest](https://msdn.microsoft.com/library/azure/dn905862.aspx)|●|○| |6| |\n|[boosted decision tree](https://msdn.microsoft.com/library/azure/dn905801.aspx)|●|○| |5|Large memory footprint|\n|[fast forest quantile](https://msdn.microsoft.com/library/azure/dn913093.aspx)|●|○| |9|Distributions rather than point predictions|\n|[neural network](https://msdn.microsoft.com/library/azure/dn905924.aspx)|●| | |9|[Additional customization is possible](http://go.microsoft.com/fwlink/?LinkId=402867)|\n|[Poisson ](https://msdn.microsoft.com/library/azure/dn905988.aspx)| | |●|5|Technically log-linear. For predicting counts|\n|[ordinal](https://msdn.microsoft.com/library/azure/dn906029.aspx)| | | |0|For predicting rank-ordering|\n|**Anomaly detection**| | | | | |\n|[support vector machine](https://msdn.microsoft.com/library/azure/dn913103.aspx)|○|○| |2|Especially good for large feature sets|\n|[PCA-based anomaly detection ](https://msdn.microsoft.com/library/azure/dn913102.aspx)| |○|●|3| |\n|[K-means](https://msdn.microsoft.com/library/azure/5049a09b-bd90-4c4e-9b46-7c87e3a36810/)| |○|●|4|A clustering algorithm|\n\n\n**Algorithm properties:**\n\n**●** - shows excellent accuracy, fast training times, and the use of linearity\n\n**○** - shows good accuracy and moderate training times\n\n## Algorithm notes\n\n### Linear regression\n\nAs mentioned previously, [linear regression](https://msdn.microsoft.com/library/azure/dn905978.aspx)\nfits a line (or plane, or hyperplane) to the data set. It's a workhorse,\nsimple and fast, but it may be overly simplistic for some problems.\nCheck here for a [linear regression\ntutorial](machine-learning-linear-regression-in-azure.md).\n\n![Data with a linear trend][3]\n\n***Data with a linear trend***\n\n### Logistic regression\n\nAlthough it confusingly includes 'regression' in the name, logistic\nregression is actually a powerful tool for\n[two-class](https://msdn.microsoft.com/library/azure/dn905994.aspx)\nand\n[multiclass](https://msdn.microsoft.com/library/azure/dn905853.aspx)\nclassification. It's fast and simple. The fact that it uses an\n'S'-shaped curve instead of a straight line makes it a natural fit for\ndividing data into groups. Logistic regression gives linear class\nboundaries, so when you use it, make sure a linear approximation is\nsomething you can live with.\n\n![Logistic regression to two-class data with just one feature][4]\n\n***A logistic regression to two-class data with just one feature*** *- the\nclass boundary is the point at which the logistic curve is just as close to both classes*\n\n### Trees, forests, and jungles\n\nDecision forests\n([regression](https://msdn.microsoft.com/library/azure/dn905862.aspx),\n[two-class](https://msdn.microsoft.com/library/azure/dn906008.aspx),\nand\n[multiclass](https://msdn.microsoft.com/library/azure/dn906015.aspx)),\ndecision jungles\n([two-class](https://msdn.microsoft.com/library/azure/dn905976.aspx)\nand\n[multiclass](https://msdn.microsoft.com/library/azure/dn905963.aspx)),\nand boosted decision trees\n([regression](https://msdn.microsoft.com/library/azure/dn905801.aspx)\nand\n[two-class](https://msdn.microsoft.com/library/azure/dn906025.aspx))\nare all based on decision trees, a foundational machine learning\nconcept. There are many variants of decision trees, but they all do the\nsame thing—subdivide the feature space into regions with mostly the same\nlabel. These can be regions of consistent category or of constant value,\ndepending on whether you are doing classification or regression.\n\n![Decision tree subdivides a feature space][5]\n\n***A decision tree subdivides a feature space into regions of roughly\nuniform values***\n\nBecause a feature space can be subdivided into arbitrarily small\nregions, it's easy to imagine dividing it finely enough to have one data\npoint per region—an extreme example of overfitting. In order to avoid\nthis, a large set of trees are constructed with special mathematical\ncare taken that the trees are not correlated. The average of this\n\"decision forest\" is a tree that avoids overfitting. Decision forests\ncan use a lot of memory. Decision jungles are a variant that consumes\nless memory at the expense of a slightly longer training time.\n\nBoosted decision trees avoid overfitting by limiting how many times they\ncan subdivide and how few data points are allowed in each region. The\nalgorithm constructs a sequence of trees, each of which learns to\ncompensate for the error left by the tree before. The result is a very\naccurate learner that tends to use a lot of memory. For the full\ntechnical description, check out [Friedman's original\npaper](http://www-stat.stanford.edu/~jhf/ftp/trebst.pdf).\n\n[Fast forest quantile\nregression](https://msdn.microsoft.com/library/azure/dn913093.aspx)\nis a variation of decision trees for the special case where you want to\nknow not only the typical (median) value of the data within a region,\nbut also its distribution in the form of quantiles.\n\n### Neural networks and perceptrons\n\nNeural networks are brain-inspired learning algorithms covering\n[multiclass](https://msdn.microsoft.com/library/azure/dn906030.aspx),\n[two-class](https://msdn.microsoft.com/library/azure/dn905947.aspx),\nand\n[regression](https://msdn.microsoft.com/library/azure/dn905924.aspx)\nproblems. They come in an infinite variety, but the neural networks\nwithin Azure Machine Learning are all of the form of directed acyclic graphs. That\nmeans that input features are passed forward (never backward) through a\nsequence of layers before being turned into outputs. In each layer,\ninputs are weighted in various combinations, summed, and passed on to\nthe next layer. This combination of simple calculations results in the\nability to learn sophisticated class boundaries and data trends,\nseemingly by magic. Many-layered networks of this sort perform the \"deep\nlearning\" that fuels so much tech reporting and science fiction.\n\nThis high performance doesn't come for free, though. Neural networks can\ntake a long time to train, particularly for large data sets with lots of\nfeatures. They also have more parameters than most algorithms, which\nmeans that parameter sweeping expands the training time a great deal.\nAnd for those overachievers who wish to [specify their own network\nstructure](http://go.microsoft.com/fwlink/?LinkId=402867), the\npossibilities are inexhaustible.\n\n![Boundaries learned by neural networks][6]\n---------------------------\n\n***The boundaries learned by neural networks can be complex and\nirregular***\n\nThe [two-class averaged\nperceptron](https://msdn.microsoft.com/library/azure/dn906036.aspx)\nis neural networks' answer to skyrocketing training times. It uses a\nnetwork structure that gives linear class boundaries. It is almost\nprimitive by today's standards, but it has a long history of working\nrobustly and is small enough to learn quickly.\n\n### SVMs\n\nSupport vector machines (SVMs) find the boundary that separates classes\nby as wide a margin as possible. When the two classes can't be clearly\nseparated, the algorithms find the best boundary they can. As written in\nAzure Machine Learning, the [two-class\nSVM](https://msdn.microsoft.com/library/azure/dn905835.aspx) does\nthis with a straight line only. (In SVM-speak, it uses a linear kernel.)\nBecause it makes this linear approximation, it is able to run fairly\nquickly. Where it really shines is with feature-intense data, like text\nor genomic. In these cases SVMs are able to separate classes more\nquickly and with less overfitting than most other algorithms, in\naddition to requiring only a modest amount of memory.\n\n![Support vector machine class boundary][7]\n\n***A typical support vector machine class boundary maximizes the margin\nseparating two classes***\n\nAnother product of Microsoft Research, the [two-class locally deep\nSVM](https://msdn.microsoft.com/library/azure/dn913070.aspx) is a\nnon-linear variant of SVM that retains most of the speed and memory\nefficiency of the linear version. It is ideal for cases where the linear\napproach doesn't give accurate enough answers. The developers kept it\nfast by breaking the problem down into a bunch of small linear SVM\nproblems. Read the [full\ndescription](http://research.microsoft.com/um/people/manik/pubs/Jose13.pdf)\nfor the details on how they pulled off this trick.\n\nUsing a clever extension of nonlinear SVMs, the [one-class\nSVM](https://msdn.microsoft.com/library/azure/dn913103.aspx) draws\na boundary that tightly outlines the entire data set. It is useful for\nanomaly detection. Any new data points that fall far outside that\nboundary are unusual enough to be noteworthy.\n\n### Bayesian methods\n\nBayesian methods have a highly desirable quality: they avoid\noverfitting. They do this by making some assumptions beforehand about\nthe likely distribution of the answer. Another byproduct of this\napproach is that they have very few parameters. Azure Machine Learning has both\nBayesian algorithms for both classification ([Two-class Bayes' point\nmachine](https://msdn.microsoft.com/library/azure/dn905930.aspx))\nand regression ([Bayesian linear\nregression](https://msdn.microsoft.com/library/azure/dn906022.aspx)).\nNote that these assume that the data can be split or fit with a straight\nline.\n\nOn an historical note, Bayes' point machines were developed at Microsoft\nResearch. They have some exceptionally beautiful theoretical work behind\nthem. The interested student is directed to the [original article in\nJMLR](http://jmlr.org/papers/volume1/herbrich01a/herbrich01a.pdf) and an\n[insightful blog by Chris\nBishop](http://blogs.technet.com/b/machinelearning/archive/2014/10/30/embracing-uncertainty-probabilistic-inference.aspx).\n\n### Specialized algorithms\n\nIf you have a very specific goal you may be in luck. Within the Azure Machine Learning\ncollection there are algorithms that specialize in rank prediction\n([ordinal\nregression](https://msdn.microsoft.com/library/azure/dn906029.aspx)),\ncount prediction ([Poisson\nregression](https://msdn.microsoft.com/library/azure/dn905988.aspx)),\nand anomaly detection (one based on [principal components\nanalysis](https://msdn.microsoft.com/library/azure/dn913102.aspx)\nand one based on [support vector\nmachine](https://msdn.microsoft.com/library/azure/dn913103.aspx)s).\nAnd there is a lone clustering algorithm as well\n([K-means](https://msdn.microsoft.com/library/azure/5049a09b-bd90-4c4e-9b46-7c87e3a36810/)).\n\n![PCA-based anomaly detection][8]\n\n***PCA-based anomaly detection*** *- the vast majority of the data falls\ninto a stereotypical distribution; points deviating dramatically from\nthat distribution are suspect*\n\n![Data set grouped using K-means][9]\n\n***A data set is grouped into 5 clusters using K-means***\n\nThere is also an ensemble [one-v-all multiclass\nclassifier](https://msdn.microsoft.com/library/azure/dn905887.aspx),\nwhich breaks the N-class classification problem into N-1 two-class\nclassification problems. The accuracy, training time, and linearity\nproperties are determined by the two-class classifiers used.\n\n![Two-class classifiers combined to form a three-class classifier][10]\n\n***A pair of two-class classifiers combine to form a three-class\nclassifier***\n\nAzure Machine Learning also includes access to a powerful machine learning framework\nunder the title of [Vowpal\nWabbit](https://msdn.microsoft.com/library/azure/8383eb49-c0a3-45db-95c8-eb56a1fef5bf).\nVW defies categorization here, since it can learn both classification\nand regression problems and can even learn from partially unlabeled\ndata. You can configure it to use any one of a number of learning\nalgorithms, loss functions, and optimization algorithms. It was designed\nfrom the ground up to be efficient, parallel, and extremely fast. It\nhandles ridiculously large feature sets with little apparent effort.\nStarted and led by Microsoft Research's own John Langford, VW is a\nFormula One entry in a field of stock car algorithms. Not every problem\nfits VW, but if yours does, it may be worth your while to climb the\nlearning curve on its interface. It's also available as [stand-alone\nopen source code](https://github.com/JohnLangford/vowpal_wabbit) in\nseveral languages.\n\n\n<!-- Media -->\n\n[1]: ./media/machine-learning-algorithm-choice/image1.png\n[2]: ./media/machine-learning-algorithm-choice/image2.png\n[3]: ./media/machine-learning-algorithm-choice/image3.png\n[4]: ./media/machine-learning-algorithm-choice/image4.png\n[5]: ./media/machine-learning-algorithm-choice/image5.png\n[6]: ./media/machine-learning-algorithm-choice/image6.png\n[7]: ./media/machine-learning-algorithm-choice/image7.png\n[8]: ./media/machine-learning-algorithm-choice/image8.png\n[9]: ./media/machine-learning-algorithm-choice/image9.png\n[10]: ./media/machine-learning-algorithm-choice/image10.png\n"
}