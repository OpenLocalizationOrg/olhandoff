{
  "nodes": [
    {
      "pos": [
        27,
        99
      ],
      "content": "Step 2: Upload data into a Machine Learning experiment | Microsoft Azure"
    },
    {
      "pos": [
        118,
        236
      ],
      "content": "Step 2 of the Develop a predictive solution walkthrough: Upload stored public data into Azure Machine Learning Studio."
    },
    {
      "pos": [
        553,
        635
      ],
      "content": "Walkthrough Step 2: Upload existing data into an Azure Machine Learning experiment"
    },
    {
      "pos": [
        637,
        809
      ],
      "content": "This is the second step of the walkthrough, <bpt id=\"p1\">[</bpt>Develop a predictive analytics solution in Azure Machine Learning<ept id=\"p1\">](machine-learning-walkthrough-develop-predictive-solution.md)</ept>"
    },
    {
      "pos": [
        816,
        908
      ],
      "content": "<bpt id=\"p2\">[</bpt>Create a Machine Learning workspace<ept id=\"p2\">](machine-learning-walkthrough-1-create-ml-workspace.md)</ept>"
    },
    {
      "pos": [
        913,
        937
      ],
      "content": "<bpt id=\"p3\">**</bpt>Upload existing data<ept id=\"p3\">**</ept>"
    },
    {
      "pos": [
        942,
        1024
      ],
      "content": "<bpt id=\"p4\">[</bpt>Create a new experiment<ept id=\"p4\">](machine-learning-walkthrough-3-create-new-experiment.md)</ept>"
    },
    {
      "pos": [
        1029,
        1121
      ],
      "content": "<bpt id=\"p5\">[</bpt>Train and evaluate the models<ept id=\"p5\">](machine-learning-walkthrough-4-train-and-evaluate-models.md)</ept>"
    },
    {
      "pos": [
        1126,
        1205
      ],
      "content": "<bpt id=\"p6\">[</bpt>Deploy the web service<ept id=\"p6\">](machine-learning-walkthrough-5-publish-web-service.md)</ept>"
    },
    {
      "pos": [
        1210,
        1288
      ],
      "content": "<bpt id=\"p7\">[</bpt>Access the web service<ept id=\"p7\">](machine-learning-walkthrough-6-access-web-service.md)</ept>"
    },
    {
      "pos": [
        1302,
        1707
      ],
      "content": "To develop a predictive model for credit risk, we need data that we can use to train and then test the model. For this walkthrough, we'll use the \"UCI Statlog (German Credit Data) Data Set\" from the UCI Machine Learning repository. You can find it here:  \n<ph id=\"ph2\">&lt;a href=\"http://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)\"&gt;</ph>http://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)<ph id=\"ph3\">&lt;/a&gt;</ph>",
      "nodes": [
        {
          "content": "To develop a predictive model for credit risk, we need data that we can use to train and then test the model.",
          "pos": [
            0,
            109
          ]
        },
        {
          "content": "For this walkthrough, we'll use the \"UCI Statlog (German Credit Data) Data Set\" from the UCI Machine Learning repository.",
          "pos": [
            110,
            231
          ]
        },
        {
          "content": "You can find it here:  \n<ph id=\"ph2\">&lt;a href=\"http://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)\"&gt;</ph>http://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)<ph id=\"ph3\">&lt;/a&gt;</ph>",
          "pos": [
            232,
            453
          ]
        }
      ]
    },
    {
      "pos": [
        1709,
        1795
      ],
      "content": "We'll use the file named <bpt id=\"p8\">**</bpt>german.data<ept id=\"p8\">**</ept>. Download this file to your local hard drive.",
      "nodes": [
        {
          "content": "We'll use the file named <bpt id=\"p8\">**</bpt>german.data<ept id=\"p8\">**</ept>.",
          "pos": [
            0,
            79
          ]
        },
        {
          "content": "Download this file to your local hard drive.",
          "pos": [
            80,
            124
          ]
        }
      ]
    },
    {
      "pos": [
        1799,
        2167
      ],
      "content": "This dataset contains rows of 20 variables for 1000 past applicants for credit. These 20 variables represent the dataset's feature vector, which provides identifying characteristics for each credit applicant. An additional column in each row represents the applicant's calculated credit risk, with 700 applicants identified as a low credit risk and 300 as a high risk.",
      "nodes": [
        {
          "content": "This dataset contains rows of 20 variables for 1000 past applicants for credit.",
          "pos": [
            0,
            79
          ]
        },
        {
          "content": "These 20 variables represent the dataset's feature vector, which provides identifying characteristics for each credit applicant.",
          "pos": [
            80,
            208
          ]
        },
        {
          "content": "An additional column in each row represents the applicant's calculated credit risk, with 700 applicants identified as a low credit risk and 300 as a high risk.",
          "pos": [
            209,
            368
          ]
        }
      ]
    },
    {
      "pos": [
        2169,
        2452
      ],
      "content": "The UCI website provides a description of the attributes of the feature vector, which include financial information, credit history, employment status, and personal information. For each applicant, a binary rating has been given indicating whether they are a low or high credit risk.",
      "nodes": [
        {
          "content": "The UCI website provides a description of the attributes of the feature vector, which include financial information, credit history, employment status, and personal information.",
          "pos": [
            0,
            177
          ]
        },
        {
          "content": "For each applicant, a binary rating has been given indicating whether they are a low or high credit risk.",
          "pos": [
            178,
            283
          ]
        }
      ]
    },
    {
      "pos": [
        2456,
        2654
      ],
      "content": "We'll use this data to train a predictive analytics model. When we're done, our model should be able to accept information for new individuals and predict whether they are a low or high credit risk.",
      "nodes": [
        {
          "content": "We'll use this data to train a predictive analytics model.",
          "pos": [
            0,
            58
          ]
        },
        {
          "content": "When we're done, our model should be able to accept information for new individuals and predict whether they are a low or high credit risk.",
          "pos": [
            59,
            198
          ]
        }
      ]
    },
    {
      "pos": [
        2658,
        3262
      ],
      "content": "Here's one interesting twist. The description of the dataset explains that misclassifying a person as a low credit risk when they are actually a high credit risk is 5 times more costly to the financial institution than misclassifying a low credit risk as high. One simple way to take this into account in our experiment is by duplicating (5 times) those entries that represent someone with a high credit risk. Then, if the model misclassifies a high credit risk as low, it will do that misclassification 5 times, once for each duplicate. This will increase the cost of this error in the training results.",
      "nodes": [
        {
          "content": "Here's one interesting twist.",
          "pos": [
            0,
            29
          ]
        },
        {
          "content": "The description of the dataset explains that misclassifying a person as a low credit risk when they are actually a high credit risk is 5 times more costly to the financial institution than misclassifying a low credit risk as high.",
          "pos": [
            30,
            260
          ]
        },
        {
          "content": "One simple way to take this into account in our experiment is by duplicating (5 times) those entries that represent someone with a high credit risk.",
          "pos": [
            261,
            409
          ]
        },
        {
          "content": "Then, if the model misclassifies a high credit risk as low, it will do that misclassification 5 times, once for each duplicate.",
          "pos": [
            410,
            537
          ]
        },
        {
          "content": "This will increase the cost of this error in the training results.",
          "pos": [
            538,
            604
          ]
        }
      ]
    },
    {
      "pos": [
        3268,
        3294
      ],
      "content": "Convert the dataset format"
    },
    {
      "pos": [
        3295,
        3486
      ],
      "content": "The original dataset uses a blank-separated format. Machine Learning Studio works better with a comma-separated value (CSV) file, so we'll convert the dataset by replacing spaces with commas.",
      "nodes": [
        {
          "content": "The original dataset uses a blank-separated format.",
          "pos": [
            0,
            51
          ]
        },
        {
          "content": "Machine Learning Studio works better with a comma-separated value (CSV) file, so we'll convert the dataset by replacing spaces with commas.",
          "pos": [
            52,
            191
          ]
        }
      ]
    },
    {
      "pos": [
        3490,
        3593
      ],
      "content": "There are many ways to convert this data. One way is by using the following Windows PowerShell command:",
      "nodes": [
        {
          "content": "There are many ways to convert this data.",
          "pos": [
            0,
            41
          ]
        },
        {
          "content": "One way is by using the following Windows PowerShell command:",
          "pos": [
            42,
            103
          ]
        }
      ]
    },
    {
      "pos": [
        3662,
        3707
      ],
      "content": "Another way is by using the Unix sed command:"
    },
    {
      "pos": [
        3757,
        3891
      ],
      "content": "In either case, we have created a comma-separated version of the data in a file named <bpt id=\"p9\">**</bpt>german.csv<ept id=\"p9\">**</ept><ph id=\"ph4\"/> that we'll use in our experiment."
    },
    {
      "pos": [
        3895,
        3940
      ],
      "content": "Upload the dataset to Machine Learning Studio"
    },
    {
      "pos": [
        3942,
        4195
      ],
      "content": "Once the data has been converted to CSV format, we need to upload it into Machine Learning Studio. \n<ph id=\"ph5\"/>For more information about getting started with Machine Learning Studio, see <bpt id=\"p10\">[</bpt>Microsoft Azure Machine Learning Studio Home<ept id=\"p10\">](https://studio.azureml.net/)</ept>.",
      "nodes": [
        {
          "content": "Once the data has been converted to CSV format, we need to upload it into Machine Learning Studio.",
          "pos": [
            0,
            98
          ]
        },
        {
          "content": "<ph id=\"ph5\"/>For more information about getting started with Machine Learning Studio, see <bpt id=\"p10\">[</bpt>Microsoft Azure Machine Learning Studio Home<ept id=\"p10\">](https://studio.azureml.net/)</ept>.",
          "pos": [
            100,
            307
          ]
        }
      ]
    },
    {
      "pos": [
        4201,
        4373
      ],
      "content": "Open Machine Learning Studio (<bpt id=\"p11\">[</bpt>https://studio.azureml.net<ept id=\"p11\">](https://studio.azureml.net)</ept>). When asked to sign in, use the account you specified as the owner of the workspace.",
      "nodes": [
        {
          "content": "Open Machine Learning Studio (<bpt id=\"p11\">[</bpt>https://studio.azureml.net<ept id=\"p11\">](https://studio.azureml.net)</ept>).",
          "pos": [
            0,
            128
          ]
        },
        {
          "content": "When asked to sign in, use the account you specified as the owner of the workspace.",
          "pos": [
            129,
            212
          ]
        }
      ]
    },
    {
      "pos": [
        4378,
        4428
      ],
      "content": "Click the <bpt id=\"p12\">**</bpt>Studio<ept id=\"p12\">**</ept><ph id=\"ph6\"/> tab at the top of the window."
    },
    {
      "pos": [
        4433,
        4476
      ],
      "content": "Click <bpt id=\"p13\">**</bpt>+NEW<ept id=\"p13\">**</ept><ph id=\"ph7\"/> at the bottom of the window."
    },
    {
      "pos": [
        4481,
        4500
      ],
      "content": "Select <bpt id=\"p14\">**</bpt>DATASET<ept id=\"p14\">**</ept>."
    },
    {
      "pos": [
        4505,
        4532
      ],
      "content": "Select <bpt id=\"p15\">**</bpt>FROM LOCAL FILE<ept id=\"p15\">**</ept>."
    },
    {
      "pos": [
        4537,
        4639
      ],
      "content": "In the <bpt id=\"p16\">**</bpt>Upload a new dataset<ept id=\"p16\">**</ept><ph id=\"ph8\"/> dialog, click <bpt id=\"p17\">**</bpt>Browse<ept id=\"p17\">**</ept><ph id=\"ph9\"/> and find the <bpt id=\"p18\">**</bpt>german.csv<ept id=\"p18\">**</ept><ph id=\"ph10\"/> file you created."
    },
    {
      "pos": [
        4644,
        4740
      ],
      "content": "Enter a name for the dataset. For this walkthrough, we'll call it \"UCI German Credit Card Data\".",
      "nodes": [
        {
          "content": "Enter a name for the dataset.",
          "pos": [
            0,
            29
          ]
        },
        {
          "content": "For this walkthrough, we'll call it \"UCI German Credit Card Data\".",
          "pos": [
            30,
            96
          ]
        }
      ]
    },
    {
      "pos": [
        4745,
        4813
      ],
      "content": "For data type, select <bpt id=\"p19\">**</bpt>Generic CSV File With no header (.nh.csv)<ept id=\"p19\">**</ept>."
    },
    {
      "pos": [
        4818,
        4850
      ],
      "content": "Add a description if you’d like."
    },
    {
      "pos": [
        4855,
        4868
      ],
      "content": "Click <bpt id=\"p20\">**</bpt>OK<ept id=\"p20\">**</ept>."
    },
    {
      "pos": [
        4872,
        4896
      ],
      "content": "<ph id=\"ph11\">![</ph>Upload the dataset<ph id=\"ph12\">][1]</ph>"
    },
    {
      "pos": [
        4901,
        4978
      ],
      "content": "This uploads the data into a dataset module that we can use in an experiment."
    },
    {
      "pos": [
        4982,
        5105
      ],
      "content": "<ph id=\"ph13\">[AZURE.TIP]</ph><ph id=\"ph14\"/> To manage datasets that you've uploaded to Studio, click the <bpt id=\"p21\">**</bpt>DATASETS<ept id=\"p21\">**</ept><ph id=\"ph15\"/> tab to the left of the Studio window."
    },
    {
      "pos": [
        5107,
        5299
      ],
      "content": "For more information about importing various types of data into an experiment, see <bpt id=\"p22\">[</bpt>Import your training data into Azure Machine Learning Studio<ept id=\"p22\">](machine-learning-data-science-import-data.md)</ept>."
    },
    {
      "pos": [
        5301,
        5393
      ],
      "content": "<bpt id=\"p23\">**</bpt>Next: <bpt id=\"p24\">[</bpt>Create a new experiment<ept id=\"p24\">](machine-learning-walkthrough-3-create-new-experiment.md)</ept><ept id=\"p23\">**</ept>"
    }
  ],
  "content": "<properties\n    pageTitle=\"Step 2: Upload data into a Machine Learning experiment | Microsoft Azure\"\n    description=\"Step 2 of the Develop a predictive solution walkthrough: Upload stored public data into Azure Machine Learning Studio.\"\n    services=\"machine-learning\"\n    documentationCenter=\"\"\n    authors=\"garyericson\"\n    manager=\"paulettm\"\n    editor=\"cgronlun\"/>\n\n<tags\n    ms.service=\"machine-learning\"\n    ms.workload=\"tbd\"\n    ms.tgt_pltfrm=\"na\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\"\n    ms.date=\"02/03/2016\" \n    ms.author=\"garye\"/>\n\n\n# Walkthrough Step 2: Upload existing data into an Azure Machine Learning experiment\n\nThis is the second step of the walkthrough, [Develop a predictive analytics solution in Azure Machine Learning](machine-learning-walkthrough-develop-predictive-solution.md)\n\n\n1.  [Create a Machine Learning workspace](machine-learning-walkthrough-1-create-ml-workspace.md)\n2.  **Upload existing data**\n3.  [Create a new experiment](machine-learning-walkthrough-3-create-new-experiment.md)\n4.  [Train and evaluate the models](machine-learning-walkthrough-4-train-and-evaluate-models.md)\n5.  [Deploy the web service](machine-learning-walkthrough-5-publish-web-service.md)\n6.  [Access the web service](machine-learning-walkthrough-6-access-web-service.md)\n\n----------\n\nTo develop a predictive model for credit risk, we need data that we can use to train and then test the model. For this walkthrough, we'll use the \"UCI Statlog (German Credit Data) Data Set\" from the UCI Machine Learning repository. You can find it here:  \n<a href=\"http://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)\">http://archive.ics.uci.edu/ml/datasets/Statlog+(German+Credit+Data)</a>\n\nWe'll use the file named **german.data**. Download this file to your local hard drive.  \n\nThis dataset contains rows of 20 variables for 1000 past applicants for credit. These 20 variables represent the dataset's feature vector, which provides identifying characteristics for each credit applicant. An additional column in each row represents the applicant's calculated credit risk, with 700 applicants identified as a low credit risk and 300 as a high risk.\n\nThe UCI website provides a description of the attributes of the feature vector, which include financial information, credit history, employment status, and personal information. For each applicant, a binary rating has been given indicating whether they are a low or high credit risk.  \n\nWe'll use this data to train a predictive analytics model. When we're done, our model should be able to accept information for new individuals and predict whether they are a low or high credit risk.  \n\nHere's one interesting twist. The description of the dataset explains that misclassifying a person as a low credit risk when they are actually a high credit risk is 5 times more costly to the financial institution than misclassifying a low credit risk as high. One simple way to take this into account in our experiment is by duplicating (5 times) those entries that represent someone with a high credit risk. Then, if the model misclassifies a high credit risk as low, it will do that misclassification 5 times, once for each duplicate. This will increase the cost of this error in the training results.  \n\n##Convert the dataset format\nThe original dataset uses a blank-separated format. Machine Learning Studio works better with a comma-separated value (CSV) file, so we'll convert the dataset by replacing spaces with commas.  \n\nThere are many ways to convert this data. One way is by using the following Windows PowerShell command:   \n\n    cat german.data | %{$_ -replace \" \",\",\"} | sc german.csv  \n\nAnother way is by using the Unix sed command:  \n\n    sed 's/ /,/g' german.data > german.csv  \n\nIn either case, we have created a comma-separated version of the data in a file named **german.csv** that we'll use in our experiment.\n\n##Upload the dataset to Machine Learning Studio\n\nOnce the data has been converted to CSV format, we need to upload it into Machine Learning Studio. \nFor more information about getting started with Machine Learning Studio, see [Microsoft Azure Machine Learning Studio Home](https://studio.azureml.net/).\n\n1.  Open Machine Learning Studio ([https://studio.azureml.net](https://studio.azureml.net)). When asked to sign in, use the account you specified as the owner of the workspace.\n1.  Click the **Studio** tab at the top of the window.\n1.  Click **+NEW** at the bottom of the window.\n1.  Select **DATASET**.\n1.  Select **FROM LOCAL FILE**.\n1.  In the **Upload a new dataset** dialog, click **Browse** and find the **german.csv** file you created.\n1.  Enter a name for the dataset. For this walkthrough, we'll call it \"UCI German Credit Card Data\".\n1.  For data type, select **Generic CSV File With no header (.nh.csv)**.\n1.  Add a description if you’d like.\n1.  Click **OK**.  \n\n![Upload the dataset][1]  \n\n\nThis uploads the data into a dataset module that we can use in an experiment.\n\n> [AZURE.TIP] To manage datasets that you've uploaded to Studio, click the **DATASETS** tab to the left of the Studio window.\n\nFor more information about importing various types of data into an experiment, see [Import your training data into Azure Machine Learning Studio](machine-learning-data-science-import-data.md).\n\n**Next: [Create a new experiment](machine-learning-walkthrough-3-create-new-experiment.md)**\n\n[1]: ./media/machine-learning-walkthrough-2-upload-data/upload1.png\n"
}