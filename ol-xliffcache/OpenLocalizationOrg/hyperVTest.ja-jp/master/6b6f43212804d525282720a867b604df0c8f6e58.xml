{
  "nodes": [
    {
      "pos": [
        28,
        105
      ],
      "content": "Customize Hadoop clusters for the Cortana Analytics Process | Microsoft Azure"
    },
    {
      "pos": [
        125,
        205
      ],
      "content": "Popular Python modules made available in custom Azure HDInsight Hadoop clusters."
    },
    {
      "pos": [
        553,
        628
      ],
      "content": "Customize Azure HDInsight Hadoop clusters for the Cortana Analytics Process"
    },
    {
      "pos": [
        634,
        646
      ],
      "content": "Introduction"
    },
    {
      "pos": [
        648,
        994
      ],
      "content": "This article describes how to customize an HDInsight Hadoop cluster by installing 64-bit Anaconda (Python 2.7) on each node when the cluster is being provisioned in HDInsight service. This customization prepares the cluster for use with the Cortana Analytics Process. It also shows how to access the headnode to submit custom jobs to the cluster.",
      "nodes": [
        {
          "content": "This article describes how to customize an HDInsight Hadoop cluster by installing 64-bit Anaconda (Python 2.7) on each node when the cluster is being provisioned in HDInsight service.",
          "pos": [
            0,
            183
          ]
        },
        {
          "content": "This customization prepares the cluster for use with the Cortana Analytics Process.",
          "pos": [
            184,
            267
          ]
        },
        {
          "content": "It also shows how to access the headnode to submit custom jobs to the cluster.",
          "pos": [
            268,
            346
          ]
        }
      ]
    },
    {
      "pos": [
        996,
        1395
      ],
      "content": "This customization makes many popular Python modules that are included in Anaconda conveniently available for use in user defined functions (UDFs) that are designed to process Hive records in the cluster. For instructions on the procedures used in this scenario, see <bpt id=\"p1\">[</bpt>Submit Hive Queries to HDInsight Hadoop clusters in the advanced analytics process<ept id=\"p1\">](machine-learning-data-science-hive-queries.md)</ept>.",
      "nodes": [
        {
          "content": "This customization makes many popular Python modules that are included in Anaconda conveniently available for use in user defined functions (UDFs) that are designed to process Hive records in the cluster.",
          "pos": [
            0,
            204
          ]
        },
        {
          "content": "For instructions on the procedures used in this scenario, see <bpt id=\"p1\">[</bpt>Submit Hive Queries to HDInsight Hadoop clusters in the advanced analytics process<ept id=\"p1\">](machine-learning-data-science-hive-queries.md)</ept>.",
          "pos": [
            205,
            437
          ]
        }
      ]
    },
    {
      "pos": [
        1397,
        1539
      ],
      "content": "The menu below links to topics that describe how to set up the various data science environments used by the Cortana Analytics Process (CAPS)."
    },
    {
      "pos": [
        1661,
        1701
      ],
      "content": "Customize Azure HDInsight Hadoop Cluster"
    },
    {
      "pos": [
        1703,
        1989
      ],
      "content": "To create a customized HDInsight Hadoop cluster, users need to log on to <bpt id=\"p2\">[</bpt><bpt id=\"p3\">**</bpt>Classic Portal of Azure<ept id=\"p3\">**</ept><ept id=\"p2\">](https://manage.windowsazure.com/)</ept>, click <bpt id=\"p4\">**</bpt>New<ept id=\"p4\">**</ept><ph id=\"ph3\"/> at the left bottom corner, and then select DATA SERVICES -&gt; HDINSIGHT -&gt; <bpt id=\"p5\">**</bpt>CUSTOM CREATE<ept id=\"p5\">**</ept><ph id=\"ph4\"/> to bring up the <bpt id=\"p6\">**</bpt>Cluster Details<ept id=\"p6\">**</ept><ph id=\"ph5\"/> window."
    },
    {
      "pos": [
        1992,
        2014
      ],
      "content": "<ph id=\"ph6\">![</ph>Create workspace<ph id=\"ph7\">][1]</ph>"
    },
    {
      "pos": [
        2016,
        2189
      ],
      "content": "Input the name of the cluster to be created on configuration page 1, and accept default values for the other fields. Click on the arrow to go to the next configuration page.",
      "nodes": [
        {
          "content": "Input the name of the cluster to be created on configuration page 1, and accept default values for the other fields.",
          "pos": [
            0,
            116
          ]
        },
        {
          "content": "Click on the arrow to go to the next configuration page.",
          "pos": [
            117,
            173
          ]
        }
      ]
    },
    {
      "pos": [
        2192,
        2214
      ],
      "content": "<ph id=\"ph8\">![</ph>Create workspace<ph id=\"ph9\">][2]</ph>"
    },
    {
      "pos": [
        2216,
        2434
      ],
      "content": "On configuration page 2, input the number of <bpt id=\"p7\">**</bpt>DATA NODES<ept id=\"p7\">**</ept>, select the <bpt id=\"p8\">**</bpt>REGION/VIRTUAL NETWORK<ept id=\"p8\">**</ept>, and select the sizes of the <bpt id=\"p9\">**</bpt>HEAD NODE<ept id=\"p9\">**</ept><ph id=\"ph10\"/> and the <bpt id=\"p10\">**</bpt>DATA NODE<ept id=\"p10\">**</ept>. Click the arrow to go to the next configuration page.",
      "nodes": [
        {
          "content": "On configuration page 2, input the number of <bpt id=\"p7\">**</bpt>DATA NODES<ept id=\"p7\">**</ept>, select the <bpt id=\"p8\">**</bpt>REGION/VIRTUAL NETWORK<ept id=\"p8\">**</ept>, and select the sizes of the <bpt id=\"p9\">**</bpt>HEAD NODE<ept id=\"p9\">**</ept><ph id=\"ph10\"/> and the <bpt id=\"p10\">**</bpt>DATA NODE<ept id=\"p10\">**</ept>.",
          "pos": [
            0,
            333
          ]
        },
        {
          "content": "Click the arrow to go to the next configuration page.",
          "pos": [
            334,
            387
          ]
        }
      ]
    },
    {
      "pos": [
        2437,
        2741
      ],
      "content": "<ph id=\"ph11\">[AZURE.NOTE]</ph><ph id=\"ph12\"/> The <bpt id=\"p11\">**</bpt>REGION/VIRTUAL NETWORK<ept id=\"p11\">**</ept><ph id=\"ph13\"/> has to be the same as the region of the storage account that is going to be used for the HDInsight Hadoop cluster. Otherwise, in fourth configuration page, the storage account that the users want to use will not appear on the dropdown list of <bpt id=\"p12\">**</bpt>ACCOUNT NAME<ept id=\"p12\">**</ept>.",
      "nodes": [
        {
          "content": "<ph id=\"ph11\">[AZURE.NOTE]</ph><ph id=\"ph12\"/> The <bpt id=\"p11\">**</bpt>REGION/VIRTUAL NETWORK<ept id=\"p11\">**</ept><ph id=\"ph13\"/> has to be the same as the region of the storage account that is going to be used for the HDInsight Hadoop cluster.",
          "pos": [
            0,
            247
          ]
        },
        {
          "content": "Otherwise, in fourth configuration page, the storage account that the users want to use will not appear on the dropdown list of <bpt id=\"p12\">**</bpt>ACCOUNT NAME<ept id=\"p12\">**</ept>.",
          "pos": [
            248,
            433
          ]
        }
      ]
    },
    {
      "pos": [
        2743,
        2765
      ],
      "content": "<ph id=\"ph14\">![</ph>Create workspace<ph id=\"ph15\">][3]</ph>"
    },
    {
      "pos": [
        2767,
        2974
      ],
      "content": "On configuration page 3, provide a user name and password for the HDInsight Hadoop cluster. <bpt id=\"p13\">**</bpt>Do not<ept id=\"p13\">**</ept><ph id=\"ph16\"/> select the <bpt id=\"p14\">_</bpt>Enter the Hive/Oozie Metastore<ept id=\"p14\">_</ept>. Then, click the arrow to go to the next configuration page.",
      "nodes": [
        {
          "content": "On configuration page 3, provide a user name and password for the HDInsight Hadoop cluster.",
          "pos": [
            0,
            91
          ]
        },
        {
          "content": "<bpt id=\"p13\">**</bpt>Do not<ept id=\"p13\">**</ept><ph id=\"ph16\"/> select the <bpt id=\"p14\">_</bpt>Enter the Hive/Oozie Metastore<ept id=\"p14\">_</ept>.",
          "pos": [
            92,
            242
          ]
        },
        {
          "content": "Then, click the arrow to go to the next configuration page.",
          "pos": [
            243,
            302
          ]
        }
      ]
    },
    {
      "pos": [
        2977,
        2999
      ],
      "content": "<ph id=\"ph17\">![</ph>Create workspace<ph id=\"ph18\">][4]</ph>"
    },
    {
      "pos": [
        3001,
        3319
      ],
      "content": "On configuration page 4, specify the storage account name, the default container of the HDInsight Hadoop cluster. If users select <bpt id=\"p15\">_</bpt>Create default container<ept id=\"p15\">_</ept><ph id=\"ph19\"/> in the <bpt id=\"p16\">**</bpt>DEFAULT CONTAINER<ept id=\"p16\">**</ept><ph id=\"ph20\"/> drop down list, a container with the same name as the cluster will be created. Click the arrow to go to the last configuration page.",
      "nodes": [
        {
          "content": "On configuration page 4, specify the storage account name, the default container of the HDInsight Hadoop cluster.",
          "pos": [
            0,
            113
          ]
        },
        {
          "content": "If users select <bpt id=\"p15\">_</bpt>Create default container<ept id=\"p15\">_</ept><ph id=\"ph19\"/> in the <bpt id=\"p16\">**</bpt>DEFAULT CONTAINER<ept id=\"p16\">**</ept><ph id=\"ph20\"/> drop down list, a container with the same name as the cluster will be created.",
          "pos": [
            114,
            374
          ]
        },
        {
          "content": "Click the arrow to go to the last configuration page.",
          "pos": [
            375,
            428
          ]
        }
      ]
    },
    {
      "pos": [
        3321,
        3343
      ],
      "content": "<ph id=\"ph21\">![</ph>Create workspace<ph id=\"ph22\">][5]</ph>"
    },
    {
      "pos": [
        3345,
        3484
      ],
      "content": "On the final <bpt id=\"p17\">**</bpt>Script Actions<ept id=\"p17\">**</ept><ph id=\"ph23\"/> configuration page, click <bpt id=\"p18\">**</bpt>add script action<ept id=\"p18\">**</ept><ph id=\"ph24\"/> button, and fill the text fields with the following values."
    },
    {
      "pos": [
        3489,
        3545
      ],
      "content": "<bpt id=\"p19\">**</bpt>NAME<ept id=\"p19\">**</ept><ph id=\"ph25\"/> - any string as the name of this script action."
    },
    {
      "pos": [
        3549,
        3586
      ],
      "content": "<bpt id=\"p20\">**</bpt>NODE TYPE<ept id=\"p20\">**</ept><ph id=\"ph26\"/> - select <bpt id=\"p21\">**</bpt>All nodes<ept id=\"p21\">**</ept>."
    },
    {
      "pos": [
        3590,
        3688
      ],
      "content": "<bpt id=\"p22\">**</bpt>SCRIPT URI<ept id=\"p22\">**</ept><ph id=\"ph27\"/> - <bpt id=\"p23\">*</bpt>http://getgoing.blob.core.windows.net/publicscripts/Azure_HDI_Setup_Windows.ps1<ept id=\"p23\">*</ept>"
    },
    {
      "pos": [
        3696,
        3752
      ],
      "content": "<bpt id=\"p24\">*</bpt>publicscripts<ept id=\"p24\">*</ept><ph id=\"ph28\"/> is a public container in storage account"
    },
    {
      "pos": [
        3760,
        3845
      ],
      "content": "<bpt id=\"p25\">*</bpt>getgoing<ept id=\"p25\">*</ept><ph id=\"ph29\"/> we use to share PowerShell script files to facilitate users work in Azure."
    },
    {
      "pos": [
        3849,
        3879
      ],
      "content": "<bpt id=\"p26\">**</bpt>PARAMETERS<ept id=\"p26\">**</ept><ph id=\"ph30\"/> - (leave blank)"
    },
    {
      "pos": [
        3881,
        3979
      ],
      "content": "Finally, click on the check mark to start the creation of the customized HDInsight Hadoop cluster."
    },
    {
      "pos": [
        3982,
        4004
      ],
      "content": "<ph id=\"ph31\">![</ph>Create workspace<ph id=\"ph32\">][6]</ph>"
    },
    {
      "pos": [
        4033,
        4071
      ],
      "content": "Access the Head Node of Hadoop Cluster"
    },
    {
      "pos": [
        4073,
        4207
      ],
      "content": "Users must enable remote access to the Hadoop cluster in Azure before they can access the head node of the Hadoop cluster through RDP."
    },
    {
      "pos": [
        4213,
        4481
      ],
      "content": "Log in to the <bpt id=\"p27\">[</bpt><bpt id=\"p28\">**</bpt>Classic Portal of Azure<ept id=\"p28\">**</ept><ept id=\"p27\">](https://manage.windowsazure.com/)</ept>, select <bpt id=\"p29\">**</bpt>HDInsight<ept id=\"p29\">**</ept><ph id=\"ph33\"/> on the left, select your Hadoop cluster from the list of clusters, click the <bpt id=\"p30\">**</bpt>CONFIGURATION<ept id=\"p30\">**</ept><ph id=\"ph34\"/> tab, and then click the <bpt id=\"p31\">**</bpt>ENABLE REMOTE<ept id=\"p31\">**</ept><ph id=\"ph35\"/> icon at the bottom of the page."
    },
    {
      "pos": [
        4491,
        4513
      ],
      "content": "<ph id=\"ph36\">![</ph>Create workspace<ph id=\"ph37\">][7]</ph>"
    },
    {
      "pos": [
        4518,
        4746
      ],
      "content": "In the <bpt id=\"p32\">**</bpt>Configure Remote Desktop<ept id=\"p32\">**</ept><ph id=\"ph38\"/> window, enter the USER NAME and PASSWORD fields, and select the expiration date for remote access. Then click the check mark to enable the remote access to the head node of the Hadoop cluster.",
      "nodes": [
        {
          "content": "In the <bpt id=\"p32\">**</bpt>Configure Remote Desktop<ept id=\"p32\">**</ept><ph id=\"ph38\"/> window, enter the USER NAME and PASSWORD fields, and select the expiration date for remote access.",
          "pos": [
            0,
            189
          ]
        },
        {
          "content": "Then click the check mark to enable the remote access to the head node of the Hadoop cluster.",
          "pos": [
            190,
            283
          ]
        }
      ]
    },
    {
      "pos": [
        4785,
        4958
      ],
      "content": "The user name and password for the remote access are not the user name and password that you use when you created the Hadoop cluster. These are a separate set of credentials",
      "nodes": [
        {
          "content": "The user name and password for the remote access are not the user name and password that you use when you created the Hadoop cluster.",
          "pos": [
            0,
            133
          ]
        },
        {
          "content": "These are a separate set of credentials",
          "pos": [
            134,
            173
          ]
        }
      ]
    },
    {
      "pos": [
        4973,
        5060
      ],
      "content": "The expiration date of the remote access has to be within 7 days from the current date."
    },
    {
      "pos": [
        5066,
        5088
      ],
      "content": "<ph id=\"ph40\">![</ph>Create workspace<ph id=\"ph41\">][8]</ph>"
    },
    {
      "pos": [
        5093,
        5331
      ],
      "content": "After remote access is enabled, click <bpt id=\"p33\">**</bpt>CONNECT<ept id=\"p33\">**</ept><ph id=\"ph42\"/> at the bottom of the page to remote into the head node. You log on to the head node of the Hadoop cluster by entering the credentials for the remote access user that you specified earlier.",
      "nodes": [
        {
          "content": "After remote access is enabled, click <bpt id=\"p33\">**</bpt>CONNECT<ept id=\"p33\">**</ept><ph id=\"ph42\"/> at the bottom of the page to remote into the head node.",
          "pos": [
            0,
            160
          ]
        },
        {
          "content": "You log on to the head node of the Hadoop cluster by entering the credentials for the remote access user that you specified earlier.",
          "pos": [
            161,
            293
          ]
        }
      ]
    },
    {
      "pos": [
        5338,
        5360
      ],
      "content": "<ph id=\"ph43\">![</ph>Create workspace<ph id=\"ph44\">][9]</ph>"
    },
    {
      "pos": [
        5362,
        5691
      ],
      "content": "The next steps in the advanced analytics process are mapped in the <bpt id=\"p34\">[</bpt>Learning Guide: Advanced data processing in Azure<ept id=\"p34\">](machine-learning-data-science-advanced-data-processing.md)</ept><ph id=\"ph45\"/> and may include steps that move data into HDInsight, process and sample it there in preparation for learning from the data with Azure Machine Learning."
    },
    {
      "pos": [
        5693,
        6046
      ],
      "content": "See <bpt id=\"p35\">[</bpt>Submit Hive Queries to HDInsight Hadoop clusters in the advanced analytics process<ept id=\"p35\">](machine-learning-data-science-process-hive-tables.md)</ept><ph id=\"ph46\"/> for instructions on how to access the Python modules that are included in Anaconda from the head node of the cluster in user defined functions (UDFs) that are used to process Hive records stored in the cluster."
    }
  ],
  "content": "<properties \n    pageTitle=\"Customize Hadoop clusters for the Cortana Analytics Process | Microsoft Azure\" \n    description=\"Popular Python modules made available in custom Azure HDInsight Hadoop clusters.\"\n    services=\"machine-learning\" \n    documentationCenter=\"\" \n    authors=\"hangzh-msft\" \n    manager=\"paulettm\" \n    editor=\"cgronlun\"  />\n\n<tags \n    ms.service=\"machine-learning\" \n    ms.workload=\"data-services\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"02/08/2016\" \n    ms.author=\"hangzh;bradsev\" />\n\n# Customize Azure HDInsight Hadoop clusters for the Cortana Analytics Process \n\n## Introduction\n\nThis article describes how to customize an HDInsight Hadoop cluster by installing 64-bit Anaconda (Python 2.7) on each node when the cluster is being provisioned in HDInsight service. This customization prepares the cluster for use with the Cortana Analytics Process. It also shows how to access the headnode to submit custom jobs to the cluster.\n\nThis customization makes many popular Python modules that are included in Anaconda conveniently available for use in user defined functions (UDFs) that are designed to process Hive records in the cluster. For instructions on the procedures used in this scenario, see [Submit Hive Queries to HDInsight Hadoop clusters in the advanced analytics process](machine-learning-data-science-hive-queries.md).\n\nThe menu below links to topics that describe how to set up the various data science environments used by the Cortana Analytics Process (CAPS).\n\n[AZURE.INCLUDE [data-science-environment-setup](../../includes/cap-setup-environments.md)]\n\n\n## <a name=\"customize\"></a>Customize Azure HDInsight Hadoop Cluster\n\nTo create a customized HDInsight Hadoop cluster, users need to log on to [**Classic Portal of Azure**](https://manage.windowsazure.com/), click **New** at the left bottom corner, and then select DATA SERVICES -> HDINSIGHT -> **CUSTOM CREATE** to bring up the **Cluster Details** window. \n\n![Create workspace][1]\n\nInput the name of the cluster to be created on configuration page 1, and accept default values for the other fields. Click on the arrow to go to the next configuration page. \n\n![Create workspace][2]\n\nOn configuration page 2, input the number of **DATA NODES**, select the **REGION/VIRTUAL NETWORK**, and select the sizes of the **HEAD NODE** and the **DATA NODE**. Click the arrow to go to the next configuration page.\n\n>[AZURE.NOTE] The **REGION/VIRTUAL NETWORK** has to be the same as the region of the storage account that is going to be used for the HDInsight Hadoop cluster. Otherwise, in fourth configuration page, the storage account that the users want to use will not appear on the dropdown list of **ACCOUNT NAME**.\n\n![Create workspace][3]\n\nOn configuration page 3, provide a user name and password for the HDInsight Hadoop cluster. **Do not** select the _Enter the Hive/Oozie Metastore_. Then, click the arrow to go to the next configuration page. \n\n![Create workspace][4]\n\nOn configuration page 4, specify the storage account name, the default container of the HDInsight Hadoop cluster. If users select _Create default container_ in the **DEFAULT CONTAINER** drop down list, a container with the same name as the cluster will be created. Click the arrow to go to the last configuration page.\n\n![Create workspace][5]\n\nOn the final **Script Actions** configuration page, click **add script action** button, and fill the text fields with the following values.\n \n* **NAME** - any string as the name of this script action. \n* **NODE TYPE** - select **All nodes**. \n* **SCRIPT URI** - *http://getgoing.blob.core.windows.net/publicscripts/Azure_HDI_Setup_Windows.ps1* \n    * *publicscripts* is a public container in storage account \n    * *getgoing* we use to share PowerShell script files to facilitate users work in Azure. \n* **PARAMETERS** - (leave blank)\n\nFinally, click on the check mark to start the creation of the customized HDInsight Hadoop cluster. \n\n![Create workspace][6]\n\n## <a name=\"headnode\"></a> Access the Head Node of Hadoop Cluster\n\nUsers must enable remote access to the Hadoop cluster in Azure before they can access the head node of the Hadoop cluster through RDP. \n\n1. Log in to the [**Classic Portal of Azure**](https://manage.windowsazure.com/), select **HDInsight** on the left, select your Hadoop cluster from the list of clusters, click the **CONFIGURATION** tab, and then click the **ENABLE REMOTE** icon at the bottom of the page.\n    \n    ![Create workspace][7]\n\n2. In the **Configure Remote Desktop** window, enter the USER NAME and PASSWORD fields, and select the expiration date for remote access. Then click the check mark to enable the remote access to the head node of the Hadoop cluster.\n    \n    >[AZURE.NOTE] \n    >\n    >1. The user name and password for the remote access are not the user name and password that you use when you created the Hadoop cluster. These are a separate set of credentials\n    >\n    >2. The expiration date of the remote access has to be within 7 days from the current date.\n\n    ![Create workspace][8]\n\n3. After remote access is enabled, click **CONNECT** at the bottom of the page to remote into the head node. You log on to the head node of the Hadoop cluster by entering the credentials for the remote access user that you specified earlier.\n\n     ![Create workspace][9]\n\nThe next steps in the advanced analytics process are mapped in the [Learning Guide: Advanced data processing in Azure](machine-learning-data-science-advanced-data-processing.md) and may include steps that move data into HDInsight, process and sample it there in preparation for learning from the data with Azure Machine Learning.\n\nSee [Submit Hive Queries to HDInsight Hadoop clusters in the advanced analytics process](machine-learning-data-science-process-hive-tables.md) for instructions on how to access the Python modules that are included in Anaconda from the head node of the cluster in user defined functions (UDFs) that are used to process Hive records stored in the cluster.\n\n[1]: ./media/machine-learning-data-science-customize-hadoop-cluster/customize-cluster-img1.png\n[2]: ./media/machine-learning-data-science-customize-hadoop-cluster/customize-cluster-img2.png\n[3]: ./media/machine-learning-data-science-customize-hadoop-cluster/customize-cluster-img3.png\n[4]: ./media/machine-learning-data-science-customize-hadoop-cluster/customize-cluster-img4.png\n[5]: ./media/machine-learning-data-science-customize-hadoop-cluster/customize-cluster-img5.png\n[6]: ./media/machine-learning-data-science-customize-hadoop-cluster/script-actions.png\n[7]: ./media/machine-learning-data-science-customize-hadoop-cluster/enable-remote-access-1.png\n[8]: ./media/machine-learning-data-science-customize-hadoop-cluster/enable-remote-access-2.png\n[9]: ./media/machine-learning-data-science-customize-hadoop-cluster/enable-remote-access-3.png\n\n "
}