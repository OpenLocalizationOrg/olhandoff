{
  "nodes": [
    {
      "pos": [
        27,
        96
      ],
      "content": "Tasks to prepare data for enhanced machine learning | Microsoft Azure"
    },
    {
      "pos": [
        115,
        177
      ],
      "content": "Pre-process and clean data to prepare it for machine learning."
    },
    {
      "pos": [
        503,
        554
      ],
      "content": "Tasks to prepare data for enhanced machine learning"
    },
    {
      "pos": [
        559,
        571
      ],
      "content": "Introduction"
    },
    {
      "pos": [
        572,
        1158
      ],
      "content": "Pre-processing and cleaning data are important tasks that typically must be conducted before dataset can be used effectively for machine learning. Raw data is often noisy and unreliable, and may be missing values. Using such data for modeling can produce misleading results. These tasks are part of the Cortana Analytics Process (CAP) and typically follow an initial exploration of a dataset used to discover and plan the pre-processing required. For more detailed instructions on the CAP process, see the steps outlined in the <bpt id=\"p1\">[</bpt>Cortana Analytics Process<ept id=\"p1\">](cortana-analytics-process.md)</ept>.",
      "nodes": [
        {
          "content": "Pre-processing and cleaning data are important tasks that typically must be conducted before dataset can be used effectively for machine learning.",
          "pos": [
            0,
            146
          ]
        },
        {
          "content": "Raw data is often noisy and unreliable, and may be missing values.",
          "pos": [
            147,
            213
          ]
        },
        {
          "content": "Using such data for modeling can produce misleading results.",
          "pos": [
            214,
            274
          ]
        },
        {
          "content": "These tasks are part of the Cortana Analytics Process (CAP) and typically follow an initial exploration of a dataset used to discover and plan the pre-processing required.",
          "pos": [
            275,
            446
          ]
        },
        {
          "content": "For more detailed instructions on the CAP process, see the steps outlined in the <bpt id=\"p1\">[</bpt>Cortana Analytics Process<ept id=\"p1\">](cortana-analytics-process.md)</ept>.",
          "pos": [
            447,
            624
          ]
        }
      ]
    },
    {
      "pos": [
        1160,
        1565
      ],
      "content": "Pre-processing and cleaning tasks, like the data exploration task, can be carried out in a wide variety of environments, such as SQL or Hive or Azure Machine Learning Studio, and with various tools and languages, such as R or Python, depending where your data is stored and how it is formatted. Since CAP is iterative in nature, these tasks can take place at various steps in the  workflow of the process.",
      "nodes": [
        {
          "content": "Pre-processing and cleaning tasks, like the data exploration task, can be carried out in a wide variety of environments, such as SQL or Hive or Azure Machine Learning Studio, and with various tools and languages, such as R or Python, depending where your data is stored and how it is formatted.",
          "pos": [
            0,
            294
          ]
        },
        {
          "content": "Since CAP is iterative in nature, these tasks can take place at various steps in the  workflow of the process.",
          "pos": [
            295,
            405
          ]
        }
      ]
    },
    {
      "pos": [
        1567,
        1723
      ],
      "content": "This article introduces various data processing concepts and tasks that can be undertaken either before or after ingesting data into Azure Machine Learning."
    },
    {
      "pos": [
        1726,
        1967
      ],
      "content": "For an example of data exploration and pre-processing done inside Azure Machine Learning studio, see the <bpt id=\"p2\">[</bpt>Pre-processing data in Azure ML Studio<ept id=\"p2\">](https://azure.microsoft.com/documentation/videos/preprocessing-data-in-azure-ml-studio/)</ept><ph id=\"ph2\"/> video."
    },
    {
      "pos": [
        1973,
        2004
      ],
      "content": "Why pre-process and clean data?"
    },
    {
      "pos": [
        2006,
        2207
      ],
      "content": "Real world data is gathered from various sources and processes and it may contain irregularities or corrupt data compromising the quality of the dataset. The typical data quality issues that arise are:",
      "nodes": [
        {
          "content": "Real world data is gathered from various sources and processes and it may contain irregularities or corrupt data compromising the quality of the dataset.",
          "pos": [
            0,
            153
          ]
        },
        {
          "content": "The typical data quality issues that arise are:",
          "pos": [
            154,
            201
          ]
        }
      ]
    },
    {
      "pos": [
        2211,
        2278
      ],
      "content": "<bpt id=\"p3\">**</bpt>Incomplete<ept id=\"p3\">**</ept>: Data lacks attributes or containing missing values."
    },
    {
      "pos": [
        2281,
        2336
      ],
      "content": "<bpt id=\"p4\">**</bpt>Noisy<ept id=\"p4\">**</ept>: Data contains erroneous records or outliers."
    },
    {
      "pos": [
        2339,
        2408
      ],
      "content": "<bpt id=\"p5\">**</bpt>Inconsistent<ept id=\"p5\">**</ept>: Data contains conflicting records or discrepancies."
    },
    {
      "pos": [
        2410,
        2707
      ],
      "content": "Quality data is a prerequisite for quality predictive models. To avoid \"garbage in, garbage out\" and improve data quality and therefore model performance, it is imperative to conduct a data health screen to spot data issues early and decide on the corresponding data processing and cleaning steps.",
      "nodes": [
        {
          "content": "Quality data is a prerequisite for quality predictive models.",
          "pos": [
            0,
            61
          ]
        },
        {
          "content": "To avoid \"garbage in, garbage out\" and improve data quality and therefore model performance, it is imperative to conduct a data health screen to spot data issues early and decide on the corresponding data processing and cleaning steps.",
          "pos": [
            62,
            297
          ]
        }
      ]
    },
    {
      "pos": [
        2712,
        2772
      ],
      "content": "What are some typical data health screens that are employed?"
    },
    {
      "pos": [
        2774,
        2827
      ],
      "content": "We can check the general quality of data by checking:"
    },
    {
      "pos": [
        2831,
        2857
      ],
      "content": "The number of <bpt id=\"p6\">**</bpt>records<ept id=\"p6\">**</ept>."
    },
    {
      "pos": [
        2860,
        2907
      ],
      "content": "The number of <bpt id=\"p7\">**</bpt>attributes<ept id=\"p7\">**</ept><ph id=\"ph3\"/> (or <bpt id=\"p8\">**</bpt>features<ept id=\"p8\">**</ept>)."
    },
    {
      "pos": [
        2910,
        2973
      ],
      "content": "The attribute <bpt id=\"p9\">**</bpt>data types<ept id=\"p9\">**</ept><ph id=\"ph4\"/> (nominal, ordinal, or continuous)."
    },
    {
      "pos": [
        2976,
        3009
      ],
      "content": "The number of <bpt id=\"p10\">**</bpt>missing values<ept id=\"p10\">**</ept>."
    },
    {
      "pos": [
        3012,
        3044
      ],
      "content": "<bpt id=\"p11\">**</bpt>Well-formedness<ept id=\"p11\">**</ept><ph id=\"ph5\"/> of the data."
    },
    {
      "pos": [
        3052,
        3179
      ],
      "content": "If the data is in TSV or CSV, check that the column separators and line separators always correctly separate columns and lines."
    },
    {
      "pos": [
        3187,
        3299
      ],
      "content": "If the data is in HTML or XML format, check whether the data is well formed based on their respective standards."
    },
    {
      "pos": [
        3307,
        3422
      ],
      "content": "Parsing may also be necessary in order to extract structured information from semi-structured or unstructured data."
    },
    {
      "pos": [
        3425,
        3587
      ],
      "content": "<bpt id=\"p12\">**</bpt>Inconsistent data records<ept id=\"p12\">**</ept>. Check the range of values are allowed. e.g. If the data contains student GPA, check if the GPA is in the designated range, say 0~4.",
      "nodes": [
        {
          "content": "<bpt id=\"p12\">**</bpt>Inconsistent data records<ept id=\"p12\">**</ept>.",
          "pos": [
            0,
            70
          ]
        },
        {
          "content": "Check the range of values are allowed.",
          "pos": [
            71,
            109
          ]
        },
        {
          "content": "e.g. If the data contains student GPA, check if the GPA is in the designated range, say 0~4.",
          "pos": [
            110,
            202
          ]
        }
      ]
    },
    {
      "pos": [
        3589,
        3878
      ],
      "content": "When you find issues with data, <bpt id=\"p13\">**</bpt>processing steps<ept id=\"p13\">**</ept><ph id=\"ph6\"/> are necessary which often involves cleaning missing values, data normalization, discretization, text processing to remove and/or replace embedded characters which may affect data alignment, mixed data types in common fields, and others."
    },
    {
      "pos": [
        3880,
        4195
      ],
      "content": "<bpt id=\"p14\">**</bpt>Azure Machine Learning consumes well-formed tabular data<ept id=\"p14\">**</ept>.  If the data is already in tabular form, data pre-processing can be performed directly with Azure Machine Learning in the ML Studio.  If data is not in tabular form, say it is in XML, parsing may be required in order to convert the data to tabular form.",
      "nodes": [
        {
          "content": "<bpt id=\"p14\">**</bpt>Azure Machine Learning consumes well-formed tabular data<ept id=\"p14\">**</ept>.",
          "pos": [
            0,
            101
          ]
        },
        {
          "content": "If the data is already in tabular form, data pre-processing can be performed directly with Azure Machine Learning in the ML Studio.",
          "pos": [
            103,
            234
          ]
        },
        {
          "content": "If data is not in tabular form, say it is in XML, parsing may be required in order to convert the data to tabular form.",
          "pos": [
            236,
            355
          ]
        }
      ]
    },
    {
      "pos": [
        4202,
        4258
      ],
      "content": "What are some of the major tasks in data pre-processing?"
    },
    {
      "pos": [
        4262,
        4351
      ],
      "content": "<bpt id=\"p15\">**</bpt>Data cleaning<ept id=\"p15\">**</ept>:  Fill in or missing values, detect and remove noisy data and outliers."
    },
    {
      "pos": [
        4354,
        4426
      ],
      "content": "<bpt id=\"p16\">**</bpt>Data transformation<ept id=\"p16\">**</ept>:  Normalize data to reduce dimensions and noise."
    },
    {
      "pos": [
        4429,
        4509
      ],
      "content": "<bpt id=\"p17\">**</bpt>Data reduction<ept id=\"p17\">**</ept>:  Sample data records or attributes for easier data handling."
    },
    {
      "pos": [
        4512,
        4648
      ],
      "content": "<bpt id=\"p18\">**</bpt>Data discretization<ept id=\"p18\">**</ept>:  Convert continuous attributes to categorical attributes for ease of use with certain machine learning methods."
    },
    {
      "pos": [
        4651,
        4834
      ],
      "content": "<bpt id=\"p19\">**</bpt>Text cleaning<ept id=\"p19\">**</ept>: remove embedded characters which may cause data misalignment, for e.g., embedded tabs in a tab-separated data file, embedded new lines which may break records, etc."
    },
    {
      "pos": [
        4836,
        4898
      ],
      "content": "The sections below detail some of these data processing steps."
    },
    {
      "pos": [
        4903,
        4935
      ],
      "content": "How to deal with missing values?"
    },
    {
      "pos": [
        4937,
        5102
      ],
      "content": "To deal with missing values, it is best to first identify the reason for the missing values to better handle the problem. Typical missing value handling methods are:",
      "nodes": [
        {
          "content": "To deal with missing values, it is best to first identify the reason for the missing values to better handle the problem.",
          "pos": [
            0,
            121
          ]
        },
        {
          "content": "Typical missing value handling methods are:",
          "pos": [
            122,
            165
          ]
        }
      ]
    },
    {
      "pos": [
        5106,
        5154
      ],
      "content": "<bpt id=\"p20\">**</bpt>Deletion<ept id=\"p20\">**</ept>: Remove records with missing values"
    },
    {
      "pos": [
        5157,
        5281
      ],
      "content": "<bpt id=\"p21\">**</bpt>Dummy substitution<ept id=\"p21\">**</ept>: Replace missing values with a dummy value: e.g, <bpt id=\"p22\">_</bpt>unknown<ept id=\"p22\">_</ept><ph id=\"ph7\"/> for categorical or 0 for numerical values."
    },
    {
      "pos": [
        5284,
        5382
      ],
      "content": "<bpt id=\"p23\">**</bpt>Mean substitution<ept id=\"p23\">**</ept>: If the missing data is numerical, replace the missing values with the mean."
    },
    {
      "pos": [
        5385,
        5502
      ],
      "content": "<bpt id=\"p24\">**</bpt>Frequent substitution<ept id=\"p24\">**</ept>: If the missing data is categorical, replace the missing values with the most frequent item"
    },
    {
      "pos": [
        5505,
        5606
      ],
      "content": "<bpt id=\"p25\">**</bpt>Regression substitution<ept id=\"p25\">**</ept>: Use a regression method to replace missing values with regressed values."
    },
    {
      "pos": [
        5613,
        5635
      ],
      "content": "How to normalize data?"
    },
    {
      "pos": [
        5637,
        5748
      ],
      "content": "Data normalization re-scales numerical values to a specified range. Popular data normalization methods include:",
      "nodes": [
        {
          "content": "Data normalization re-scales numerical values to a specified range.",
          "pos": [
            0,
            67
          ]
        },
        {
          "content": "Popular data normalization methods include:",
          "pos": [
            68,
            111
          ]
        }
      ]
    },
    {
      "pos": [
        5752,
        5894
      ],
      "content": "<bpt id=\"p26\">**</bpt>Min-Max Normalization<ept id=\"p26\">**</ept>: Linearly transform the data to a range, say between 0 and 1, where the min value is scaled to 0 and max value to 1."
    },
    {
      "pos": [
        5897,
        6051
      ],
      "content": "<bpt id=\"p27\">**</bpt>Z-score Normalization<ept id=\"p27\">**</ept>: Scale data based on mean and standard deviation: divide the difference between the data and the mean by the standard deviation."
    },
    {
      "pos": [
        6054,
        6141
      ],
      "content": "<bpt id=\"p28\">**</bpt>Decimal scaling<ept id=\"p28\">**</ept>: Scale the data by moving the decimal point of the attribute value."
    },
    {
      "pos": [
        6148,
        6171
      ],
      "content": "How to discretize data?"
    },
    {
      "pos": [
        6173,
        6293
      ],
      "content": "Data can be discretized by converting continuous values to nominal attributes or intervals. Some ways of doing this are:",
      "nodes": [
        {
          "content": "Data can be discretized by converting continuous values to nominal attributes or intervals.",
          "pos": [
            0,
            91
          ]
        },
        {
          "content": "Some ways of doing this are:",
          "pos": [
            92,
            120
          ]
        }
      ]
    },
    {
      "pos": [
        6297,
        6471
      ],
      "content": "<bpt id=\"p29\">**</bpt>Equal-Width Binning<ept id=\"p29\">**</ept>: Divide the range of all possible values of an attribute into N groups of the same size, and assign the values that fall in a bin with the bin number."
    },
    {
      "pos": [
        6474,
        6679
      ],
      "content": "<bpt id=\"p30\">**</bpt>Equal-Height Binning<ept id=\"p30\">**</ept>: Divide the range of all possible values of an attribute into N groups, each containing the same number of instances, then assign the values that fall in a bin with the bin number."
    },
    {
      "pos": [
        6686,
        6705
      ],
      "content": "How to reduce data?"
    },
    {
      "pos": [
        6708,
        6856
      ],
      "content": "There are various methods to reduce data size for easier data handling. Depending on data size and the domain, the following methods can be applied:",
      "nodes": [
        {
          "content": "There are various methods to reduce data size for easier data handling.",
          "pos": [
            0,
            71
          ]
        },
        {
          "content": "Depending on data size and the domain, the following methods can be applied:",
          "pos": [
            72,
            148
          ]
        }
      ]
    },
    {
      "pos": [
        6860,
        6961
      ],
      "content": "<bpt id=\"p31\">**</bpt>Record Sampling<ept id=\"p31\">**</ept>: Sample the data records and only choose the representative subset from the data."
    },
    {
      "pos": [
        6964,
        7056
      ],
      "content": "<bpt id=\"p32\">**</bpt>Attribute Sampling<ept id=\"p32\">**</ept>: Select only a subset of the most important attributes from the data."
    },
    {
      "pos": [
        7061,
        7296
      ],
      "content": "<bpt id=\"p33\">**</bpt>Aggregation<ept id=\"p33\">**</ept>: Divide the data into groups and store the numbers for each group. For example, the daily revenue numbers of a restaurant chain over the past 20 years can be aggregated to monthly revenue to reduce the size of the data.",
      "nodes": [
        {
          "content": "<bpt id=\"p33\">**</bpt>Aggregation<ept id=\"p33\">**</ept>: Divide the data into groups and store the numbers for each group.",
          "pos": [
            0,
            122
          ]
        },
        {
          "content": "For example, the daily revenue numbers of a restaurant chain over the past 20 years can be aggregated to monthly revenue to reduce the size of the data.",
          "pos": [
            123,
            275
          ]
        }
      ]
    },
    {
      "pos": [
        7303,
        7326
      ],
      "content": "How to clean text data?"
    },
    {
      "pos": [
        7328,
        7928
      ],
      "content": "<bpt id=\"p34\">**</bpt>Text fields in tabular data<ept id=\"p34\">**</ept><ph id=\"ph8\"/> may include characters which affect columns alignment and/or record boundaries. For e.g., embedded tabs in a tab-separated file cause column misalignment, and embedded new line characters break record lines. Improper text encoding handling while writing/reading text leads to information loss, inadvertent introduction of unreadable characters, e.g., nulls, and may also affect text parsing. Careful parsing and editing may be required in order to clean text fields for proper alignment and/or to extract structured data from unstructured or semi-structured text data.",
      "nodes": [
        {
          "content": "<bpt id=\"p34\">**</bpt>Text fields in tabular data<ept id=\"p34\">**</ept><ph id=\"ph8\"/> may include characters which affect columns alignment and/or record boundaries.",
          "pos": [
            0,
            165
          ]
        },
        {
          "content": "For e.g., embedded tabs in a tab-separated file cause column misalignment, and embedded new line characters break record lines.",
          "pos": [
            166,
            293
          ]
        },
        {
          "content": "Improper text encoding handling while writing/reading text leads to information loss, inadvertent introduction of unreadable characters, e.g., nulls, and may also affect text parsing.",
          "pos": [
            294,
            477
          ]
        },
        {
          "content": "Careful parsing and editing may be required in order to clean text fields for proper alignment and/or to extract structured data from unstructured or semi-structured text data.",
          "pos": [
            478,
            654
          ]
        }
      ]
    },
    {
      "pos": [
        7930,
        8440
      ],
      "content": "<bpt id=\"p35\">**</bpt>Data exploration<ept id=\"p35\">**</ept><ph id=\"ph9\"/> offers an early view into the data. A number of data issues can be uncovered during this step and  corresponding methods can be applied to address those issues.  It is important to ask questions such as what is the source of the issue and how the issue may have been introduced. This also helps you decide on the data processing steps that need to be taken to resolve them. The kind of insights one intends to derive from the data can also be used to prioritize the data processing effort.",
      "nodes": [
        {
          "content": "<bpt id=\"p35\">**</bpt>Data exploration<ept id=\"p35\">**</ept><ph id=\"ph9\"/> offers an early view into the data.",
          "pos": [
            0,
            110
          ]
        },
        {
          "content": "A number of data issues can be uncovered during this step and  corresponding methods can be applied to address those issues.",
          "pos": [
            111,
            235
          ]
        },
        {
          "content": "It is important to ask questions such as what is the source of the issue and how the issue may have been introduced.",
          "pos": [
            237,
            353
          ]
        },
        {
          "content": "This also helps you decide on the data processing steps that need to be taken to resolve them.",
          "pos": [
            354,
            448
          ]
        },
        {
          "content": "The kind of insights one intends to derive from the data can also be used to prioritize the data processing effort.",
          "pos": [
            449,
            564
          ]
        }
      ]
    },
    {
      "pos": [
        8445,
        8455
      ],
      "content": "References"
    },
    {
      "pos": [
        8458,
        8578
      ],
      "content": "<bpt id=\"p36\">*</bpt>Data Mining: Concepts and Techniques<ept id=\"p36\">*</ept>, Third Edition, Morgan Kaufmann, 2011, Jiawei Han, Micheline Kamber, and Jian Pei"
    }
  ],
  "content": "<properties\n    pageTitle=\"Tasks to prepare data for enhanced machine learning | Microsoft Azure\"\n    description=\"Pre-process and clean data to prepare it for machine learning.\"\n    services=\"machine-learning\"\n    documentationCenter=\"\"\n    authors=\"bradsev\"\n    manager=\"paulettm\"\n    editor=\"cgronlun\" />\n\n<tags\n    ms.service=\"machine-learning\"\n    ms.workload=\"data-services\"\n    ms.tgt_pltfrm=\"na\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\"\n    ms.date=\"02/08/2016\"\n    ms.author=\"bradsev\" />\n\n\n# Tasks to prepare data for enhanced machine learning\n\n## Introduction\nPre-processing and cleaning data are important tasks that typically must be conducted before dataset can be used effectively for machine learning. Raw data is often noisy and unreliable, and may be missing values. Using such data for modeling can produce misleading results. These tasks are part of the Cortana Analytics Process (CAP) and typically follow an initial exploration of a dataset used to discover and plan the pre-processing required. For more detailed instructions on the CAP process, see the steps outlined in the [Cortana Analytics Process](cortana-analytics-process.md).\n\nPre-processing and cleaning tasks, like the data exploration task, can be carried out in a wide variety of environments, such as SQL or Hive or Azure Machine Learning Studio, and with various tools and languages, such as R or Python, depending where your data is stored and how it is formatted. Since CAP is iterative in nature, these tasks can take place at various steps in the  workflow of the process.\n\nThis article introduces various data processing concepts and tasks that can be undertaken either before or after ingesting data into Azure Machine Learning. \n\nFor an example of data exploration and pre-processing done inside Azure Machine Learning studio, see the [Pre-processing data in Azure ML Studio](https://azure.microsoft.com/documentation/videos/preprocessing-data-in-azure-ml-studio/) video.\n\n\n## Why pre-process and clean data?\n\nReal world data is gathered from various sources and processes and it may contain irregularities or corrupt data compromising the quality of the dataset. The typical data quality issues that arise are:\n\n* **Incomplete**: Data lacks attributes or containing missing values.\n* **Noisy**: Data contains erroneous records or outliers.\n* **Inconsistent**: Data contains conflicting records or discrepancies.\n\nQuality data is a prerequisite for quality predictive models. To avoid \"garbage in, garbage out\" and improve data quality and therefore model performance, it is imperative to conduct a data health screen to spot data issues early and decide on the corresponding data processing and cleaning steps.\n\n## What are some typical data health screens that are employed?\n\nWe can check the general quality of data by checking:\n\n* The number of **records**.\n* The number of **attributes** (or **features**).\n* The attribute **data types** (nominal, ordinal, or continuous).\n* The number of **missing values**.\n* **Well-formedness** of the data. \n    * If the data is in TSV or CSV, check that the column separators and line separators always correctly separate columns and lines. \n    * If the data is in HTML or XML format, check whether the data is well formed based on their respective standards. \n    * Parsing may also be necessary in order to extract structured information from semi-structured or unstructured data.\n* **Inconsistent data records**. Check the range of values are allowed. e.g. If the data contains student GPA, check if the GPA is in the designated range, say 0~4.\n\nWhen you find issues with data, **processing steps** are necessary which often involves cleaning missing values, data normalization, discretization, text processing to remove and/or replace embedded characters which may affect data alignment, mixed data types in common fields, and others.\n\n**Azure Machine Learning consumes well-formed tabular data**.  If the data is already in tabular form, data pre-processing can be performed directly with Azure Machine Learning in the ML Studio.  If data is not in tabular form, say it is in XML, parsing may be required in order to convert the data to tabular form.  \n\n## What are some of the major tasks in data pre-processing?\n\n* **Data cleaning**:  Fill in or missing values, detect and remove noisy data and outliers.\n* **Data transformation**:  Normalize data to reduce dimensions and noise.\n* **Data reduction**:  Sample data records or attributes for easier data handling.\n* **Data discretization**:  Convert continuous attributes to categorical attributes for ease of use with certain machine learning methods.\n* **Text cleaning**: remove embedded characters which may cause data misalignment, for e.g., embedded tabs in a tab-separated data file, embedded new lines which may break records, etc.\n\nThe sections below detail some of these data processing steps.\n\n## How to deal with missing values?\n\nTo deal with missing values, it is best to first identify the reason for the missing values to better handle the problem. Typical missing value handling methods are:\n\n* **Deletion**: Remove records with missing values\n* **Dummy substitution**: Replace missing values with a dummy value: e.g, _unknown_ for categorical or 0 for numerical values.\n* **Mean substitution**: If the missing data is numerical, replace the missing values with the mean.\n* **Frequent substitution**: If the missing data is categorical, replace the missing values with the most frequent item\n* **Regression substitution**: Use a regression method to replace missing values with regressed values.  \n\n## How to normalize data?\n\nData normalization re-scales numerical values to a specified range. Popular data normalization methods include:\n\n* **Min-Max Normalization**: Linearly transform the data to a range, say between 0 and 1, where the min value is scaled to 0 and max value to 1.\n* **Z-score Normalization**: Scale data based on mean and standard deviation: divide the difference between the data and the mean by the standard deviation.\n* **Decimal scaling**: Scale the data by moving the decimal point of the attribute value.  \n\n## How to discretize data?\n\nData can be discretized by converting continuous values to nominal attributes or intervals. Some ways of doing this are:\n\n* **Equal-Width Binning**: Divide the range of all possible values of an attribute into N groups of the same size, and assign the values that fall in a bin with the bin number.\n* **Equal-Height Binning**: Divide the range of all possible values of an attribute into N groups, each containing the same number of instances, then assign the values that fall in a bin with the bin number.  \n\n## How to reduce data? \n\nThere are various methods to reduce data size for easier data handling. Depending on data size and the domain, the following methods can be applied:\n\n* **Record Sampling**: Sample the data records and only choose the representative subset from the data.\n* **Attribute Sampling**: Select only a subset of the most important attributes from the data.  \n* **Aggregation**: Divide the data into groups and store the numbers for each group. For example, the daily revenue numbers of a restaurant chain over the past 20 years can be aggregated to monthly revenue to reduce the size of the data.  \n\n## How to clean text data?\n\n**Text fields in tabular data** may include characters which affect columns alignment and/or record boundaries. For e.g., embedded tabs in a tab-separated file cause column misalignment, and embedded new line characters break record lines. Improper text encoding handling while writing/reading text leads to information loss, inadvertent introduction of unreadable characters, e.g., nulls, and may also affect text parsing. Careful parsing and editing may be required in order to clean text fields for proper alignment and/or to extract structured data from unstructured or semi-structured text data.\n\n**Data exploration** offers an early view into the data. A number of data issues can be uncovered during this step and  corresponding methods can be applied to address those issues.  It is important to ask questions such as what is the source of the issue and how the issue may have been introduced. This also helps you decide on the data processing steps that need to be taken to resolve them. The kind of insights one intends to derive from the data can also be used to prioritize the data processing effort.\n\n## References\n\n>*Data Mining: Concepts and Techniques*, Third Edition, Morgan Kaufmann, 2011, Jiawei Han, Micheline Kamber, and Jian Pei\n "
}