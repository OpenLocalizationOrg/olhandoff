<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" source-language="en-US" target-language="ja-jp">
    <body>
      <group id="main" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Use Python components in a Storm topology on HDinsight | Microsoft Azure</source>
          <target state="new">Use Python components in a Storm topology on HDinsight | Microsoft Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Learn how you can use Python components from with Apache Storm on Azure HDInsight.</source>
          <target state="new">Learn how you can use Python components from with Apache Storm on Azure HDInsight.</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>You will learn how to use Python components from both a Java based, and Clojure based Storm topology.</source>
          <target state="new">You will learn how to use Python components from both a Java based, and Clojure based Storm topology.</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>Develop Apache Storm topologies using Python on HDInsight</source>
          <target state="new">Develop Apache Storm topologies using Python on HDInsight</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>Apache Storm supports multiple languages, even allowing you to combine components from several languages in one topology.</source>
          <target state="new">Apache Storm supports multiple languages, even allowing you to combine components from several languages in one topology.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>In this document, you will learn how to use Python components in your Java and Clojure-based Storm topologies on HDInsight.</source>
          <target state="new">In this document, you will learn how to use Python components in your Java and Clojure-based Storm topologies on HDInsight.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>Prerequisites</source>
          <target state="new">Prerequisites</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>Python 2.7 or higher</source>
          <target state="new">Python 2.7 or higher</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>Java JDK 1.7 or higher</source>
          <target state="new">Java JDK 1.7 or higher</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>Leiningen<ept id="p1">](http://leiningen.org/)</ept></source>
          <target state="new"><bpt id="p1">[</bpt>Leiningen<ept id="p1">](http://leiningen.org/)</ept></target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>Storm multi-language support</source>
          <target state="new">Storm multi-language support</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>Storm was designed to work with components written using any programming language, however this requires that the components understand how to work with the <bpt id="p2">[</bpt>Thrift definition for Storm<ept id="p2">](https://github.com/apache/storm/blob/master/storm-core/src/storm.thrift)</ept>.</source>
          <target state="new">Storm was designed to work with components written using any programming language, however this requires that the components understand how to work with the <bpt id="p2">[</bpt>Thrift definition for Storm<ept id="p2">](https://github.com/apache/storm/blob/master/storm-core/src/storm.thrift)</ept>.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>For Python, a module is provided as part of the Apache Storm project that allows you to easily interface with Storm.</source>
          <target state="new">For Python, a module is provided as part of the Apache Storm project that allows you to easily interface with Storm.</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>You can find this module at <bpt id="p3">[</bpt>https://github.com/apache/storm/blob/master/storm-multilang/python/src/main/resources/resources/storm.py<ept id="p3">](https://github.com/apache/storm/blob/master/storm-multilang/python/src/main/resources/resources/storm.py)</ept>.</source>
          <target state="new">You can find this module at <bpt id="p3">[</bpt>https://github.com/apache/storm/blob/master/storm-multilang/python/src/main/resources/resources/storm.py<ept id="p3">](https://github.com/apache/storm/blob/master/storm-multilang/python/src/main/resources/resources/storm.py)</ept>.</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>Since Apache Storm is a Java process that runs on the Java Virtual Machine (JVM,) components written in other languages are executed as subprocesses.</source>
          <target state="new">Since Apache Storm is a Java process that runs on the Java Virtual Machine (JVM,) components written in other languages are executed as subprocesses.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>The Storm bits running in the JVM communicates with these subprocesses using JSON messages sent over stdin/stdout.</source>
          <target state="new">The Storm bits running in the JVM communicates with these subprocesses using JSON messages sent over stdin/stdout.</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>More details on communication between components can be found in the <bpt id="p4">[</bpt>Multi-lang Protocol<ept id="p4">](https://storm.apache.org/documentation/Multilang-protocol.html)</ept><ph id="ph2" /> documentation.</source>
          <target state="new">More details on communication between components can be found in the <bpt id="p4">[</bpt>Multi-lang Protocol<ept id="p4">](https://storm.apache.org/documentation/Multilang-protocol.html)</ept><ph id="ph2" /> documentation.</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>The Storm module</source>
          <target state="new">The Storm module</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>The storm module (https://github.com/apache/storm/blob/master/storm-multilang/python/src/main/resources/resources/storm.py,) provides the bits needed to create Python components that work with Storm.</source>
          <target state="new">The storm module (https://github.com/apache/storm/blob/master/storm-multilang/python/src/main/resources/resources/storm.py,) provides the bits needed to create Python components that work with Storm.</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>This provides things like <ph id="ph3">`storm.emit`</ph><ph id="ph4" /> to emit tuples, and <ph id="ph5">`storm.logInfo`</ph><ph id="ph6" /> to write to the logs.</source>
          <target state="new">This provides things like <ph id="ph3">`storm.emit`</ph><ph id="ph4" /> to emit tuples, and <ph id="ph5">`storm.logInfo`</ph><ph id="ph6" /> to write to the logs.</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>I would encourage you to read over this file and understand what it provides.</source>
          <target state="new">I would encourage you to read over this file and understand what it provides.</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>Challenges</source>
          <target state="new">Challenges</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>Using the <bpt id="p5">__</bpt>storm.py<ept id="p5">__</ept><ph id="ph7" /> module, you can create Python spouts that consume data, and bolts that process data, however the overall Storm topology definition that wires up communication between components is still written using Java or Clojure.</source>
          <target state="new">Using the <bpt id="p5">__</bpt>storm.py<ept id="p5">__</ept><ph id="ph7" /> module, you can create Python spouts that consume data, and bolts that process data, however the overall Storm topology definition that wires up communication between components is still written using Java or Clojure.</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>Additionally, if you use Java, you must also create Java components that act as an interface to the Python components.</source>
          <target state="new">Additionally, if you use Java, you must also create Java components that act as an interface to the Python components.</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>Also, since Storm clusters run in a distributed fashion, you must ensure that any modules required by your Python components are available on all worker nodes in the cluster.</source>
          <target state="new">Also, since Storm clusters run in a distributed fashion, you must ensure that any modules required by your Python components are available on all worker nodes in the cluster.</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>Storm doesn't provide any easy way to accomplish this for multi-lang resources - you either have to include all dependencies as part of the jar file for the topology, or manually install dependencies on each worker node in the cluster.</source>
          <target state="new">Storm doesn't provide any easy way to accomplish this for multi-lang resources - you either have to include all dependencies as part of the jar file for the topology, or manually install dependencies on each worker node in the cluster.</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>There are some projects that attempt to overcome these shortcomings, such as <bpt id="p6">[</bpt>Pyleus<ept id="p6">](https://github.com/Yelp/pyleus)</ept><ph id="ph8" /> and <bpt id="p7">[</bpt>Streamparse<ept id="p7">](https://github.com/Parsely/streamparse)</ept>.</source>
          <target state="new">There are some projects that attempt to overcome these shortcomings, such as <bpt id="p6">[</bpt>Pyleus<ept id="p6">](https://github.com/Yelp/pyleus)</ept><ph id="ph8" /> and <bpt id="p7">[</bpt>Streamparse<ept id="p7">](https://github.com/Parsely/streamparse)</ept>.</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>While both of these can be ran on Linux-based HDInsight clusters, they are not the primary focus of this document as they require customizations during cluster setup and are not fully tested on HDInsight clusters.</source>
          <target state="new">While both of these can be ran on Linux-based HDInsight clusters, they are not the primary focus of this document as they require customizations during cluster setup and are not fully tested on HDInsight clusters.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>Notes on using these frameworks with HDInsight are included at the end of this document.</source>
          <target state="new">Notes on using these frameworks with HDInsight are included at the end of this document.</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>Java vs. Clojure topology definition</source>
          <target state="new">Java vs. Clojure topology definition</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>Of the two methods of defining a topology, Clojure is by far the easiest/cleanest as you can directly referenc python components in the topology definition.</source>
          <target state="new">Of the two methods of defining a topology, Clojure is by far the easiest/cleanest as you can directly referenc python components in the topology definition.</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>For Java-based topology definitions, you must also define Java components that handle things like declaring the fields in the tuples returned from the Python components.</source>
          <target state="new">For Java-based topology definitions, you must also define Java components that handle things like declaring the fields in the tuples returned from the Python components.</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>Both methods are described in this document, along with example projects.</source>
          <target state="new">Both methods are described in this document, along with example projects.</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>Python components with a Java topology</source>
          <target state="new">Python components with a Java topology</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source><ph id="ph9">[AZURE.NOTE]</ph><ph id="ph10" /> This example is available at <bpt id="p8">[</bpt>https://github.com/Azure-Samples/hdinsight-python-storm-wordcount<ept id="p8">](https://github.com/Azure-Samples/hdinsight-python-storm-wordcount)</ept><ph id="ph11" /> in the <bpt id="p9">__</bpt>JavaTopology<ept id="p9">__</ept><ph id="ph12" /> directory.</source>
          <target state="new"><ph id="ph9">[AZURE.NOTE]</ph><ph id="ph10" /> This example is available at <bpt id="p8">[</bpt>https://github.com/Azure-Samples/hdinsight-python-storm-wordcount<ept id="p8">](https://github.com/Azure-Samples/hdinsight-python-storm-wordcount)</ept><ph id="ph11" /> in the <bpt id="p9">__</bpt>JavaTopology<ept id="p9">__</ept><ph id="ph12" /> directory.</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>This is a Maven based project.</source>
          <target state="new">This is a Maven based project.</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>If you are unfamiliar with Maven, see <bpt id="p10">[</bpt>Develop Java-based topologies with Apache Storm on HDInsight<ept id="p10">](hdinsight-storm-develop-java-topology.md)</ept><ph id="ph13" /> for more information on creating a Maven project for a Storm topology.</source>
          <target state="new">If you are unfamiliar with Maven, see <bpt id="p10">[</bpt>Develop Java-based topologies with Apache Storm on HDInsight<ept id="p10">](hdinsight-storm-develop-java-topology.md)</ept><ph id="ph13" /> for more information on creating a Maven project for a Storm topology.</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>A Java-based topology that uses Python (or other JVM language components,) initially appears to use Java components; but if you look in each of the Java spouts/bolts, you'll see code similar to the following:</source>
          <target state="new">A Java-based topology that uses Python (or other JVM language components,) initially appears to use Java components; but if you look in each of the Java spouts/bolts, you'll see code similar to the following:</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>This is where Java invokes Python and runs the script that contains the actual bolt logic.</source>
          <target state="new">This is where Java invokes Python and runs the script that contains the actual bolt logic.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>The Java spouts/bolts (for this example,) simply declare the fields in the tuple that will be emitted by the underlying Python component.</source>
          <target state="new">The Java spouts/bolts (for this example,) simply declare the fields in the tuple that will be emitted by the underlying Python component.</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>The actual Python files are stored in the <ph id="ph14">`/multilang/resources`</ph><ph id="ph15" /> directory in this example.</source>
          <target state="new">The actual Python files are stored in the <ph id="ph14">`/multilang/resources`</ph><ph id="ph15" /> directory in this example.</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>The <ph id="ph16">`/multilang`</ph><ph id="ph17" /> directory is referenced in the <bpt id="p11">__</bpt>pom.xml<ept id="p11">__</ept>:</source>
          <target state="new">The <ph id="ph16">`/multilang`</ph><ph id="ph17" /> directory is referenced in the <bpt id="p11">__</bpt>pom.xml<ept id="p11">__</ept>:</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source><ph id="ph18">&lt;resources&gt;</ph><ph id="ph19">
    &lt;resource&gt;</ph><ph id="ph20">
        &lt;!-- Where the Python bits are kept --&gt;</ph><ph id="ph21">
        &lt;directory&gt;</ph>${basedir}/multilang<ph id="ph22">&lt;/directory&gt;</ph><ph id="ph23">
    &lt;/resource&gt;</ph><ph id="ph24">
&lt;/resources&gt;</ph></source>
          <target state="new"><ph id="ph18">&lt;resources&gt;</ph><ph id="ph19">
    &lt;resource&gt;</ph><ph id="ph20">
        &lt;!-- Where the Python bits are kept --&gt;</ph><ph id="ph21">
        &lt;directory&gt;</ph>${basedir}/multilang<ph id="ph22">&lt;/directory&gt;</ph><ph id="ph23">
    &lt;/resource&gt;</ph><ph id="ph24">
&lt;/resources&gt;</ph></target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>This includes all the files in the <ph id="ph25">`/multilang`</ph><ph id="ph26" /> folder in the jar that will be built from this project.</source>
          <target state="new">This includes all the files in the <ph id="ph25">`/multilang`</ph><ph id="ph26" /> folder in the jar that will be built from this project.</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source><ph id="ph27">[AZURE.IMPORTANT]</ph><ph id="ph28" /> Note that this only specifies the <ph id="ph29">`/multilang`</ph><ph id="ph30" /> directory and not <ph id="ph31">`/multilang/resources`</ph>.</source>
          <target state="new"><ph id="ph27">[AZURE.IMPORTANT]</ph><ph id="ph28" /> Note that this only specifies the <ph id="ph29">`/multilang`</ph><ph id="ph30" /> directory and not <ph id="ph31">`/multilang/resources`</ph>.</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>Storm expects non-JVM resources in a <ph id="ph32">`resources`</ph><ph id="ph33" /> directory, so it is looked for internally already.</source>
          <target state="new">Storm expects non-JVM resources in a <ph id="ph32">`resources`</ph><ph id="ph33" /> directory, so it is looked for internally already.</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>Placing components in this folder allows you to just reference by name in the Java code.</source>
          <target state="new">Placing components in this folder allows you to just reference by name in the Java code.</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>For example, <ph id="ph34">`super("python", "countbolt.py");`</ph>.</source>
          <target state="new">For example, <ph id="ph34">`super("python", "countbolt.py");`</ph>.</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>Another way to think of it is that Storm sees the <ph id="ph35">`resources`</ph><ph id="ph36" /> directory as the root (/) when accessing multi-lang resources.</source>
          <target state="new">Another way to think of it is that Storm sees the <ph id="ph35">`resources`</ph><ph id="ph36" /> directory as the root (/) when accessing multi-lang resources.</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>For this example project, the <ph id="ph37">`storm.py`</ph><ph id="ph38" /> module is included in the <ph id="ph39">`/multilang/resources`</ph><ph id="ph40" /> directory.</source>
          <target state="new">For this example project, the <ph id="ph37">`storm.py`</ph><ph id="ph38" /> module is included in the <ph id="ph39">`/multilang/resources`</ph><ph id="ph40" /> directory.</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>Build and run the project</source>
          <target state="new">Build and run the project</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>To run this project locally, just use the following Maven command to build and run in local mode:</source>
          <target state="new">To run this project locally, just use the following Maven command to build and run in local mode:</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>Use ctrl+c to kill the process.</source>
          <target state="new">Use ctrl+c to kill the process.</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>To deploy the project to an HDInsight cluster running Apache Storm, use the following steps:</source>
          <target state="new">To deploy the project to an HDInsight cluster running Apache Storm, use the following steps:</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source>Build an uber jar:</source>
          <target state="new">Build an uber jar:</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>This will create a file named <bpt id="p12">__</bpt>WordCount--1.0-SNAPSHOT.jar<ept id="p12">__</ept><ph id="ph41" /> in the <ph id="ph42">`/target`</ph><ph id="ph43" /> directory for this project.</source>
          <target state="new">This will create a file named <bpt id="p12">__</bpt>WordCount--1.0-SNAPSHOT.jar<ept id="p12">__</ept><ph id="ph41" /> in the <ph id="ph42">`/target`</ph><ph id="ph43" /> directory for this project.</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>Upload the jar file to the Hadoop cluster using one of the following methods:</source>
          <target state="new">Upload the jar file to the Hadoop cluster using one of the following methods:</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>For <bpt id="p13">__</bpt>Linux-based<ept id="p13">__</ept><ph id="ph44" /> HDInsight clusters: Use <ph id="ph45">`scp WordCount-1.0-SNAPSHOT.jar USERNAME@CLUSTERNAME-ssh.azurehdinsight.net:WordCount-1.0-SNAPSHOT.jar`</ph><ph id="ph46" /> to copy the jar file to the cluster, replacing USERNAME with your SSH user name and CLUSTERNAME with the HDInsight cluster name.</source>
          <target state="new">For <bpt id="p13">__</bpt>Linux-based<ept id="p13">__</ept><ph id="ph44" /> HDInsight clusters: Use <ph id="ph45">`scp WordCount-1.0-SNAPSHOT.jar USERNAME@CLUSTERNAME-ssh.azurehdinsight.net:WordCount-1.0-SNAPSHOT.jar`</ph><ph id="ph46" /> to copy the jar file to the cluster, replacing USERNAME with your SSH user name and CLUSTERNAME with the HDInsight cluster name.</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>Once the file has finished uploading, connect to the cluster using SSH and start the topology using <ph id="ph47">`storm jar WordCount-1.0-SNAPSHOT.jar com.microsoft.example.WordCount wordcount`</ph></source>
          <target state="new">Once the file has finished uploading, connect to the cluster using SSH and start the topology using <ph id="ph47">`storm jar WordCount-1.0-SNAPSHOT.jar com.microsoft.example.WordCount wordcount`</ph></target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source>For <bpt id="p14">__</bpt>Windows-based<ept id="p14">__</ept><ph id="ph48" /> HDInsight clusters: Connect to the Storm Dashboard by going to HTTPS://CLUSTERNAME.azurehdinsight.net/ in your browser.</source>
          <target state="new">For <bpt id="p14">__</bpt>Windows-based<ept id="p14">__</ept><ph id="ph48" /> HDInsight clusters: Connect to the Storm Dashboard by going to HTTPS://CLUSTERNAME.azurehdinsight.net/ in your browser.</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>Replace CLUSTERNAME with your HDInsight cluster name and provide the admin name and password when prompted.</source>
          <target state="new">Replace CLUSTERNAME with your HDInsight cluster name and provide the admin name and password when prompted.</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>Using the form, perform the following actions:</source>
          <target state="new">Using the form, perform the following actions:</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source><bpt id="p15">__</bpt>Jar File<ept id="p15">__</ept>: Select <bpt id="p16">__</bpt>Browse<ept id="p16">__</ept>, then select the <bpt id="p17">__</bpt>WordCount-1.0-SNAPSHOT.jar<ept id="p17">__</ept><ph id="ph49" /> file</source>
          <target state="new"><bpt id="p15">__</bpt>Jar File<ept id="p15">__</ept>: Select <bpt id="p16">__</bpt>Browse<ept id="p16">__</ept>, then select the <bpt id="p17">__</bpt>WordCount-1.0-SNAPSHOT.jar<ept id="p17">__</ept><ph id="ph49" /> file</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source><bpt id="p18">__</bpt>Class Name<ept id="p18">__</ept>: Enter <ph id="ph50">`com.microsoft.example.WordCount`</ph></source>
          <target state="new"><bpt id="p18">__</bpt>Class Name<ept id="p18">__</ept>: Enter <ph id="ph50">`com.microsoft.example.WordCount`</ph></target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source><bpt id="p19">__</bpt>Additional Paramters<ept id="p19">__</ept>: Enter a friendly name such as <ph id="ph51">`wordcount`</ph><ph id="ph52" /> to identify the topology</source>
          <target state="new"><bpt id="p19">__</bpt>Additional Paramters<ept id="p19">__</ept>: Enter a friendly name such as <ph id="ph51">`wordcount`</ph><ph id="ph52" /> to identify the topology</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>Finally, select <bpt id="p20">__</bpt>Submit<ept id="p20">__</ept><ph id="ph53" /> to start the topology.</source>
          <target state="new">Finally, select <bpt id="p20">__</bpt>Submit<ept id="p20">__</ept><ph id="ph53" /> to start the topology.</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source><ph id="ph54">[AZURE.NOTE]</ph><ph id="ph55" /> Once started, a Storm topology runs until stopped (killed.) To stop the topology, use either the <ph id="ph56">`storm kill TOPOLOGYNAME`</ph><ph id="ph57" /> command from the command-line (SSH session to a Linux cluster for example,) or by using the Storm UI, select the topology, and then select the <bpt id="p21">__</bpt>Kill<ept id="p21">__</ept><ph id="ph58" /> button.</source>
          <target state="new"><ph id="ph54">[AZURE.NOTE]</ph><ph id="ph55" /> Once started, a Storm topology runs until stopped (killed.) To stop the topology, use either the <ph id="ph56">`storm kill TOPOLOGYNAME`</ph><ph id="ph57" /> command from the command-line (SSH session to a Linux cluster for example,) or by using the Storm UI, select the topology, and then select the <bpt id="p21">__</bpt>Kill<ept id="p21">__</ept><ph id="ph58" /> button.</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source>Python components with a Clojure topology</source>
          <target state="new">Python components with a Clojure topology</target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source><ph id="ph59">[AZURE.NOTE]</ph><ph id="ph60" /> This example is available at <bpt id="p22">[</bpt>https://github.com/Azure-Samples/hdinsight-python-storm-wordcount<ept id="p22">](https://github.com/Azure-Samples/hdinsight-python-storm-wordcount)</ept><ph id="ph61" /> in the <bpt id="p23">__</bpt>ClojureTopology<ept id="p23">__</ept><ph id="ph62" /> directory.</source>
          <target state="new"><ph id="ph59">[AZURE.NOTE]</ph><ph id="ph60" /> This example is available at <bpt id="p22">[</bpt>https://github.com/Azure-Samples/hdinsight-python-storm-wordcount<ept id="p22">](https://github.com/Azure-Samples/hdinsight-python-storm-wordcount)</ept><ph id="ph61" /> in the <bpt id="p23">__</bpt>ClojureTopology<ept id="p23">__</ept><ph id="ph62" /> directory.</target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>This topology was created by using <bpt id="p24">[</bpt>Leiningen<ept id="p24">](http://leiningen.org)</ept><ph id="ph63" /> to <bpt id="p25">[</bpt>create a new Clojure project<ept id="p25">](https://github.com/technomancy/leiningen/blob/stable/doc/TUTORIAL.md#creating-a-project)</ept>.</source>
          <target state="new">This topology was created by using <bpt id="p24">[</bpt>Leiningen<ept id="p24">](http://leiningen.org)</ept><ph id="ph63" /> to <bpt id="p25">[</bpt>create a new Clojure project<ept id="p25">](https://github.com/technomancy/leiningen/blob/stable/doc/TUTORIAL.md#creating-a-project)</ept>.</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>After that, the following modifications to the scaffolded project were made:</source>
          <target state="new">After that, the following modifications to the scaffolded project were made:</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source><bpt id="p26">__</bpt>project.clj<ept id="p26">__</ept>: Added dependencies for Storm, and exclusions for items that may cause a problem when deployed to the HDInsight server.</source>
          <target state="new"><bpt id="p26">__</bpt>project.clj<ept id="p26">__</ept>: Added dependencies for Storm, and exclusions for items that may cause a problem when deployed to the HDInsight server.</target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source><bpt id="p27">__</bpt>resources/resources<ept id="p27">__</ept>: Leiningen creates a default <ph id="ph64">`resources`</ph><ph id="ph65" /> directory, however the files stored here appear to get added to the root of the jar file created from this project, and Storm expects files in a sub-directory named <ph id="ph66">`resources`</ph>.</source>
          <target state="new"><bpt id="p27">__</bpt>resources/resources<ept id="p27">__</ept>: Leiningen creates a default <ph id="ph64">`resources`</ph><ph id="ph65" /> directory, however the files stored here appear to get added to the root of the jar file created from this project, and Storm expects files in a sub-directory named <ph id="ph66">`resources`</ph>.</target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source>So a sub-directory was added and the Python files are stored in <ph id="ph67">`resources/resources`</ph>.</source>
          <target state="new">So a sub-directory was added and the Python files are stored in <ph id="ph67">`resources/resources`</ph>.</target>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source>At run-time, this will be treated as the root (/) for accessing Python components.</source>
          <target state="new">At run-time, this will be treated as the root (/) for accessing Python components.</target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source><bpt id="p28">__</bpt>src/wordcount/core.clj<ept id="p28">__</ept>: This file contains the topology definition, and is referenced from the <bpt id="p29">__</bpt>project.clj<ept id="p29">__</ept><ph id="ph68" /> file.</source>
          <target state="new"><bpt id="p28">__</bpt>src/wordcount/core.clj<ept id="p28">__</ept>: This file contains the topology definition, and is referenced from the <bpt id="p29">__</bpt>project.clj<ept id="p29">__</ept><ph id="ph68" /> file.</target>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source>For more information on using Clojure to define a Storm topology, see <bpt id="p30">[</bpt>Clojure DSL<ept id="p30">](https://storm.apache.org/documentation/Clojure-DSL.html)</ept>.</source>
          <target state="new">For more information on using Clojure to define a Storm topology, see <bpt id="p30">[</bpt>Clojure DSL<ept id="p30">](https://storm.apache.org/documentation/Clojure-DSL.html)</ept>.</target>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source>Build and run the project</source>
          <target state="new">Build and run the project</target>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source><bpt id="p31">__</bpt>To build and run the project locally<ept id="p31">__</ept>, use the following command:</source>
          <target state="new"><bpt id="p31">__</bpt>To build and run the project locally<ept id="p31">__</ept>, use the following command:</target>
        </trans-unit>
        <trans-unit id="180" translate="yes" xml:space="preserve">
          <source>To stop the topology, use <bpt id="p32">__</bpt>Ctrl+C<ept id="p32">__</ept>.</source>
          <target state="new">To stop the topology, use <bpt id="p32">__</bpt>Ctrl+C<ept id="p32">__</ept>.</target>
        </trans-unit>
        <trans-unit id="181" translate="yes" xml:space="preserve">
          <source><bpt id="p33">__</bpt>To build an uberjar and deploy to HDInsight<ept id="p33">__</ept>, use the following steps:</source>
          <target state="new"><bpt id="p33">__</bpt>To build an uberjar and deploy to HDInsight<ept id="p33">__</ept>, use the following steps:</target>
        </trans-unit>
        <trans-unit id="182" translate="yes" xml:space="preserve">
          <source>Create an uberjar containing the topology and required dependencies:</source>
          <target state="new">Create an uberjar containing the topology and required dependencies:</target>
        </trans-unit>
        <trans-unit id="183" translate="yes" xml:space="preserve">
          <source>This will create a new file named <ph id="ph69">`wordcount-1.0-SNAPSHOT.jar`</ph><ph id="ph70" /> in the <ph id="ph71">`target\uberjar+uberjar`</ph><ph id="ph72" /> directory.</source>
          <target state="new">This will create a new file named <ph id="ph69">`wordcount-1.0-SNAPSHOT.jar`</ph><ph id="ph70" /> in the <ph id="ph71">`target\uberjar+uberjar`</ph><ph id="ph72" /> directory.</target>
        </trans-unit>
        <trans-unit id="184" translate="yes" xml:space="preserve">
          <source>Use one of the following methods to deploy and run the topology to an HDInsight cluster:</source>
          <target state="new">Use one of the following methods to deploy and run the topology to an HDInsight cluster:</target>
        </trans-unit>
        <trans-unit id="185" translate="yes" xml:space="preserve">
          <source><bpt id="p34">__</bpt>Linux-based HDInsight<ept id="p34">__</ept></source>
          <target state="new"><bpt id="p34">__</bpt>Linux-based HDInsight<ept id="p34">__</ept></target>
        </trans-unit>
        <trans-unit id="186" translate="yes" xml:space="preserve">
          <source>Copy the file to the HDInsight cluster head node using <ph id="ph73">`scp`</ph>.</source>
          <target state="new">Copy the file to the HDInsight cluster head node using <ph id="ph73">`scp`</ph>.</target>
        </trans-unit>
        <trans-unit id="187" translate="yes" xml:space="preserve">
          <source>For example:</source>
          <target state="new">For example:</target>
        </trans-unit>
        <trans-unit id="188" translate="yes" xml:space="preserve">
          <source>Replace USERNAME with an SSH user for your cluster, and CLUSTERNAME with your HDInsight cluster name.</source>
          <target state="new">Replace USERNAME with an SSH user for your cluster, and CLUSTERNAME with your HDInsight cluster name.</target>
        </trans-unit>
        <trans-unit id="189" translate="yes" xml:space="preserve">
          <source>Once the file has been copied to the cluster, use SSH to connect to the cluster and submit the job.</source>
          <target state="new">Once the file has been copied to the cluster, use SSH to connect to the cluster and submit the job.</target>
        </trans-unit>
        <trans-unit id="190" translate="yes" xml:space="preserve">
          <source>For information on using SSH with HDInsight, see one of the following:</source>
          <target state="new">For information on using SSH with HDInsight, see one of the following:</target>
        </trans-unit>
        <trans-unit id="191" translate="yes" xml:space="preserve">
          <source><bpt id="p35">[</bpt>Use SSH with Linux-based HDInsight from Linux, Unix, or OS X<ept id="p35">](hdinsight-hadoop-linux-use-ssh-unix.md)</ept></source>
          <target state="new"><bpt id="p35">[</bpt>Use SSH with Linux-based HDInsight from Linux, Unix, or OS X<ept id="p35">](hdinsight-hadoop-linux-use-ssh-unix.md)</ept></target>
        </trans-unit>
        <trans-unit id="192" translate="yes" xml:space="preserve">
          <source><bpt id="p36">[</bpt>Use SSH with Linux-based HDInsight from Windows<ept id="p36">](hdinsight-hadoop-linux-use-ssh-windows.md)</ept></source>
          <target state="new"><bpt id="p36">[</bpt>Use SSH with Linux-based HDInsight from Windows<ept id="p36">](hdinsight-hadoop-linux-use-ssh-windows.md)</ept></target>
        </trans-unit>
        <trans-unit id="193" translate="yes" xml:space="preserve">
          <source>Once connected, use the following to start the topology:</source>
          <target state="new">Once connected, use the following to start the topology:</target>
        </trans-unit>
        <trans-unit id="194" translate="yes" xml:space="preserve">
          <source><bpt id="p37">__</bpt>Windows-based HDInsight<ept id="p37">__</ept></source>
          <target state="new"><bpt id="p37">__</bpt>Windows-based HDInsight<ept id="p37">__</ept></target>
        </trans-unit>
        <trans-unit id="195" translate="yes" xml:space="preserve">
          <source>Connect to the Storm Dashboard by going to HTTPS://CLUSTERNAME.azurehdinsight.net/ in your browser.</source>
          <target state="new">Connect to the Storm Dashboard by going to HTTPS://CLUSTERNAME.azurehdinsight.net/ in your browser.</target>
        </trans-unit>
        <trans-unit id="196" translate="yes" xml:space="preserve">
          <source>Replace CLUSTERNAME with your HDInsight cluster name and provide the admin name and password when prompted.</source>
          <target state="new">Replace CLUSTERNAME with your HDInsight cluster name and provide the admin name and password when prompted.</target>
        </trans-unit>
        <trans-unit id="197" translate="yes" xml:space="preserve">
          <source>Using the form, perform the following actions:</source>
          <target state="new">Using the form, perform the following actions:</target>
        </trans-unit>
        <trans-unit id="198" translate="yes" xml:space="preserve">
          <source><bpt id="p38">__</bpt>Jar File<ept id="p38">__</ept>: Select <bpt id="p39">__</bpt>Browse<ept id="p39">__</ept>, then select the <bpt id="p40">__</bpt>wordcount-1.0-SNAPSHOT.jar<ept id="p40">__</ept><ph id="ph74" /> file</source>
          <target state="new"><bpt id="p38">__</bpt>Jar File<ept id="p38">__</ept>: Select <bpt id="p39">__</bpt>Browse<ept id="p39">__</ept>, then select the <bpt id="p40">__</bpt>wordcount-1.0-SNAPSHOT.jar<ept id="p40">__</ept><ph id="ph74" /> file</target>
        </trans-unit>
        <trans-unit id="199" translate="yes" xml:space="preserve">
          <source><bpt id="p41">__</bpt>Class Name<ept id="p41">__</ept>: Enter <ph id="ph75">`wordcount.core`</ph></source>
          <target state="new"><bpt id="p41">__</bpt>Class Name<ept id="p41">__</ept>: Enter <ph id="ph75">`wordcount.core`</ph></target>
        </trans-unit>
        <trans-unit id="200" translate="yes" xml:space="preserve">
          <source><bpt id="p42">__</bpt>Additional Paramters<ept id="p42">__</ept>: Enter a friendly name such as <ph id="ph76">`wordcount`</ph><ph id="ph77" /> to identify the topology</source>
          <target state="new"><bpt id="p42">__</bpt>Additional Paramters<ept id="p42">__</ept>: Enter a friendly name such as <ph id="ph76">`wordcount`</ph><ph id="ph77" /> to identify the topology</target>
        </trans-unit>
        <trans-unit id="201" translate="yes" xml:space="preserve">
          <source>Finally, select <bpt id="p43">__</bpt>Submit<ept id="p43">__</ept><ph id="ph78" /> to start the topology.</source>
          <target state="new">Finally, select <bpt id="p43">__</bpt>Submit<ept id="p43">__</ept><ph id="ph78" /> to start the topology.</target>
        </trans-unit>
        <trans-unit id="202" translate="yes" xml:space="preserve">
          <source><ph id="ph79">[AZURE.NOTE]</ph><ph id="ph80" /> Once started, a Storm topology runs until stopped (killed.) To stop the topology, use either the <ph id="ph81">`storm kill TOPOLOGYNAME`</ph><ph id="ph82" /> command from the command-line (SSH session to a Linux cluster,) or by using the Storm UI, select the topology, and then select the <bpt id="p44">__</bpt>Kill<ept id="p44">__</ept><ph id="ph83" /> button.</source>
          <target state="new"><ph id="ph79">[AZURE.NOTE]</ph><ph id="ph80" /> Once started, a Storm topology runs until stopped (killed.) To stop the topology, use either the <ph id="ph81">`storm kill TOPOLOGYNAME`</ph><ph id="ph82" /> command from the command-line (SSH session to a Linux cluster,) or by using the Storm UI, select the topology, and then select the <bpt id="p44">__</bpt>Kill<ept id="p44">__</ept><ph id="ph83" /> button.</target>
        </trans-unit>
        <trans-unit id="203" translate="yes" xml:space="preserve">
          <source>Pyleus framework</source>
          <target state="new">Pyleus framework</target>
        </trans-unit>
        <trans-unit id="204" translate="yes" xml:space="preserve">
          <source><bpt id="p45">[</bpt>Pyleus<ept id="p45">](https://github.com/Yelp/pyleus)</ept><ph id="ph84" /> is a framework that attempts to make it easier to use Python with Storm by providing the following:</source>
          <target state="new"><bpt id="p45">[</bpt>Pyleus<ept id="p45">](https://github.com/Yelp/pyleus)</ept><ph id="ph84" /> is a framework that attempts to make it easier to use Python with Storm by providing the following:</target>
        </trans-unit>
        <trans-unit id="205" translate="yes" xml:space="preserve">
          <source><bpt id="p46">__</bpt>YAML-based topology definitions<ept id="p46">__</ept>: This provides an easier way to define the topology, that doesn't require knowledge of Java or Clojure</source>
          <target state="new"><bpt id="p46">__</bpt>YAML-based topology definitions<ept id="p46">__</ept>: This provides an easier way to define the topology, that doesn't require knowledge of Java or Clojure</target>
        </trans-unit>
        <trans-unit id="206" translate="yes" xml:space="preserve">
          <source><bpt id="p47">__</bpt>MessagePack-based serializer<ept id="p47">__</ept>: MessagePack is used as the default serialization, instead of JSON.</source>
          <target state="new"><bpt id="p47">__</bpt>MessagePack-based serializer<ept id="p47">__</ept>: MessagePack is used as the default serialization, instead of JSON.</target>
        </trans-unit>
        <trans-unit id="207" translate="yes" xml:space="preserve">
          <source>This can result in faster messaging between components</source>
          <target state="new">This can result in faster messaging between components</target>
        </trans-unit>
        <trans-unit id="208" translate="yes" xml:space="preserve">
          <source><bpt id="p48">__</bpt>Dependency management<ept id="p48">__</ept>: Virtualenv is used to ensure that Python dependencies are deployed to all worker nodes.</source>
          <target state="new"><bpt id="p48">__</bpt>Dependency management<ept id="p48">__</ept>: Virtualenv is used to ensure that Python dependencies are deployed to all worker nodes.</target>
        </trans-unit>
        <trans-unit id="209" translate="yes" xml:space="preserve">
          <source>This requires Virtualenv to be installed on the worker nodes</source>
          <target state="new">This requires Virtualenv to be installed on the worker nodes</target>
        </trans-unit>
        <trans-unit id="210" translate="yes" xml:space="preserve">
          <source><ph id="ph85">[AZURE.IMPORTANT]</ph><ph id="ph86" /> Pyleus requires Storm on your development environment.</source>
          <target state="new"><ph id="ph85">[AZURE.IMPORTANT]</ph><ph id="ph86" /> Pyleus requires Storm on your development environment.</target>
        </trans-unit>
        <trans-unit id="211" translate="yes" xml:space="preserve">
          <source>Using the base Apache Storm 0.9.3 distribution seems to result in jars that are incompatible with the version of Storm provided with HDInsight.</source>
          <target state="new">Using the base Apache Storm 0.9.3 distribution seems to result in jars that are incompatible with the version of Storm provided with HDInsight.</target>
        </trans-unit>
        <trans-unit id="212" translate="yes" xml:space="preserve">
          <source>So the following steps use the HDInsight cluster as the development environment.</source>
          <target state="new">So the following steps use the HDInsight cluster as the development environment.</target>
        </trans-unit>
        <trans-unit id="213" translate="yes" xml:space="preserve">
          <source>You can successfuly build the example Pyleus topologies, using the HDInsight head node as the build environment:</source>
          <target state="new">You can successfuly build the example Pyleus topologies, using the HDInsight head node as the build environment:</target>
        </trans-unit>
        <trans-unit id="214" translate="yes" xml:space="preserve">
          <source>When provisioning a new Storm on HDInsight cluster, you must ensure that Python Virtualenv is present on the cluster nodes.</source>
          <target state="new">When provisioning a new Storm on HDInsight cluster, you must ensure that Python Virtualenv is present on the cluster nodes.</target>
        </trans-unit>
        <trans-unit id="215" translate="yes" xml:space="preserve">
          <source>When creating a new Linux-based HDInsight cluster, use the following Script Action settings with <bpt id="p49">[</bpt>Cluster customization<ept id="p49">](hdinsight-hadoop-customize-cluster.md)</ept>:</source>
          <target state="new">When creating a new Linux-based HDInsight cluster, use the following Script Action settings with <bpt id="p49">[</bpt>Cluster customization<ept id="p49">](hdinsight-hadoop-customize-cluster.md)</ept>:</target>
        </trans-unit>
        <trans-unit id="216" translate="yes" xml:space="preserve">
          <source><bpt id="p50">__</bpt>Name<ept id="p50">__</ept>: Just provide a friendly name here</source>
          <target state="new"><bpt id="p50">__</bpt>Name<ept id="p50">__</ept>: Just provide a friendly name here</target>
        </trans-unit>
        <trans-unit id="217" translate="yes" xml:space="preserve">
          <source>__ Script URI__: Use <ph id="ph87">`https://hditutorialdata.blob.core.windows.net/customizecluster/pythonvirtualenv.sh`</ph><ph id="ph88" /> as the value.</source>
          <target state="new">__ Script URI__: Use <ph id="ph87">`https://hditutorialdata.blob.core.windows.net/customizecluster/pythonvirtualenv.sh`</ph><ph id="ph88" /> as the value.</target>
        </trans-unit>
        <trans-unit id="218" translate="yes" xml:space="preserve">
          <source>This script will install Python Virtualenv on the nodes.</source>
          <target state="new">This script will install Python Virtualenv on the nodes.</target>
        </trans-unit>
        <trans-unit id="219" translate="yes" xml:space="preserve">
          <source><ph id="ph89">[AZURE.NOTE]</ph><ph id="ph90" /> It will also create some directories that are used by the Streamparse framework later in this document.</source>
          <target state="new"><ph id="ph89">[AZURE.NOTE]</ph><ph id="ph90" /> It will also create some directories that are used by the Streamparse framework later in this document.</target>
        </trans-unit>
        <trans-unit id="220" translate="yes" xml:space="preserve">
          <source><bpt id="p51">__</bpt>Nimbus<ept id="p51">__</ept>: Check this entry so that the script is applied to the Nimbus (head) nodes.</source>
          <target state="new"><bpt id="p51">__</bpt>Nimbus<ept id="p51">__</ept>: Check this entry so that the script is applied to the Nimbus (head) nodes.</target>
        </trans-unit>
        <trans-unit id="221" translate="yes" xml:space="preserve">
          <source><bpt id="p52">__</bpt>Supervisor<ept id="p52">__</ept>: Check ths entry so that the script is applied to the supervisor (worker) nodes</source>
          <target state="new"><bpt id="p52">__</bpt>Supervisor<ept id="p52">__</ept>: Check ths entry so that the script is applied to the supervisor (worker) nodes</target>
        </trans-unit>
        <trans-unit id="222" translate="yes" xml:space="preserve">
          <source>Leave other entries blank.</source>
          <target state="new">Leave other entries blank.</target>
        </trans-unit>
        <trans-unit id="223" translate="yes" xml:space="preserve">
          <source>Once the cluster has been created, connect using SSH:</source>
          <target state="new">Once the cluster has been created, connect using SSH:</target>
        </trans-unit>
        <trans-unit id="224" translate="yes" xml:space="preserve">
          <source><bpt id="p53">[</bpt>Use SSH with Linux-based HDInsight from Linux, Unix, or OS X<ept id="p53">](hdinsight-hadoop-linux-use-ssh-unix.md)</ept></source>
          <target state="new"><bpt id="p53">[</bpt>Use SSH with Linux-based HDInsight from Linux, Unix, or OS X<ept id="p53">](hdinsight-hadoop-linux-use-ssh-unix.md)</ept></target>
        </trans-unit>
        <trans-unit id="225" translate="yes" xml:space="preserve">
          <source><bpt id="p54">[</bpt>Use SSH with Linux-based HDInsight from Windows<ept id="p54">](hdinsight-hadoop-linux-use-ssh-windows.md)</ept></source>
          <target state="new"><bpt id="p54">[</bpt>Use SSH with Linux-based HDInsight from Windows<ept id="p54">](hdinsight-hadoop-linux-use-ssh-windows.md)</ept></target>
        </trans-unit>
        <trans-unit id="226" translate="yes" xml:space="preserve">
          <source>From the SSH connect, use the following to create a new virtual environment and install Pyleus:</source>
          <target state="new">From the SSH connect, use the following to create a new virtual environment and install Pyleus:</target>
        </trans-unit>
        <trans-unit id="227" translate="yes" xml:space="preserve">
          <source>Next, download the Pyleus git repository and build the WordCount example:</source>
          <target state="new">Next, download the Pyleus git repository and build the WordCount example:</target>
        </trans-unit>
        <trans-unit id="228" translate="yes" xml:space="preserve">
          <source>Once the build completes, you will have a new file named <ph id="ph91">`word_count.jar`</ph><ph id="ph92" /> in the current directory.</source>
          <target state="new">Once the build completes, you will have a new file named <ph id="ph91">`word_count.jar`</ph><ph id="ph92" /> in the current directory.</target>
        </trans-unit>
        <trans-unit id="229" translate="yes" xml:space="preserve">
          <source>To submit the topology to the Storm cluster, use the following command:</source>
          <target state="new">To submit the topology to the Storm cluster, use the following command:</target>
        </trans-unit>
        <trans-unit id="230" translate="yes" xml:space="preserve">
          <source>The <ph id="ph93">`-n`</ph><ph id="ph94" /> parameter specifies the Nimbus host.</source>
          <target state="new">The <ph id="ph93">`-n`</ph><ph id="ph94" /> parameter specifies the Nimbus host.</target>
        </trans-unit>
        <trans-unit id="231" translate="yes" xml:space="preserve">
          <source>Since we are on the head node, we can use <ph id="ph95">`localhost`</ph>.</source>
          <target state="new">Since we are on the head node, we can use <ph id="ph95">`localhost`</ph>.</target>
        </trans-unit>
        <trans-unit id="232" translate="yes" xml:space="preserve">
          <source>You can also use the <ph id="ph96">`pyleus`</ph><ph id="ph97" /> command to perform other Storm actions.</source>
          <target state="new">You can also use the <ph id="ph96">`pyleus`</ph><ph id="ph97" /> command to perform other Storm actions.</target>
        </trans-unit>
        <trans-unit id="233" translate="yes" xml:space="preserve">
          <source>Use the following to list the running topologies, and then kill the <ph id="ph98">`word_count`</ph><ph id="ph99" /> topology:</source>
          <target state="new">Use the following to list the running topologies, and then kill the <ph id="ph98">`word_count`</ph><ph id="ph99" /> topology:</target>
        </trans-unit>
        <trans-unit id="234" translate="yes" xml:space="preserve">
          <source>Streamparse framework</source>
          <target state="new">Streamparse framework</target>
        </trans-unit>
        <trans-unit id="235" translate="yes" xml:space="preserve">
          <source><bpt id="p55">[</bpt>Streamparse<ept id="p55">](https://github.com/Parsely/streamparse)</ept><ph id="ph100" /> is a framework that attempts to make it easier to use Python with Storm by providing the following:</source>
          <target state="new"><bpt id="p55">[</bpt>Streamparse<ept id="p55">](https://github.com/Parsely/streamparse)</ept><ph id="ph100" /> is a framework that attempts to make it easier to use Python with Storm by providing the following:</target>
        </trans-unit>
        <trans-unit id="236" translate="yes" xml:space="preserve">
          <source><bpt id="p56">__</bpt>Scaffolding<ept id="p56">__</ept>: This allows you to easily create the scaffolding for a project, then modify files to add your logic</source>
          <target state="new"><bpt id="p56">__</bpt>Scaffolding<ept id="p56">__</ept>: This allows you to easily create the scaffolding for a project, then modify files to add your logic</target>
        </trans-unit>
        <trans-unit id="237" translate="yes" xml:space="preserve">
          <source><bpt id="p57">__</bpt>Clojure DSL functions<ept id="p57">__</ept>: These reduce the verbosity of using Python components in a Clojure topology definition</source>
          <target state="new"><bpt id="p57">__</bpt>Clojure DSL functions<ept id="p57">__</ept>: These reduce the verbosity of using Python components in a Clojure topology definition</target>
        </trans-unit>
        <trans-unit id="238" translate="yes" xml:space="preserve">
          <source><bpt id="p58">__</bpt>Dependency management<ept id="p58">__</ept>: Virtualenv is used to ensure that Python dependencies are deployed to all worker nodes.</source>
          <target state="new"><bpt id="p58">__</bpt>Dependency management<ept id="p58">__</ept>: Virtualenv is used to ensure that Python dependencies are deployed to all worker nodes.</target>
        </trans-unit>
        <trans-unit id="239" translate="yes" xml:space="preserve">
          <source>This requires Virtualenv to be installed on the worker nodes</source>
          <target state="new">This requires Virtualenv to be installed on the worker nodes</target>
        </trans-unit>
        <trans-unit id="240" translate="yes" xml:space="preserve">
          <source><bpt id="p59">__</bpt>Remote deployment<ept id="p59">__</ept>: Streamparse can use SSH automation to deploy components to worker nodes, and will can create an SSH tunnel to communicate with Nimbus.</source>
          <target state="new"><bpt id="p59">__</bpt>Remote deployment<ept id="p59">__</ept>: Streamparse can use SSH automation to deploy components to worker nodes, and will can create an SSH tunnel to communicate with Nimbus.</target>
        </trans-unit>
        <trans-unit id="241" translate="yes" xml:space="preserve">
          <source>So you can easily deploy from your development environment to Linux-based cluster such as HDInsight</source>
          <target state="new">So you can easily deploy from your development environment to Linux-based cluster such as HDInsight</target>
        </trans-unit>
        <trans-unit id="242" translate="yes" xml:space="preserve">
          <source><ph id="ph101">[AZURE.IMPORTANT]</ph><ph id="ph102" /> Streamparse relies on components that expect <bpt id="p60">[</bpt>Unix signals<ept id="p60">](https://en.wikipedia.org/wiki/Unix_signal)</ept>, which are not available on Windows.</source>
          <target state="new"><ph id="ph101">[AZURE.IMPORTANT]</ph><ph id="ph102" /> Streamparse relies on components that expect <bpt id="p60">[</bpt>Unix signals<ept id="p60">](https://en.wikipedia.org/wiki/Unix_signal)</ept>, which are not available on Windows.</target>
        </trans-unit>
        <trans-unit id="243" translate="yes" xml:space="preserve">
          <source>Your development environment must be Linux, Unix, or OS X, and the HDInsight cluster must be Linux-based.</source>
          <target state="new">Your development environment must be Linux, Unix, or OS X, and the HDInsight cluster must be Linux-based.</target>
        </trans-unit>
        <trans-unit id="244" translate="yes" xml:space="preserve">
          <source>When provisioning a new Storm on HDInsight cluster, you must ensure that Python Virtualenv is present on the cluster nodes.</source>
          <target state="new">When provisioning a new Storm on HDInsight cluster, you must ensure that Python Virtualenv is present on the cluster nodes.</target>
        </trans-unit>
        <trans-unit id="245" translate="yes" xml:space="preserve">
          <source>When creating a new Linux-based HDInsight cluster, use the following Script Action settings with <bpt id="p61">[</bpt>Cluster customization<ept id="p61">](hdinsight-hadoop-customize-cluster.md)</ept>:</source>
          <target state="new">When creating a new Linux-based HDInsight cluster, use the following Script Action settings with <bpt id="p61">[</bpt>Cluster customization<ept id="p61">](hdinsight-hadoop-customize-cluster.md)</ept>:</target>
        </trans-unit>
        <trans-unit id="246" translate="yes" xml:space="preserve">
          <source><bpt id="p62">__</bpt>Name<ept id="p62">__</ept>: Just provide a friendly name here</source>
          <target state="new"><bpt id="p62">__</bpt>Name<ept id="p62">__</ept>: Just provide a friendly name here</target>
        </trans-unit>
        <trans-unit id="247" translate="yes" xml:space="preserve">
          <source>__ Script URI__: Use <ph id="ph103">`https://hditutorialdata.blob.core.windows.net/customizecluster/pythonvirtualenv.sh`</ph><ph id="ph104" /> as the value.</source>
          <target state="new">__ Script URI__: Use <ph id="ph103">`https://hditutorialdata.blob.core.windows.net/customizecluster/pythonvirtualenv.sh`</ph><ph id="ph104" /> as the value.</target>
        </trans-unit>
        <trans-unit id="248" translate="yes" xml:space="preserve">
          <source>This script will install Python Virtualenv on the nodes, as well as create directories used by Streamparse</source>
          <target state="new">This script will install Python Virtualenv on the nodes, as well as create directories used by Streamparse</target>
        </trans-unit>
        <trans-unit id="249" translate="yes" xml:space="preserve">
          <source><bpt id="p63">__</bpt>Nimbus<ept id="p63">__</ept>: Check this entry so that the script is applied to the Nimbus (head) nodes.</source>
          <target state="new"><bpt id="p63">__</bpt>Nimbus<ept id="p63">__</ept>: Check this entry so that the script is applied to the Nimbus (head) nodes.</target>
        </trans-unit>
        <trans-unit id="250" translate="yes" xml:space="preserve">
          <source><bpt id="p64">__</bpt>Supervisor<ept id="p64">__</ept>: Check ths entry so that the script is applied to the supervisor (worker) nodes</source>
          <target state="new"><bpt id="p64">__</bpt>Supervisor<ept id="p64">__</ept>: Check ths entry so that the script is applied to the supervisor (worker) nodes</target>
        </trans-unit>
        <trans-unit id="251" translate="yes" xml:space="preserve">
          <source>Leave other entries blank.</source>
          <target state="new">Leave other entries blank.</target>
        </trans-unit>
        <trans-unit id="252" translate="yes" xml:space="preserve">
          <source><ph id="ph105">[AZURE.WARNING]</ph><ph id="ph106" /> You must also use a <bpt id="p65">__</bpt>public key<ept id="p65">__</ept><ph id="ph107" /> to secure the SSH user for your HDInsight cluster in order to remotely deploy using Streamparse.</source>
          <target state="new"><ph id="ph105">[AZURE.WARNING]</ph><ph id="ph106" /> You must also use a <bpt id="p65">__</bpt>public key<ept id="p65">__</ept><ph id="ph107" /> to secure the SSH user for your HDInsight cluster in order to remotely deploy using Streamparse.</target>
        </trans-unit>
        <trans-unit id="253" translate="yes" xml:space="preserve">
          <source>For more information on using keys with SSH on HDInsight, see one of the following documents:</source>
          <target state="new">For more information on using keys with SSH on HDInsight, see one of the following documents:</target>
        </trans-unit>
        <trans-unit id="254" translate="yes" xml:space="preserve">
          <source><bpt id="p66">[</bpt>Use SSH with Linux-based HDInsight from Linux, Unix, or OS X<ept id="p66">](hdinsight-hadoop-linux-use-ssh-unix.md)</ept></source>
          <target state="new"><bpt id="p66">[</bpt>Use SSH with Linux-based HDInsight from Linux, Unix, or OS X<ept id="p66">](hdinsight-hadoop-linux-use-ssh-unix.md)</ept></target>
        </trans-unit>
        <trans-unit id="255" translate="yes" xml:space="preserve">
          <source><bpt id="p67">[</bpt>Use SSH with Linux-based HDInsight from Windows<ept id="p67">](hdinsight-hadoop-linux-use-ssh-windows.md)</ept></source>
          <target state="new"><bpt id="p67">[</bpt>Use SSH with Linux-based HDInsight from Windows<ept id="p67">](hdinsight-hadoop-linux-use-ssh-windows.md)</ept></target>
        </trans-unit>
        <trans-unit id="256" translate="yes" xml:space="preserve">
          <source>While the cluster is provisioning, install Streamparse on your development environment using the following command:</source>
          <target state="new">While the cluster is provisioning, install Streamparse on your development environment using the following command:</target>
        </trans-unit>
        <trans-unit id="257" translate="yes" xml:space="preserve">
          <source>Once Streamparse has installed, use the following command to create an example project:</source>
          <target state="new">Once Streamparse has installed, use the following command to create an example project:</target>
        </trans-unit>
        <trans-unit id="258" translate="yes" xml:space="preserve">
          <source>This will create a new directory named <ph id="ph108">`wordcount`</ph>, and populate it with an example Word Count project.</source>
          <target state="new">This will create a new directory named <ph id="ph108">`wordcount`</ph>, and populate it with an example Word Count project.</target>
        </trans-unit>
        <trans-unit id="259" translate="yes" xml:space="preserve">
          <source>Change directories into the <ph id="ph109">`wordcount`</ph><ph id="ph110" /> directory and start the topology in local mode:</source>
          <target state="new">Change directories into the <ph id="ph109">`wordcount`</ph><ph id="ph110" /> directory and start the topology in local mode:</target>
        </trans-unit>
        <trans-unit id="260" translate="yes" xml:space="preserve">
          <source>Use Ctrl+C to stop the topology.</source>
          <target state="new">Use Ctrl+C to stop the topology.</target>
        </trans-unit>
        <trans-unit id="261" translate="yes" xml:space="preserve">
          <source>Deploy the topology</source>
          <target state="new">Deploy the topology</target>
        </trans-unit>
        <trans-unit id="262" translate="yes" xml:space="preserve">
          <source>Once your Linux-based HDInsight cluster has been created, use the following steps to deploy the topology to the cluster:</source>
          <target state="new">Once your Linux-based HDInsight cluster has been created, use the following steps to deploy the topology to the cluster:</target>
        </trans-unit>
        <trans-unit id="263" translate="yes" xml:space="preserve">
          <source>Use the following command to find the fully qualified domain names of the worker nodes for your cluster:</source>
          <target state="new">Use the following command to find the fully qualified domain names of the worker nodes for your cluster:</target>
        </trans-unit>
        <trans-unit id="264" translate="yes" xml:space="preserve">
          <source>This will retrieve the hosts information for the cluster, pipe it to grep, and return on the entries for the worker nodes.</source>
          <target state="new">This will retrieve the hosts information for the cluster, pipe it to grep, and return on the entries for the worker nodes.</target>
        </trans-unit>
        <trans-unit id="265" translate="yes" xml:space="preserve">
          <source>You should see results similar to the following:</source>
          <target state="new">You should see results similar to the following:</target>
        </trans-unit>
        <trans-unit id="266" translate="yes" xml:space="preserve">
          <source>Save the <ph id="ph111">`"workernode0.1kft5e4nx2tevg5b2pdwxqx1fb.jx.internal.cloudapp.net"`</ph><ph id="ph112" /> information, as it will be used in the next step.</source>
          <target state="new">Save the <ph id="ph111">`"workernode0.1kft5e4nx2tevg5b2pdwxqx1fb.jx.internal.cloudapp.net"`</ph><ph id="ph112" /> information, as it will be used in the next step.</target>
        </trans-unit>
        <trans-unit id="267" translate="yes" xml:space="preserve">
          <source>Open the <bpt id="p68">__</bpt>config.json<ept id="p68">__</ept><ph id="ph113" /> file in the <ph id="ph114">`wordcount`</ph><ph id="ph115" /> directory, and change the following entries:</source>
          <target state="new">Open the <bpt id="p68">__</bpt>config.json<ept id="p68">__</ept><ph id="ph113" /> file in the <ph id="ph114">`wordcount`</ph><ph id="ph115" /> directory, and change the following entries:</target>
        </trans-unit>
        <trans-unit id="268" translate="yes" xml:space="preserve">
          <source><bpt id="p69">__</bpt>user<ept id="p69">__</ept>: Set this to the SSH user account name that you configured for the HDInsight cluster.</source>
          <target state="new"><bpt id="p69">__</bpt>user<ept id="p69">__</ept>: Set this to the SSH user account name that you configured for the HDInsight cluster.</target>
        </trans-unit>
        <trans-unit id="269" translate="yes" xml:space="preserve">
          <source>This will be used to authenticate to the cluster when deploying the project</source>
          <target state="new">This will be used to authenticate to the cluster when deploying the project</target>
        </trans-unit>
        <trans-unit id="270" translate="yes" xml:space="preserve">
          <source><bpt id="p70">__</bpt>nimbus<ept id="p70">__</ept>: Set this to <ph id="ph116">`CLUSTERNAME-ssh.azurehdinsight.net`</ph>.</source>
          <target state="new"><bpt id="p70">__</bpt>nimbus<ept id="p70">__</ept>: Set this to <ph id="ph116">`CLUSTERNAME-ssh.azurehdinsight.net`</ph>.</target>
        </trans-unit>
        <trans-unit id="271" translate="yes" xml:space="preserve">
          <source>Replace CLUSTERNAME with the name of your cluster.</source>
          <target state="new">Replace CLUSTERNAME with the name of your cluster.</target>
        </trans-unit>
        <trans-unit id="272" translate="yes" xml:space="preserve">
          <source>This is used when communicating with the Nimbus node, which is the head node of the cluster</source>
          <target state="new">This is used when communicating with the Nimbus node, which is the head node of the cluster</target>
        </trans-unit>
        <trans-unit id="273" translate="yes" xml:space="preserve">
          <source><bpt id="p71">__</bpt>workers<ept id="p71">__</ept>: Populate the workers entry with the host names for the worker nodes that you retrieved using curl.</source>
          <target state="new"><bpt id="p71">__</bpt>workers<ept id="p71">__</ept>: Populate the workers entry with the host names for the worker nodes that you retrieved using curl.</target>
        </trans-unit>
        <trans-unit id="274" translate="yes" xml:space="preserve">
          <source>For example:</source>
          <target state="new">For example:</target>
        </trans-unit>
        <trans-unit id="275" translate="yes" xml:space="preserve">
          <source>"workers": [</source>
          <target state="new">"workers": [</target>
        </trans-unit>
        <trans-unit id="276" translate="yes" xml:space="preserve">
          <source>"workernode0.1kft5e4nx2tevg5b2pdwxqx1fb.jx.internal.cloudapp.net",</source>
          <target state="new">"workernode0.1kft5e4nx2tevg5b2pdwxqx1fb.jx.internal.cloudapp.net",</target>
        </trans-unit>
        <trans-unit id="277" translate="yes" xml:space="preserve">
          <source>"workernode1.1kft5e4nx2tevg5b2pdwxqx1fb.jx.internal.cloudapp.net"</source>
          <target state="new">"workernode1.1kft5e4nx2tevg5b2pdwxqx1fb.jx.internal.cloudapp.net"</target>
        </trans-unit>
        <trans-unit id="278" translate="yes" xml:space="preserve">
          <source>]</source>
          <target state="new">]</target>
        </trans-unit>
        <trans-unit id="280" translate="yes" xml:space="preserve">
          <source>Since Streamparse deploying on HDInsight needs to forward your authentication through the head node to the workers, <ph id="ph117">`ssh-agent`</ph><ph id="ph118" /> must be started on your workstation.</source>
          <target state="new">Since Streamparse deploying on HDInsight needs to forward your authentication through the head node to the workers, <ph id="ph117">`ssh-agent`</ph><ph id="ph118" /> must be started on your workstation.</target>
        </trans-unit>
        <trans-unit id="281" translate="yes" xml:space="preserve">
          <source>For most operating systems, it is started automatically.</source>
          <target state="new">For most operating systems, it is started automatically.</target>
        </trans-unit>
        <trans-unit id="282" translate="yes" xml:space="preserve">
          <source>Use the following command to verify that it is running:</source>
          <target state="new">Use the following command to verify that it is running:</target>
        </trans-unit>
        <trans-unit id="283" translate="yes" xml:space="preserve">
          <source>This will return a response similar to the following if <ph id="ph119">`ssh-agent`</ph><ph id="ph120" /> is running:</source>
          <target state="new">This will return a response similar to the following if <ph id="ph119">`ssh-agent`</ph><ph id="ph120" /> is running:</target>
        </trans-unit>
        <trans-unit id="284" translate="yes" xml:space="preserve">
          <source><ph id="ph121">[AZURE.NOTE]</ph><ph id="ph122" /> The complete path may be different depending on your operating system.</source>
          <target state="new"><ph id="ph121">[AZURE.NOTE]</ph><ph id="ph122" /> The complete path may be different depending on your operating system.</target>
        </trans-unit>
        <trans-unit id="285" translate="yes" xml:space="preserve">
          <source>For example, on OS X the path may be similar to <ph id="ph123">`/private/tmp/com.apple.launchd.vq2rfuxaso/Listeners`</ph>.</source>
          <target state="new">For example, on OS X the path may be similar to <ph id="ph123">`/private/tmp/com.apple.launchd.vq2rfuxaso/Listeners`</ph>.</target>
        </trans-unit>
        <trans-unit id="286" translate="yes" xml:space="preserve">
          <source>But it should return some path if the agent is running.</source>
          <target state="new">But it should return some path if the agent is running.</target>
        </trans-unit>
        <trans-unit id="287" translate="yes" xml:space="preserve">
          <source>If nothing is returned, use the <ph id="ph124">`ssh-agent`</ph><ph id="ph125" /> command to start the agent.</source>
          <target state="new">If nothing is returned, use the <ph id="ph124">`ssh-agent`</ph><ph id="ph125" /> command to start the agent.</target>
        </trans-unit>
        <trans-unit id="288" translate="yes" xml:space="preserve">
          <source>Verify that the agent knows about the key you use to authenticate to the HDInsight server.</source>
          <target state="new">Verify that the agent knows about the key you use to authenticate to the HDInsight server.</target>
        </trans-unit>
        <trans-unit id="289" translate="yes" xml:space="preserve">
          <source>Use the following command to list the keys that are available to the agent:</source>
          <target state="new">Use the following command to list the keys that are available to the agent:</target>
        </trans-unit>
        <trans-unit id="290" translate="yes" xml:space="preserve">
          <source>This will return the private keys that have been added to the agent.</source>
          <target state="new">This will return the private keys that have been added to the agent.</target>
        </trans-unit>
        <trans-unit id="291" translate="yes" xml:space="preserve">
          <source>You can compare the results to the content of the private key you generated when creating an SSH key to authenticate to HDInsight.</source>
          <target state="new">You can compare the results to the content of the private key you generated when creating an SSH key to authenticate to HDInsight.</target>
        </trans-unit>
        <trans-unit id="292" translate="yes" xml:space="preserve">
          <source>If no information is returned, or the returned information does not match your private key, use the following to add the private key to the agent:</source>
          <target state="new">If no information is returned, or the returned information does not match your private key, use the following to add the private key to the agent:</target>
        </trans-unit>
        <trans-unit id="293" translate="yes" xml:space="preserve">
          <source>For example, <ph id="ph126">`ssh-add ~/.ssh/id_rsa`</ph>.</source>
          <target state="new">For example, <ph id="ph126">`ssh-add ~/.ssh/id_rsa`</ph>.</target>
        </trans-unit>
        <trans-unit id="294" translate="yes" xml:space="preserve">
          <source>You must also configure SSH so that it knows forwarding should be used for your HDInsight cluster.</source>
          <target state="new">You must also configure SSH so that it knows forwarding should be used for your HDInsight cluster.</target>
        </trans-unit>
        <trans-unit id="295" translate="yes" xml:space="preserve">
          <source>Add the following to <ph id="ph127">`~/.ssh/config`</ph>.</source>
          <target state="new">Add the following to <ph id="ph127">`~/.ssh/config`</ph>.</target>
        </trans-unit>
        <trans-unit id="296" translate="yes" xml:space="preserve">
          <source>If this file does not exist, create it and use the following as the contents:</source>
          <target state="new">If this file does not exist, create it and use the following as the contents:</target>
        </trans-unit>
        <trans-unit id="297" translate="yes" xml:space="preserve">
          <source>Replace CLUSTERNAME with the name of your HDInsight cluster.</source>
          <target state="new">Replace CLUSTERNAME with the name of your HDInsight cluster.</target>
        </trans-unit>
        <trans-unit id="298" translate="yes" xml:space="preserve">
          <source>This configures the SSH agent on your workstation to enable the forwarding of your SSH credentials through any *.azurehdinsight.net system that you connect to.</source>
          <target state="new">This configures the SSH agent on your workstation to enable the forwarding of your SSH credentials through any *.azurehdinsight.net system that you connect to.</target>
        </trans-unit>
        <trans-unit id="299" translate="yes" xml:space="preserve">
          <source>In this case, the head node of your cluster.</source>
          <target state="new">In this case, the head node of your cluster.</target>
        </trans-unit>
        <trans-unit id="300" translate="yes" xml:space="preserve">
          <source>Next, it configures the command used to proxy SSH traffic from the headnode to the individual worker nodes (internal.cloudapp.net.) This allows Streamparse to connect to the head node, then from there to each of the worker nodes, using the key authentication for your SSH account.</source>
          <target state="new">Next, it configures the command used to proxy SSH traffic from the headnode to the individual worker nodes (internal.cloudapp.net.) This allows Streamparse to connect to the head node, then from there to each of the worker nodes, using the key authentication for your SSH account.</target>
        </trans-unit>
        <trans-unit id="301" translate="yes" xml:space="preserve">
          <source>Finally, use the following command to submit the topology from your local development environment, to the HDInsight cluster:</source>
          <target state="new">Finally, use the following command to submit the topology from your local development environment, to the HDInsight cluster:</target>
        </trans-unit>
        <trans-unit id="302" translate="yes" xml:space="preserve">
          <source>This will connect to the HDInsight cluster, deploy the topology and any Python dependencies, then start the topology.</source>
          <target state="new">This will connect to the HDInsight cluster, deploy the topology and any Python dependencies, then start the topology.</target>
        </trans-unit>
        <trans-unit id="303" translate="yes" xml:space="preserve">
          <source>Next steps</source>
          <target state="new">Next steps</target>
        </trans-unit>
        <trans-unit id="304" translate="yes" xml:space="preserve">
          <source>In this document, you learned how to use Python components from a Storm topology.</source>
          <target state="new">In this document, you learned how to use Python components from a Storm topology.</target>
        </trans-unit>
        <trans-unit id="305" translate="yes" xml:space="preserve">
          <source>See the following documents for other ways to use Python with HDInsight:</source>
          <target state="new">See the following documents for other ways to use Python with HDInsight:</target>
        </trans-unit>
        <trans-unit id="306" translate="yes" xml:space="preserve">
          <source><bpt id="p72">[</bpt>How to use Python for streaming MapReduce jobs<ept id="p72">](hdinsight-hadoop-streaming-python.md)</ept></source>
          <target state="new"><bpt id="p72">[</bpt>How to use Python for streaming MapReduce jobs<ept id="p72">](hdinsight-hadoop-streaming-python.md)</ept></target>
        </trans-unit>
        <trans-unit id="307" translate="yes" xml:space="preserve">
          <source><bpt id="p73">[</bpt>How to use Python User Defined Functions (UDF) in Pig and Hive<ept id="p73">](hdinsight-python.md)</ept></source>
          <target state="new"><bpt id="p73">[</bpt>How to use Python User Defined Functions (UDF) in Pig and Hive<ept id="p73">](hdinsight-python.md)</ept></target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">542a6ad9a411c563533279419ed0e460fdaea799</xliffext:olfilehash>
  </header>
</xliff>