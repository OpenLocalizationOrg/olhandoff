{
  "nodes": [
    {
      "pos": [
        26,
        71
      ],
      "content": "Use Hadoop Pig in HDInsight | Microsoft Azure"
    },
    {
      "pos": [
        89,
        135
      ],
      "content": "Learn how to use Pig with Hadoop on HDInsight."
    },
    {
      "pos": [
        453,
        485
      ],
      "content": "Use Pig with Hadoop on HDInsight"
    },
    {
      "pos": [
        565,
        808
      ],
      "content": "<bpt id=\"p1\">[</bpt>Apache Pig<ept id=\"p1\">](http://pig.apache.org/)</ept><ph id=\"ph3\"/> is a platform for creating programs for Hadoop by using a procedural language known as <bpt id=\"p2\">*</bpt>Pig Latin<ept id=\"p2\">*</ept>. Pig is an alternative to Java for creating <bpt id=\"p3\">*</bpt>MapReduce<ept id=\"p3\">*</ept><ph id=\"ph4\"/> solutions, and it is included with Azure HDInsight.",
      "nodes": [
        {
          "content": "<bpt id=\"p1\">[</bpt>Apache Pig<ept id=\"p1\">](http://pig.apache.org/)</ept><ph id=\"ph3\"/> is a platform for creating programs for Hadoop by using a procedural language known as <bpt id=\"p2\">*</bpt>Pig Latin<ept id=\"p2\">*</ept>.",
          "pos": [
            0,
            226
          ]
        },
        {
          "content": "Pig is an alternative to Java for creating <bpt id=\"p3\">*</bpt>MapReduce<ept id=\"p3\">*</ept><ph id=\"ph4\"/> solutions, and it is included with Azure HDInsight.",
          "pos": [
            227,
            385
          ]
        }
      ]
    },
    {
      "pos": [
        810,
        877
      ],
      "content": "In this article, you will learn how you can use Pig with HDInsight."
    },
    {
      "pos": [
        879,
        881
      ],
      "content": "##"
    },
    {
      "pos": [
        897,
        909
      ],
      "content": "Why use Pig?"
    },
    {
      "pos": [
        911,
        1212
      ],
      "content": "One of the challenges of processing data by using MapReduce in Hadoop is implementing your processing logic by using only a map and a reduce function. For complex processing, you often have to break processing into multiple MapReduce operations that are chained together to achieve the desired result.",
      "nodes": [
        {
          "content": "One of the challenges of processing data by using MapReduce in Hadoop is implementing your processing logic by using only a map and a reduce function.",
          "pos": [
            0,
            150
          ]
        },
        {
          "content": "For complex processing, you often have to break processing into multiple MapReduce operations that are chained together to achieve the desired result.",
          "pos": [
            151,
            301
          ]
        }
      ]
    },
    {
      "pos": [
        1214,
        1400
      ],
      "content": "Instead of forcing you to use only map and reduce functions, Pig allows you to define processing as a series of transformations that the data flows through to produce the desired output."
    },
    {
      "pos": [
        1402,
        1593
      ],
      "content": "The Pig Latin language allows you to describe the data flow from raw input, through one or more transformations, to produce the desired output. Pig Latin programs follow this general pattern:",
      "nodes": [
        {
          "content": "The Pig Latin language allows you to describe the data flow from raw input, through one or more transformations, to produce the desired output.",
          "pos": [
            0,
            143
          ]
        },
        {
          "content": "Pig Latin programs follow this general pattern:",
          "pos": [
            144,
            191
          ]
        }
      ]
    },
    {
      "pos": [
        1597,
        1655
      ],
      "content": "<bpt id=\"p4\">**</bpt>Load<ept id=\"p4\">**</ept>: Read data to be manipulated from the file system"
    },
    {
      "pos": [
        1658,
        1692
      ],
      "content": "<bpt id=\"p5\">**</bpt>Transform<ept id=\"p5\">**</ept>: Manipulate the data"
    },
    {
      "pos": [
        1695,
        1766
      ],
      "content": "<bpt id=\"p6\">**</bpt>Dump or store<ept id=\"p6\">**</ept>: Output data to the screen or store it for processing"
    },
    {
      "pos": [
        1768,
        1930
      ],
      "content": "Pig Latin also supports user-defined functions (UDF), which allows you to invoke external components that implement logic that is difficult to model in Pig Latin."
    },
    {
      "pos": [
        1932,
        2148
      ],
      "content": "For more information about Pig Latin, see <bpt id=\"p7\">[</bpt>Pig Latin Reference Manual 1<ept id=\"p7\">](http://pig.apache.org/docs/r0.7.0/piglatin_ref1.html)</ept><ph id=\"ph5\"/> and <bpt id=\"p8\">[</bpt>Pig Latin Reference Manual 2<ept id=\"p8\">](http://pig.apache.org/docs/r0.7.0/piglatin_ref2.html)</ept>."
    },
    {
      "pos": [
        2150,
        2217
      ],
      "content": "For an example of using UDFs with Pig, see the following documents:"
    },
    {
      "pos": [
        2221,
        2356
      ],
      "content": "<bpt id=\"p9\">[</bpt>Use DataFu with Pig in HDInsight<ept id=\"p9\">](hdinsight-hadoop-use-pig-datafu-udf.md)</ept><ph id=\"ph6\"/> - DataFu is a collection of useful UDFs maintained by Apache"
    },
    {
      "pos": [
        2360,
        2424
      ],
      "content": "<bpt id=\"p10\">[</bpt>Use Python with Pig and Hive in HDInsight<ept id=\"p10\">](hdinsight-python.md)</ept>"
    },
    {
      "pos": [
        2428,
        2515
      ],
      "content": "<bpt id=\"p11\">[</bpt>Use C# with Hive and Pig in HDInsight<ept id=\"p11\">](hdinsight-hadoop-hive-pig-udf-dotnet-csharp.md)</ept>"
    },
    {
      "pos": [
        2517,
        2519
      ],
      "content": "##"
    },
    {
      "pos": [
        2536,
        2557
      ],
      "content": "About the sample data"
    },
    {
      "pos": [
        2559,
        2817
      ],
      "content": "This example uses a <bpt id=\"p12\">*</bpt>log4j<ept id=\"p12\">*</ept><ph id=\"ph7\"/> sample file, which is stored at <bpt id=\"p13\">**</bpt>/example/data/sample.log<ept id=\"p13\">**</ept><ph id=\"ph8\"/> in your blob storage container. Each log inside the file consists of a line of fields that contains a <ph id=\"ph9\">`[LOG LEVEL]`</ph><ph id=\"ph10\"/> field to show the type and the severity, for example:",
      "nodes": [
        {
          "content": "This example uses a <bpt id=\"p12\">*</bpt>log4j<ept id=\"p12\">*</ept><ph id=\"ph7\"/> sample file, which is stored at <bpt id=\"p13\">**</bpt>/example/data/sample.log<ept id=\"p13\">**</ept><ph id=\"ph8\"/> in your blob storage container.",
          "pos": [
            0,
            228
          ]
        },
        {
          "content": "Each log inside the file consists of a line of fields that contains a <ph id=\"ph9\">`[LOG LEVEL]`</ph><ph id=\"ph10\"/> field to show the type and the severity, for example:",
          "pos": [
            229,
            399
          ]
        }
      ]
    },
    {
      "pos": [
        2898,
        2946
      ],
      "content": "In the previous example, the log level is ERROR."
    },
    {
      "pos": [
        2950,
        3351
      ],
      "content": "<ph id=\"ph11\">[AZURE.NOTE]</ph><ph id=\"ph12\"/> You can also generate a log4j file by using the <bpt id=\"p14\">[</bpt>Apache Log4j<ept id=\"p14\">](http://en.wikipedia.org/wiki/Log4j)</ept><ph id=\"ph13\"/> logging tool and then upload that file to your blob. See <bpt id=\"p15\">[</bpt>Upload Data to HDInsight<ept id=\"p15\">](hdinsight-upload-data.md)</ept><ph id=\"ph14\"/> for instructions. For more information about how blobs in Azure storage are used with HDInsight, see <bpt id=\"p16\">[</bpt>Use Azure Blob Storage with HDInsight<ept id=\"p16\">](hdinsight-hadoop-use-blob-storage.md)</ept>.",
      "nodes": [
        {
          "content": "<ph id=\"ph11\">[AZURE.NOTE]</ph><ph id=\"ph12\"/> You can also generate a log4j file by using the <bpt id=\"p14\">[</bpt>Apache Log4j<ept id=\"p14\">](http://en.wikipedia.org/wiki/Log4j)</ept><ph id=\"ph13\"/> logging tool and then upload that file to your blob.",
          "pos": [
            0,
            253
          ]
        },
        {
          "content": "See <bpt id=\"p15\">[</bpt>Upload Data to HDInsight<ept id=\"p15\">](hdinsight-upload-data.md)</ept><ph id=\"ph14\"/> for instructions.",
          "pos": [
            254,
            383
          ]
        },
        {
          "content": "For more information about how blobs in Azure storage are used with HDInsight, see <bpt id=\"p16\">[</bpt>Use Azure Blob Storage with HDInsight<ept id=\"p16\">](hdinsight-hadoop-use-blob-storage.md)</ept>.",
          "pos": [
            384,
            585
          ]
        }
      ]
    },
    {
      "pos": [
        3353,
        3623
      ],
      "content": "The sample data is stored in Azure Blob storage, which HDInsight uses as the default file system for Hadoop clusters. HDInsight can access files stored in blobs by using the <bpt id=\"p17\">**</bpt>wasb<ept id=\"p17\">**</ept><ph id=\"ph15\"/> prefix. For example, to access the sample.log file, you would use the following syntax:",
      "nodes": [
        {
          "content": "The sample data is stored in Azure Blob storage, which HDInsight uses as the default file system for Hadoop clusters.",
          "pos": [
            0,
            117
          ]
        },
        {
          "content": "HDInsight can access files stored in blobs by using the <bpt id=\"p17\">**</bpt>wasb<ept id=\"p17\">**</ept><ph id=\"ph15\"/> prefix.",
          "pos": [
            118,
            245
          ]
        },
        {
          "content": "For example, to access the sample.log file, you would use the following syntax:",
          "pos": [
            246,
            325
          ]
        }
      ]
    },
    {
      "pos": [
        3662,
        3795
      ],
      "content": "Because WASB is the default storage for HDInsight, you can also access the file by using <bpt id=\"p18\">**</bpt>/example/data/sample.log<ept id=\"p18\">**</ept><ph id=\"ph16\"/> from Pig Latin."
    },
    {
      "pos": [
        3799,
        4243
      ],
      "content": "<ph id=\"ph17\">[AZURE.NOTE]</ph><ph id=\"ph18\"/> The syntax, <bpt id=\"p19\">**</bpt>wasb:///<ept id=\"p19\">**</ept>, is used to access files stored in the default storage container for your HDInsight cluster. If you specified additional storage accounts when you provisioned your cluster, and you want to access files stored in these accounts, you can access the data by specifying the container name and storage account address, for example: <bpt id=\"p20\">**</bpt>wasb://mycontainer@mystorage.blob.core.windows.net/example/data/sample.log<ept id=\"p20\">**</ept>.",
      "nodes": [
        {
          "content": "<ph id=\"ph17\">[AZURE.NOTE]</ph><ph id=\"ph18\"/> The syntax, <bpt id=\"p19\">**</bpt>wasb:///<ept id=\"p19\">**</ept>, is used to access files stored in the default storage container for your HDInsight cluster.",
          "pos": [
            0,
            204
          ]
        },
        {
          "content": "If you specified additional storage accounts when you provisioned your cluster, and you want to access files stored in these accounts, you can access the data by specifying the container name and storage account address, for example: <bpt id=\"p20\">**</bpt>wasb://mycontainer@mystorage.blob.core.windows.net/example/data/sample.log<ept id=\"p20\">**</ept>.",
          "pos": [
            205,
            558
          ]
        }
      ]
    },
    {
      "pos": [
        4246,
        4248
      ],
      "content": "##"
    },
    {
      "pos": [
        4264,
        4284
      ],
      "content": "About the sample job"
    },
    {
      "pos": [
        4286,
        4561
      ],
      "content": "The following Pig Latin job loads the <bpt id=\"p21\">**</bpt>sample.log<ept id=\"p21\">**</ept><ph id=\"ph19\"/> file from the default storage for your HDInsight cluster. Then it performs a series of transformations that result in a count of how many times each log level occurred in the input data. The results are dumped into STDOUT.",
      "nodes": [
        {
          "content": "The following Pig Latin job loads the <bpt id=\"p21\">**</bpt>sample.log<ept id=\"p21\">**</ept><ph id=\"ph19\"/> file from the default storage for your HDInsight cluster.",
          "pos": [
            0,
            165
          ]
        },
        {
          "content": "Then it performs a series of transformations that result in a count of how many times each log level occurred in the input data.",
          "pos": [
            166,
            294
          ]
        },
        {
          "content": "The results are dumped into STDOUT.",
          "pos": [
            295,
            330
          ]
        }
      ]
    },
    {
      "pos": [
        5010,
        5093
      ],
      "content": "The following image shows a breakdown of what each transformation does to the data."
    },
    {
      "pos": [
        5095,
        5180
      ],
      "content": "<ph id=\"ph20\">![</ph>Graphical representation of the transformations<ph id=\"ph21\">][image-hdi-pig-data-transformation]</ph>"
    },
    {
      "pos": [
        5182,
        5184
      ],
      "content": "##"
    },
    {
      "pos": [
        5200,
        5221
      ],
      "content": "Run the Pig Latin job"
    },
    {
      "pos": [
        5223,
        5391
      ],
      "content": "HDInsight can run Pig Latin jobs by using a variety of methods. Use the following table to decide which method is right for you, then follow the link for a walkthrough.",
      "nodes": [
        {
          "content": "HDInsight can run Pig Latin jobs by using a variety of methods.",
          "pos": [
            0,
            63
          ]
        },
        {
          "content": "Use the following table to decide which method is right for you, then follow the link for a walkthrough.",
          "pos": [
            64,
            168
          ]
        }
      ]
    },
    {
      "pos": [
        5395,
        5422
      ],
      "content": "<bpt id=\"p22\">**</bpt>Use this<ept id=\"p22\">**</ept><ph id=\"ph22\"/> if you want..."
    },
    {
      "pos": [
        5459,
        5486
      ],
      "content": "...an <bpt id=\"p23\">**</bpt>interactive<ept id=\"p23\">**</ept><ph id=\"ph23\"/> shell"
    },
    {
      "pos": [
        5489,
        5512
      ],
      "content": "...<bpt id=\"p24\">**</bpt>batch<ept id=\"p24\">**</ept><ph id=\"ph24\"/> processing"
    },
    {
      "pos": [
        5515,
        5556
      ],
      "content": "...with this <bpt id=\"p25\">**</bpt>cluster operating system<ept id=\"p25\">**</ept>"
    },
    {
      "pos": [
        5559,
        5599
      ],
      "content": "...from this <bpt id=\"p26\">**</bpt>client operating system<ept id=\"p26\">**</ept>"
    },
    {
      "pos": [
        5813,
        5851
      ],
      "content": "<bpt id=\"p27\">[</bpt>SSH<ept id=\"p27\">](hdinsight-hadoop-use-pig-ssh.md)</ept>"
    },
    {
      "pos": [
        5890,
        5891
      ],
      "content": "✔"
    },
    {
      "pos": [
        5918,
        5919
      ],
      "content": "✔"
    },
    {
      "pos": [
        5933,
        5938
      ],
      "content": "Linux"
    },
    {
      "pos": [
        5977,
        6010
      ],
      "content": "Linux, Unix, Mac OS X, or Windows"
    },
    {
      "pos": [
        6022,
        6062
      ],
      "content": "<bpt id=\"p28\">[</bpt>Curl<ept id=\"p28\">](hdinsight-hadoop-use-pig-curl.md)</ept>"
    },
    {
      "pos": [
        6096,
        6102
      ],
      "content": "&amp;nbsp;"
    },
    {
      "pos": [
        6127,
        6128
      ],
      "content": "✔"
    },
    {
      "pos": [
        6142,
        6158
      ],
      "content": "Linux or Windows"
    },
    {
      "pos": [
        6186,
        6219
      ],
      "content": "Linux, Unix, Mac OS X, or Windows"
    },
    {
      "pos": [
        6231,
        6292
      ],
      "content": "<bpt id=\"p29\">[</bpt>.NET SDK for Hadoop<ept id=\"p29\">](hdinsight-hadoop-use-pig-dotnet-sdk.md)</ept>"
    },
    {
      "pos": [
        6305,
        6311
      ],
      "content": "&amp;nbsp;"
    },
    {
      "pos": [
        6336,
        6337
      ],
      "content": "✔"
    },
    {
      "pos": [
        6351,
        6367
      ],
      "content": "Linux or Windows"
    },
    {
      "pos": [
        6395,
        6412
      ],
      "content": "Windows (for now)"
    },
    {
      "pos": [
        6440,
        6500
      ],
      "content": "<bpt id=\"p30\">[</bpt>Windows PowerShell<ept id=\"p30\">](hdinsight-hadoop-use-pig-powershell.md)</ept>"
    },
    {
      "pos": [
        6514,
        6520
      ],
      "content": "&amp;nbsp;"
    },
    {
      "pos": [
        6545,
        6546
      ],
      "content": "✔"
    },
    {
      "pos": [
        6560,
        6576
      ],
      "content": "Linux or Windows"
    },
    {
      "pos": [
        6604,
        6611
      ],
      "content": "Windows"
    },
    {
      "pos": [
        6649,
        6709
      ],
      "content": "<bpt id=\"p31\">[</bpt>Remote Desktop<ept id=\"p31\">](hdinsight-hadoop-use-pig-remote-desktop.md)</ept>"
    },
    {
      "pos": [
        6726,
        6727
      ],
      "content": "✔"
    },
    {
      "pos": [
        6754,
        6755
      ],
      "content": "✔"
    },
    {
      "pos": [
        6769,
        6776
      ],
      "content": "Windows"
    },
    {
      "pos": [
        6813,
        6820
      ],
      "content": "Windows"
    },
    {
      "pos": [
        6861,
        6946
      ],
      "content": "Running Pig jobs on Azure HDInsight using on-premises SQL Server Integration Services"
    },
    {
      "pos": [
        6948,
        7125
      ],
      "content": "You can also use SQL Server Integration Services (SSIS) to run a Pig job. The Azure Feature Pack for SSIS provides the following components that work with Pig jobs on HDInsight.",
      "nodes": [
        {
          "content": "You can also use SQL Server Integration Services (SSIS) to run a Pig job.",
          "pos": [
            0,
            73
          ]
        },
        {
          "content": "The Azure Feature Pack for SSIS provides the following components that work with Pig jobs on HDInsight.",
          "pos": [
            74,
            177
          ]
        }
      ]
    },
    {
      "pos": [
        7130,
        7165
      ],
      "content": "<bpt id=\"p32\">[</bpt>Azure HDInsight Pig Task<ept id=\"p32\">][pigtask]</ept>"
    },
    {
      "pos": [
        7168,
        7226
      ],
      "content": "<bpt id=\"p33\">[</bpt>Azure Subscription Connection Manager<ept id=\"p33\">][connectionmanager]</ept>"
    },
    {
      "pos": [
        7229,
        7295
      ],
      "content": "Learn more about the Azure Feature Pack for SSIS <bpt id=\"p34\">[</bpt>here<ept id=\"p34\">][ssispack]</ept>."
    },
    {
      "pos": [
        7298,
        7300
      ],
      "content": "##"
    },
    {
      "pos": [
        7322,
        7332
      ],
      "content": "Next steps"
    },
    {
      "pos": [
        7334,
        7466
      ],
      "content": "Now that you have learned how to use Pig with HDInsight, use the following links to explore other ways to work with Azure HDInsight."
    },
    {
      "pos": [
        7470,
        7519
      ],
      "content": "<bpt id=\"p35\">[</bpt>Upload data to HDInsight<ept id=\"p35\">][hdinsight-upload-data]</ept>"
    },
    {
      "pos": [
        7522,
        7567
      ],
      "content": "<bpt id=\"p36\">[</bpt>Use Hive with HDInsight<ept id=\"p36\">][hdinsight-use-hive]</ept>"
    },
    {
      "pos": [
        7570,
        7630
      ],
      "content": "<bpt id=\"p37\">[</bpt>Use MapReduce jobs with HDInsight<ept id=\"p37\">][hdinsight-use-mapreduce]</ept>"
    }
  ],
  "content": "<properties\n   pageTitle=\"Use Hadoop Pig in HDInsight | Microsoft Azure\"\n   description=\"Learn how to use Pig with Hadoop on HDInsight.\"\n   services=\"hdinsight\"\n   documentationCenter=\"\"\n   authors=\"Blackmist\"\n   manager=\"paulettm\"\n   editor=\"cgronlun\"\n    tags=\"azure-portal\"/>\n\n<tags\n   ms.service=\"hdinsight\"\n   ms.devlang=\"na\"\n   ms.topic=\"article\"\n   ms.tgt_pltfrm=\"na\"\n   ms.workload=\"big-data\"\n   ms.date=\"01/28/2016\"\n   ms.author=\"larryfr\"/>\n\n# Use Pig with Hadoop on HDInsight\n\n[AZURE.INCLUDE [pig-selector](../../includes/hdinsight-selector-use-pig.md)]\n\n[Apache Pig](http://pig.apache.org/) is a platform for creating programs for Hadoop by using a procedural language known as *Pig Latin*. Pig is an alternative to Java for creating *MapReduce* solutions, and it is included with Azure HDInsight.\n\nIn this article, you will learn how you can use Pig with HDInsight.\n\n##<a id=\"why\"></a>Why use Pig?\n\nOne of the challenges of processing data by using MapReduce in Hadoop is implementing your processing logic by using only a map and a reduce function. For complex processing, you often have to break processing into multiple MapReduce operations that are chained together to achieve the desired result.\n\nInstead of forcing you to use only map and reduce functions, Pig allows you to define processing as a series of transformations that the data flows through to produce the desired output.\n\nThe Pig Latin language allows you to describe the data flow from raw input, through one or more transformations, to produce the desired output. Pig Latin programs follow this general pattern:\n\n- **Load**: Read data to be manipulated from the file system\n- **Transform**: Manipulate the data\n- **Dump or store**: Output data to the screen or store it for processing\n\nPig Latin also supports user-defined functions (UDF), which allows you to invoke external components that implement logic that is difficult to model in Pig Latin.\n\nFor more information about Pig Latin, see [Pig Latin Reference Manual 1](http://pig.apache.org/docs/r0.7.0/piglatin_ref1.html) and [Pig Latin Reference Manual 2](http://pig.apache.org/docs/r0.7.0/piglatin_ref2.html).\n\nFor an example of using UDFs with Pig, see the following documents:\n\n* [Use DataFu with Pig in HDInsight](hdinsight-hadoop-use-pig-datafu-udf.md) - DataFu is a collection of useful UDFs maintained by Apache\n\n* [Use Python with Pig and Hive in HDInsight](hdinsight-python.md)\n\n* [Use C# with Hive and Pig in HDInsight](hdinsight-hadoop-hive-pig-udf-dotnet-csharp.md)\n\n##<a id=\"data\"></a>About the sample data\n\nThis example uses a *log4j* sample file, which is stored at **/example/data/sample.log** in your blob storage container. Each log inside the file consists of a line of fields that contains a `[LOG LEVEL]` field to show the type and the severity, for example:\n\n    2012-02-03 20:26:41 SampleClass3 [ERROR] verbose detail for id 1527353937\n\nIn the previous example, the log level is ERROR.\n\n> [AZURE.NOTE] You can also generate a log4j file by using the [Apache Log4j](http://en.wikipedia.org/wiki/Log4j) logging tool and then upload that file to your blob. See [Upload Data to HDInsight](hdinsight-upload-data.md) for instructions. For more information about how blobs in Azure storage are used with HDInsight, see [Use Azure Blob Storage with HDInsight](hdinsight-hadoop-use-blob-storage.md).\n\nThe sample data is stored in Azure Blob storage, which HDInsight uses as the default file system for Hadoop clusters. HDInsight can access files stored in blobs by using the **wasb** prefix. For example, to access the sample.log file, you would use the following syntax:\n\n    wasb:///example/data/sample.log\n\nBecause WASB is the default storage for HDInsight, you can also access the file by using **/example/data/sample.log** from Pig Latin.\n\n> [AZURE.NOTE] The syntax, **wasb:///**, is used to access files stored in the default storage container for your HDInsight cluster. If you specified additional storage accounts when you provisioned your cluster, and you want to access files stored in these accounts, you can access the data by specifying the container name and storage account address, for example: **wasb://mycontainer@mystorage.blob.core.windows.net/example/data/sample.log**.\n\n\n##<a id=\"job\"></a>About the sample job\n\nThe following Pig Latin job loads the **sample.log** file from the default storage for your HDInsight cluster. Then it performs a series of transformations that result in a count of how many times each log level occurred in the input data. The results are dumped into STDOUT.\n\n    LOGS = LOAD 'wasb:///example/data/sample.log';\n    LEVELS = foreach LOGS generate REGEX_EXTRACT($0, '(TRACE|DEBUG|INFO|WARN|ERROR|FATAL)', 1)  as LOGLEVEL;\n    FILTEREDLEVELS = FILTER LEVELS by LOGLEVEL is not null;\n    GROUPEDLEVELS = GROUP FILTEREDLEVELS by LOGLEVEL;\n    FREQUENCIES = foreach GROUPEDLEVELS generate group as LOGLEVEL, COUNT(FILTEREDLEVELS.LOGLEVEL) as COUNT;\n    RESULT = order FREQUENCIES by COUNT desc;\n    DUMP RESULT;\n\nThe following image shows a breakdown of what each transformation does to the data.\n\n![Graphical representation of the transformations][image-hdi-pig-data-transformation]\n\n##<a id=\"run\"></a>Run the Pig Latin job\n\nHDInsight can run Pig Latin jobs by using a variety of methods. Use the following table to decide which method is right for you, then follow the link for a walkthrough.\n\n| **Use this** if you want...                                   | ...an **interactive** shell | ...**batch** processing | ...with this **cluster operating system** | ...from this **client operating system** |\n|:--------------------------------------------------------------|:---------------------------:|:-----------------------:|:------------------------------------------|:-----------------------------------------|\n| [SSH](hdinsight-hadoop-use-pig-ssh.md)                        |              ✔              |            ✔            | Linux                                     | Linux, Unix, Mac OS X, or Windows        |\n| [Curl](hdinsight-hadoop-use-pig-curl.md)                      |           &nbsp;            |            ✔            | Linux or Windows                          | Linux, Unix, Mac OS X, or Windows        |\n| [.NET SDK for Hadoop](hdinsight-hadoop-use-pig-dotnet-sdk.md) |           &nbsp;            |            ✔            | Linux or Windows                          | Windows (for now)                        |\n| [Windows PowerShell](hdinsight-hadoop-use-pig-powershell.md)  |           &nbsp;            |            ✔            | Linux or Windows                          | Windows                                  |\n| [Remote Desktop](hdinsight-hadoop-use-pig-remote-desktop.md)  |              ✔              |            ✔            | Windows                                   | Windows                                  |\n\n\n## Running Pig jobs on Azure HDInsight using on-premises SQL Server Integration Services\n\nYou can also use SQL Server Integration Services (SSIS) to run a Pig job. The Azure Feature Pack for SSIS provides the following components that work with Pig jobs on HDInsight.\n\n\n- [Azure HDInsight Pig Task][pigtask]\n- [Azure Subscription Connection Manager][connectionmanager]\n\n\nLearn more about the Azure Feature Pack for SSIS [here][ssispack].\n\n\n##<a id=\"nextsteps\"></a>Next steps\n\nNow that you have learned how to use Pig with HDInsight, use the following links to explore other ways to work with Azure HDInsight.\n\n* [Upload data to HDInsight][hdinsight-upload-data]\n* [Use Hive with HDInsight][hdinsight-use-hive]\n* [Use MapReduce jobs with HDInsight][hdinsight-use-mapreduce]\n\n[check]: ./media/hdinsight-use-pig/hdi.checkmark.png\n\n[apachepig-home]: http://pig.apache.org/\n[putty]: http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html\n[curl]: http://curl.haxx.se/\n[pigtask]: http://msdn.microsoft.com/library/mt146781(v=sql.120).aspx\n[connectionmanager]: http://msdn.microsoft.com/library/mt146773(v=sql.120).aspx\n[ssispack]: http://msdn.microsoft.com/library/mt146770(v=sql.120).aspx\n\n[hdinsight-storage]: hdinsight-use-blob-storage.md\n[hdinsight-upload-data]: hdinsight-upload-data.md\n[hdinsight-get-started]: ../hdinsight-get-started.md\n[hdinsight-admin-powershell]: hdinsight-administer-use-powershell.md\n\n[hdinsight-use-hive]: hdinsight-use-hive.md\n[hdinsight-use-mapreduce]: hdinsight-use-mapreduce.md\n\n[hdinsight-provision]: hdinsight-provision-clusters.md\n[hdinsight-submit-jobs]: hdinsight-submit-hadoop-jobs-programmatically.md#mapreduce-sdk\n\n[Powershell-install-configure]: ../install-configure-powershell.md\n\n[powershell-start]: http://technet.microsoft.com/library/hh847889.aspx\n\n[image-hdi-log4j-sample]: ./media/hdinsight-use-pig/HDI.wholesamplefile.png\n[image-hdi-pig-data-transformation]: ./media/hdinsight-use-pig/HDI.DataTransformation.gif\n[image-hdi-pig-powershell]: ./media/hdinsight-use-pig/hdi.pig.powershell.png\n[image-hdi-pig-architecture]: ./media/hdinsight-use-pig/HDI.Pig.Architecture.png"
}