{
  "nodes": [
    {
      "pos": [
        27,
        105
      ],
      "content": "Access Hadoop YARN application logs on Linux-based HDInsight | Microsoft Azure"
    },
    {
      "pos": [
        124,
        256
      ],
      "content": "Learn how to access YARN application logs on a Linux-based HDInsight (Hadoop) cluster using both the command-line and a web browser."
    },
    {
      "pos": [
        587,
        640
      ],
      "content": "Access YARN application logs on Linux-based HDInsight"
    },
    {
      "pos": [
        642,
        802
      ],
      "content": "This document explains how to access the logs for YARN (Yet Another Resource Negotiator) applications that have finished on a Hadoop cluster in Azure HDInsight."
    },
    {
      "pos": [
        806,
        1045
      ],
      "content": "<ph id=\"ph2\">[AZURE.NOTE]</ph><ph id=\"ph3\"/> The information in this document is specific to Linux-based HDInsight clusters. For information on Windows-based clusters, see <bpt id=\"p1\">[</bpt>Access YARN application logs on Windows-based HDInsight<ept id=\"p1\">](hdinsight-hadoop-access-yarn-app-logs.md)</ept>",
      "nodes": [
        {
          "content": "<ph id=\"ph2\">[AZURE.NOTE]</ph><ph id=\"ph3\"/> The information in this document is specific to Linux-based HDInsight clusters.",
          "pos": [
            0,
            124
          ]
        },
        {
          "content": "For information on Windows-based clusters, see <bpt id=\"p1\">[</bpt>Access YARN application logs on Windows-based HDInsight<ept id=\"p1\">](hdinsight-hadoop-access-yarn-app-logs.md)</ept>",
          "pos": [
            125,
            309
          ]
        }
      ]
    },
    {
      "pos": [
        1050,
        1063
      ],
      "content": "Prerequisites"
    },
    {
      "pos": [
        1067,
        1099
      ],
      "content": "A Linux-based HDInsight cluster."
    },
    {
      "pos": [
        1103,
        1227
      ],
      "content": "You must <bpt id=\"p2\">[</bpt>create an SSH tunnel<ept id=\"p2\">](hdinsight-linux-ambari-ssh-tunnel.md)</ept><ph id=\"ph4\"/> before you can access the ResourceManager Logs web UI."
    },
    {
      "pos": [
        1265,
        1285
      ],
      "content": "YARN Timeline Server"
    },
    {
      "pos": [
        1287,
        1557
      ],
      "content": "The <bpt id=\"p3\">[</bpt>YARN Timeline Server<ept id=\"p3\">](http://hadoop.apache.org/docs/r2.4.0/hadoop-yarn/hadoop-yarn-site/TimelineServer.html)</ept><ph id=\"ph5\"/> provides generic information on completed applications as well as framework-specific application information through two different interfaces. Specifically:",
      "nodes": [
        {
          "content": "The <bpt id=\"p3\">[</bpt>YARN Timeline Server<ept id=\"p3\">](http://hadoop.apache.org/docs/r2.4.0/hadoop-yarn/hadoop-yarn-site/TimelineServer.html)</ept><ph id=\"ph5\"/> provides generic information on completed applications as well as framework-specific application information through two different interfaces.",
          "pos": [
            0,
            308
          ]
        },
        {
          "content": "Specifically:",
          "pos": [
            309,
            322
          ]
        }
      ]
    },
    {
      "pos": [
        1561,
        1690
      ],
      "content": "Storage and retrieval of generic application information on HDInsight clusters has been enabled with version 3.1.1.374 or higher."
    },
    {
      "pos": [
        1693,
        1822
      ],
      "content": "The framework-specific application information component of the Timeline Server is not currently available on HDInsight clusters."
    },
    {
      "pos": [
        1824,
        1897
      ],
      "content": "Generic information on applications includes the following sorts of data:"
    },
    {
      "pos": [
        1901,
        1958
      ],
      "content": "The application ID, a unique identifier of an application"
    },
    {
      "pos": [
        1961,
        1997
      ],
      "content": "The user who started the application"
    },
    {
      "pos": [
        2000,
        2056
      ],
      "content": "Information on attempts made to complete the application"
    },
    {
      "pos": [
        2059,
        2111
      ],
      "content": "The containers used by any given application attempt"
    },
    {
      "pos": [
        2146,
        2172
      ],
      "content": "YARN applications and logs"
    },
    {
      "pos": [
        2174,
        2836
      ],
      "content": "YARN supports multiple programming models (MapReduce being one of them) by decoupling resource management from application scheduling/monitoring. This is done through a global <bpt id=\"p4\">*</bpt>ResourceManager<ept id=\"p4\">*</ept><ph id=\"ph6\"/> (RM), per-worker-node <bpt id=\"p5\">*</bpt>NodeManagers<ept id=\"p5\">*</ept><ph id=\"ph7\"/> (NMs), and per-application <bpt id=\"p6\">*</bpt>ApplicationMasters<ept id=\"p6\">*</ept><ph id=\"ph8\"/> (AMs). The per-application AM negotiates resources (CPU, memory, disk, network) for running your application with the RM. The RM works with NMs to grant these resources, which are granted as <bpt id=\"p7\">*</bpt>containers<ept id=\"p7\">*</ept>. The AM is responsible for tracking the progress of the containers assigned to it by the RM. An application may require many containers depending on the nature of the application.",
      "nodes": [
        {
          "content": "YARN supports multiple programming models (MapReduce being one of them) by decoupling resource management from application scheduling/monitoring.",
          "pos": [
            0,
            145
          ]
        },
        {
          "content": "This is done through a global <bpt id=\"p4\">*</bpt>ResourceManager<ept id=\"p4\">*</ept><ph id=\"ph6\"/> (RM), per-worker-node <bpt id=\"p5\">*</bpt>NodeManagers<ept id=\"p5\">*</ept><ph id=\"ph7\"/> (NMs), and per-application <bpt id=\"p6\">*</bpt>ApplicationMasters<ept id=\"p6\">*</ept><ph id=\"ph8\"/> (AMs).",
          "pos": [
            146,
            441
          ]
        },
        {
          "content": "The per-application AM negotiates resources (CPU, memory, disk, network) for running your application with the RM.",
          "pos": [
            442,
            556
          ]
        },
        {
          "content": "The RM works with NMs to grant these resources, which are granted as <bpt id=\"p7\">*</bpt>containers<ept id=\"p7\">*</ept>.",
          "pos": [
            557,
            677
          ]
        },
        {
          "content": "The AM is responsible for tracking the progress of the containers assigned to it by the RM.",
          "pos": [
            678,
            769
          ]
        },
        {
          "content": "An application may require many containers depending on the nature of the application.",
          "pos": [
            770,
            856
          ]
        }
      ]
    },
    {
      "pos": [
        2838,
        3388
      ],
      "content": "Furthermore, each application may consist of multiple <bpt id=\"p8\">*</bpt>application attempts<ept id=\"p8\">*</ept><ph id=\"ph9\"/> in order to finish in the presence of crashes or due to the loss of communication between an AM and an RM. Hence, containers are granted to a specific attempt of an application. In a sense, a container provides the context for basic unit of work performed by a YARN application, and all work that is done within the context of a container is performed on the single worker node on which the container was allocated. See <bpt id=\"p9\">[</bpt>YARN Concepts<ept id=\"p9\">][YARN-concepts]</ept><ph id=\"ph10\"/> for further reference.",
      "nodes": [
        {
          "content": "Furthermore, each application may consist of multiple <bpt id=\"p8\">*</bpt>application attempts<ept id=\"p8\">*</ept><ph id=\"ph9\"/> in order to finish in the presence of crashes or due to the loss of communication between an AM and an RM.",
          "pos": [
            0,
            235
          ]
        },
        {
          "content": "Hence, containers are granted to a specific attempt of an application.",
          "pos": [
            236,
            306
          ]
        },
        {
          "content": "In a sense, a container provides the context for basic unit of work performed by a YARN application, and all work that is done within the context of a container is performed on the single worker node on which the container was allocated.",
          "pos": [
            307,
            544
          ]
        },
        {
          "content": "See <bpt id=\"p9\">[</bpt>YARN Concepts<ept id=\"p9\">][YARN-concepts]</ept><ph id=\"ph10\"/> for further reference.",
          "pos": [
            545,
            655
          ]
        }
      ]
    },
    {
      "pos": [
        3390,
        4318
      ],
      "content": "Application logs (and the associated container logs) are critical in debugging problematic Hadoop applications. YARN provides a nice framework for collecting, aggregating, and storing application logs with the <bpt id=\"p10\">[</bpt>Log Aggregation<ept id=\"p10\">][log-aggregation]</ept><ph id=\"ph11\"/> feature. The Log Aggregation feature makes accessing application logs more deterministic, as it aggregates logs across all containers on a worker node and stores them as one aggregated log file per worker node on the default file system after an application finishes. Your application may use hundreds or thousands of containers, but logs for all containers run on a single worker node will always be aggregated to a single file, resulting in one log file per worker node used by your application. Log Aggregation is enabled by default on HDInsight clusters (version 3.0 and above), and aggregated logs can be found in the default container of your cluster at the following location:",
      "nodes": [
        {
          "content": "Application logs (and the associated container logs) are critical in debugging problematic Hadoop applications.",
          "pos": [
            0,
            111
          ]
        },
        {
          "content": "YARN provides a nice framework for collecting, aggregating, and storing application logs with the <bpt id=\"p10\">[</bpt>Log Aggregation<ept id=\"p10\">][log-aggregation]</ept><ph id=\"ph11\"/> feature.",
          "pos": [
            112,
            308
          ]
        },
        {
          "content": "The Log Aggregation feature makes accessing application logs more deterministic, as it aggregates logs across all containers on a worker node and stores them as one aggregated log file per worker node on the default file system after an application finishes.",
          "pos": [
            309,
            567
          ]
        },
        {
          "content": "Your application may use hundreds or thousands of containers, but logs for all containers run on a single worker node will always be aggregated to a single file, resulting in one log file per worker node used by your application.",
          "pos": [
            568,
            797
          ]
        },
        {
          "content": "Log Aggregation is enabled by default on HDInsight clusters (version 3.0 and above), and aggregated logs can be found in the default container of your cluster at the following location:",
          "pos": [
            798,
            983
          ]
        }
      ]
    },
    {
      "pos": [
        4370,
        4538
      ],
      "content": "In that location, <bpt id=\"p11\">*</bpt>user<ept id=\"p11\">*</ept><ph id=\"ph12\"/> is the name of the user who started the application, and <bpt id=\"p12\">*</bpt>applicationId<ept id=\"p12\">*</ept><ph id=\"ph13\"/> is the unique identifier of an application as assigned by the YARN RM."
    },
    {
      "pos": [
        4540,
        4814
      ],
      "content": "The aggregated logs are not directly readable, as they are written in a <bpt id=\"p13\">[</bpt>TFile<ept id=\"p13\">][T-file]</ept>, <bpt id=\"p14\">[</bpt>binary format<ept id=\"p14\">][binary-format]</ept><ph id=\"ph14\"/> indexed by container. You must use the YARN ResourceManager logs or CLI tools to view these logs as plain text for applications or containers of interest.",
      "nodes": [
        {
          "content": "The aggregated logs are not directly readable, as they are written in a <bpt id=\"p13\">[</bpt>TFile<ept id=\"p13\">][T-file]</ept>, <bpt id=\"p14\">[</bpt>binary format<ept id=\"p14\">][binary-format]</ept><ph id=\"ph14\"/> indexed by container.",
          "pos": [
            0,
            236
          ]
        },
        {
          "content": "You must use the YARN ResourceManager logs or CLI tools to view these logs as plain text for applications or containers of interest.",
          "pos": [
            237,
            369
          ]
        }
      ]
    },
    {
      "pos": [
        4819,
        4833
      ],
      "content": "YARN CLI tools"
    },
    {
      "pos": [
        4835,
        5009
      ],
      "content": "In order to use the YARN CLI tools, you must first connect to the HDInsight cluster using SSH. Use one of the following documents for information on using SSH with HDInsight:",
      "nodes": [
        {
          "content": "In order to use the YARN CLI tools, you must first connect to the HDInsight cluster using SSH.",
          "pos": [
            0,
            94
          ]
        },
        {
          "content": "Use one of the following documents for information on using SSH with HDInsight:",
          "pos": [
            95,
            174
          ]
        }
      ]
    },
    {
      "pos": [
        5013,
        5125
      ],
      "content": "<bpt id=\"p15\">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X<ept id=\"p15\">](hdinsight-hadoop-linux-use-ssh-unix.md)</ept>"
    },
    {
      "pos": [
        5129,
        5231
      ],
      "content": "<bpt id=\"p16\">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Windows<ept id=\"p16\">](hdinsight-hadoop-linux-use-ssh-windows.md)</ept>"
    },
    {
      "pos": [
        5237,
        5316
      ],
      "content": "You can view these logs as plain text by running one of the following commands:"
    },
    {
      "pos": [
        5565,
        5731
      ],
      "content": "You must specify the &amp;lt;applicationId&gt;, &amp;lt;user-who-started-the-application&gt;, &amp;lt;containerId&gt;, and &amp;ltworker-node-address&gt; information when running these commands."
    },
    {
      "pos": [
        5735,
        5758
      ],
      "content": "YARN ResourceManager UI"
    },
    {
      "pos": [
        5760,
        5995
      ],
      "content": "The YARN ResourceManager UI runs on the cluster headnode, and can be accessed through the Ambari web UI; however, you must first <bpt id=\"p17\">[</bpt>create an SSH tunnel<ept id=\"p17\">](hdinsight-linux-ambari-ssh-tunnel.md)</ept><ph id=\"ph15\"/> before you can access the ResourceManager UI."
    },
    {
      "pos": [
        5997,
        6080
      ],
      "content": "Once you have created an SSH tunnel, use the following steps to view the YARN logs:"
    },
    {
      "pos": [
        6085,
        6218
      ],
      "content": "In your web browser, navigate to https://CLUSTERNAME.azurehdinsight.net. Replace CLUSTERNAME with the name of your HDInsight cluster.",
      "nodes": [
        {
          "content": "In your web browser, navigate to https://CLUSTERNAME.azurehdinsight.net.",
          "pos": [
            0,
            72
          ]
        },
        {
          "content": "Replace CLUSTERNAME with the name of your HDInsight cluster.",
          "pos": [
            73,
            133
          ]
        }
      ]
    },
    {
      "pos": [
        6223,
        6278
      ],
      "content": "From the list of services on the left, select <bpt id=\"p18\">__</bpt>YARN<ept id=\"p18\">__</ept>."
    },
    {
      "pos": [
        6284,
        6377
      ],
      "content": "<ph id=\"ph16\">![</ph>Yarn service selected<ph id=\"ph17\">](./media/hdinsight-hadoop-access-yarn-app-logs-linux/yarnservice.png)</ph>"
    },
    {
      "pos": [
        6382,
        6494
      ],
      "content": "From the <bpt id=\"p19\">__</bpt>Quick Links<ept id=\"p19\">__</ept><ph id=\"ph18\"/> dropdown, select one of the cluster head nodes and then select <bpt id=\"p20\">__</bpt>ResourceManager Log<ept id=\"p20\">__</ept>."
    },
    {
      "pos": [
        6500,
        6592
      ],
      "content": "<ph id=\"ph19\">![</ph>Yarn quick linnks<ph id=\"ph20\">](./media/hdinsight-hadoop-access-yarn-app-logs-linux/yarnquicklinks.png)</ph>"
    },
    {
      "pos": [
        6602,
        6658
      ],
      "content": "You will be presented with a list of links to YARN logs."
    }
  ],
  "content": "<properties\n    pageTitle=\"Access Hadoop YARN application logs on Linux-based HDInsight | Microsoft Azure\"\n    description=\"Learn how to access YARN application logs on a Linux-based HDInsight (Hadoop) cluster using both the command-line and a web browser.\"\n    services=\"hdinsight\"\n    documentationCenter=\"\"\n    tags=\"azure-portal\"\n    authors=\"Blackmist\" \n    manager=\"paulettm\"\n    editor=\"cgronlun\"/>\n\n<tags\n    ms.service=\"hdinsight\"\n    ms.workload=\"big-data\"\n    ms.tgt_pltfrm=\"na\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\"\n    ms.date=\"02/05/2016\"\n    ms.author=\"larryfr\"/>\n\n# Access YARN application logs on Linux-based HDInsight\n\nThis document explains how to access the logs for YARN (Yet Another Resource Negotiator) applications that have finished on a Hadoop cluster in Azure HDInsight.\n\n> [AZURE.NOTE] The information in this document is specific to Linux-based HDInsight clusters. For information on Windows-based clusters, see [Access YARN application logs on Windows-based HDInsight](hdinsight-hadoop-access-yarn-app-logs.md)\n\n## Prerequisites\n\n* A Linux-based HDInsight cluster.\n\n* You must [create an SSH tunnel](hdinsight-linux-ambari-ssh-tunnel.md) before you can access the ResourceManager Logs web UI.\n\n## <a name=\"YARNTimelineServer\"></a>YARN Timeline Server\n\nThe [YARN Timeline Server](http://hadoop.apache.org/docs/r2.4.0/hadoop-yarn/hadoop-yarn-site/TimelineServer.html) provides generic information on completed applications as well as framework-specific application information through two different interfaces. Specifically:\n\n* Storage and retrieval of generic application information on HDInsight clusters has been enabled with version 3.1.1.374 or higher.\n* The framework-specific application information component of the Timeline Server is not currently available on HDInsight clusters.\n\nGeneric information on applications includes the following sorts of data:\n\n* The application ID, a unique identifier of an application\n* The user who started the application\n* Information on attempts made to complete the application\n* The containers used by any given application attempt\n\n## <a name=\"YARNAppsAndLogs\"></a>YARN applications and logs\n\nYARN supports multiple programming models (MapReduce being one of them) by decoupling resource management from application scheduling/monitoring. This is done through a global *ResourceManager* (RM), per-worker-node *NodeManagers* (NMs), and per-application *ApplicationMasters* (AMs). The per-application AM negotiates resources (CPU, memory, disk, network) for running your application with the RM. The RM works with NMs to grant these resources, which are granted as *containers*. The AM is responsible for tracking the progress of the containers assigned to it by the RM. An application may require many containers depending on the nature of the application.\n\nFurthermore, each application may consist of multiple *application attempts* in order to finish in the presence of crashes or due to the loss of communication between an AM and an RM. Hence, containers are granted to a specific attempt of an application. In a sense, a container provides the context for basic unit of work performed by a YARN application, and all work that is done within the context of a container is performed on the single worker node on which the container was allocated. See [YARN Concepts][YARN-concepts] for further reference.\n\nApplication logs (and the associated container logs) are critical in debugging problematic Hadoop applications. YARN provides a nice framework for collecting, aggregating, and storing application logs with the [Log Aggregation][log-aggregation] feature. The Log Aggregation feature makes accessing application logs more deterministic, as it aggregates logs across all containers on a worker node and stores them as one aggregated log file per worker node on the default file system after an application finishes. Your application may use hundreds or thousands of containers, but logs for all containers run on a single worker node will always be aggregated to a single file, resulting in one log file per worker node used by your application. Log Aggregation is enabled by default on HDInsight clusters (version 3.0 and above), and aggregated logs can be found in the default container of your cluster at the following location:\n\n    wasb:///app-logs/<user>/logs/<applicationId>\n\nIn that location, *user* is the name of the user who started the application, and *applicationId* is the unique identifier of an application as assigned by the YARN RM.\n\nThe aggregated logs are not directly readable, as they are written in a [TFile][T-file], [binary format][binary-format] indexed by container. You must use the YARN ResourceManager logs or CLI tools to view these logs as plain text for applications or containers of interest. \n\n##YARN CLI tools\n\nIn order to use the YARN CLI tools, you must first connect to the HDInsight cluster using SSH. Use one of the following documents for information on using SSH with HDInsight:\n\n- [Use SSH with Linux-based Hadoop on HDInsight from Linux, Unix, or OS X](hdinsight-hadoop-linux-use-ssh-unix.md)\n\n- [Use SSH with Linux-based Hadoop on HDInsight from Windows](hdinsight-hadoop-linux-use-ssh-windows.md)\n    \nYou can view these logs as plain text by running one of the following commands:\n\n    yarn logs -applicationId <applicationId> -appOwner <user-who-started-the-application>\n    yarn logs -applicationId <applicationId> -appOwner <user-who-started-the-application> -containerId <containerId> -nodeAddress <worker-node-address>\n    \nYou must specify the &lt;applicationId>, &lt;user-who-started-the-application>, &lt;containerId>, and &ltworker-node-address> information when running these commands.\n\n##YARN ResourceManager UI\n\nThe YARN ResourceManager UI runs on the cluster headnode, and can be accessed through the Ambari web UI; however, you must first [create an SSH tunnel](hdinsight-linux-ambari-ssh-tunnel.md) before you can access the ResourceManager UI.\n\nOnce you have created an SSH tunnel, use the following steps to view the YARN logs:\n\n1. In your web browser, navigate to https://CLUSTERNAME.azurehdinsight.net. Replace CLUSTERNAME with the name of your HDInsight cluster.\n\n2. From the list of services on the left, select __YARN__.\n\n    ![Yarn service selected](./media/hdinsight-hadoop-access-yarn-app-logs-linux/yarnservice.png)\n\n3. From the __Quick Links__ dropdown, select one of the cluster head nodes and then select __ResourceManager Log__.\n\n    ![Yarn quick linnks](./media/hdinsight-hadoop-access-yarn-app-logs-linux/yarnquicklinks.png)\n    \n    You will be presented with a list of links to YARN logs.\n\n[YARN-timeline-server]:http://hadoop.apache.org/docs/r2.4.0/hadoop-yarn/hadoop-yarn-site/TimelineServer.html\n[log-aggregation]:http://hortonworks.com/blog/simplifying-user-logs-management-and-access-in-yarn/\n[T-file]:https://issues.apache.org/jira/secure/attachment/12396286/TFile%20Specification%2020081217.pdf\n[binary-format]:https://issues.apache.org/jira/browse/HADOOP-3315\n[YARN-concepts]:http://hortonworks.com/blog/apache-hadoop-yarn-concepts-and-applications/\n"
}