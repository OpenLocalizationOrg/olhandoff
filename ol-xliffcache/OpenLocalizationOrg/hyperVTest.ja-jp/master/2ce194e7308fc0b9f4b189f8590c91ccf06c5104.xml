{
  "nodes": [
    {
      "pos": [
        26,
        91
      ],
      "content": "Use Hadoop Hive and Remote Desktop in HDInsight | Microsoft Azure"
    },
    {
      "pos": [
        109,
        253
      ],
      "content": "Learn how to connect to Hadoop cluster in HDInsight by using Remote Desktop, and then run Hive queries by using the Hive Command-Line Interface."
    },
    {
      "pos": [
        571,
        624
      ],
      "content": "Use Hive with Hadoop on HDInsight with Remote Desktop"
    },
    {
      "pos": [
        706,
        875
      ],
      "content": "In this article, you will learn how to connect to an HDInsight cluster by using Remote Desktop, and then run Hive queries by using the Hive Command-Line Interface (CLI)."
    },
    {
      "pos": [
        879,
        1136
      ],
      "content": "<ph id=\"ph3\">[AZURE.NOTE]</ph><ph id=\"ph4\"/> This document does not provide a detailed description of what the HiveQL statements that are used in the examples do. For information about the HiveQL that is used in this example, see <bpt id=\"p1\">[</bpt>Use Hive with Hadoop on HDInsight<ept id=\"p1\">](hdinsight-use-hive.md)</ept>.",
      "nodes": [
        {
          "content": "<ph id=\"ph3\">[AZURE.NOTE]</ph><ph id=\"ph4\"/> This document does not provide a detailed description of what the HiveQL statements that are used in the examples do.",
          "pos": [
            0,
            162
          ]
        },
        {
          "content": "For information about the HiveQL that is used in this example, see <bpt id=\"p1\">[</bpt>Use Hive with Hadoop on HDInsight<ept id=\"p1\">](hdinsight-use-hive.md)</ept>.",
          "pos": [
            163,
            327
          ]
        }
      ]
    },
    {
      "pos": [
        1138,
        1140
      ],
      "content": "##"
    },
    {
      "pos": [
        1159,
        1172
      ],
      "content": "Prerequisites"
    },
    {
      "pos": [
        1174,
        1241
      ],
      "content": "To complete the steps in this article, you will need the following:"
    },
    {
      "pos": [
        1245,
        1300
      ],
      "content": "A Windows-based HDInsight (Hadoop on HDInsight) cluster"
    },
    {
      "pos": [
        1304,
        1364
      ],
      "content": "A client computer running Windows 10, Window 8, or Windows 7"
    },
    {
      "pos": [
        1366,
        1368
      ],
      "content": "##"
    },
    {
      "pos": [
        1388,
        1415
      ],
      "content": "Connect with Remote Desktop"
    },
    {
      "pos": [
        1417,
        1611
      ],
      "content": "Enable Remote Desktop for the HDInsight cluster, then connect to it by following the instructions at <bpt id=\"p2\">[</bpt>Connect to HDInsight clusters using RDP<ept id=\"p2\">](hdinsight-administer-use-management-portal.md#rdp)</ept>."
    },
    {
      "pos": [
        1613,
        1615
      ],
      "content": "##"
    },
    {
      "pos": [
        1632,
        1652
      ],
      "content": "Use the Hive command"
    },
    {
      "pos": [
        1654,
        1762
      ],
      "content": "When you have connected to the desktop for the HDInsight cluster, use the following steps to work with Hive:"
    },
    {
      "pos": [
        1767,
        1829
      ],
      "content": "From the HDInsight desktop, start the <bpt id=\"p3\">**</bpt>Hadoop Command Line<ept id=\"p3\">**</ept>."
    },
    {
      "pos": [
        1834,
        1884
      ],
      "content": "Enter the following command to start the Hive CLI:"
    },
    {
      "pos": [
        1920,
        1988
      ],
      "content": "When the CLI has started, you will see the Hive CLI prompt: <ph id=\"ph5\">`hive&gt;`</ph>."
    },
    {
      "pos": [
        1993,
        2099
      ],
      "content": "Using the CLI, enter the following statements to create a new table named <bpt id=\"p4\">**</bpt>log4jLogs<ept id=\"p4\">**</ept><ph id=\"ph6\"/> using sample data:"
    },
    {
      "pos": [
        2497,
        2544
      ],
      "content": "These statements perform the following actions:"
    },
    {
      "pos": [
        2552,
        2632
      ],
      "content": "<bpt id=\"p5\">**</bpt>DROP TABLE<ept id=\"p5\">**</ept>: Deletes the table and the data file if the table already exists."
    },
    {
      "pos": [
        2640,
        2807
      ],
      "content": "<bpt id=\"p6\">**</bpt>CREATE EXTERNAL TABLE<ept id=\"p6\">**</ept>: Creates a new 'external' table in Hive. External tables store only the table definition in Hive (the data is left in the original location).",
      "nodes": [
        {
          "content": "<bpt id=\"p6\">**</bpt>CREATE EXTERNAL TABLE<ept id=\"p6\">**</ept>: Creates a new 'external' table in Hive.",
          "pos": [
            0,
            104
          ]
        },
        {
          "content": "External tables store only the table definition in Hive (the data is left in the original location).",
          "pos": [
            105,
            205
          ]
        }
      ]
    },
    {
      "pos": [
        2819,
        3069
      ],
      "content": "<ph id=\"ph7\">[AZURE.NOTE]</ph><ph id=\"ph8\"/> External tables should be used when you expect the underlying data to be updated by an external source (such as an automated data upload process) or by another MapReduce operation, but you always want Hive queries to use the latest data."
    },
    {
      "pos": [
        3090,
        3173
      ],
      "content": "Dropping an external table does <bpt id=\"p7\">**</bpt>not<ept id=\"p7\">**</ept><ph id=\"ph9\"/> delete the data, only the table definition."
    },
    {
      "pos": [
        3181,
        3297
      ],
      "content": "<bpt id=\"p8\">**</bpt>ROW FORMAT<ept id=\"p8\">**</ept>: Tells Hive how the data is formatted. In this case, the fields in each log are separated by a space.",
      "nodes": [
        {
          "content": "<bpt id=\"p8\">**</bpt>ROW FORMAT<ept id=\"p8\">**</ept>: Tells Hive how the data is formatted.",
          "pos": [
            0,
            91
          ]
        },
        {
          "content": "In this case, the fields in each log are separated by a space.",
          "pos": [
            92,
            154
          ]
        }
      ]
    },
    {
      "pos": [
        3305,
        3433
      ],
      "content": "<bpt id=\"p9\">**</bpt>STORED AS TEXTFILE LOCATION<ept id=\"p9\">**</ept>: Tells Hive where the data is stored (the example/data directory) and that it is stored as text."
    },
    {
      "pos": [
        3441,
        3622
      ],
      "content": "<bpt id=\"p10\">**</bpt>SELECT<ept id=\"p10\">**</ept>: Selects a count of all rows where column <bpt id=\"p11\">**</bpt>t4<ept id=\"p11\">**</ept><ph id=\"ph10\"/> contains the value <bpt id=\"p12\">**</bpt>[ERROR]<ept id=\"p12\">**</ept>. This should return a value of <bpt id=\"p13\">**</bpt>3<ept id=\"p13\">**</ept><ph id=\"ph11\"/> because there are three rows that contain this value.",
      "nodes": [
        {
          "content": "<bpt id=\"p10\">**</bpt>SELECT<ept id=\"p10\">**</ept>: Selects a count of all rows where column <bpt id=\"p11\">**</bpt>t4<ept id=\"p11\">**</ept><ph id=\"ph10\"/> contains the value <bpt id=\"p12\">**</bpt>[ERROR]<ept id=\"p12\">**</ept>.",
          "pos": [
            0,
            226
          ]
        },
        {
          "content": "This should return a value of <bpt id=\"p13\">**</bpt>3<ept id=\"p13\">**</ept><ph id=\"ph11\"/> because there are three rows that contain this value.",
          "pos": [
            227,
            371
          ]
        }
      ]
    },
    {
      "pos": [
        3630,
        3913
      ],
      "content": "<bpt id=\"p14\">**</bpt>INPUT__FILE__NAME LIKE '%.log'<ept id=\"p14\">**</ept><ph id=\"ph12\"/> - Tells Hive that we should only return data from files ending in .log. This restricts the search to the sample.log file that contains the data, and keeps it from returning data from other example data files that do not match the schema we defined.",
      "nodes": [
        {
          "content": "<bpt id=\"p14\">**</bpt>INPUT__FILE__NAME LIKE '%.log'<ept id=\"p14\">**</ept><ph id=\"ph12\"/> - Tells Hive that we should only return data from files ending in .log.",
          "pos": [
            0,
            161
          ]
        },
        {
          "content": "This restricts the search to the sample.log file that contains the data, and keeps it from returning data from other example data files that do not match the schema we defined.",
          "pos": [
            162,
            338
          ]
        }
      ]
    },
    {
      "pos": [
        3919,
        4001
      ],
      "content": "Use the following statements to create a new 'internal' table named <bpt id=\"p15\">**</bpt>errorLogs<ept id=\"p15\">**</ept>:"
    },
    {
      "pos": [
        4293,
        4340
      ],
      "content": "These statements perform the following actions:"
    },
    {
      "pos": [
        4348,
        4576
      ],
      "content": "<bpt id=\"p16\">**</bpt>CREATE TABLE IF NOT EXISTS<ept id=\"p16\">**</ept>: Creates a table if it does not already exist. Because the <bpt id=\"p17\">**</bpt>EXTERNAL<ept id=\"p17\">**</ept><ph id=\"ph13\"/> keyword is not used, this is an internal table, which is stored in the Hive data warehouse and is managed completely by Hive.",
      "nodes": [
        {
          "content": "<bpt id=\"p16\">**</bpt>CREATE TABLE IF NOT EXISTS<ept id=\"p16\">**</ept>: Creates a table if it does not already exist.",
          "pos": [
            0,
            117
          ]
        },
        {
          "content": "Because the <bpt id=\"p17\">**</bpt>EXTERNAL<ept id=\"p17\">**</ept><ph id=\"ph13\"/> keyword is not used, this is an internal table, which is stored in the Hive data warehouse and is managed completely by Hive.",
          "pos": [
            118,
            323
          ]
        }
      ]
    },
    {
      "pos": [
        4588,
        4689
      ],
      "content": "<ph id=\"ph14\">[AZURE.NOTE]</ph><ph id=\"ph15\"/> Unlike <bpt id=\"p18\">**</bpt>EXTERNAL<ept id=\"p18\">**</ept><ph id=\"ph16\"/> tables, dropping an internal table also deletes the underlying data."
    },
    {
      "pos": [
        4697,
        4842
      ],
      "content": "<bpt id=\"p19\">**</bpt>STORED AS ORC<ept id=\"p19\">**</ept>: Stores the data in optimized row columnar (ORC) format. This is a highly optimized and efficient format for storing Hive data.",
      "nodes": [
        {
          "content": "<bpt id=\"p19\">**</bpt>STORED AS ORC<ept id=\"p19\">**</ept>: Stores the data in optimized row columnar (ORC) format.",
          "pos": [
            0,
            114
          ]
        },
        {
          "content": "This is a highly optimized and efficient format for storing Hive data.",
          "pos": [
            115,
            185
          ]
        }
      ]
    },
    {
      "pos": [
        4850,
        5002
      ],
      "content": "<bpt id=\"p20\">**</bpt>INSERT OVERWRITE ... SELECT<ept id=\"p20\">**</ept>: Selects rows from the <bpt id=\"p21\">**</bpt>log4jLogs<ept id=\"p21\">**</ept><ph id=\"ph17\"/> table that contain <bpt id=\"p22\">**</bpt>[ERROR]<ept id=\"p22\">**</ept>, then inserts the data into the <bpt id=\"p23\">**</bpt>errorLogs<ept id=\"p23\">**</ept><ph id=\"ph18\"/> table."
    },
    {
      "pos": [
        5008,
        5181
      ],
      "content": "To verify that only rows that contain <bpt id=\"p24\">**</bpt>[ERROR]<ept id=\"p24\">**</ept><ph id=\"ph19\"/> in column t4 were stored to the <bpt id=\"p25\">**</bpt>errorLogs<ept id=\"p25\">**</ept><ph id=\"ph20\"/> table, use the following statement to return all the rows from <bpt id=\"p26\">**</bpt>errorLogs<ept id=\"p26\">**</ept>:"
    },
    {
      "pos": [
        5221,
        5300
      ],
      "content": "Three rows of data should be returned, all containing <bpt id=\"p27\">**</bpt>[ERROR]<ept id=\"p27\">**</ept><ph id=\"ph21\"/> in column t4."
    },
    {
      "pos": [
        5302,
        5304
      ],
      "content": "##"
    },
    {
      "pos": [
        5324,
        5331
      ],
      "content": "Summary"
    },
    {
      "pos": [
        5333,
        5498
      ],
      "content": "As you can see, the the Hive command provides an easy way to interactively run Hive queries on an HDInsight cluster, monitor the job status, and retrieve the output."
    },
    {
      "pos": [
        5500,
        5502
      ],
      "content": "##"
    },
    {
      "pos": [
        5524,
        5534
      ],
      "content": "Next steps"
    },
    {
      "pos": [
        5536,
        5584
      ],
      "content": "For general information about Hive in HDInsight:"
    },
    {
      "pos": [
        5588,
        5646
      ],
      "content": "<bpt id=\"p28\">[</bpt>Use Hive with Hadoop on HDInsight<ept id=\"p28\">](hdinsight-use-hive.md)</ept>"
    },
    {
      "pos": [
        5648,
        5719
      ],
      "content": "For information about other ways you can work with Hadoop on HDInsight:"
    },
    {
      "pos": [
        5723,
        5779
      ],
      "content": "<bpt id=\"p29\">[</bpt>Use Pig with Hadoop on HDInsight<ept id=\"p29\">](hdinsight-use-pig.md)</ept>"
    },
    {
      "pos": [
        5783,
        5851
      ],
      "content": "<bpt id=\"p30\">[</bpt>Use MapReduce with Hadoop on HDInsight<ept id=\"p30\">](hdinsight-use-mapreduce.md)</ept>"
    },
    {
      "pos": [
        5853,
        5939
      ],
      "content": "If you are using Tez with Hive, see the following documents for debugging information:"
    },
    {
      "pos": [
        5943,
        6013
      ],
      "content": "<bpt id=\"p31\">[</bpt>Use the Tez UI on Windows-based HDInsight<ept id=\"p31\">](hdinsight-debug-tez-ui.md)</ept>"
    },
    {
      "pos": [
        6017,
        6103
      ],
      "content": "<bpt id=\"p32\">[</bpt>Use the Ambari Tez view on Linux-based HDInsight<ept id=\"p32\">](hdinsight-debug-ambari-tez-view.md)</ept>"
    }
  ],
  "content": "<properties\n   pageTitle=\"Use Hadoop Hive and Remote Desktop in HDInsight | Microsoft Azure\"\n   description=\"Learn how to connect to Hadoop cluster in HDInsight by using Remote Desktop, and then run Hive queries by using the Hive Command-Line Interface.\"\n   services=\"hdinsight\"\n   documentationCenter=\"\"\n   authors=\"Blackmist\"\n   manager=\"paulettm\"\n   editor=\"cgronlun\"\n    tags=\"azure-portal\"/>\n\n<tags\n   ms.service=\"hdinsight\"\n   ms.devlang=\"na\"\n   ms.topic=\"article\"\n   ms.tgt_pltfrm=\"na\"\n   ms.workload=\"big-data\"\n   ms.date=\"02/16/2016\"\n   ms.author=\"larryfr\"/>\n\n# Use Hive with Hadoop on HDInsight with Remote Desktop\n\n[AZURE.INCLUDE [hive-selector](../../includes/hdinsight-selector-use-hive.md)]\n\nIn this article, you will learn how to connect to an HDInsight cluster by using Remote Desktop, and then run Hive queries by using the Hive Command-Line Interface (CLI).\n\n> [AZURE.NOTE] This document does not provide a detailed description of what the HiveQL statements that are used in the examples do. For information about the HiveQL that is used in this example, see [Use Hive with Hadoop on HDInsight](hdinsight-use-hive.md).\n\n##<a id=\"prereq\"></a>Prerequisites\n\nTo complete the steps in this article, you will need the following:\n\n* A Windows-based HDInsight (Hadoop on HDInsight) cluster\n\n* A client computer running Windows 10, Window 8, or Windows 7\n\n##<a id=\"connect\"></a>Connect with Remote Desktop\n\nEnable Remote Desktop for the HDInsight cluster, then connect to it by following the instructions at [Connect to HDInsight clusters using RDP](hdinsight-administer-use-management-portal.md#rdp).\n\n##<a id=\"hive\"></a>Use the Hive command\n\nWhen you have connected to the desktop for the HDInsight cluster, use the following steps to work with Hive:\n\n1. From the HDInsight desktop, start the **Hadoop Command Line**.\n\n2. Enter the following command to start the Hive CLI:\n\n        %hive_home%\\bin\\hive\n\n    When the CLI has started, you will see the Hive CLI prompt: `hive>`.\n\n3. Using the CLI, enter the following statements to create a new table named **log4jLogs** using sample data:\n\n        DROP TABLE log4jLogs;\n        CREATE EXTERNAL TABLE log4jLogs (t1 string, t2 string, t3 string, t4 string, t5 string, t6 string, t7 string)\n        ROW FORMAT DELIMITED FIELDS TERMINATED BY ' '\n        STORED AS TEXTFILE LOCATION 'wasb:///example/data/';\n        SELECT t4 AS sev, COUNT(*) AS count FROM log4jLogs WHERE t4 = '[ERROR]' AND INPUT__FILE__NAME LIKE '%.log' GROUP BY t4;\n\n    These statements perform the following actions:\n\n    * **DROP TABLE**: Deletes the table and the data file if the table already exists.\n\n    * **CREATE EXTERNAL TABLE**: Creates a new 'external' table in Hive. External tables store only the table definition in Hive (the data is left in the original location).\n\n        > [AZURE.NOTE] External tables should be used when you expect the underlying data to be updated by an external source (such as an automated data upload process) or by another MapReduce operation, but you always want Hive queries to use the latest data.\n        >\n        > Dropping an external table does **not** delete the data, only the table definition.\n\n    * **ROW FORMAT**: Tells Hive how the data is formatted. In this case, the fields in each log are separated by a space.\n\n    * **STORED AS TEXTFILE LOCATION**: Tells Hive where the data is stored (the example/data directory) and that it is stored as text.\n\n    * **SELECT**: Selects a count of all rows where column **t4** contains the value **[ERROR]**. This should return a value of **3** because there are three rows that contain this value.\n\n    * **INPUT__FILE__NAME LIKE '%.log'** - Tells Hive that we should only return data from files ending in .log. This restricts the search to the sample.log file that contains the data, and keeps it from returning data from other example data files that do not match the schema we defined.\n\n\n4. Use the following statements to create a new 'internal' table named **errorLogs**:\n\n        CREATE TABLE IF NOT EXISTS errorLogs (t1 string, t2 string, t3 string, t4 string, t5 string, t6 string, t7 string) STORED AS ORC;\n        INSERT OVERWRITE TABLE errorLogs SELECT t1, t2, t3, t4, t5, t6, t7 FROM log4jLogs WHERE t4 = '[ERROR]' AND INPUT__FILE__NAME LIKE '%.log';\n\n    These statements perform the following actions:\n\n    * **CREATE TABLE IF NOT EXISTS**: Creates a table if it does not already exist. Because the **EXTERNAL** keyword is not used, this is an internal table, which is stored in the Hive data warehouse and is managed completely by Hive.\n\n        > [AZURE.NOTE] Unlike **EXTERNAL** tables, dropping an internal table also deletes the underlying data.\n\n    * **STORED AS ORC**: Stores the data in optimized row columnar (ORC) format. This is a highly optimized and efficient format for storing Hive data.\n\n    * **INSERT OVERWRITE ... SELECT**: Selects rows from the **log4jLogs** table that contain **[ERROR]**, then inserts the data into the **errorLogs** table.\n\n    To verify that only rows that contain **[ERROR]** in column t4 were stored to the **errorLogs** table, use the following statement to return all the rows from **errorLogs**:\n\n        SELECT * from errorLogs;\n\n    Three rows of data should be returned, all containing **[ERROR]** in column t4.\n\n##<a id=\"summary\"></a>Summary\n\nAs you can see, the the Hive command provides an easy way to interactively run Hive queries on an HDInsight cluster, monitor the job status, and retrieve the output.\n\n##<a id=\"nextsteps\"></a>Next steps\n\nFor general information about Hive in HDInsight:\n\n* [Use Hive with Hadoop on HDInsight](hdinsight-use-hive.md)\n\nFor information about other ways you can work with Hadoop on HDInsight:\n\n* [Use Pig with Hadoop on HDInsight](hdinsight-use-pig.md)\n\n* [Use MapReduce with Hadoop on HDInsight](hdinsight-use-mapreduce.md)\n\nIf you are using Tez with Hive, see the following documents for debugging information:\n\n* [Use the Tez UI on Windows-based HDInsight](hdinsight-debug-tez-ui.md)\n\n* [Use the Ambari Tez view on Linux-based HDInsight](hdinsight-debug-ambari-tez-view.md)\n\n[1]: ../HDInsight/hdinsight-hadoop-visual-studio-tools-get-started.md\n\n[hdinsight-sdk-documentation]: http://msdnstage.redmond.corp.microsoft.com/library/dn479185.aspx\n\n[azure-purchase-options]: http://azure.microsoft.com/pricing/purchase-options/\n[azure-member-offers]: http://azure.microsoft.com/pricing/member-offers/\n[azure-free-trial]: http://azure.microsoft.com/pricing/free-trial/\n\n[apache-tez]: http://tez.apache.org\n[apache-hive]: http://hive.apache.org/\n[apache-log4j]: http://en.wikipedia.org/wiki/Log4j\n[hive-on-tez-wiki]: https://cwiki.apache.org/confluence/display/Hive/Hive+on+Tez\n[import-to-excel]: http://azure.microsoft.com/documentation/articles/hdinsight-connect-excel-power-query/\n\n\n[hdinsight-use-oozie]: hdinsight-use-oozie.md\n[hdinsight-analyze-flight-data]: hdinsight-analyze-flight-delay-data.md\n\n\n\n\n\n[hdinsight-provision]: hdinsight-provision-clusters.md\n[hdinsight-submit-jobs]: hdinsight-submit-hadoop-jobs-programmatically.md\n[hdinsight-upload-data]: hdinsight-upload-data.md\n\n\n[Powershell-install-configure]: ../powershell-install-configure.md\n[powershell-here-strings]: http://technet.microsoft.com/library/ee692792.aspx\n\n"
}