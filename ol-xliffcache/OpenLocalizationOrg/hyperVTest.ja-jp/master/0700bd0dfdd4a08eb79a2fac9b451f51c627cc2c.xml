{
  "nodes": [
    {
      "pos": [
        28,
        83
      ],
      "content": "Submit Spark jobs remotely using Livy | Microsoft Azure"
    },
    {
      "pos": [
        103,
        179
      ],
      "content": "Learn how to use Livy with HDInsight clusters to submit Spark jobs remotely."
    },
    {
      "pos": [
        520,
        598
      ],
      "content": "Submit Spark jobs remotely using Livy with Spark clusters on HDInsight (Linux)"
    },
    {
      "pos": [
        600,
        873
      ],
      "content": "Apache Spark cluster on Azure HDInsight includes Livy, a REST interface for submitting jobs remotely to a Spark cluster from anywhere. For detailed documentation, see <bpt id=\"p1\">[</bpt>Livy<ept id=\"p1\">](https://github.com/cloudera/hue/tree/master/apps/spark/java#welcome-to-livy-the-rest-spark-server)</ept>.",
      "nodes": [
        {
          "content": "Apache Spark cluster on Azure HDInsight includes Livy, a REST interface for submitting jobs remotely to a Spark cluster from anywhere.",
          "pos": [
            0,
            134
          ]
        },
        {
          "content": "For detailed documentation, see <bpt id=\"p1\">[</bpt>Livy<ept id=\"p1\">](https://github.com/cloudera/hue/tree/master/apps/spark/java#welcome-to-livy-the-rest-spark-server)</ept>.",
          "pos": [
            135,
            311
          ]
        }
      ]
    },
    {
      "pos": [
        875,
        1090
      ],
      "content": "You can use Livy to run interactive Spark shells or submit batch jobs to be run on Spark. This article talks about using Livy to submit batch jobs. The syntax below uses Curl to make REST calls to the Livy endpoint.",
      "nodes": [
        {
          "content": "You can use Livy to run interactive Spark shells or submit batch jobs to be run on Spark.",
          "pos": [
            0,
            89
          ]
        },
        {
          "content": "This article talks about using Livy to submit batch jobs.",
          "pos": [
            90,
            147
          ]
        },
        {
          "content": "The syntax below uses Curl to make REST calls to the Livy endpoint.",
          "pos": [
            148,
            215
          ]
        }
      ]
    },
    {
      "pos": [
        1092,
        1110
      ],
      "content": "<bpt id=\"p2\">**</bpt>Prerequisites:<ept id=\"p2\">**</ept>"
    },
    {
      "pos": [
        1112,
        1140
      ],
      "content": "You must have the following:"
    },
    {
      "pos": [
        1144,
        1298
      ],
      "content": "An Azure subscription. See <bpt id=\"p3\">[</bpt>Get Azure free trial<ept id=\"p3\">](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/)</ept>.",
      "nodes": [
        {
          "content": "An Azure subscription.",
          "pos": [
            0,
            22
          ]
        },
        {
          "content": "See <bpt id=\"p3\">[</bpt>Get Azure free trial<ept id=\"p3\">](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/)</ept>.",
          "pos": [
            23,
            192
          ]
        }
      ]
    },
    {
      "pos": [
        1301,
        1462
      ],
      "content": "An Apache Spark cluster on HDInsight Linux. For instructions, see <bpt id=\"p4\">[</bpt>Create Apache Spark clusters in Azure HDInsight<ept id=\"p4\">](hdinsight-apache-spark-jupyter-spark-sql.md)</ept>.",
      "nodes": [
        {
          "content": "An Apache Spark cluster on HDInsight Linux.",
          "pos": [
            0,
            43
          ]
        },
        {
          "content": "For instructions, see <bpt id=\"p4\">[</bpt>Create Apache Spark clusters in Azure HDInsight<ept id=\"p4\">](hdinsight-apache-spark-jupyter-spark-sql.md)</ept>.",
          "pos": [
            44,
            199
          ]
        }
      ]
    },
    {
      "pos": [
        1467,
        1497
      ],
      "content": "Submit a batch job the cluster"
    },
    {
      "pos": [
        1499,
        1871
      ],
      "content": "Before you submit a batch job, you must upload the application jar on the cluster storage associated with the cluster. You can use <bpt id=\"p5\">[</bpt><bpt id=\"p6\">**</bpt>AzCopy<ept id=\"p6\">**</ept><ept id=\"p5\">](storage/storage-use-azcopy.md)</ept>, a command line utility, to do so. There are a lot of other clients you can use to upload data. You can find more about them at <bpt id=\"p7\">[</bpt>Upload data for Hadoop jobs in HDInsight<ept id=\"p7\">](hdinsight-upload-data.md)</ept>.",
      "nodes": [
        {
          "content": "Before you submit a batch job, you must upload the application jar on the cluster storage associated with the cluster.",
          "pos": [
            0,
            118
          ]
        },
        {
          "content": "You can use <bpt id=\"p5\">[</bpt><bpt id=\"p6\">**</bpt>AzCopy<ept id=\"p6\">**</ept><ept id=\"p5\">](storage/storage-use-azcopy.md)</ept>, a command line utility, to do so.",
          "pos": [
            119,
            285
          ]
        },
        {
          "content": "There are a lot of other clients you can use to upload data.",
          "pos": [
            286,
            346
          ]
        },
        {
          "content": "You can find more about them at <bpt id=\"p7\">[</bpt>Upload data for Hadoop jobs in HDInsight<ept id=\"p7\">](hdinsight-upload-data.md)</ept>.",
          "pos": [
            347,
            486
          ]
        }
      ]
    },
    {
      "pos": [
        2098,
        2111
      ],
      "content": "<bpt id=\"p8\">**</bpt>Examples<ept id=\"p8\">**</ept>:"
    },
    {
      "pos": [
        2115,
        2163
      ],
      "content": "If the jar file is on the cluster storage (WASB)"
    },
    {
      "pos": [
        2468,
        2580
      ],
      "content": "If the you want to pass the jar filename and the classname as part of an input file (in this example, input.txt)"
    },
    {
      "pos": [
        2769,
        2818
      ],
      "content": "Get information on batches running on the cluster"
    },
    {
      "pos": [
        2948,
        2961
      ],
      "content": "<bpt id=\"p9\">**</bpt>Examples<ept id=\"p9\">**</ept>:"
    },
    {
      "pos": [
        2965,
        3028
      ],
      "content": "If you want to retrieve all the batches running on the cluster:"
    },
    {
      "pos": [
        3144,
        3205
      ],
      "content": "If you want to retrieve a specific batch with a given batchId"
    },
    {
      "pos": [
        3333,
        3351
      ],
      "content": "Delete a batch job"
    },
    {
      "pos": [
        3494,
        3506
      ],
      "content": "<bpt id=\"p10\">**</bpt>Example<ept id=\"p10\">**</ept>:"
    },
    {
      "pos": [
        3632,
        3650
      ],
      "content": "Show me an example"
    },
    {
      "pos": [
        3652,
        4053
      ],
      "content": "In this section, we look at examples on how to use Livy to submit a Spark application, monitor the progress of the application, and then delete the job. The application we use in this example is the one developed in the article <bpt id=\"p11\">[</bpt>Create a standalone Scala application and to run on HDInsight Spark cluster<ept id=\"p11\">](hdinsight-apache-spark-create-standalone-application.md)</ept>. The steps below assume the following:",
      "nodes": [
        {
          "content": "In this section, we look at examples on how to use Livy to submit a Spark application, monitor the progress of the application, and then delete the job.",
          "pos": [
            0,
            152
          ]
        },
        {
          "content": "The application we use in this example is the one developed in the article <bpt id=\"p11\">[</bpt>Create a standalone Scala application and to run on HDInsight Spark cluster<ept id=\"p11\">](hdinsight-apache-spark-create-standalone-application.md)</ept>.",
          "pos": [
            153,
            403
          ]
        },
        {
          "content": "The steps below assume the following:",
          "pos": [
            404,
            441
          ]
        }
      ]
    },
    {
      "pos": [
        4057,
        4157
      ],
      "content": "You have already copied over the application jar to the storage account associated with the cluster."
    },
    {
      "pos": [
        4160,
        4233
      ],
      "content": "You have CuRL installed on the computer where you are trying these steps."
    },
    {
      "pos": [
        4235,
        4263
      ],
      "content": "Perform the following steps."
    },
    {
      "pos": [
        4268,
        4460
      ],
      "content": "Let us first verify that Livy is running on the cluster. We can do so by getting a list of running batches. If this is the first time you are running a job using Livy, this should return zero.",
      "nodes": [
        {
          "content": "Let us first verify that Livy is running on the cluster.",
          "pos": [
            0,
            56
          ]
        },
        {
          "content": "We can do so by getting a list of running batches.",
          "pos": [
            57,
            107
          ]
        },
        {
          "content": "If this is the first time you are running a job using Livy, this should return zero.",
          "pos": [
            108,
            192
          ]
        }
      ]
    },
    {
      "pos": [
        4578,
        4628
      ],
      "content": "You should get an output similar to the following:"
    },
    {
      "pos": [
        5014,
        5105
      ],
      "content": "Notice how the last line in the output says <bpt id=\"p12\">**</bpt>total:0<ept id=\"p12\">**</ept>, which suggests no running batches."
    },
    {
      "pos": [
        5110,
        5333
      ],
      "content": "Let us now submit a batch job. The snippet below uses an input file (input.txt) to pass the jar name and the class name as parameters. This is the recommended approach if you are running these steps from a Windows computer.",
      "nodes": [
        {
          "content": "Let us now submit a batch job.",
          "pos": [
            0,
            30
          ]
        },
        {
          "content": "The snippet below uses an input file (input.txt) to pass the jar name and the class name as parameters.",
          "pos": [
            31,
            134
          ]
        },
        {
          "content": "This is the recommended approach if you are running these steps from a Windows computer.",
          "pos": [
            135,
            223
          ]
        }
      ]
    },
    {
      "pos": [
        5514,
        5578
      ],
      "content": "The parameters in the file <bpt id=\"p13\">**</bpt>input.txt<ept id=\"p13\">**</ept><ph id=\"ph2\"/> are defined as follows:"
    },
    {
      "pos": [
        5700,
        5750
      ],
      "content": "You should see an output similar to the following:"
    },
    {
      "pos": [
        6166,
        6275
      ],
      "content": "Notice how the last line of the output says <bpt id=\"p14\">**</bpt>state:starting<ept id=\"p14\">**</ept>. It also says, <bpt id=\"p15\">**</bpt>id:0<ept id=\"p15\">**</ept>. This is the batch ID.",
      "nodes": [
        {
          "content": "Notice how the last line of the output says <bpt id=\"p14\">**</bpt>state:starting<ept id=\"p14\">**</ept>.",
          "pos": [
            0,
            103
          ]
        },
        {
          "content": "It also says, <bpt id=\"p15\">**</bpt>id:0<ept id=\"p15\">**</ept>.",
          "pos": [
            104,
            167
          ]
        },
        {
          "content": "This is the batch ID.",
          "pos": [
            168,
            189
          ]
        }
      ]
    },
    {
      "pos": [
        6280,
        6358
      ],
      "content": "You can now retrieve the the status of this specific batch using the batch ID."
    },
    {
      "pos": [
        6478,
        6528
      ],
      "content": "You should see an output similar to the following:"
    },
    {
      "pos": [
        7390,
        7485
      ],
      "content": "The output now shows <bpt id=\"p16\">**</bpt>state:success<ept id=\"p16\">**</ept>, which suggests that the job was successfully completed."
    },
    {
      "pos": [
        7490,
        7532
      ],
      "content": "If you want, you can now delete the batch."
    },
    {
      "pos": [
        7656,
        7706
      ],
      "content": "You should see an output similar to the following:"
    },
    {
      "pos": [
        8075,
        8334
      ],
      "content": "The last line of the output shows that the batch was successfully deleted. If you delete a job while it is running, it will essentially kill the job. If you delete a job that has completed, successfully or otherwise, it deletes the job information completely.",
      "nodes": [
        {
          "content": "The last line of the output shows that the batch was successfully deleted.",
          "pos": [
            0,
            74
          ]
        },
        {
          "content": "If you delete a job while it is running, it will essentially kill the job.",
          "pos": [
            75,
            149
          ]
        },
        {
          "content": "If you delete a job that has completed, successfully or otherwise, it deletes the job information completely.",
          "pos": [
            150,
            259
          ]
        }
      ]
    },
    {
      "pos": [
        8361,
        8369
      ],
      "content": "See also"
    },
    {
      "pos": [
        8374,
        8453
      ],
      "content": "<bpt id=\"p17\">[</bpt>Overview: Apache Spark on Azure HDInsight<ept id=\"p17\">](hdinsight-apache-spark-overview.md)</ept>"
    },
    {
      "pos": [
        8459,
        8468
      ],
      "content": "Scenarios"
    },
    {
      "pos": [
        8472,
        8601
      ],
      "content": "<bpt id=\"p18\">[</bpt>Spark with BI: Perform interactive data analysis using Spark in HDInsight with BI tools<ept id=\"p18\">](hdinsight-apache-spark-use-bi-tools.md)</ept>"
    },
    {
      "pos": [
        8605,
        8770
      ],
      "content": "<bpt id=\"p19\">[</bpt>Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data<ept id=\"p19\">](hdinsight-apache-spark-ipython-notebook-machine-learning.md)</ept>"
    },
    {
      "pos": [
        8774,
        8920
      ],
      "content": "<bpt id=\"p20\">[</bpt>Spark with Machine Learning: Use Spark in HDInsight to predict food inspection results<ept id=\"p20\">](hdinsight-apache-spark-machine-learning-mllib-ipython.md)</ept>"
    },
    {
      "pos": [
        8924,
        9057
      ],
      "content": "<bpt id=\"p21\">[</bpt>Spark Streaming: Use Spark in HDInsight for building real-time streaming applications<ept id=\"p21\">](hdinsight-apache-spark-eventhub-streaming.md)</ept>"
    },
    {
      "pos": [
        9061,
        9171
      ],
      "content": "<bpt id=\"p22\">[</bpt>Website log analysis using Spark in HDInsight<ept id=\"p22\">](hdinsight-apache-spark-custom-library-website-log-analysis.md)</ept>"
    },
    {
      "pos": [
        9177,
        9204
      ],
      "content": "Create and run applications"
    },
    {
      "pos": [
        9208,
        9310
      ],
      "content": "<bpt id=\"p23\">[</bpt>Create a standalone application using Scala<ept id=\"p23\">](hdinsight-apache-spark-create-standalone-application.md)</ept>"
    },
    {
      "pos": [
        9316,
        9336
      ],
      "content": "Tools and extensions"
    },
    {
      "pos": [
        9340,
        9479
      ],
      "content": "<bpt id=\"p24\">[</bpt>Use HDInsight Tools Plugin for IntelliJ IDEA to create and submit Spark Scala applicatons<ept id=\"p24\">](hdinsight-apache-spark-intellij-tool-plugin.md)</ept>"
    },
    {
      "pos": [
        9483,
        9590
      ],
      "content": "<bpt id=\"p25\">[</bpt>Use Zeppelin notebooks with a Spark cluster on HDInsight<ept id=\"p25\">](hdinsight-apache-spark-use-zeppelin-notebook.md)</ept>"
    },
    {
      "pos": [
        9594,
        9717
      ],
      "content": "<bpt id=\"p26\">[</bpt>Kernels available for Jupyter notebook in Spark cluster for HDInsight<ept id=\"p26\">](hdinsight-apache-spark-jupyter-notebook-kernels.md)</ept>"
    },
    {
      "pos": [
        9723,
        9739
      ],
      "content": "Manage resources"
    },
    {
      "pos": [
        9743,
        9853
      ],
      "content": "<bpt id=\"p27\">[</bpt>Manage resources for the Apache Spark cluster in Azure HDInsight<ept id=\"p27\">](hdinsight-apache-spark-resource-manager.md)</ept>"
    }
  ],
  "content": "<properties \n    pageTitle=\"Submit Spark jobs remotely using Livy | Microsoft Azure\" \n    description=\"Learn how to use Livy with HDInsight clusters to submit Spark jobs remotely.\" \n    services=\"hdinsight\" \n    documentationCenter=\"\" \n    authors=\"nitinme\" \n    manager=\"paulettm\" \n    editor=\"cgronlun\"\n    tags=\"azure-portal\"/>\n\n<tags \n    ms.service=\"hdinsight\" \n    ms.workload=\"big-data\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"02/05/2016\" \n    ms.author=\"nitinme\"/>\n\n\n# Submit Spark jobs remotely using Livy with Spark clusters on HDInsight (Linux)\n\nApache Spark cluster on Azure HDInsight includes Livy, a REST interface for submitting jobs remotely to a Spark cluster from anywhere. For detailed documentation, see [Livy](https://github.com/cloudera/hue/tree/master/apps/spark/java#welcome-to-livy-the-rest-spark-server).\n\nYou can use Livy to run interactive Spark shells or submit batch jobs to be run on Spark. This article talks about using Livy to submit batch jobs. The syntax below uses Curl to make REST calls to the Livy endpoint.\n\n**Prerequisites:**\n\nYou must have the following:\n\n- An Azure subscription. See [Get Azure free trial](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/).\n- An Apache Spark cluster on HDInsight Linux. For instructions, see [Create Apache Spark clusters in Azure HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md).\n\n## Submit a batch job the cluster\n\nBefore you submit a batch job, you must upload the application jar on the cluster storage associated with the cluster. You can use [**AzCopy**](storage/storage-use-azcopy.md), a command line utility, to do so. There are a lot of other clients you can use to upload data. You can find more about them at [Upload data for Hadoop jobs in HDInsight](hdinsight-upload-data.md).\n\n    curl -k --user \"<hdinsight user>:<user password>\" -v -H <content-type> -X POST -d '{ \"file\":\"<path to application jar>\", \"className\":\"<classname in jar>\" }' 'https://<spark_cluster_name>.azurehdinsight.net/livy/batches'\n\n**Examples**:\n\n* If the jar file is on the cluster storage (WASB)\n\n        curl -k --user \"admin:mypassword1!\" -v -H 'Content-Type: application/json' -X POST -d '{ \"file\":\"wasb://mycontainer@mystorageaccount.blob.core.windows.net/data/SparkSimpleTest.jar\", \"className\":\"com.microsoft.spark.test.SimpleFile\" }' \"https://mysparkcluster.azurehdinsight.net/livy/batches\"\n\n* If the you want to pass the jar filename and the classname as part of an input file (in this example, input.txt)\n        \n        curl -k  --user \"admin:mypassword1!\" -v -H \"Content-Type: application/json\" -X POST --data @C:\\Temp\\input.txt \"https://mysparkcluster.azurehdinsight.net/livy/batches\"\n\n## Get information on batches running on the cluster\n\n    curl -k --user \"<hdinsight user>:<user password>\" -v -X GET \"https://<spark_cluster_name>.azurehdinsight.net/livy/batches\"\n\n**Examples**:\n\n* If you want to retrieve all the batches running on the cluster:\n\n        curl -k --user \"admin:mypassword1!\" -v -X GET \"https://mysparkcluster.azurehdinsight.net/livy/batches\"\n\n* If you want to retrieve a specific batch with a given batchId\n\n        curl -k --user \"admin:mypassword1!\" -v -X GET \"https://mysparkcluster.azurehdinsight.net/livy/batches/{batchId}\"\n\n\n## Delete a batch job\n\n    curl -k --user \"<hdinsight user>:<user password>\" -v -X DELETE \"https://<spark_cluster_name>.azurehdinsight.net/livy/batches/{batchId}\"\n\n**Example**:\n\n    curl -k --user \"admin:mypassword1!\" -v -X DELETE \"https://mysparkcluster.azurehdinsight.net/livy/batches/{batchId}\"\n\n## Show me an example\n\nIn this section, we look at examples on how to use Livy to submit a Spark application, monitor the progress of the application, and then delete the job. The application we use in this example is the one developed in the article [Create a standalone Scala application and to run on HDInsight Spark cluster](hdinsight-apache-spark-create-standalone-application.md). The steps below assume the following:\n\n* You have already copied over the application jar to the storage account associated with the cluster.\n* You have CuRL installed on the computer where you are trying these steps.\n\nPerform the following steps.\n\n1. Let us first verify that Livy is running on the cluster. We can do so by getting a list of running batches. If this is the first time you are running a job using Livy, this should return zero.\n\n        curl -k --user \"admin:mypassword1!\" -v -X GET \"https://mysparkcluster.azurehdinsight.net/livy/batches\"\n\n    You should get an output similar to the following:\n\n        < HTTP/1.1 200 OK\n        < Content-Type: application/json; charset=UTF-8\n        < Server: Microsoft-IIS/8.5\n        < X-Powered-By: ARR/2.5\n        < X-Powered-By: ASP.NET\n        < Date: Fri, 20 Nov 2015 23:47:53 GMT\n        < Content-Length: 34\n        <\n        {\"from\":0,\"total\":0,\"sessions\":[]}* Connection #0 to host mysparkcluster.azurehdinsight.net left intact\n\n    Notice how the last line in the output says **total:0**, which suggests no running batches.\n\n2. Let us now submit a batch job. The snippet below uses an input file (input.txt) to pass the jar name and the class name as parameters. This is the recommended approach if you are running these steps from a Windows computer.\n\n        curl -k --user \"admin:mypassword1!\" -v -H \"Content-Type: application/json\" -X POST --data @C:\\Temp\\input.txt \"https://mysparkcluster.azurehdinsight.net/livy/batches\"\n\n    The parameters in the file **input.txt** are defined as follows:\n\n        { \"file\":\"wasb:///example/jars/SparkSimpleApp.jar\", \"className\":\"com.microsoft.spark.example.WasbIOTest\" }\n\n    You should see an output similar to the following:\n\n        < HTTP/1.1 201 Created\n        < Content-Type: application/json; charset=UTF-8\n        < Location: /0\n        < Server: Microsoft-IIS/8.5\n        < X-Powered-By: ARR/2.5\n        < X-Powered-By: ASP.NET\n        < Date: Fri, 20 Nov 2015 23:51:30 GMT\n        < Content-Length: 36\n        <\n        {\"id\":0,\"state\":\"starting\",\"log\":[]}* Connection #0 to host mysparkcluster.azurehdinsight.net left intact\n\n    Notice how the last line of the output says **state:starting**. It also says, **id:0**. This is the batch ID.\n\n3. You can now retrieve the the status of this specific batch using the batch ID.\n\n        curl -k --user \"admin:mypassword1!\" -v -X GET \"https://mysparkcluster.azurehdinsight.net/livy/batches/0\"\n\n    You should see an output similar to the following:\n\n        < HTTP/1.1 200 OK\n        < Content-Type: application/json; charset=UTF-8\n        < Server: Microsoft-IIS/8.5\n        < X-Powered-By: ARR/2.5\n        < X-Powered-By: ASP.NET\n        < Date: Fri, 20 Nov 2015 23:54:42 GMT\n        < Content-Length: 509\n        <\n        {\"id\":0,\"state\":\"success\",\"log\":[\"\\t diagnostics: N/A\",\"\\t ApplicationMaster host: 10.0.0.4\",\"\\t ApplicationMaster RPC port: 0\",\"\\t queue: default\",\"\\t start time: 1448063505350\",\"\\t final status: SUCCEEDED\",\"\\t tracking URL: http://hn0-myspar.lpel1gnnvxne3gwzqkfq5u5uzh.jx.internal.cloudapp.net:8088/proxy/application_1447984474852_0002/\",\"\\t user: root\",\"15/11/20 23:52:47 INFO Utils: Shutdown hook called\",\"15/11/20 23:52:47 INFO Utils: Deleting directory /tmp/spark-b72cd2bf-280b-4c57-8ceb-9e3e69ac7d0c\"]}* Connection #0 to host mysparkcluster.azurehdinsight.net left intact\n\n    The output now shows **state:success**, which suggests that the job was successfully completed.\n\n4. If you want, you can now delete the batch. \n\n        curl -k --user \"admin:mypassword1!\" -v -X DELETE \"https://mysparkcluster.azurehdinsight.net/livy/batches/0\"\n\n    You should see an output similar to the following:\n\n        < HTTP/1.1 200 OK\n        < Content-Type: application/json; charset=UTF-8\n        < Server: Microsoft-IIS/8.5\n        < X-Powered-By: ARR/2.5\n        < X-Powered-By: ASP.NET\n        < Date: Sat, 21 Nov 2015 18:51:54 GMT\n        < Content-Length: 17\n        <\n        {\"msg\":\"deleted\"}* Connection #0 to host mysparkcluster.azurehdinsight.net left intact\n\n    The last line of the output shows that the batch was successfully deleted. If you delete a job while it is running, it will essentially kill the job. If you delete a job that has completed, successfully or otherwise, it deletes the job information completely.\n\n## <a name=\"seealso\"></a>See also\n\n\n* [Overview: Apache Spark on Azure HDInsight](hdinsight-apache-spark-overview.md)\n\n### Scenarios\n\n* [Spark with BI: Perform interactive data analysis using Spark in HDInsight with BI tools](hdinsight-apache-spark-use-bi-tools.md)\n\n* [Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data](hdinsight-apache-spark-ipython-notebook-machine-learning.md)\n\n* [Spark with Machine Learning: Use Spark in HDInsight to predict food inspection results](hdinsight-apache-spark-machine-learning-mllib-ipython.md)\n\n* [Spark Streaming: Use Spark in HDInsight for building real-time streaming applications](hdinsight-apache-spark-eventhub-streaming.md)\n\n* [Website log analysis using Spark in HDInsight](hdinsight-apache-spark-custom-library-website-log-analysis.md)\n\n### Create and run applications\n\n* [Create a standalone application using Scala](hdinsight-apache-spark-create-standalone-application.md)\n\n### Tools and extensions\n\n* [Use HDInsight Tools Plugin for IntelliJ IDEA to create and submit Spark Scala applicatons](hdinsight-apache-spark-intellij-tool-plugin.md)\n\n* [Use Zeppelin notebooks with a Spark cluster on HDInsight](hdinsight-apache-spark-use-zeppelin-notebook.md)\n\n* [Kernels available for Jupyter notebook in Spark cluster for HDInsight](hdinsight-apache-spark-jupyter-notebook-kernels.md)\n\n### Manage resources\n\n* [Manage resources for the Apache Spark cluster in Azure HDInsight](hdinsight-apache-spark-resource-manager.md)\n"
}