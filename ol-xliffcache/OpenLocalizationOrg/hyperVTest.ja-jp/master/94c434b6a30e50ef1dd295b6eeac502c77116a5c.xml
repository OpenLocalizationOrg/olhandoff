{
  "nodes": [
    {
      "pos": [
        26,
        91
      ],
      "content": "Use Hadoop Pig with SSH on an HDInsight cluster | Microsoft Azure"
    },
    {
      "pos": [
        109,
        259
      ],
      "content": "Learn how connect to a Linux-based Hadoop cluster with SSH, and then use the Pig command to run Pig Latin statements interactively, or as a batch job."
    },
    {
      "pos": [
        576,
        640
      ],
      "content": "Run Pig jobs on a Linux-based cluster with the Pig command (SSH)"
    },
    {
      "pos": [
        720,
        943
      ],
      "content": "In this document you will walk through the process of connecting to a Linux-based Azure HDInsight cluster by using Secure Shell (SSH), then using the Pig command to run Pig Latin statements interactively, or as a batch job."
    },
    {
      "pos": [
        945,
        1084
      ],
      "content": "The Pig Latin programming language allows you to describe transformations that are applied to the input data to produce the desired output."
    },
    {
      "pos": [
        1088,
        1266
      ],
      "content": "<ph id=\"ph3\">[AZURE.NOTE]</ph><ph id=\"ph4\"/> If you are already familiar with using Linux-based Hadoop servers, but are new to HDInsight, see <bpt id=\"p1\">[</bpt>Linux-based HDInsight Tips<ept id=\"p1\">](hdinsight-hadoop-linux-information.md)</ept>."
    },
    {
      "pos": [
        1268,
        1270
      ],
      "content": "##"
    },
    {
      "pos": [
        1289,
        1302
      ],
      "content": "Prerequisites"
    },
    {
      "pos": [
        1304,
        1371
      ],
      "content": "To complete the steps in this article, you will need the following."
    },
    {
      "pos": [
        1375,
        1429
      ],
      "content": "A Linux-based HDInsight (Hadoop on HDInsight) cluster."
    },
    {
      "pos": [
        1433,
        1623
      ],
      "content": "An SSH client. Linux, Unix, and Mac OS should come with an SSH client. Windows users must download a client, such as <bpt id=\"p2\">[</bpt>PuTTY<ept id=\"p2\">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.",
      "nodes": [
        {
          "content": "An SSH client.",
          "pos": [
            0,
            14
          ]
        },
        {
          "content": "Linux, Unix, and Mac OS should come with an SSH client.",
          "pos": [
            15,
            70
          ]
        },
        {
          "content": "Windows users must download a client, such as <bpt id=\"p2\">[</bpt>PuTTY<ept id=\"p2\">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.",
          "pos": [
            71,
            228
          ]
        }
      ]
    },
    {
      "pos": [
        1625,
        1627
      ],
      "content": "##"
    },
    {
      "pos": [
        1643,
        1659
      ],
      "content": "Connect with SSH"
    },
    {
      "pos": [
        1661,
        1917
      ],
      "content": "Connect to the fully qualified domain name (FQDN) of your HDInsight cluster by using the SSH command. The FQDN will be the name you gave the cluster, then <bpt id=\"p3\">**</bpt>.azurehdinsight.net<ept id=\"p3\">**</ept>. For example, the following would connect to a cluster named <bpt id=\"p4\">**</bpt>myhdinsight<ept id=\"p4\">**</ept>.",
      "nodes": [
        {
          "content": "Connect to the fully qualified domain name (FQDN) of your HDInsight cluster by using the SSH command.",
          "pos": [
            0,
            101
          ]
        },
        {
          "content": "The FQDN will be the name you gave the cluster, then <bpt id=\"p3\">**</bpt>.azurehdinsight.net<ept id=\"p3\">**</ept>.",
          "pos": [
            102,
            217
          ]
        },
        {
          "content": "For example, the following would connect to a cluster named <bpt id=\"p4\">**</bpt>myhdinsight<ept id=\"p4\">**</ept>.",
          "pos": [
            218,
            332
          ]
        }
      ]
    },
    {
      "pos": [
        1969,
        2148
      ],
      "content": "<bpt id=\"p5\">**</bpt>If you provided a certificate key for SSH authentication<ept id=\"p5\">**</ept><ph id=\"ph5\"/> when you created the HDInsight cluster, you may need to specify the location of the private key on your client system."
    },
    {
      "pos": [
        2215,
        2361
      ],
      "content": "<bpt id=\"p6\">**</bpt>If you provided a password for SSH authentication<ept id=\"p6\">**</ept><ph id=\"ph6\"/> when you created the HDInsight cluster, you will need to provide the password when prompted."
    },
    {
      "pos": [
        2363,
        2531
      ],
      "content": "For more information on using SSH with HDInsight, see <bpt id=\"p7\">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Linux, OS X, and Unix<ept id=\"p7\">](hdinsight-hadoop-linux-use-ssh-unix.md)</ept>."
    },
    {
      "pos": [
        2536,
        2565
      ],
      "content": "PuTTY (Windows-based clients)"
    },
    {
      "pos": [
        2567,
        2805
      ],
      "content": "Windows does not provide a built-in SSH client. We recommend using <bpt id=\"p8\">**</bpt>PuTTY<ept id=\"p8\">**</ept>, which can be downloaded from <bpt id=\"p9\">[</bpt>http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html<ept id=\"p9\">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.",
      "nodes": [
        {
          "content": "Windows does not provide a built-in SSH client.",
          "pos": [
            0,
            47
          ]
        },
        {
          "content": "We recommend using <bpt id=\"p8\">**</bpt>PuTTY<ept id=\"p8\">**</ept>, which can be downloaded from <bpt id=\"p9\">[</bpt>http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html<ept id=\"p9\">](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)</ept>.",
          "pos": [
            48,
            314
          ]
        }
      ]
    },
    {
      "pos": [
        2807,
        2952
      ],
      "content": "For more information on using PuTTY, see <bpt id=\"p10\">[</bpt>Use SSH with Linux-based Hadoop on HDInsight from Windows <ept id=\"p10\">](hdinsight-hadoop-linux-use-ssh-windows.md)</ept>."
    },
    {
      "pos": [
        2954,
        2956
      ],
      "content": "##"
    },
    {
      "pos": [
        2972,
        2991
      ],
      "content": "Use the Pig command"
    },
    {
      "pos": [
        2996,
        3086
      ],
      "content": "Once connected, start the Pig command-line interface (CLI) by using the following command."
    },
    {
      "pos": [
        3105,
        3154
      ],
      "content": "After a moment, you should see a <ph id=\"ph7\">`grunt&gt;`</ph><ph id=\"ph8\"/> prompt."
    },
    {
      "pos": [
        3159,
        3189
      ],
      "content": "Enter the following statement."
    },
    {
      "pos": [
        3251,
        3378
      ],
      "content": "This command loads the contents of the sample.log file into LOGS. You can view the contents of the file by using the following.",
      "nodes": [
        {
          "content": "This command loads the contents of the sample.log file into LOGS.",
          "pos": [
            0,
            65
          ]
        },
        {
          "content": "You can view the contents of the file by using the following.",
          "pos": [
            66,
            127
          ]
        }
      ]
    },
    {
      "pos": [
        3403,
        3535
      ],
      "content": "Next, transform the data by applying a regular expression to extract only the logging level from each record by using the following."
    },
    {
      "pos": [
        3655,
        3752
      ],
      "content": "You can use <bpt id=\"p11\">**</bpt>DUMP<ept id=\"p11\">**</ept><ph id=\"ph9\"/> to view the data after the transformation. In this case, use <ph id=\"ph10\">`DUMP LEVELS;`</ph>.",
      "nodes": [
        {
          "content": "You can use <bpt id=\"p11\">**</bpt>DUMP<ept id=\"p11\">**</ept><ph id=\"ph9\"/> to view the data after the transformation.",
          "pos": [
            0,
            117
          ]
        },
        {
          "content": "In this case, use <ph id=\"ph10\">`DUMP LEVELS;`</ph>.",
          "pos": [
            118,
            170
          ]
        }
      ]
    },
    {
      "pos": [
        3757,
        3894
      ],
      "content": "Continue applying transformations by using the following statements. Use <ph id=\"ph11\">`DUMP`</ph><ph id=\"ph12\"/> to view the result of the transformation after each step.",
      "nodes": [
        {
          "content": "Continue applying transformations by using the following statements.",
          "pos": [
            0,
            68
          ]
        },
        {
          "content": "Use <ph id=\"ph11\">`DUMP`</ph><ph id=\"ph12\"/> to view the result of the transformation after each step.",
          "pos": [
            69,
            171
          ]
        }
      ]
    },
    {
      "pos": [
        3900,
        4768
      ],
      "leadings": [
        "",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   ",
        "   "
      ],
      "content": "<ph id=\"ph13\">&lt;table&gt;</ph><ph id=\"ph14\">\n &lt;tr&gt;</ph><ph id=\"ph15\">\n &lt;th&gt;</ph>Statement<ph id=\"ph16\">&lt;/th&gt;</ph><ph id=\"ph17\">&lt;th&gt;</ph>What it does<ph id=\"ph18\">&lt;/th&gt;</ph><ph id=\"ph19\">\n &lt;/tr&gt;</ph><ph id=\"ph20\">\n &lt;tr&gt;</ph><ph id=\"ph21\">\n &lt;td&gt;</ph>FILTEREDLEVELS = FILTER LEVELS by LOGLEVEL is not null;<ph id=\"ph22\">&lt;/td&gt;</ph><ph id=\"ph23\">&lt;td&gt;</ph>Removes rows that contain a null value for the log level and stores the results into FILTEREDLEVELS.<ph id=\"ph24\">&lt;/td&gt;</ph><ph id=\"ph25\">\n &lt;/tr&gt;</ph><ph id=\"ph26\">\n &lt;tr&gt;</ph><ph id=\"ph27\">\n &lt;td&gt;</ph>GROUPEDLEVELS = GROUP FILTEREDLEVELS by LOGLEVEL;<ph id=\"ph28\">&lt;/td&gt;</ph><ph id=\"ph29\">&lt;td&gt;</ph>Groups the rows by log level and stores the results into GROUPEDLEVELS.<ph id=\"ph30\">&lt;/td&gt;</ph><ph id=\"ph31\">\n &lt;/tr&gt;</ph><ph id=\"ph32\">\n &lt;tr&gt;</ph><ph id=\"ph33\">\n &lt;td&gt;</ph>FREQUENCIES = foreach GROUPEDLEVELS generate group as LOGLEVEL, COUNT(FILTEREDLEVELS.LOGLEVEL) as COUNT;<ph id=\"ph34\">&lt;/td&gt;</ph><ph id=\"ph35\">&lt;td&gt;</ph>Creates a new set of data that contains each unique log level value and how many times it occurs. This is stored into FREQUENCIES.<ph id=\"ph36\">&lt;/td&gt;</ph><ph id=\"ph37\">\n &lt;/tr&gt;</ph><ph id=\"ph38\">\n &lt;tr&gt;</ph><ph id=\"ph39\">\n &lt;td&gt;</ph>RESULT = order FREQUENCIES by COUNT desc;<ph id=\"ph40\">&lt;/td&gt;</ph><ph id=\"ph41\">&lt;td&gt;</ph>Orders the log levels by count (descending) and stores into RESULT.<ph id=\"ph42\">&lt;/td&gt;</ph><ph id=\"ph43\">\n &lt;/tr&gt;</ph><ph id=\"ph44\">\n &lt;/table&gt;</ph>",
      "nodes": [
        {
          "content": "<ph id=\"ph13\">&lt;table&gt;</ph><ph id=\"ph14\">\n &lt;tr&gt;</ph><ph id=\"ph15\">\n &lt;th&gt;</ph>Statement<ph id=\"ph16\">&lt;/th&gt;</ph><ph id=\"ph17\">&lt;th&gt;</ph>What it does<ph id=\"ph18\">&lt;/th&gt;</ph><ph id=\"ph19\">\n &lt;/tr&gt;</ph><ph id=\"ph20\">\n &lt;tr&gt;</ph><ph id=\"ph21\">\n &lt;td&gt;</ph>FILTEREDLEVELS = FILTER LEVELS by LOGLEVEL is not null;<ph id=\"ph22\">&lt;/td&gt;</ph><ph id=\"ph23\">&lt;td&gt;</ph>Removes rows that contain a null value for the log level and stores the results into FILTEREDLEVELS.<ph id=\"ph24\">&lt;/td&gt;</ph><ph id=\"ph25\">\n &lt;/tr&gt;</ph><ph id=\"ph26\">\n &lt;tr&gt;</ph><ph id=\"ph27\">\n &lt;td&gt;</ph>GROUPEDLEVELS = GROUP FILTEREDLEVELS by LOGLEVEL;<ph id=\"ph28\">&lt;/td&gt;</ph><ph id=\"ph29\">&lt;td&gt;</ph>Groups the rows by log level and stores the results into GROUPEDLEVELS.<ph id=\"ph30\">&lt;/td&gt;</ph><ph id=\"ph31\">\n &lt;/tr&gt;</ph><ph id=\"ph32\">\n &lt;tr&gt;</ph><ph id=\"ph33\">\n &lt;td&gt;</ph>FREQUENCIES = foreach GROUPEDLEVELS generate group as LOGLEVEL, COUNT(FILTEREDLEVELS.LOGLEVEL) as COUNT;<ph id=\"ph34\">&lt;/td&gt;</ph><ph id=\"ph35\">&lt;td&gt;</ph>Creates a new set of data that contains each unique log level value and how many times it occurs.",
          "pos": [
            0,
            1199
          ]
        },
        {
          "content": "This is stored into FREQUENCIES.<ph id=\"ph36\">&lt;/td&gt;</ph><ph id=\"ph37\">\n &lt;/tr&gt;</ph><ph id=\"ph38\">\n &lt;tr&gt;</ph><ph id=\"ph39\">\n &lt;td&gt;</ph>RESULT = order FREQUENCIES by COUNT desc;<ph id=\"ph40\">&lt;/td&gt;</ph><ph id=\"ph41\">&lt;td&gt;</ph>Orders the log levels by count (descending) and stores into RESULT.<ph id=\"ph42\">&lt;/td&gt;</ph><ph id=\"ph43\">\n &lt;/tr&gt;</ph><ph id=\"ph44\">\n &lt;/table&gt;</ph>",
          "pos": [
            1200,
            1620
          ]
        }
      ]
    },
    {
      "pos": [
        4773,
        4993
      ],
      "content": "You can also save the results of a transformation by using the <ph id=\"ph45\">`STORE`</ph><ph id=\"ph46\"/> statement. For example, the following saves the <ph id=\"ph47\">`RESULT`</ph><ph id=\"ph48\"/> to the <bpt id=\"p12\">**</bpt>/example/data/pigout<ept id=\"p12\">**</ept><ph id=\"ph49\"/> directory on the default storage container for your cluster.",
      "nodes": [
        {
          "content": "You can also save the results of a transformation by using the <ph id=\"ph45\">`STORE`</ph><ph id=\"ph46\"/> statement.",
          "pos": [
            0,
            115
          ]
        },
        {
          "content": "For example, the following saves the <ph id=\"ph47\">`RESULT`</ph><ph id=\"ph48\"/> to the <bpt id=\"p12\">**</bpt>/example/data/pigout<ept id=\"p12\">**</ept><ph id=\"ph49\"/> directory on the default storage container for your cluster.",
          "pos": [
            116,
            343
          ]
        }
      ]
    },
    {
      "pos": [
        5059,
        5208
      ],
      "content": "<ph id=\"ph50\">[AZURE.NOTE]</ph><ph id=\"ph51\"/> The data is stored in the specified directory in files named <bpt id=\"p13\">**</bpt>part-nnnnn<ept id=\"p13\">**</ept>. If the directory already exists, you will receive an error.",
      "nodes": [
        {
          "content": "<ph id=\"ph50\">[AZURE.NOTE]</ph><ph id=\"ph51\"/> The data is stored in the specified directory in files named <bpt id=\"p13\">**</bpt>part-nnnnn<ept id=\"p13\">**</ept>.",
          "pos": [
            0,
            163
          ]
        },
        {
          "content": "If the directory already exists, you will receive an error.",
          "pos": [
            164,
            223
          ]
        }
      ]
    },
    {
      "pos": [
        5213,
        5269
      ],
      "content": "To exit the grunt prompt, enter the following statement."
    },
    {
      "pos": [
        5289,
        5310
      ],
      "content": "Pig Latin batch files"
    },
    {
      "pos": [
        5312,
        5382
      ],
      "content": "You can also use the Pig command to run Pig Latin contained in a file."
    },
    {
      "pos": [
        5387,
        5600
      ],
      "content": "After exiting the grunt prompt, use the following command to pipe STDIN into a file named <bpt id=\"p14\">**</bpt>pigbatch.pig<ept id=\"p14\">**</ept>. This file will be created in the home directory for the account you are logged in to for the SSH session.",
      "nodes": [
        {
          "content": "After exiting the grunt prompt, use the following command to pipe STDIN into a file named <bpt id=\"p14\">**</bpt>pigbatch.pig<ept id=\"p14\">**</ept>.",
          "pos": [
            0,
            147
          ]
        },
        {
          "content": "This file will be created in the home directory for the account you are logged in to for the SSH session.",
          "pos": [
            148,
            253
          ]
        }
      ]
    },
    {
      "pos": [
        5635,
        5704
      ],
      "content": "Type or paste the following lines, and then use Ctrl+D when finished."
    },
    {
      "pos": [
        6184,
        6260
      ],
      "content": "Use the following to run the <bpt id=\"p15\">**</bpt>pigbatch.pig<ept id=\"p15\">**</ept><ph id=\"ph52\"/> file by using the Pig command."
    },
    {
      "pos": [
        6294,
        6439
      ],
      "content": "Once the batch job finishes, you should see the following output, which should be the same as when you used <ph id=\"ph53\">`DUMP RESULT;`</ph><ph id=\"ph54\"/> in the previous steps."
    },
    {
      "pos": [
        6554,
        6556
      ],
      "content": "##"
    },
    {
      "pos": [
        6576,
        6583
      ],
      "content": "Summary"
    },
    {
      "pos": [
        6585,
        6739
      ],
      "content": "As you can see, the Pig command allows you to interactively run MapReduce operations by using Pig Latin, as well as run statements stored in a batch file."
    },
    {
      "pos": [
        6741,
        6743
      ],
      "content": "##"
    },
    {
      "pos": [
        6765,
        6775
      ],
      "content": "Next steps"
    },
    {
      "pos": [
        6777,
        6821
      ],
      "content": "For general information on Pig in HDInsight."
    },
    {
      "pos": [
        6825,
        6881
      ],
      "content": "<bpt id=\"p16\">[</bpt>Use Pig with Hadoop on HDInsight<ept id=\"p16\">](hdinsight-use-pig.md)</ept>"
    },
    {
      "pos": [
        6883,
        6951
      ],
      "content": "For information on other ways you can work with Hadoop on HDInsight."
    },
    {
      "pos": [
        6955,
        7013
      ],
      "content": "<bpt id=\"p17\">[</bpt>Use Hive with Hadoop on HDInsight<ept id=\"p17\">](hdinsight-use-hive.md)</ept>"
    },
    {
      "pos": [
        7017,
        7085
      ],
      "content": "<bpt id=\"p18\">[</bpt>Use MapReduce with Hadoop on HDInsight<ept id=\"p18\">](hdinsight-use-mapreduce.md)</ept>"
    }
  ],
  "content": "<properties\n   pageTitle=\"Use Hadoop Pig with SSH on an HDInsight cluster | Microsoft Azure\"\n   description=\"Learn how connect to a Linux-based Hadoop cluster with SSH, and then use the Pig command to run Pig Latin statements interactively, or as a batch job.\"\n   services=\"hdinsight\"\n   documentationCenter=\"\"\n   authors=\"Blackmist\"\n   manager=\"paulettm\"\n   editor=\"cgronlun\"\n    tags=\"azure-portal\"/>\n\n<tags\n   ms.service=\"hdinsight\"\n   ms.devlang=\"na\"\n   ms.topic=\"article\"\n   ms.tgt_pltfrm=\"na\"\n   ms.workload=\"big-data\"\n   ms.date=\"02/05/2016\"\n   ms.author=\"larryfr\"/>\n\n#Run Pig jobs on a Linux-based cluster with the Pig command (SSH)\n\n[AZURE.INCLUDE [pig-selector](../../includes/hdinsight-selector-use-pig.md)]\n\nIn this document you will walk through the process of connecting to a Linux-based Azure HDInsight cluster by using Secure Shell (SSH), then using the Pig command to run Pig Latin statements interactively, or as a batch job.\n\nThe Pig Latin programming language allows you to describe transformations that are applied to the input data to produce the desired output.\n\n> [AZURE.NOTE] If you are already familiar with using Linux-based Hadoop servers, but are new to HDInsight, see [Linux-based HDInsight Tips](hdinsight-hadoop-linux-information.md).\n\n##<a id=\"prereq\"></a>Prerequisites\n\nTo complete the steps in this article, you will need the following.\n\n* A Linux-based HDInsight (Hadoop on HDInsight) cluster.\n\n* An SSH client. Linux, Unix, and Mac OS should come with an SSH client. Windows users must download a client, such as [PuTTY](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html).\n\n##<a id=\"ssh\"></a>Connect with SSH\n\nConnect to the fully qualified domain name (FQDN) of your HDInsight cluster by using the SSH command. The FQDN will be the name you gave the cluster, then **.azurehdinsight.net**. For example, the following would connect to a cluster named **myhdinsight**.\n\n    ssh admin@myhdinsight-ssh.azurehdinsight.net\n\n**If you provided a certificate key for SSH authentication** when you created the HDInsight cluster, you may need to specify the location of the private key on your client system.\n\n    ssh admin@myhdinsight-ssh.azurehdinsight.net -i ~/mykey.key\n\n**If you provided a password for SSH authentication** when you created the HDInsight cluster, you will need to provide the password when prompted.\n\nFor more information on using SSH with HDInsight, see [Use SSH with Linux-based Hadoop on HDInsight from Linux, OS X, and Unix](hdinsight-hadoop-linux-use-ssh-unix.md).\n\n###PuTTY (Windows-based clients)\n\nWindows does not provide a built-in SSH client. We recommend using **PuTTY**, which can be downloaded from [http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html).\n\nFor more information on using PuTTY, see [Use SSH with Linux-based Hadoop on HDInsight from Windows ](hdinsight-hadoop-linux-use-ssh-windows.md).\n\n##<a id=\"pig\"></a>Use the Pig command\n\n2. Once connected, start the Pig command-line interface (CLI) by using the following command.\n\n        pig\n\n    After a moment, you should see a `grunt>` prompt.\n\n3. Enter the following statement.\n\n        LOGS = LOAD 'wasb:///example/data/sample.log';\n\n    This command loads the contents of the sample.log file into LOGS. You can view the contents of the file by using the following.\n\n        DUMP LOGS;\n\n4. Next, transform the data by applying a regular expression to extract only the logging level from each record by using the following.\n\n        LEVELS = foreach LOGS generate REGEX_EXTRACT($0, '(TRACE|DEBUG|INFO|WARN|ERROR|FATAL)', 1)  as LOGLEVEL;\n\n    You can use **DUMP** to view the data after the transformation. In this case, use `DUMP LEVELS;`.\n\n5. Continue applying transformations by using the following statements. Use `DUMP` to view the result of the transformation after each step.\n\n    <table>\n    <tr>\n    <th>Statement</th><th>What it does</th>\n    </tr>\n    <tr>\n    <td>FILTEREDLEVELS = FILTER LEVELS by LOGLEVEL is not null;</td><td>Removes rows that contain a null value for the log level and stores the results into FILTEREDLEVELS.</td>\n    </tr>\n    <tr>\n    <td>GROUPEDLEVELS = GROUP FILTEREDLEVELS by LOGLEVEL;</td><td>Groups the rows by log level and stores the results into GROUPEDLEVELS.</td>\n    </tr>\n    <tr>\n    <td>FREQUENCIES = foreach GROUPEDLEVELS generate group as LOGLEVEL, COUNT(FILTEREDLEVELS.LOGLEVEL) as COUNT;</td><td>Creates a new set of data that contains each unique log level value and how many times it occurs. This is stored into FREQUENCIES.</td>\n    </tr>\n    <tr>\n    <td>RESULT = order FREQUENCIES by COUNT desc;</td><td>Orders the log levels by count (descending) and stores into RESULT.</td>\n    </tr>\n    </table>\n\n6. You can also save the results of a transformation by using the `STORE` statement. For example, the following saves the `RESULT` to the **/example/data/pigout** directory on the default storage container for your cluster.\n\n        STORE RESULT into 'wasb:///example/data/pigout';\n\n    > [AZURE.NOTE] The data is stored in the specified directory in files named **part-nnnnn**. If the directory already exists, you will receive an error.\n\n7. To exit the grunt prompt, enter the following statement.\n\n        QUIT;\n\n###Pig Latin batch files\n\nYou can also use the Pig command to run Pig Latin contained in a file.\n\n3. After exiting the grunt prompt, use the following command to pipe STDIN into a file named **pigbatch.pig**. This file will be created in the home directory for the account you are logged in to for the SSH session.\n\n        cat > ~/pigbatch.pig\n\n4. Type or paste the following lines, and then use Ctrl+D when finished.\n\n        LOGS = LOAD 'wasb:///example/data/sample.log';\n        LEVELS = foreach LOGS generate REGEX_EXTRACT($0, '(TRACE|DEBUG|INFO|WARN|ERROR|FATAL)', 1)  as LOGLEVEL;\n        FILTEREDLEVELS = FILTER LEVELS by LOGLEVEL is not null;\n        GROUPEDLEVELS = GROUP FILTEREDLEVELS by LOGLEVEL;\n        FREQUENCIES = foreach GROUPEDLEVELS generate group as LOGLEVEL, COUNT(FILTEREDLEVELS.LOGLEVEL) as COUNT;\n        RESULT = order FREQUENCIES by COUNT desc;\n        DUMP RESULT;\n\n5. Use the following to run the **pigbatch.pig** file by using the Pig command.\n\n        pig ~/pigbatch.pig\n\n    Once the batch job finishes, you should see the following output, which should be the same as when you used `DUMP RESULT;` in the previous steps.\n\n        (TRACE,816)\n        (DEBUG,434)\n        (INFO,96)\n        (WARN,11)\n        (ERROR,6)\n        (FATAL,2)\n\n##<a id=\"summary\"></a>Summary\n\nAs you can see, the Pig command allows you to interactively run MapReduce operations by using Pig Latin, as well as run statements stored in a batch file.\n\n##<a id=\"nextsteps\"></a>Next steps\n\nFor general information on Pig in HDInsight.\n\n* [Use Pig with Hadoop on HDInsight](hdinsight-use-pig.md)\n\nFor information on other ways you can work with Hadoop on HDInsight.\n\n* [Use Hive with Hadoop on HDInsight](hdinsight-use-hive.md)\n\n* [Use MapReduce with Hadoop on HDInsight](hdinsight-use-mapreduce.md)\n"
}