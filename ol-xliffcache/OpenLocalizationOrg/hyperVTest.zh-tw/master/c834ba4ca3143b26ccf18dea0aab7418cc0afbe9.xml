{
  "nodes": [
    {
      "pos": [
        28,
        85
      ],
      "content": "What is the Cortana Analytics Process?  | Microsoft Azure",
      "nodes": [
        {
          "content": "What is the Cortana Analytics Process?",
          "pos": [
            0,
            38
          ]
        },
        {
          "content": "| Microsoft Azure",
          "pos": [
            40,
            57
          ]
        }
      ]
    },
    {
      "pos": [
        105,
        242
      ],
      "content": "The Cortana Analytics Process is a systematic data science method for building intelligent applications that leverage advanced analytics."
    },
    {
      "pos": [
        587,
        631
      ],
      "content": "What is the Cortana Analytics Process (CAP)?"
    },
    {
      "pos": [
        633,
        992
      ],
      "content": "The Cortana Analytics Process (CAP) is a systematic data science method that outlines a sequence of steps that leverages advanced analytics to build intelligent applications. The CAP steps provide <bpt id=\"p1\">**</bpt>guidance<ept id=\"p1\">**</ept><ph id=\"ph2\"/> on how to define the problem, analyze relevant data, build  and evaluate predictive models, and then deploy those models in intelligent applications.",
      "nodes": [
        {
          "content": "The Cortana Analytics Process (CAP) is a systematic data science method that outlines a sequence of steps that leverages advanced analytics to build intelligent applications.",
          "pos": [
            0,
            174
          ]
        },
        {
          "content": "The CAP steps provide <bpt id=\"p1\">**</bpt>guidance<ept id=\"p1\">**</ept><ph id=\"ph2\"/> on how to define the problem, analyze relevant data, build  and evaluate predictive models, and then deploy those models in intelligent applications.",
          "pos": [
            175,
            411
          ]
        }
      ]
    },
    {
      "pos": [
        995,
        1047
      ],
      "content": "Here are the steps in <bpt id=\"p2\">**</bpt>Cortana Analytics Process<ept id=\"p2\">**</ept>:"
    },
    {
      "pos": [
        1051,
        1152
      ],
      "content": "<ph id=\"ph3\">![</ph>CAP-workflow<ph id=\"ph4\">](./media/machine-learning-data-science-the-cortana-analytics-process/CAP-workflow.png)</ph>"
    },
    {
      "pos": [
        1154,
        1462
      ],
      "content": "The process is <bpt id=\"p3\">**</bpt>iterative<ept id=\"p3\">**</ept>: the understanding of new and existing or refinements in the model evolves and requires reworking steps previously completed in the sequence. Existing organizational development and project planning processes are <bpt id=\"p4\">**</bpt>easily adapted<ept id=\"p4\">**</ept><ph id=\"ph5\"/> to work with the CAP-defined sequence of steps.",
      "nodes": [
        {
          "content": "The process is <bpt id=\"p3\">**</bpt>iterative<ept id=\"p3\">**</ept>: the understanding of new and existing or refinements in the model evolves and requires reworking steps previously completed in the sequence.",
          "pos": [
            0,
            208
          ]
        },
        {
          "content": "Existing organizational development and project planning processes are <bpt id=\"p4\">**</bpt>easily adapted<ept id=\"p4\">**</ept><ph id=\"ph5\"/> to work with the CAP-defined sequence of steps.",
          "pos": [
            209,
            398
          ]
        }
      ]
    },
    {
      "pos": [
        1465,
        1648
      ],
      "content": "The steps in the process are diagrammed and linked in the <bpt id=\"p5\">[</bpt>CAP learning path<ept id=\"p5\">](https://azure.microsoft.com/documentation/learning-paths/cortana-analytics-process/)</ept><ph id=\"ph6\"/> and described below."
    },
    {
      "pos": [
        1655,
        1672
      ],
      "content": "Preparation Steps"
    },
    {
      "pos": [
        1678,
        1708
      ],
      "content": "P1. Plan the analytics project",
      "nodes": [
        {
          "content": "P1.",
          "pos": [
            0,
            3
          ]
        },
        {
          "content": "Plan the analytics project",
          "pos": [
            4,
            30
          ]
        }
      ]
    },
    {
      "pos": [
        1711,
        2658
      ],
      "content": "Start an analytics project by defining your business goals and problems. They are specified in terms of <bpt id=\"p6\">**</bpt>business requirements<ept id=\"p6\">**</ept>. A central objective of this step is to identify the key business variables (sales forecast or the probability of an order being fraudulent, for example) that the analysis needs to predict to satisfy these requirements. Additional planning is then usually essential to develop an understanding of the <bpt id=\"p7\">**</bpt>data sources<ept id=\"p7\">**</ept><ph id=\"ph7\"/> needed to address the objectives of the project from an analytical perspective. It is not uncommon, for example, to find that existing systems need to collect and log additional kinds of data to address the problem and achieve the project goals. For guidance, see <bpt id=\"p8\">[</bpt>Plan your environment for the Cortana Analytics Process<ept id=\"p8\">](machine-learning-data-science-plan-your-environment.md)</ept><ph id=\"ph8\"/> and <bpt id=\"p9\">[</bpt>Scenarios for advanced analytics in Azure Machine Learning<ept id=\"p9\">](machine-learning-data-science-plan-sample-scenarios.md)</ept>.",
      "nodes": [
        {
          "content": "Start an analytics project by defining your business goals and problems.",
          "pos": [
            0,
            72
          ]
        },
        {
          "content": "They are specified in terms of <bpt id=\"p6\">**</bpt>business requirements<ept id=\"p6\">**</ept>.",
          "pos": [
            73,
            168
          ]
        },
        {
          "content": "A central objective of this step is to identify the key business variables (sales forecast or the probability of an order being fraudulent, for example) that the analysis needs to predict to satisfy these requirements.",
          "pos": [
            169,
            387
          ]
        },
        {
          "content": "Additional planning is then usually essential to develop an understanding of the <bpt id=\"p7\">**</bpt>data sources<ept id=\"p7\">**</ept><ph id=\"ph7\"/> needed to address the objectives of the project from an analytical perspective.",
          "pos": [
            388,
            617
          ]
        },
        {
          "content": "It is not uncommon, for example, to find that existing systems need to collect and log additional kinds of data to address the problem and achieve the project goals.",
          "pos": [
            618,
            783
          ]
        },
        {
          "content": "For guidance, see <bpt id=\"p8\">[</bpt>Plan your environment for the Cortana Analytics Process<ept id=\"p8\">](machine-learning-data-science-plan-your-environment.md)</ept><ph id=\"ph8\"/> and <bpt id=\"p9\">[</bpt>Scenarios for advanced analytics in Azure Machine Learning<ept id=\"p9\">](machine-learning-data-science-plan-sample-scenarios.md)</ept>.",
          "pos": [
            784,
            1127
          ]
        }
      ]
    },
    {
      "pos": [
        2665,
        2696
      ],
      "content": "P2. Setup analytics environment",
      "nodes": [
        {
          "content": "P2.",
          "pos": [
            0,
            3
          ]
        },
        {
          "content": "Setup analytics environment",
          "pos": [
            4,
            31
          ]
        }
      ]
    },
    {
      "pos": [
        2699,
        2786
      ],
      "content": "An analytics environment for the Cortana Analytics Process involves several components:"
    },
    {
      "pos": [
        2791,
        2862
      ],
      "content": "<bpt id=\"p10\">**</bpt>data workspaces<ept id=\"p10\">**</ept><ph id=\"ph9\"/> where the data is staged for analysis and modeling,"
    },
    {
      "pos": [
        2866,
        2950
      ],
      "content": "a <bpt id=\"p11\">**</bpt>processing infrastructure<ept id=\"p11\">**</ept><ph id=\"ph10\"/> for pre-processing, exploring, and modeling the data"
    },
    {
      "pos": [
        2953,
        3090
      ],
      "content": "a <bpt id=\"p12\">**</bpt>runtime infrastructure<ept id=\"p12\">**</ept><ph id=\"ph11\"/> to operationalize the analytical models and run the intelligent client applications that consume the models."
    },
    {
      "pos": [
        3094,
        3601
      ],
      "content": "The analytics infrastructure that needs to be setup is often part of an environment that is separate from core operational systems. But it typically leverages data from multiple systems within the enterprise as well as from sources external to the company. The analytics infrastructure can be purely cloud-based, or an on-premises setup, or a hybrid of the two. For options, see <bpt id=\"p13\">[</bpt>Set up data science environments for use in the Cortana Analytics Process<ept id=\"p13\">](machine-learning-data-science-environment-setup.md)</ept>.",
      "nodes": [
        {
          "content": "The analytics infrastructure that needs to be setup is often part of an environment that is separate from core operational systems.",
          "pos": [
            0,
            131
          ]
        },
        {
          "content": "But it typically leverages data from multiple systems within the enterprise as well as from sources external to the company.",
          "pos": [
            132,
            256
          ]
        },
        {
          "content": "The analytics infrastructure can be purely cloud-based, or an on-premises setup, or a hybrid of the two.",
          "pos": [
            257,
            361
          ]
        },
        {
          "content": "For options, see <bpt id=\"p13\">[</bpt>Set up data science environments for use in the Cortana Analytics Process<ept id=\"p13\">](machine-learning-data-science-environment-setup.md)</ept>.",
          "pos": [
            362,
            547
          ]
        }
      ]
    },
    {
      "pos": [
        3606,
        3622
      ],
      "content": "Analytics Steps:"
    },
    {
      "pos": [
        3629,
        3675
      ],
      "content": "1. Ingest Data into the analytical environment"
    },
    {
      "pos": [
        3678,
        4145
      ],
      "content": "The first step is to bring the relevant data from various sources, either from within or from outside the enterprise, into an analytic environments where the data can be processed. The <bpt id=\"p14\">**</bpt>format<ept id=\"p14\">**</ept><ph id=\"ph12\"/> of the data at source may differ from the format required by the destination. So some data transformation may also have to be done by the ingestion tooling. For options, see <bpt id=\"p15\">[</bpt>Load data into storage environments for analytics<ept id=\"p15\">](machine-learning-data-science-ingest-data.md)</ept>",
      "nodes": [
        {
          "content": "The first step is to bring the relevant data from various sources, either from within or from outside the enterprise, into an analytic environments where the data can be processed.",
          "pos": [
            0,
            180
          ]
        },
        {
          "content": "The <bpt id=\"p14\">**</bpt>format<ept id=\"p14\">**</ept><ph id=\"ph12\"/> of the data at source may differ from the format required by the destination.",
          "pos": [
            181,
            328
          ]
        },
        {
          "content": "So some data transformation may also have to be done by the ingestion tooling.",
          "pos": [
            329,
            407
          ]
        },
        {
          "content": "For options, see <bpt id=\"p15\">[</bpt>Load data into storage environments for analytics<ept id=\"p15\">](machine-learning-data-science-ingest-data.md)</ept>",
          "pos": [
            408,
            562
          ]
        }
      ]
    },
    {
      "pos": [
        4147,
        4699
      ],
      "content": "In addition to the initial ingestion of data, many  intelligent applications are required to refresh the data regularly as part of an ongoing learning process. This can be done by setting up a <bpt id=\"p16\">**</bpt>data pipeline<ept id=\"p16\">**</ept><ph id=\"ph13\"/> or workflow. This forms part of the iterative part of the process that includes rebuilding and re-evaluating the analytical models used by the intelligent application deploying the solution. See, for example, <bpt id=\"p17\">[</bpt>Move data from an on-premise SQL server to SQL Azure with Azure Data Factory<ept id=\"p17\">](machine-learning-data-science-move-sql-azure-adf.md)</ept>.",
      "nodes": [
        {
          "content": "In addition to the initial ingestion of data, many  intelligent applications are required to refresh the data regularly as part of an ongoing learning process.",
          "pos": [
            0,
            159
          ]
        },
        {
          "content": "This can be done by setting up a <bpt id=\"p16\">**</bpt>data pipeline<ept id=\"p16\">**</ept><ph id=\"ph13\"/> or workflow.",
          "pos": [
            160,
            278
          ]
        },
        {
          "content": "This forms part of the iterative part of the process that includes rebuilding and re-evaluating the analytical models used by the intelligent application deploying the solution.",
          "pos": [
            279,
            456
          ]
        },
        {
          "content": "See, for example, <bpt id=\"p17\">[</bpt>Move data from an on-premise SQL server to SQL Azure with Azure Data Factory<ept id=\"p17\">](machine-learning-data-science-move-sql-azure-adf.md)</ept>.",
          "pos": [
            457,
            647
          ]
        }
      ]
    },
    {
      "pos": [
        4705,
        4736
      ],
      "content": "2. Explore and pre-process data"
    },
    {
      "pos": [
        4739,
        5305
      ],
      "content": "The next step is to obtain a deeper understanding of the data by investigating its <bpt id=\"p18\">**</bpt>summary statistics<ept id=\"p18\">**</ept><ph id=\"ph14\"/> , relationships, and by using techniques such <bpt id=\"p19\">**</bpt>visualization<ept id=\"p19\">**</ept>. This is also where issues of <bpt id=\"p20\">**</bpt>data quality<ept id=\"p20\">**</ept><ph id=\"ph15\"/> and integrity, such as missing values, data type mismatches, and inconsistent data relationships, are handled. Pre-processing transforms are used to clean up the raw data before further analytics and modeling can take place. For a description, see <bpt id=\"p21\">[</bpt>Tasks to prepare data for enhanced machine learning<ept id=\"p21\">](machine-learning-data-science-prepare-data.md)</ept>.",
      "nodes": [
        {
          "content": "The next step is to obtain a deeper understanding of the data by investigating its <bpt id=\"p18\">**</bpt>summary statistics<ept id=\"p18\">**</ept><ph id=\"ph14\"/> , relationships, and by using techniques such <bpt id=\"p19\">**</bpt>visualization<ept id=\"p19\">**</ept>.",
          "pos": [
            0,
            265
          ]
        },
        {
          "content": "This is also where issues of <bpt id=\"p20\">**</bpt>data quality<ept id=\"p20\">**</ept><ph id=\"ph15\"/> and integrity, such as missing values, data type mismatches, and inconsistent data relationships, are handled.",
          "pos": [
            266,
            477
          ]
        },
        {
          "content": "Pre-processing transforms are used to clean up the raw data before further analytics and modeling can take place.",
          "pos": [
            478,
            591
          ]
        },
        {
          "content": "For a description, see <bpt id=\"p21\">[</bpt>Tasks to prepare data for enhanced machine learning<ept id=\"p21\">](machine-learning-data-science-prepare-data.md)</ept>.",
          "pos": [
            592,
            756
          ]
        }
      ]
    },
    {
      "pos": [
        5311,
        5330
      ],
      "content": "3. Develop Features"
    },
    {
      "pos": [
        5333,
        6035
      ],
      "content": "Data scientists, in collaboration with domain experts,  must identify the features that capture the salient properties of the data set and that can best be used to predict the key business variables identified during planning. These new features can be derived from existing data or may require additional data to be collected. This process is known as <bpt id=\"p22\">**</bpt>feature engineering<ept id=\"p22\">**</ept><ph id=\"ph16\"/> and is one of the key steps in building an effective predictive analytics system. This step requires a creative combination of domain expertise and the insights obtained from the data exploration step. For guidance, see <bpt id=\"p23\">[</bpt>Feature engineering in the Cortana Analytics Process<ept id=\"p23\">](machine-learning-data-science-create-features.md)</ept>.",
      "nodes": [
        {
          "content": "Data scientists, in collaboration with domain experts,  must identify the features that capture the salient properties of the data set and that can best be used to predict the key business variables identified during planning.",
          "pos": [
            0,
            226
          ]
        },
        {
          "content": "These new features can be derived from existing data or may require additional data to be collected.",
          "pos": [
            227,
            327
          ]
        },
        {
          "content": "This process is known as <bpt id=\"p22\">**</bpt>feature engineering<ept id=\"p22\">**</ept><ph id=\"ph16\"/> and is one of the key steps in building an effective predictive analytics system.",
          "pos": [
            328,
            513
          ]
        },
        {
          "content": "This step requires a creative combination of domain expertise and the insights obtained from the data exploration step.",
          "pos": [
            514,
            633
          ]
        },
        {
          "content": "For guidance, see <bpt id=\"p23\">[</bpt>Feature engineering in the Cortana Analytics Process<ept id=\"p23\">](machine-learning-data-science-create-features.md)</ept>.",
          "pos": [
            634,
            797
          ]
        }
      ]
    },
    {
      "pos": [
        6041,
        6068
      ],
      "content": "4. Create predictive models"
    },
    {
      "pos": [
        6071,
        6492
      ],
      "content": "Data scientists build analytical models to predict the key variables identified by the business requirements defined in the planning step using data that has been cleaned and featurized. Machine learning systems support multiple <bpt id=\"p24\">**</bpt>modeling algorithms<ept id=\"p24\">**</ept><ph id=\"ph17\"/> that are applicable to a wide variety of cases. For guidance, see <bpt id=\"p25\">[</bpt>How to choose algorithms for Microsoft Azure Machine Learning<ept id=\"p25\">](machine-learning-algorithm-choice,md)</ept>.",
      "nodes": [
        {
          "content": "Data scientists build analytical models to predict the key variables identified by the business requirements defined in the planning step using data that has been cleaned and featurized.",
          "pos": [
            0,
            186
          ]
        },
        {
          "content": "Machine learning systems support multiple <bpt id=\"p24\">**</bpt>modeling algorithms<ept id=\"p24\">**</ept><ph id=\"ph17\"/> that are applicable to a wide variety of cases.",
          "pos": [
            187,
            355
          ]
        },
        {
          "content": "For guidance, see <bpt id=\"p25\">[</bpt>How to choose algorithms for Microsoft Azure Machine Learning<ept id=\"p25\">](machine-learning-algorithm-choice,md)</ept>.",
          "pos": [
            356,
            516
          ]
        }
      ]
    },
    {
      "pos": [
        6494,
        6753
      ],
      "content": "Data scientists must choose the most appropriate model for their prediction task and it is not uncommon that results from multiple models need to be combined to obtain the best results. The input data for modeling is usually divided randomly into three parts:",
      "nodes": [
        {
          "content": "Data scientists must choose the most appropriate model for their prediction task and it is not uncommon that results from multiple models need to be combined to obtain the best results.",
          "pos": [
            0,
            185
          ]
        },
        {
          "content": "The input data for modeling is usually divided randomly into three parts:",
          "pos": [
            186,
            259
          ]
        }
      ]
    },
    {
      "pos": [
        6757,
        6777
      ],
      "content": "a training data set,"
    },
    {
      "pos": [
        6781,
        6802
      ],
      "content": "a validation data set"
    },
    {
      "pos": [
        6806,
        6824
      ],
      "content": "a testing data set"
    },
    {
      "pos": [
        6827,
        7329
      ],
      "content": "The models are built using the <bpt id=\"p26\">**</bpt>training data set<ept id=\"p26\">**</ept>. The optimal combination of models (with parameters tuned) is selected by running the models and measuring the prediction errors for the <bpt id=\"p27\">**</bpt>validation data set<ept id=\"p27\">**</ept>. Finally the <bpt id=\"p28\">**</bpt>test data set<ept id=\"p28\">**</ept><ph id=\"ph18\"/> is used to evaluate the performance of the chosen model on independent data that was not used to train or validate the model.  For procedures, see <bpt id=\"p29\">[</bpt>How to evaluate model performance in Azure Machine Learning<ept id=\"p29\">](machine-learning-evaluate-model-performance.md)</ept>.",
      "nodes": [
        {
          "content": "The models are built using the <bpt id=\"p26\">**</bpt>training data set<ept id=\"p26\">**</ept>.",
          "pos": [
            0,
            93
          ]
        },
        {
          "content": "The optimal combination of models (with parameters tuned) is selected by running the models and measuring the prediction errors for the <bpt id=\"p27\">**</bpt>validation data set<ept id=\"p27\">**</ept>.",
          "pos": [
            94,
            294
          ]
        },
        {
          "content": "Finally the <bpt id=\"p28\">**</bpt>test data set<ept id=\"p28\">**</ept><ph id=\"ph18\"/> is used to evaluate the performance of the chosen model on independent data that was not used to train or validate the model.",
          "pos": [
            295,
            505
          ]
        },
        {
          "content": "For procedures, see <bpt id=\"p29\">[</bpt>How to evaluate model performance in Azure Machine Learning<ept id=\"p29\">](machine-learning-evaluate-model-performance.md)</ept>.",
          "pos": [
            507,
            677
          ]
        }
      ]
    },
    {
      "pos": [
        7335,
        7363
      ],
      "content": "5. Deploy and Consume models"
    },
    {
      "pos": [
        7366,
        7929
      ],
      "content": "Once we have a set of models that perform well, they can be <bpt id=\"p30\">**</bpt>operationalized<ept id=\"p30\">**</ept><ph id=\"ph19\"/> for other applications to consume. Depending on the business requirements, predictions are made either in <bpt id=\"p31\">**</bpt>real time<ept id=\"p31\">**</ept><ph id=\"ph20\"/> or on a <bpt id=\"p32\">**</bpt>batch<ept id=\"p32\">**</ept><ph id=\"ph21\"/> basis. To be operationalized, the models have to be exposed with an <bpt id=\"p33\">**</bpt>open API interface<ept id=\"p33\">**</ept><ph id=\"ph22\"/> that is easily consumed from various applications such online website, spreadsheets, dashboards, or line of business and backend applications. See <bpt id=\"p34\">[</bpt>Deploy an Azure Machine Learning web service<ept id=\"p34\">](machine-learning-publish-a-machine-learning-web-service.md)</ept>.",
      "nodes": [
        {
          "content": "Once we have a set of models that perform well, they can be <bpt id=\"p30\">**</bpt>operationalized<ept id=\"p30\">**</ept><ph id=\"ph19\"/> for other applications to consume.",
          "pos": [
            0,
            169
          ]
        },
        {
          "content": "Depending on the business requirements, predictions are made either in <bpt id=\"p31\">**</bpt>real time<ept id=\"p31\">**</ept><ph id=\"ph20\"/> or on a <bpt id=\"p32\">**</bpt>batch<ept id=\"p32\">**</ept><ph id=\"ph21\"/> basis.",
          "pos": [
            170,
            389
          ]
        },
        {
          "content": "To be operationalized, the models have to be exposed with an <bpt id=\"p33\">**</bpt>open API interface<ept id=\"p33\">**</ept><ph id=\"ph22\"/> that is easily consumed from various applications such online website, spreadsheets, dashboards, or line of business and backend applications.",
          "pos": [
            390,
            671
          ]
        },
        {
          "content": "See <bpt id=\"p34\">[</bpt>Deploy an Azure Machine Learning web service<ept id=\"p34\">](machine-learning-publish-a-machine-learning-web-service.md)</ept>.",
          "pos": [
            672,
            823
          ]
        }
      ]
    },
    {
      "pos": [
        7934,
        7956
      ],
      "content": "Summary and next steps"
    },
    {
      "pos": [
        7958,
        8338
      ],
      "content": "The <bpt id=\"p35\">[</bpt>Cortana Analytics Process<ept id=\"p35\">](https://azure.microsoft.com/documentation/learning-paths/cortana-analytics-process/)</ept><ph id=\"ph23\"/> is modeled as a sequence of iterated steps that <bpt id=\"p36\">**</bpt>provide guidance<ept id=\"p36\">**</ept><ph id=\"ph24\"/> on the tasks needed to use advanced analytics to build  an intelligent applications. Each step also provides details on how to use various Microsoft technologies to complete the tasks described.",
      "nodes": [
        {
          "content": "The <bpt id=\"p35\">[</bpt>Cortana Analytics Process<ept id=\"p35\">](https://azure.microsoft.com/documentation/learning-paths/cortana-analytics-process/)</ept><ph id=\"ph23\"/> is modeled as a sequence of iterated steps that <bpt id=\"p36\">**</bpt>provide guidance<ept id=\"p36\">**</ept><ph id=\"ph24\"/> on the tasks needed to use advanced analytics to build  an intelligent applications.",
          "pos": [
            0,
            380
          ]
        },
        {
          "content": "Each step also provides details on how to use various Microsoft technologies to complete the tasks described.",
          "pos": [
            381,
            490
          ]
        }
      ]
    },
    {
      "pos": [
        8341,
        8717
      ],
      "content": "While CAP does not prescribe specific types of <bpt id=\"p37\">**</bpt>documentation<ept id=\"p37\">**</ept><ph id=\"ph25\"/> artifacts, it is a best practice to document the results of the data exploration, modeling and evaluation, and to save the pertinent code so that the analysis can iterated when required. This also allows reuse of the analytics work when working on other applications involving similar data and prediction tasks.",
      "nodes": [
        {
          "content": "While CAP does not prescribe specific types of <bpt id=\"p37\">**</bpt>documentation<ept id=\"p37\">**</ept><ph id=\"ph25\"/> artifacts, it is a best practice to document the results of the data exploration, modeling and evaluation, and to save the pertinent code so that the analysis can iterated when required.",
          "pos": [
            0,
            306
          ]
        },
        {
          "content": "This also allows reuse of the analytics work when working on other applications involving similar data and prediction tasks.",
          "pos": [
            307,
            431
          ]
        }
      ]
    },
    {
      "pos": [
        8719,
        9100
      ],
      "content": "Full end-to-end walkthroughs that demonstrate all the steps in the process for <bpt id=\"p38\">**</bpt>specific scenarios<ept id=\"p38\">**</ept><ph id=\"ph26\"/> are also provided. See <bpt id=\"p39\">[</bpt>The Cortana Analytics Process in action: using SQL Server<ept id=\"p39\">](machine-learning-data-science-process-sql-walkthrough.md)</ept><ph id=\"ph27\"/> and <bpt id=\"p40\">[</bpt>The Cortana Analytics Process in action: using HDInsight Hadoop clusters<ept id=\"p40\">](machine-learning-data-science-process-hive-walkthrough.md)</ept>.",
      "nodes": [
        {
          "content": "Full end-to-end walkthroughs that demonstrate all the steps in the process for <bpt id=\"p38\">**</bpt>specific scenarios<ept id=\"p38\">**</ept><ph id=\"ph26\"/> are also provided.",
          "pos": [
            0,
            175
          ]
        },
        {
          "content": "See <bpt id=\"p39\">[</bpt>The Cortana Analytics Process in action: using SQL Server<ept id=\"p39\">](machine-learning-data-science-process-sql-walkthrough.md)</ept><ph id=\"ph27\"/> and <bpt id=\"p40\">[</bpt>The Cortana Analytics Process in action: using HDInsight Hadoop clusters<ept id=\"p40\">](machine-learning-data-science-process-hive-walkthrough.md)</ept>.",
          "pos": [
            176,
            531
          ]
        }
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"What is the Cortana Analytics Process?  | Microsoft Azure\" \n    description=\"The Cortana Analytics Process is a systematic data science method for building intelligent applications that leverage advanced analytics.\" \n    services=\"machine-learning\" \n    documentationCenter=\"\" \n    authors=\"bradsev\"\n    manager=\"paulettm\" \n    editor=\"cgronlun\" />\n\n<tags \n    ms.service=\"machine-learning\" \n    ms.workload=\"data-services\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"02/08/2016\" \n    ms.author=\"bradsev;gopitk\" /> \n\n\n# What is the Cortana Analytics Process (CAP)?\n\nThe Cortana Analytics Process (CAP) is a systematic data science method that outlines a sequence of steps that leverages advanced analytics to build intelligent applications. The CAP steps provide **guidance** on how to define the problem, analyze relevant data, build  and evaluate predictive models, and then deploy those models in intelligent applications. \n\nHere are the steps in **Cortana Analytics Process**:  \n\n![CAP-workflow](./media/machine-learning-data-science-the-cortana-analytics-process/CAP-workflow.png)\n\nThe process is **iterative**: the understanding of new and existing or refinements in the model evolves and requires reworking steps previously completed in the sequence. Existing organizational development and project planning processes are **easily adapted** to work with the CAP-defined sequence of steps. \n\nThe steps in the process are diagrammed and linked in the [CAP learning path](https://azure.microsoft.com/documentation/learning-paths/cortana-analytics-process/) and described below.  \n\n## Preparation Steps \n\n## P1. Plan the analytics project \n\nStart an analytics project by defining your business goals and problems. They are specified in terms of **business requirements**. A central objective of this step is to identify the key business variables (sales forecast or the probability of an order being fraudulent, for example) that the analysis needs to predict to satisfy these requirements. Additional planning is then usually essential to develop an understanding of the **data sources** needed to address the objectives of the project from an analytical perspective. It is not uncommon, for example, to find that existing systems need to collect and log additional kinds of data to address the problem and achieve the project goals. For guidance, see [Plan your environment for the Cortana Analytics Process](machine-learning-data-science-plan-your-environment.md) and [Scenarios for advanced analytics in Azure Machine Learning](machine-learning-data-science-plan-sample-scenarios.md).  \n\n## P2. Setup analytics environment \n\nAn analytics environment for the Cortana Analytics Process involves several components: \n\n- **data workspaces** where the data is staged for analysis and modeling, \n- a **processing infrastructure** for pre-processing, exploring, and modeling the data\n- a **runtime infrastructure** to operationalize the analytical models and run the intelligent client applications that consume the models.  \n\nThe analytics infrastructure that needs to be setup is often part of an environment that is separate from core operational systems. But it typically leverages data from multiple systems within the enterprise as well as from sources external to the company. The analytics infrastructure can be purely cloud-based, or an on-premises setup, or a hybrid of the two. For options, see [Set up data science environments for use in the Cortana Analytics Process](machine-learning-data-science-environment-setup.md).\n\n## Analytics Steps:  \n\n## 1. Ingest Data into the analytical environment \n\nThe first step is to bring the relevant data from various sources, either from within or from outside the enterprise, into an analytic environments where the data can be processed. The **format** of the data at source may differ from the format required by the destination. So some data transformation may also have to be done by the ingestion tooling. For options, see [Load data into storage environments for analytics](machine-learning-data-science-ingest-data.md)\n\nIn addition to the initial ingestion of data, many  intelligent applications are required to refresh the data regularly as part of an ongoing learning process. This can be done by setting up a **data pipeline** or workflow. This forms part of the iterative part of the process that includes rebuilding and re-evaluating the analytical models used by the intelligent application deploying the solution. See, for example, [Move data from an on-premise SQL server to SQL Azure with Azure Data Factory](machine-learning-data-science-move-sql-azure-adf.md).\n\n\n## 2. Explore and pre-process data \n\nThe next step is to obtain a deeper understanding of the data by investigating its **summary statistics** , relationships, and by using techniques such **visualization**. This is also where issues of **data quality** and integrity, such as missing values, data type mismatches, and inconsistent data relationships, are handled. Pre-processing transforms are used to clean up the raw data before further analytics and modeling can take place. For a description, see [Tasks to prepare data for enhanced machine learning](machine-learning-data-science-prepare-data.md).\n\n\n## 3. Develop Features \n\nData scientists, in collaboration with domain experts,  must identify the features that capture the salient properties of the data set and that can best be used to predict the key business variables identified during planning. These new features can be derived from existing data or may require additional data to be collected. This process is known as **feature engineering** and is one of the key steps in building an effective predictive analytics system. This step requires a creative combination of domain expertise and the insights obtained from the data exploration step. For guidance, see [Feature engineering in the Cortana Analytics Process](machine-learning-data-science-create-features.md).\n\n\n## 4. Create predictive models \n\nData scientists build analytical models to predict the key variables identified by the business requirements defined in the planning step using data that has been cleaned and featurized. Machine learning systems support multiple **modeling algorithms** that are applicable to a wide variety of cases. For guidance, see [How to choose algorithms for Microsoft Azure Machine Learning](machine-learning-algorithm-choice,md).\n\nData scientists must choose the most appropriate model for their prediction task and it is not uncommon that results from multiple models need to be combined to obtain the best results. The input data for modeling is usually divided randomly into three parts:\n\n- a training data set, \n- a validation data set \n- a testing data set \n\nThe models are built using the **training data set**. The optimal combination of models (with parameters tuned) is selected by running the models and measuring the prediction errors for the **validation data set**. Finally the **test data set** is used to evaluate the performance of the chosen model on independent data that was not used to train or validate the model.  For procedures, see [How to evaluate model performance in Azure Machine Learning](machine-learning-evaluate-model-performance.md).\n\n\n## 5. Deploy and Consume models \n\nOnce we have a set of models that perform well, they can be **operationalized** for other applications to consume. Depending on the business requirements, predictions are made either in **real time** or on a **batch** basis. To be operationalized, the models have to be exposed with an **open API interface** that is easily consumed from various applications such online website, spreadsheets, dashboards, or line of business and backend applications. See [Deploy an Azure Machine Learning web service](machine-learning-publish-a-machine-learning-web-service.md).\n\n## Summary and next steps\n\nThe [Cortana Analytics Process](https://azure.microsoft.com/documentation/learning-paths/cortana-analytics-process/) is modeled as a sequence of iterated steps that **provide guidance** on the tasks needed to use advanced analytics to build  an intelligent applications. Each step also provides details on how to use various Microsoft technologies to complete the tasks described. \n\nWhile CAP does not prescribe specific types of **documentation** artifacts, it is a best practice to document the results of the data exploration, modeling and evaluation, and to save the pertinent code so that the analysis can iterated when required. This also allows reuse of the analytics work when working on other applications involving similar data and prediction tasks.\n\nFull end-to-end walkthroughs that demonstrate all the steps in the process for **specific scenarios** are also provided. See [The Cortana Analytics Process in action: using SQL Server](machine-learning-data-science-process-sql-walkthrough.md) and [The Cortana Analytics Process in action: using HDInsight Hadoop clusters](machine-learning-data-science-process-hive-walkthrough.md).\n\n \n"
}