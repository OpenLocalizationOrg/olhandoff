{
  "nodes": [
    {
      "pos": [
        28,
        80
      ],
      "content": "Sample Data in SQL Server on Azure | Microsoft Azure"
    },
    {
      "pos": [
        100,
        134
      ],
      "content": "Sample Data in SQL Server on Azure"
    },
    {
      "pos": [
        483,
        484
      ],
      "content": "#"
    },
    {
      "pos": [
        506,
        540
      ],
      "content": "Sample data in SQL Server on Azure"
    },
    {
      "pos": [
        546,
        558
      ],
      "content": "Introduction"
    },
    {
      "pos": [
        560,
        859
      ],
      "content": "This document shows how to sample data stored in SQL Server on Azure using either SQL or the Python Programming Language. It also shows how to move sampled data into Azure Machine Learning by saving it to a file, uploading it to an Azure blob, and then reading it into Azure Machine Learning Studio.",
      "nodes": [
        {
          "content": "This document shows how to sample data stored in SQL Server on Azure using either SQL or the Python Programming Language.",
          "pos": [
            0,
            121
          ]
        },
        {
          "content": "It also shows how to move sampled data into Azure Machine Learning by saving it to a file, uploading it to an Azure blob, and then reading it into Azure Machine Learning Studio.",
          "pos": [
            122,
            299
          ]
        }
      ]
    },
    {
      "pos": [
        861,
        1051
      ],
      "content": "The Python sampling uses the <bpt id=\"p1\">[</bpt>pyodbc<ept id=\"p1\">](https://code.google.com/p/pyodbc/)</ept><ph id=\"ph2\"/> ODBC library to connect to SQL Server on Azure and the<bpt id=\"p2\">[</bpt>Pandas<ept id=\"p2\">](http://pandas.pydata.org/)</ept><ph id=\"ph3\"/> library to do the sampling."
    },
    {
      "pos": [
        1054,
        1357
      ],
      "content": "<ph id=\"ph4\">[AZURE.NOTE]</ph><ph id=\"ph5\"/> The sample SQL code in this document assumes that the data is in a SQL Server on Azure. If it is not, please refer to <bpt id=\"p3\">[</bpt>Move data to SQL Server on Azure<ept id=\"p3\">](machine-learning-data-science-move-sql-server-virtual-machine.md)</ept><ph id=\"ph6\"/> topic for instructions on how to move your data to SQL Server on Azure.",
      "nodes": [
        {
          "content": "<ph id=\"ph4\">[AZURE.NOTE]</ph><ph id=\"ph5\"/> The sample SQL code in this document assumes that the data is in a SQL Server on Azure.",
          "pos": [
            0,
            132
          ]
        },
        {
          "content": "If it is not, please refer to <bpt id=\"p3\">[</bpt>Move data to SQL Server on Azure<ept id=\"p3\">](machine-learning-data-science-move-sql-server-virtual-machine.md)</ept><ph id=\"ph6\"/> topic for instructions on how to move your data to SQL Server on Azure.",
          "pos": [
            133,
            387
          ]
        }
      ]
    },
    {
      "pos": [
        1359,
        1759
      ],
      "content": "<bpt id=\"p4\">**</bpt>Why sample your data?<ept id=\"p4\">**</ept>\nIf the dataset you plan to analyze is large, it is usually a good idea to down-sample the data to reduce it to a smaller but representative and more manageable size. This facilitates data understanding, exploration, and feature engineering. Its role in the Cortana Analytics Process is to enable fast prototyping of the data processing functions and machine learning models.",
      "nodes": [
        {
          "content": "<bpt id=\"p4\">**</bpt>Why sample your data?<ept id=\"p4\">**</ept>\nIf the dataset you plan to analyze is large, it is usually a good idea to down-sample the data to reduce it to a smaller but representative and more manageable size.",
          "pos": [
            0,
            229
          ]
        },
        {
          "content": "This facilitates data understanding, exploration, and feature engineering.",
          "pos": [
            230,
            304
          ]
        },
        {
          "content": "Its role in the Cortana Analytics Process is to enable fast prototyping of the data processing functions and machine learning models.",
          "pos": [
            305,
            438
          ]
        }
      ]
    },
    {
      "pos": [
        1761,
        1863
      ],
      "content": "The <bpt id=\"p5\">**</bpt>menu<ept id=\"p5\">**</ept><ph id=\"ph7\"/> below links to topics that describe how to sample data from various storage environments."
    },
    {
      "pos": [
        1954,
        2109
      ],
      "content": "This sampling task is a step in the <bpt id=\"p6\">[</bpt>Cortana Analytics Process (CAP)<ept id=\"p6\">](https://azure.microsoft.com/documentation/learning-paths/cortana-analytics-process/)</ept>."
    },
    {
      "pos": [
        2111,
        2113
      ],
      "content": "##"
    },
    {
      "pos": [
        2131,
        2140
      ],
      "content": "Using SQL"
    },
    {
      "pos": [
        2142,
        2327
      ],
      "content": "This section describes several methods using SQL to perform simple random sampling against the data in the database. Please choose a method based on your data size and its distribution.",
      "nodes": [
        {
          "content": "This section describes several methods using SQL to perform simple random sampling against the data in the database.",
          "pos": [
            0,
            116
          ]
        },
        {
          "content": "Please choose a method based on your data size and its distribution.",
          "pos": [
            117,
            185
          ]
        }
      ]
    },
    {
      "pos": [
        2329,
        2560
      ],
      "content": "The two items below show how to use newid in SQL Server to perform the sampling. The method you choose depends on how random you want the sample to be (pk_id in the sample code below is assumed to be an auto-generated primary key).",
      "nodes": [
        {
          "content": "The two items below show how to use newid in SQL Server to perform the sampling.",
          "pos": [
            0,
            80
          ]
        },
        {
          "content": "The method you choose depends on how random you want the sample to be (pk_id in the sample code below is assumed to be an auto-generated primary key).",
          "pos": [
            81,
            231
          ]
        }
      ]
    },
    {
      "pos": [
        2565,
        2590
      ],
      "content": "Less strict random sample"
    },
    {
      "pos": [
        2737,
        2755
      ],
      "content": "More random sample"
    },
    {
      "pos": [
        2902,
        3136
      ],
      "content": "Tablesample can be used for sampling as well as demonstrated below. This may be a better approach if your data size is large (assuming that data on different pages is not correlated) and for the query to complete in a reasonable time.",
      "nodes": [
        {
          "content": "Tablesample can be used for sampling as well as demonstrated below.",
          "pos": [
            0,
            67
          ]
        },
        {
          "content": "This may be a better approach if your data size is large (assuming that data on different pages is not correlated) and for the query to complete in a reasonable time.",
          "pos": [
            68,
            234
          ]
        }
      ]
    },
    {
      "pos": [
        3205,
        3307
      ],
      "content": "<ph id=\"ph9\">[AZURE.NOTE]</ph><ph id=\"ph10\"/> You can explore and generate features from this sampled data by storing it in a new table"
    },
    {
      "pos": [
        3310,
        3313
      ],
      "content": "###"
    },
    {
      "pos": [
        3335,
        3371
      ],
      "content": "Connecting to Azure Machine Learning"
    },
    {
      "pos": [
        3373,
        3608
      ],
      "content": "You can directly  use the sample queries above in the Azure ML Reader module to down-sample the data on the fly and bring it into an Azure ML experiment. A screen shot of using the reader module to read the sampled data is shown below:",
      "nodes": [
        {
          "content": "You can directly  use the sample queries above in the Azure ML Reader module to down-sample the data on the fly and bring it into an Azure ML experiment.",
          "pos": [
            0,
            153
          ]
        },
        {
          "content": "A screen shot of using the reader module to read the sampled data is shown below:",
          "pos": [
            154,
            235
          ]
        }
      ]
    },
    {
      "pos": [
        3613,
        3629
      ],
      "content": "<ph id=\"ph11\">![</ph>reader sql<ph id=\"ph12\">][1]</ph>"
    },
    {
      "pos": [
        3631,
        3633
      ],
      "content": "##"
    },
    {
      "pos": [
        3654,
        3691
      ],
      "content": "Using the Python programming language"
    },
    {
      "pos": [
        3694,
        3969
      ],
      "content": "This section demonstrates using the <bpt id=\"p7\">[</bpt>pyodbc library<ept id=\"p7\">](https://code.google.com/p/pyodbc/)</ept><ph id=\"ph13\"/> to establish an ODBC connect to a SQL server database in Python. The database connection string is as follows: (replace servername, dbname, username and password with your configuration):",
      "nodes": [
        {
          "content": "This section demonstrates using the <bpt id=\"p7\">[</bpt>pyodbc library<ept id=\"p7\">](https://code.google.com/p/pyodbc/)</ept><ph id=\"ph13\"/> to establish an ODBC connect to a SQL server database in Python.",
          "pos": [
            0,
            205
          ]
        },
        {
          "content": "The database connection string is as follows: (replace servername, dbname, username and password with your configuration):",
          "pos": [
            206,
            328
          ]
        }
      ]
    },
    {
      "pos": [
        4147,
        4416
      ],
      "content": "The <bpt id=\"p8\">[</bpt>Pandas<ept id=\"p8\">](http://pandas.pydata.org/)</ept><ph id=\"ph14\"/> library in Python provides a rich set of data structures and data analysis tools for data manipulation for Python programming. The code below reads a 0.1% sample of the data from a table in Azure SQL database into a Pandas data :",
      "nodes": [
        {
          "content": "The <bpt id=\"p8\">[</bpt>Pandas<ept id=\"p8\">](http://pandas.pydata.org/)</ept><ph id=\"ph14\"/> library in Python provides a rich set of data structures and data analysis tools for data manipulation for Python programming.",
          "pos": [
            0,
            219
          ]
        },
        {
          "content": "The code below reads a 0.1% sample of the data from a table in Azure SQL database into a Pandas data :",
          "pos": [
            220,
            322
          ]
        }
      ]
    },
    {
      "pos": [
        4629,
        4693
      ],
      "content": "You can now work with the sampled data in the Pandas data frame."
    },
    {
      "pos": [
        4696,
        4699
      ],
      "content": "###"
    },
    {
      "pos": [
        4724,
        4760
      ],
      "content": "Connecting to Azure Machine Learning"
    },
    {
      "pos": [
        4762,
        4994
      ],
      "content": "You can use the following sample code to save the down-sampled data to a file and upload it to an Azure blob. The data in the blob can be directly read into an Azure ML Experiment using the <bpt id=\"p9\">*</bpt>Reader Module<ept id=\"p9\">*</ept>. The steps are as follows:",
      "nodes": [
        {
          "content": "You can use the following sample code to save the down-sampled data to a file and upload it to an Azure blob.",
          "pos": [
            0,
            109
          ]
        },
        {
          "content": "The data in the blob can be directly read into an Azure ML Experiment using the <bpt id=\"p9\">*</bpt>Reader Module<ept id=\"p9\">*</ept>.",
          "pos": [
            110,
            244
          ]
        },
        {
          "content": "The steps are as follows:",
          "pos": [
            245,
            270
          ]
        }
      ]
    },
    {
      "pos": [
        5000,
        5043
      ],
      "content": "Write the pandas data frame to a local file"
    },
    {
      "pos": [
        5156,
        5187
      ],
      "content": "Upload local file to Azure blob"
    },
    {
      "pos": [
        5954,
        6045
      ],
      "content": "Read data from Azure blob using Azure ML <bpt id=\"p10\">*</bpt>Reader Module<ept id=\"p10\">*</ept><ph id=\"ph15\"/> as shown in the screen grab below:"
    },
    {
      "pos": [
        6048,
        6065
      ],
      "content": "<ph id=\"ph16\">![</ph>reader blob<ph id=\"ph17\">][2]</ph>"
    },
    {
      "pos": [
        6070,
        6117
      ],
      "content": "The Cortana Analytics Process in Action example"
    },
    {
      "pos": [
        6119,
        6333
      ],
      "content": "For an end-to-end walkthrough example of the Cortana Analytics Process a using a public dataset, see <bpt id=\"p11\">[</bpt>Cortana Analytics Process in Action: using SQL Sever<ept id=\"p11\">](machine-learning-data-science-process-sql-walkthrough.md)</ept>."
    }
  ],
  "content": "<properties \n    pageTitle=\"Sample Data in SQL Server on Azure | Microsoft Azure\" \n    description=\"Sample Data in SQL Server on Azure\" \n    services=\"machine-learning\" \n    documentationCenter=\"\" \n    authors=\"bradsev\" \n    manager=\"paulettm\" \n    editor=\"cgronlun\" />\n\n<tags \n    ms.service=\"machine-learning\" \n    ms.workload=\"data-services\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"02/07/2016\" \n    ms.author=\"fashah;garye;bradsev\" /> \n\n#<a name=\"heading\"></a>Sample data in SQL Server on Azure\n\n\n## Introduction\n\nThis document shows how to sample data stored in SQL Server on Azure using either SQL or the Python Programming Language. It also shows how to move sampled data into Azure Machine Learning by saving it to a file, uploading it to an Azure blob, and then reading it into Azure Machine Learning Studio.\n\nThe Python sampling uses the [pyodbc](https://code.google.com/p/pyodbc/) ODBC library to connect to SQL Server on Azure and the[Pandas](http://pandas.pydata.org/) library to do the sampling.\n\n>[AZURE.NOTE] The sample SQL code in this document assumes that the data is in a SQL Server on Azure. If it is not, please refer to [Move data to SQL Server on Azure](machine-learning-data-science-move-sql-server-virtual-machine.md) topic for instructions on how to move your data to SQL Server on Azure.\n\n**Why sample your data?**\nIf the dataset you plan to analyze is large, it is usually a good idea to down-sample the data to reduce it to a smaller but representative and more manageable size. This facilitates data understanding, exploration, and feature engineering. Its role in the Cortana Analytics Process is to enable fast prototyping of the data processing functions and machine learning models.\n\nThe **menu** below links to topics that describe how to sample data from various storage environments. \n\n[AZURE.INCLUDE [cap-sample-data-selector](../../includes/cap-sample-data-selector.md)]\n\nThis sampling task is a step in the [Cortana Analytics Process (CAP)](https://azure.microsoft.com/documentation/learning-paths/cortana-analytics-process/).\n\n##<a name=\"SQL\"></a>Using SQL\n\nThis section describes several methods using SQL to perform simple random sampling against the data in the database. Please choose a method based on your data size and its distribution.\n\nThe two items below show how to use newid in SQL Server to perform the sampling. The method you choose depends on how random you want the sample to be (pk_id in the sample code below is assumed to be an auto-generated primary key).\n\n1. Less strict random sample\n\n        select  * from <table_name> where <primary_key> in \n        (select top 10 percent <primary_key> from <table_name> order by newid())\n\n2. More random sample \n\n        SELECT * FROM <table_name>\n        WHERE 0.1 >= CAST(CHECKSUM(NEWID(), <primary_key>) & 0x7fffffff AS float)/ CAST (0x7fffffff AS int)\n\nTablesample can be used for sampling as well as demonstrated below. This may be a better approach if your data size is large (assuming that data on different pages is not correlated) and for the query to complete in a reasonable time.\n\n    SELECT *\n    FROM <table_name> \n    TABLESAMPLE (10 PERCENT)\n\n>[AZURE.NOTE] You can explore and generate features from this sampled data by storing it in a new table\n\n\n###<a name=\"sql-aml\"></a>Connecting to Azure Machine Learning\n\nYou can directly  use the sample queries above in the Azure ML Reader module to down-sample the data on the fly and bring it into an Azure ML experiment. A screen shot of using the reader module to read the sampled data is shown below:\n   \n![reader sql][1]\n\n##<a name=\"python\"></a>Using the Python programming language \n\nThis section demonstrates using the [pyodbc library](https://code.google.com/p/pyodbc/) to establish an ODBC connect to a SQL server database in Python. The database connection string is as follows: (replace servername, dbname, username and password with your configuration):\n\n    #Set up the SQL Azure connection\n    import pyodbc   \n    conn = pyodbc.connect('DRIVER={SQL Server};SERVER=<servername>;DATABASE=<dbname>;UID=<username>;PWD=<password>')\n\nThe [Pandas](http://pandas.pydata.org/) library in Python provides a rich set of data structures and data analysis tools for data manipulation for Python programming. The code below reads a 0.1% sample of the data from a table in Azure SQL database into a Pandas data :\n\n    import pandas as pd\n\n    # Query database and load the returned results in pandas data frame\n    data_frame = pd.read_sql('''select column1, cloumn2... from <table_name> tablesample (0.1 percent)''', conn)\n\nYou can now work with the sampled data in the Pandas data frame. \n\n###<a name=\"python-aml\"></a>Connecting to Azure Machine Learning\n\nYou can use the following sample code to save the down-sampled data to a file and upload it to an Azure blob. The data in the blob can be directly read into an Azure ML Experiment using the *Reader Module*. The steps are as follows: \n\n1. Write the pandas data frame to a local file\n\n        dataframe.to_csv(os.path.join(os.getcwd(),LOCALFILENAME), sep='\\t', encoding='utf-8', index=False)\n\n2. Upload local file to Azure blob\n\n        from azure.storage import BlobService\n        import tables\n\n        STORAGEACCOUNTNAME= <storage_account_name>\n        LOCALFILENAME= <local_file_name>\n        STORAGEACCOUNTKEY= <storage_account_key>\n        CONTAINERNAME= <container_name>\n        BLOBNAME= <blob_name>\n\n        output_blob_service=BlobService(account_name=STORAGEACCOUNTNAME,account_key=STORAGEACCOUNTKEY)    \n        localfileprocessed = os.path.join(os.getcwd(),LOCALFILENAME) #assuming file is in current working directory\n        \n        try:\n       \n        #perform upload\n        output_blob_service.put_block_blob_from_path(CONTAINERNAME,BLOBNAME,localfileprocessed)\n        \n        except:         \n            print (\"Something went wrong with uploading blob:\"+BLOBNAME)\n\n3. Read data from Azure blob using Azure ML *Reader Module* as shown in the screen grab below:\n \n![reader blob][2]\n\n## The Cortana Analytics Process in Action example\n\nFor an end-to-end walkthrough example of the Cortana Analytics Process a using a public dataset, see [Cortana Analytics Process in Action: using SQL Sever](machine-learning-data-science-process-sql-walkthrough.md).\n\n[1]: ./media/machine-learning-data-science-sample-sql-server-virtual-machine/reader_database.png\n[2]: ./media/machine-learning-data-science-sample-sql-server-virtual-machine/reader_blob.png\n\n "
}