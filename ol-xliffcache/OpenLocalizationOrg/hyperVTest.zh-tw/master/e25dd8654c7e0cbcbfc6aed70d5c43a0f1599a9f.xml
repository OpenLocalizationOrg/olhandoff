{
  "nodes": [
    {
      "pos": [
        28,
        118
      ],
      "content": "Submit Hive Queries to Hadoop clusters in the Cortana Analytics Process  | Microsoft Azure"
    },
    {
      "pos": [
        138,
        167
      ],
      "content": "Process Data from Hive Tables"
    },
    {
      "pos": [
        515,
        516
      ],
      "content": "#"
    },
    {
      "pos": [
        539,
        620
      ],
      "content": "Submit Hive Queries to HDInsight Hadoop clusters in the Cortana Analytics Process"
    },
    {
      "pos": [
        622,
        795
      ],
      "content": "This document describes various ways of submitting Hive queries to Hadoop clusters that are managed by an HDInsight service in Azure. Hive queries can be submitted by using:",
      "nodes": [
        {
          "content": "This document describes various ways of submitting Hive queries to Hadoop clusters that are managed by an HDInsight service in Azure.",
          "pos": [
            0,
            133
          ]
        },
        {
          "content": "Hive queries can be submitted by using:",
          "pos": [
            134,
            173
          ]
        }
      ]
    },
    {
      "pos": [
        800,
        854
      ],
      "content": "the Hadoop Command Line on the headnode of the cluster"
    },
    {
      "pos": [
        857,
        877
      ],
      "content": "the IPython Notebook"
    },
    {
      "pos": [
        881,
        896
      ],
      "content": "the Hive Editor"
    },
    {
      "pos": [
        899,
        923
      ],
      "content": "Azure PowerShell scripts"
    },
    {
      "pos": [
        926,
        1074
      ],
      "content": "Generic Hive queries that can be used to explore the data or to generate features that use embedded Hive User Defined Functions (UDFs) are provided."
    },
    {
      "pos": [
        1077,
        1453
      ],
      "content": "Examples of queries the specific to <bpt id=\"p1\">[</bpt>NYC Taxi Trip Data<ept id=\"p1\">](http://chriswhong.com/open-data/foil_nyc_taxi/)</ept><ph id=\"ph2\"/> scenarios are also provided in <bpt id=\"p2\">[</bpt>Github repository<ept id=\"p2\">](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/DataScienceProcess/DataScienceScripts)</ept>. These queries already have data schema specified and are ready to be submitted to run for this scenario.",
      "nodes": [
        {
          "content": "Examples of queries the specific to <bpt id=\"p1\">[</bpt>NYC Taxi Trip Data<ept id=\"p1\">](http://chriswhong.com/open-data/foil_nyc_taxi/)</ept><ph id=\"ph2\"/> scenarios are also provided in <bpt id=\"p2\">[</bpt>Github repository<ept id=\"p2\">](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/DataScienceProcess/DataScienceScripts)</ept>.",
          "pos": [
            0,
            361
          ]
        },
        {
          "content": "These queries already have data schema specified and are ready to be submitted to run for this scenario.",
          "pos": [
            362,
            466
          ]
        }
      ]
    },
    {
      "pos": [
        1456,
        1566
      ],
      "content": "In the final section, parameters that users can tune to improve the performance of Hive queries are discussed."
    },
    {
      "pos": [
        1571,
        1584
      ],
      "content": "Prerequisites"
    },
    {
      "pos": [
        1585,
        1620
      ],
      "content": "This article assumes that you have:"
    },
    {
      "pos": [
        1625,
        1773
      ],
      "content": "Created an Azure storage account. If you need instructions for this task, see <bpt id=\"p3\">[</bpt>Create an Azure Storage account<ept id=\"p3\">](../hdinsight-get-started.md#storage)</ept>",
      "nodes": [
        {
          "content": "Created an Azure storage account.",
          "pos": [
            0,
            33
          ]
        },
        {
          "content": "If you need instructions for this task, see <bpt id=\"p3\">[</bpt>Create an Azure Storage account<ept id=\"p3\">](../hdinsight-get-started.md#storage)</ept>",
          "pos": [
            34,
            186
          ]
        }
      ]
    },
    {
      "pos": [
        1777,
        1938
      ],
      "content": "Provisioned an Hadoop cluster with the HDInsight service.  If you need instructions, see <bpt id=\"p4\">[</bpt>Provision an HDInsight cluster<ept id=\"p4\">](../hdinsight-get-started.md#provision)</ept>.",
      "nodes": [
        {
          "content": "Provisioned an Hadoop cluster with the HDInsight service.",
          "pos": [
            0,
            57
          ]
        },
        {
          "content": "If you need instructions, see <bpt id=\"p4\">[</bpt>Provision an HDInsight cluster<ept id=\"p4\">](../hdinsight-get-started.md#provision)</ept>.",
          "pos": [
            59,
            199
          ]
        }
      ]
    },
    {
      "pos": [
        1941,
        2188
      ],
      "content": "Uploaded the data to Hive tables in Azure HDInsight Hadoop clusters. If it has not, please follow the instructions provided at <bpt id=\"p5\">[</bpt>Create and load data to Hive tables<ept id=\"p5\">](machine-learning-data-science-hive-tables.md)</ept><ph id=\"ph3\"/> to upload data to Hive tables first.",
      "nodes": [
        {
          "content": "Uploaded the data to Hive tables in Azure HDInsight Hadoop clusters.",
          "pos": [
            0,
            68
          ]
        },
        {
          "content": "If it has not, please follow the instructions provided at <bpt id=\"p5\">[</bpt>Create and load data to Hive tables<ept id=\"p5\">](machine-learning-data-science-hive-tables.md)</ept><ph id=\"ph3\"/> to upload data to Hive tables first.",
          "pos": [
            69,
            299
          ]
        }
      ]
    },
    {
      "pos": [
        2191,
        2372
      ],
      "content": "Enabled remote access to the cluster. If you need instructions, see <bpt id=\"p6\">[</bpt>Access the Head Node of Hadoop Cluster<ept id=\"p6\">](machine-learning-data-science-customize-hadoop-cluster.md#remoteaccess)</ept>.",
      "nodes": [
        {
          "content": "Enabled remote access to the cluster.",
          "pos": [
            0,
            37
          ]
        },
        {
          "content": "If you need instructions, see <bpt id=\"p6\">[</bpt>Access the Head Node of Hadoop Cluster<ept id=\"p6\">](machine-learning-data-science-customize-hadoop-cluster.md#remoteaccess)</ept>.",
          "pos": [
            38,
            219
          ]
        }
      ]
    },
    {
      "pos": [
        2400,
        2426
      ],
      "content": "How to submit Hive queries"
    },
    {
      "pos": [
        2431,
        2521
      ],
      "content": "<bpt id=\"p7\">[</bpt>Submit Hive queries through Hadoop Command Line in headnode of Hadoop cluster<ept id=\"p7\">](#headnode)</ept>"
    },
    {
      "pos": [
        2525,
        2581
      ],
      "content": "<bpt id=\"p8\">[</bpt>Submit Hive queries with the Hive Editor<ept id=\"p8\">](#hive-editor)</ept>"
    },
    {
      "pos": [
        2585,
        2642
      ],
      "content": "<bpt id=\"p9\">[</bpt>Submit Hive queries with Azure PowerShell Commands<ept id=\"p9\">](#ps)</ept>"
    },
    {
      "pos": [
        2645,
        2648
      ],
      "content": "###"
    },
    {
      "pos": [
        2672,
        2752
      ],
      "content": "1. Submit Hive queries through Hadoop Command Line in headnode of Hadoop cluster"
    },
    {
      "pos": [
        2754,
        2950
      ],
      "content": "If the Hive query is complex, submitting it directly in the head node of the Hadoop cluster typically leads to faster turn around than submitting it with a Hive Editor or Azure PowerShell scripts."
    },
    {
      "pos": [
        2953,
        3101
      ],
      "content": "Log in to the head node of the Hadoop cluster, open the Hadoop Command Line on the desktop of the head node, and enter command <ph id=\"ph4\">`cd %hive_home%\\bin`</ph>."
    },
    {
      "pos": [
        3103,
        3175
      ],
      "content": "Users have three ways to submit Hive queries in the Hadoop Command Line:"
    },
    {
      "pos": [
        3180,
        3188
      ],
      "content": "directly"
    },
    {
      "pos": [
        3191,
        3207
      ],
      "content": "using .hql files"
    },
    {
      "pos": [
        3210,
        3239
      ],
      "content": "with the Hive command console"
    },
    {
      "pos": [
        3246,
        3298
      ],
      "content": "Submit Hive queries directly in Hadoop Command Line."
    },
    {
      "pos": [
        3301,
        3567
      ],
      "content": "Users can run command like <ph id=\"ph5\">`hive -e \"&lt;your hive query&gt;;`</ph><ph id=\"ph6\"/> to submit simple Hive queries directly in Hadoop Command Line. Here is an example, where the red box outlines the command that submits the Hive query, and the green box outlines the output from the Hive query.",
      "nodes": [
        {
          "content": "Users can run command like <ph id=\"ph5\">`hive -e \"&lt;your hive query&gt;;`</ph><ph id=\"ph6\"/> to submit simple Hive queries directly in Hadoop Command Line.",
          "pos": [
            0,
            157
          ]
        },
        {
          "content": "Here is an example, where the red box outlines the command that submits the Hive query, and the green box outlines the output from the Hive query.",
          "pos": [
            158,
            304
          ]
        }
      ]
    },
    {
      "pos": [
        3569,
        3592
      ],
      "content": "<ph id=\"ph7\">![</ph>Create workspace<ph id=\"ph8\">][10]</ph>"
    },
    {
      "pos": [
        3599,
        3632
      ],
      "content": "Submit Hive queries in .hql files"
    },
    {
      "pos": [
        3634,
        4021
      ],
      "content": "When the Hive query is more complicated and has multiple lines, editing queries in command line or Hive command console is not practical. An alternative is to use a text editor in the head node of the Hadoop cluster to save the Hive queries in a .hql file in a local directory of the head node. Then the Hive query in the .hql file can be submitted by using the <ph id=\"ph9\">`-f`</ph><ph id=\"ph10\"/> argument as follows:",
      "nodes": [
        {
          "content": "When the Hive query is more complicated and has multiple lines, editing queries in command line or Hive command console is not practical.",
          "pos": [
            0,
            137
          ]
        },
        {
          "content": "An alternative is to use a text editor in the head node of the Hadoop cluster to save the Hive queries in a .hql file in a local directory of the head node.",
          "pos": [
            138,
            294
          ]
        },
        {
          "content": "Then the Hive query in the .hql file can be submitted by using the <ph id=\"ph9\">`-f`</ph><ph id=\"ph10\"/> argument as follows:",
          "pos": [
            295,
            420
          ]
        }
      ]
    },
    {
      "pos": [
        4068,
        4091
      ],
      "content": "<ph id=\"ph11\">![</ph>Create workspace<ph id=\"ph12\">][15]</ph>"
    },
    {
      "pos": [
        4094,
        4151
      ],
      "content": "<bpt id=\"p10\">**</bpt>Suppress progress status screen print of Hive queries<ept id=\"p10\">**</ept>"
    },
    {
      "pos": [
        4153,
        4426
      ],
      "content": "By default, after Hive query is submitted in Hadoop Command Line, the progress of the Map/Reduce job will be printed out on screen. To suppress the screen print of the Map/Reduce job progress, you can use an argument <ph id=\"ph13\">`-S`</ph><ph id=\"ph14\"/> (\"S\" in upper case) in the command line as follows:",
      "nodes": [
        {
          "content": "By default, after Hive query is submitted in Hadoop Command Line, the progress of the Map/Reduce job will be printed out on screen.",
          "pos": [
            0,
            131
          ]
        },
        {
          "content": "To suppress the screen print of the Map/Reduce job progress, you can use an argument <ph id=\"ph13\">`-S`</ph><ph id=\"ph14\"/> (\"S\" in upper case) in the command line as follows:",
          "pos": [
            132,
            307
          ]
        }
      ]
    },
    {
      "pos": [
        4507,
        4551
      ],
      "content": "Submit Hive queries in Hive command console."
    },
    {
      "pos": [
        4553,
        4951
      ],
      "content": "Users can also first enter the Hive command console by running command <ph id=\"ph15\">`hive`</ph><ph id=\"ph16\"/> in Hadoop Command Line, and then submit Hive queries in Hive command console. Here is an example. In this example, the two red boxes highlight the commands used to enter the Hive command console, and the Hive query submitted in Hive command console, respectively. The green box highlights the output from the Hive query.",
      "nodes": [
        {
          "content": "Users can also first enter the Hive command console by running command <ph id=\"ph15\">`hive`</ph><ph id=\"ph16\"/> in Hadoop Command Line, and then submit Hive queries in Hive command console.",
          "pos": [
            0,
            189
          ]
        },
        {
          "content": "Here is an example.",
          "pos": [
            190,
            209
          ]
        },
        {
          "content": "In this example, the two red boxes highlight the commands used to enter the Hive command console, and the Hive query submitted in Hive command console, respectively.",
          "pos": [
            210,
            375
          ]
        },
        {
          "content": "The green box highlights the output from the Hive query.",
          "pos": [
            376,
            432
          ]
        }
      ]
    },
    {
      "pos": [
        4954,
        4977
      ],
      "content": "<ph id=\"ph17\">![</ph>Create workspace<ph id=\"ph18\">][11]</ph>"
    },
    {
      "pos": [
        4979,
        5216
      ],
      "content": "The previous examples directly output the Hive query results on screen. Users can also write the output to a local file on the head node, or to an Azure blob. Then, users can use other tools to further analyze the output of Hive queries.",
      "nodes": [
        {
          "content": "The previous examples directly output the Hive query results on screen.",
          "pos": [
            0,
            71
          ]
        },
        {
          "content": "Users can also write the output to a local file on the head node, or to an Azure blob.",
          "pos": [
            72,
            158
          ]
        },
        {
          "content": "Then, users can use other tools to further analyze the output of Hive queries.",
          "pos": [
            159,
            237
          ]
        }
      ]
    },
    {
      "pos": [
        5218,
        5264
      ],
      "content": "<bpt id=\"p11\">**</bpt>Output Hive query results to a local file.<ept id=\"p11\">**</ept>"
    },
    {
      "pos": [
        5267,
        5409
      ],
      "content": "To output Hive query results to a local directory on the head node, users have to submit the Hive query in the Hadoop Command Line as follows:"
    },
    {
      "pos": [
        5473,
        5597
      ],
      "content": "In the following example, the output of Hive query is written into a file <ph id=\"ph19\">`hivequeryoutput.txt`</ph><ph id=\"ph20\"/> in directory <ph id=\"ph21\">`C:\\apps\\temp`</ph>."
    },
    {
      "pos": [
        5599,
        5622
      ],
      "content": "<ph id=\"ph22\">![</ph>Create workspace<ph id=\"ph23\">][12]</ph>"
    },
    {
      "pos": [
        5624,
        5670
      ],
      "content": "<bpt id=\"p12\">**</bpt>Output Hive query results to an Azure blob<ept id=\"p12\">**</ept>"
    },
    {
      "pos": [
        5672,
        5822
      ],
      "content": "Users can also output the Hive query results to an Azure blob, within the default container of the Hadoop cluster. The Hive query has to be like this:",
      "nodes": [
        {
          "content": "Users can also output the Hive query results to an Azure blob, within the default container of the Hadoop cluster.",
          "pos": [
            0,
            114
          ]
        },
        {
          "content": "The Hive query has to be like this:",
          "pos": [
            115,
            150
          ]
        }
      ]
    },
    {
      "pos": [
        5932,
        6280
      ],
      "content": "In the following example, the output of Hive query is written to a blob directory <ph id=\"ph24\">`queryoutputdir`</ph><ph id=\"ph25\"/> within the default container of the Hadoop cluster. Here, you only need to provide the directory name, without the blob name. An error will be thrown out if you provide both directory and blob names, such as <ph id=\"ph26\">`wasb:///queryoutputdir/queryoutput.txt`</ph>.",
      "nodes": [
        {
          "content": "In the following example, the output of Hive query is written to a blob directory <ph id=\"ph24\">`queryoutputdir`</ph><ph id=\"ph25\"/> within the default container of the Hadoop cluster.",
          "pos": [
            0,
            184
          ]
        },
        {
          "content": "Here, you only need to provide the directory name, without the blob name.",
          "pos": [
            185,
            258
          ]
        },
        {
          "content": "An error will be thrown out if you provide both directory and blob names, such as <ph id=\"ph26\">`wasb:///queryoutputdir/queryoutput.txt`</ph>.",
          "pos": [
            259,
            401
          ]
        }
      ]
    },
    {
      "pos": [
        6283,
        6306
      ],
      "content": "<ph id=\"ph27\">![</ph>Create workspace<ph id=\"ph28\">][13]</ph>"
    },
    {
      "pos": [
        6308,
        6567
      ],
      "content": "If you open the default container of the Hadoop cluster using tools like Azure Storage Explorer, you will see the output of the Hive query as follows. You can apply the filter (highlighted by red box) to only retrieve the blob with specified letters in names.",
      "nodes": [
        {
          "content": "If you open the default container of the Hadoop cluster using tools like Azure Storage Explorer, you will see the output of the Hive query as follows.",
          "pos": [
            0,
            150
          ]
        },
        {
          "content": "You can apply the filter (highlighted by red box) to only retrieve the blob with specified letters in names.",
          "pos": [
            151,
            259
          ]
        }
      ]
    },
    {
      "pos": [
        6569,
        6592
      ],
      "content": "<ph id=\"ph29\">![</ph>Create workspace<ph id=\"ph30\">][14]</ph>"
    },
    {
      "pos": [
        6594,
        6597
      ],
      "content": "###"
    },
    {
      "pos": [
        6624,
        6667
      ],
      "content": "2. Submit Hive queries with the Hive Editor"
    },
    {
      "pos": [
        6669,
        6890
      ],
      "content": "Users can also use Query Console (Hive Editor) by entering the URL in a web browser <ph id=\"ph31\">`https://&lt;Hadoop cluster name&gt;.azurehdinsight.net/Home/HiveEditor`</ph><ph id=\"ph32\"/> (you will be asked to input the Hadoop cluster credentials to log in),"
    },
    {
      "pos": [
        6892,
        6895
      ],
      "content": "###"
    },
    {
      "pos": [
        6913,
        6966
      ],
      "content": "3. Submit Hive queries with Azure PowerShell Commands"
    },
    {
      "pos": [
        6968,
        7158
      ],
      "content": "Users can also us PowerShell to submit Hive queries. For instructions, see <bpt id=\"p13\">[</bpt>Submit Hive jobs using PowerShell<ept id=\"p13\">](../hdinsight/hdinsight-submit-hadoop-jobs-programmatically.md#hive-powershell)</ept>.",
      "nodes": [
        {
          "content": "Users can also us PowerShell to submit Hive queries.",
          "pos": [
            0,
            52
          ]
        },
        {
          "content": "For instructions, see <bpt id=\"p13\">[</bpt>Submit Hive jobs using PowerShell<ept id=\"p13\">](../hdinsight/hdinsight-submit-hadoop-jobs-programmatically.md#hive-powershell)</ept>.",
          "pos": [
            53,
            230
          ]
        }
      ]
    },
    {
      "pos": [
        7186,
        7249
      ],
      "content": "Data Exploration, Feature Engineering and Hive Parameter Tuning"
    },
    {
      "pos": [
        7251,
        7360
      ],
      "content": "We describe the following data wrangling tasks in this section using Hive in Azure HDInsight Hadoop clusters:"
    },
    {
      "pos": [
        7365,
        7406
      ],
      "content": "<bpt id=\"p14\">[</bpt>Data Exploration<ept id=\"p14\">](#hive-dataexploration)</ept>"
    },
    {
      "pos": [
        7410,
        7456
      ],
      "content": "<bpt id=\"p15\">[</bpt>Feature Generation<ept id=\"p15\">](#hive-featureengineering)</ept>"
    },
    {
      "pos": [
        7460,
        7736
      ],
      "content": "<ph id=\"ph33\">[AZURE.NOTE]</ph><ph id=\"ph34\"/> The sample Hive queries assume that the data has been uploaded to Hive tables in Azure HDInsight Hadoop clusters. If it has not, please follow <bpt id=\"p16\">[</bpt>Create and load data to Hive tables<ept id=\"p16\">](machine-learning-data-science-hive-tables.md)</ept><ph id=\"ph35\"/> to upload data to Hive tables first.",
      "nodes": [
        {
          "content": "<ph id=\"ph33\">[AZURE.NOTE]</ph><ph id=\"ph34\"/> The sample Hive queries assume that the data has been uploaded to Hive tables in Azure HDInsight Hadoop clusters.",
          "pos": [
            0,
            160
          ]
        },
        {
          "content": "If it has not, please follow <bpt id=\"p16\">[</bpt>Create and load data to Hive tables<ept id=\"p16\">](machine-learning-data-science-hive-tables.md)</ept><ph id=\"ph35\"/> to upload data to Hive tables first.",
          "pos": [
            161,
            365
          ]
        }
      ]
    },
    {
      "pos": [
        7738,
        7741
      ],
      "content": "###"
    },
    {
      "pos": [
        7776,
        7792
      ],
      "content": "Data Exploration"
    },
    {
      "pos": [
        7793,
        7876
      ],
      "content": "Here are a few sample Hive scripts that can be used to explore data in Hive tables."
    },
    {
      "pos": [
        7881,
        8031
      ],
      "leadings": [
        "",
        "   "
      ],
      "content": "Get the count of observations per partition\n <ph id=\"ph36\">`SELECT &lt;partitionfieldname&gt;, count(*) from &lt;databasename&gt;.&lt;tablename&gt; group by &lt;partitionfieldname&gt;;`</ph>"
    },
    {
      "pos": [
        8036,
        8192
      ],
      "leadings": [
        "",
        "   "
      ],
      "content": "Get the count of observations per day\n <ph id=\"ph37\">`SELECT to_date(&lt;date_columnname&gt;), count(*) from &lt;databasename&gt;.&lt;tablename&gt; group by to_date(&lt;date_columnname&gt;);`</ph>"
    },
    {
      "pos": [
        8198,
        8307
      ],
      "leadings": [
        "",
        "   "
      ],
      "content": "Get the levels in a categorical column  \n <ph id=\"ph38\">`SELECT  distinct &lt;column_name&gt; from &lt;databasename&gt;.&lt;tablename&gt;`</ph>"
    },
    {
      "pos": [
        8312,
        8489
      ],
      "leadings": [
        "",
        "   "
      ],
      "content": "Get the number of levels in combination of two categorical columns \n <ph id=\"ph39\">`SELECT &lt;column_a&gt;, &lt;column_b&gt;, count(*) from &lt;databasename&gt;.&lt;tablename&gt; group by &lt;column_a&gt;, &lt;column_b&gt;`</ph>"
    },
    {
      "pos": [
        8494,
        8630
      ],
      "leadings": [
        "",
        "   "
      ],
      "content": "Get the distribution for numerical columns  \n <ph id=\"ph40\">`SELECT &lt;column_name&gt;, count(*) from &lt;databasename&gt;.&lt;tablename&gt; group by &lt;column_name&gt;`</ph>"
    },
    {
      "pos": [
        8635,
        8674
      ],
      "content": "Extract records from joining two tables"
    },
    {
      "pos": [
        9570,
        9573
      ],
      "content": "###"
    },
    {
      "pos": [
        9611,
        9629
      ],
      "content": "Feature Generation"
    },
    {
      "pos": [
        9631,
        9707
      ],
      "content": "In this section, we describe ways of generating features using Hive queries:"
    },
    {
      "pos": [
        9715,
        9775
      ],
      "content": "<bpt id=\"p17\">[</bpt>Frequency based Feature Generation<ept id=\"p17\">](#hive-frequencyfeature)</ept>"
    },
    {
      "pos": [
        9779,
        9855
      ],
      "content": "<bpt id=\"p18\">[</bpt>Risks of Categorical Variables in Binary Classification<ept id=\"p18\">](#hive-riskfeature)</ept>"
    },
    {
      "pos": [
        9859,
        9916
      ],
      "content": "<bpt id=\"p19\">[</bpt>Extract features from Datetime Field<ept id=\"p19\">](#hive-datefeature)</ept>"
    },
    {
      "pos": [
        9920,
        9973
      ],
      "content": "<bpt id=\"p20\">[</bpt>Extract features from Text Field<ept id=\"p20\">](#hive-textfeature)</ept>"
    },
    {
      "pos": [
        9977,
        10040
      ],
      "content": "<bpt id=\"p21\">[</bpt>Calculate distance between GPS coordinates<ept id=\"p21\">](#hive-gpsdistance)</ept>"
    },
    {
      "pos": [
        10044,
        10270
      ],
      "content": "<ph id=\"ph41\">[AZURE.NOTE]</ph><ph id=\"ph42\"/> Once you generate additional features, you can either add them as columns to the existing table or create a new table with the additional features and primary key, which can then be joined with the original table."
    },
    {
      "pos": [
        10272,
        10276
      ],
      "content": "####"
    },
    {
      "pos": [
        10313,
        10347
      ],
      "content": "Frequency based Feature Generation"
    },
    {
      "pos": [
        10349,
        10585
      ],
      "content": "Sometimes, it is valuable to calculate the frequencies of the levels of a categorical variable, or the frequencies of level combinations of multiple categorical variables. Users can use the following script to calculate the frequencies:",
      "nodes": [
        {
          "content": "Sometimes, it is valuable to calculate the frequencies of the levels of a categorical variable, or the frequencies of level combinations of multiple categorical variables.",
          "pos": [
            0,
            171
          ]
        },
        {
          "content": "Users can use the following script to calculate the frequencies:",
          "pos": [
            172,
            236
          ]
        }
      ]
    },
    {
      "pos": [
        10948,
        10952
      ],
      "content": "####"
    },
    {
      "pos": [
        10984,
        11039
      ],
      "content": "Risks of Categorical Variables in Binary Classification"
    },
    {
      "pos": [
        11041,
        11379
      ],
      "content": "In binary classification, sometimes we need to convert non-numeric categorical variables into numeric features by replacing the non-numeric levels with numeric risks, since some models might only take numeric features. In this section, we show some generic Hive queries of calculating the risk values (log odds) of a categorical variable.",
      "nodes": [
        {
          "content": "In binary classification, sometimes we need to convert non-numeric categorical variables into numeric features by replacing the non-numeric levels with numeric risks, since some models might only take numeric features.",
          "pos": [
            0,
            218
          ]
        },
        {
          "content": "In this section, we show some generic Hive queries of calculating the risk values (log odds) of a categorical variable.",
          "pos": [
            219,
            338
          ]
        }
      ]
    },
    {
      "pos": [
        12083,
        12321
      ],
      "content": "In this example, variables <ph id=\"ph43\">`smooth_param1`</ph><ph id=\"ph44\"/> and <ph id=\"ph45\">`smooth_param2`</ph><ph id=\"ph46\"/> are set to smooth the risk values calculated from data. Risks are ranged between -Inf and Inf. Risks&gt;0 stands for the probability that the target equals 1 is greater than 0.5.",
      "nodes": [
        {
          "content": "In this example, variables <ph id=\"ph43\">`smooth_param1`</ph><ph id=\"ph44\"/> and <ph id=\"ph45\">`smooth_param2`</ph><ph id=\"ph46\"/> are set to smooth the risk values calculated from data. Ri",
          "pos": [
            0,
            189
          ]
        },
        {
          "content": "sks are ranged between -Inf and Inf. Ri",
          "pos": [
            189,
            228
          ]
        },
        {
          "content": "sks&gt;0 stands for the probability that the target equals 1 is greater than 0.5.",
          "pos": [
            228,
            309
          ]
        }
      ]
    },
    {
      "pos": [
        12324,
        12493
      ],
      "content": "After the risk table is calculated, users can assign risk values to a table by joining it with the risk table. The Hive joining query has been given in previous section.",
      "nodes": [
        {
          "content": "After the risk table is calculated, users can assign risk values to a table by joining it with the risk table.",
          "pos": [
            0,
            110
          ]
        },
        {
          "content": "The Hive joining query has been given in previous section.",
          "pos": [
            111,
            169
          ]
        }
      ]
    },
    {
      "pos": [
        12495,
        12499
      ],
      "content": "####"
    },
    {
      "pos": [
        12531,
        12568
      ],
      "content": "Extract features from Datetime Fields"
    },
    {
      "pos": [
        12570,
        12961
      ],
      "content": "Hive comes along with a set of UDFs for processing datetime fields. In Hive, the default datetime format is 'yyyy-MM-dd 00:00:00' (like '1970-01-01 12:21:32'). In this section, we show examples of extracting the day of a month, and the month from a datetime field, and examples of converting a datetime string in a format other than the default format to a datetime string in default format.",
      "nodes": [
        {
          "content": "Hive comes along with a set of UDFs for processing datetime fields.",
          "pos": [
            0,
            67
          ]
        },
        {
          "content": "In Hive, the default datetime format is 'yyyy-MM-dd 00:00:00' (like '1970-01-01 12:21:32').",
          "pos": [
            68,
            159
          ]
        },
        {
          "content": "In this section, we show examples of extracting the day of a month, and the month from a datetime field, and examples of converting a datetime string in a format other than the default format to a datetime string in default format.",
          "pos": [
            160,
            391
          ]
        }
      ]
    },
    {
      "pos": [
        13069,
        13155
      ],
      "content": "This Hive query assumes that the <ph id=\"ph47\">`&lt;datetime field&gt;`</ph><ph id=\"ph48\"/> is in the default datetime format."
    },
    {
      "pos": [
        13157,
        13458
      ],
      "content": "If a datetime field is not in the default format, we need to first convert the datetile field into Unix time stamp, and then convert the Unix time stamp to a datetime string in the default format. After the datetime is in default format, users can apply the embedded datetime UDFs to extract features.",
      "nodes": [
        {
          "content": "If a datetime field is not in the default format, we need to first convert the datetile field into Unix time stamp, and then convert the Unix time stamp to a datetime string in the default format.",
          "pos": [
            0,
            196
          ]
        },
        {
          "content": "After the datetime is in default format, users can apply the embedded datetime UDFs to extract features.",
          "pos": [
            197,
            301
          ]
        }
      ]
    },
    {
      "pos": [
        13599,
        13784
      ],
      "content": "In this query, if the <ph id=\"ph49\">`&lt;datetime field&gt;`</ph><ph id=\"ph50\"/> has the pattern like <ph id=\"ph51\">`03/26/2015 12:04:39`</ph>, the <ph id=\"ph52\">`'&lt;pattern of the datetime field&gt;'`</ph><ph id=\"ph53\"/> should be <ph id=\"ph54\">`'MM/dd/yyyy HH:mm:ss'`</ph>. To test it, users can run",
      "nodes": [
        {
          "content": "In this query, if the <ph id=\"ph49\">`&lt;datetime field&gt;`</ph><ph id=\"ph50\"/> has the pattern like <ph id=\"ph51\">`03/26/2015 12:04:39`</ph>, the <ph id=\"ph52\">`'&lt;pattern of the datetime field&gt;'`</ph><ph id=\"ph53\"/> should be <ph id=\"ph54\">`'MM/dd/yyyy HH:mm:ss'`</ph>.",
          "pos": [
            0,
            277
          ]
        },
        {
          "content": "To test it, users can run",
          "pos": [
            278,
            303
          ]
        }
      ]
    },
    {
      "pos": [
        13915,
        14040
      ],
      "content": "In this query, <ph id=\"ph55\">`hivesampletable`</ph><ph id=\"ph56\"/> comes with all Azure HDInsight Hadoop clusters by default when the clusters are provisioned."
    },
    {
      "pos": [
        14044,
        14048
      ],
      "content": "####"
    },
    {
      "pos": [
        14080,
        14113
      ],
      "content": "Extract features from Text Fields"
    },
    {
      "pos": [
        14115,
        14299
      ],
      "content": "Assume that the Hive table has a text field, which is a string of words separated by space, the following query extract the length of the string, and the number of words in the string."
    },
    {
      "pos": [
        14434,
        14438
      ],
      "content": "####"
    },
    {
      "pos": [
        14470,
        14512
      ],
      "content": "Calculate distance between GPS coordinates"
    },
    {
      "pos": [
        14514,
        14712
      ],
      "content": "The query given in this section can be directly applied on the NYC Taxi Trip Data. The purpose of this query is to show how to apply the embedded mathematical functions in Hive to generate features.",
      "nodes": [
        {
          "content": "The query given in this section can be directly applied on the NYC Taxi Trip Data.",
          "pos": [
            0,
            82
          ]
        },
        {
          "content": "The purpose of this query is to show how to apply the embedded mathematical functions in Hive to generate features.",
          "pos": [
            83,
            198
          ]
        }
      ]
    },
    {
      "pos": [
        14715,
        14985
      ],
      "content": "The fields that are used in this query are GPS coordinates of pickup and dropoff locations, named pickup\\_longitude, pickup\\_latitude, dropoff\\_longitude, and dropoff\\_latitude. The queries to calculate the direct distance between the pickup and dropoff coordinates are:",
      "nodes": [
        {
          "content": "The fields that are used in this query are GPS coordinates of pickup and dropoff locations, named pickup\\_longitude, pickup\\_latitude, dropoff\\_longitude, and dropoff\\_latitude.",
          "pos": [
            0,
            177
          ]
        },
        {
          "content": "The queries to calculate the direct distance between the pickup and dropoff coordinates are:",
          "pos": [
            178,
            270
          ]
        }
      ]
    },
    {
      "pos": [
        15938,
        16519
      ],
      "content": "The mathematical equations of calculating distance between two GPS coordinates can be found at <bpt id=\"p22\">[</bpt>Movable Type Scripts<ept id=\"p22\">](http://www.movable-type.co.uk/scripts/latlong.html)</ept>, authored by Peter Lapisu. In his Javascript, the function toRad() is just <ph id=\"ph57\">`lat_or_lon*pi/180`</ph>, which converts degrees to radians. Here, <ph id=\"ph58\">`lat_or_lon`</ph><ph id=\"ph59\"/> is the latitude or longitude. Since Hive does not provide function <ph id=\"ph60\">`atan2`</ph>, but provides function <ph id=\"ph61\">`atan`</ph>, the <ph id=\"ph62\">`atan2`</ph><ph id=\"ph63\"/> function is implemented by <ph id=\"ph64\">`atan`</ph><ph id=\"ph65\"/> function in the above Hive query, based on its definition in <bpt id=\"p23\">[</bpt>Wikipedia<ept id=\"p23\">](http://en.wikipedia.org/wiki/Atan2)</ept>.",
      "nodes": [
        {
          "content": "The mathematical equations of calculating distance between two GPS coordinates can be found at <bpt id=\"p22\">[</bpt>Movable Type Scripts<ept id=\"p22\">](http://www.movable-type.co.uk/scripts/latlong.html)</ept>, authored by Peter Lapisu.",
          "pos": [
            0,
            236
          ]
        },
        {
          "content": "In his Javascript, the function toRad() is just <ph id=\"ph57\">`lat_or_lon*pi/180`</ph>, which converts degrees to radians.",
          "pos": [
            237,
            359
          ]
        },
        {
          "content": "Here, <ph id=\"ph58\">`lat_or_lon`</ph><ph id=\"ph59\"/> is the latitude or longitude.",
          "pos": [
            360,
            442
          ]
        },
        {
          "content": "Since Hive does not provide function <ph id=\"ph60\">`atan2`</ph>, but provides function <ph id=\"ph61\">`atan`</ph>, the <ph id=\"ph62\">`atan2`</ph><ph id=\"ph63\"/> function is implemented by <ph id=\"ph64\">`atan`</ph><ph id=\"ph65\"/> function in the above Hive query, based on its definition in <bpt id=\"p23\">[</bpt>Wikipedia<ept id=\"p23\">](http://en.wikipedia.org/wiki/Atan2)</ept>.",
          "pos": [
            443,
            820
          ]
        }
      ]
    },
    {
      "pos": [
        16522,
        16544
      ],
      "content": "<ph id=\"ph66\">![</ph>Create workspace<ph id=\"ph67\">][1]</ph>"
    },
    {
      "pos": [
        16546,
        16731
      ],
      "content": "A full list of Hive embedded UDFs can be found in the <bpt id=\"p24\">[</bpt>UDF Language Manual<ept id=\"p24\">](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-MathematicalFunctions)</ept>."
    },
    {
      "pos": [
        16759,
        16819
      ],
      "content": "Advanced topics: Tune Hive Parameters to Improve Query Speed"
    },
    {
      "pos": [
        16821,
        17158
      ],
      "content": "The default parameter settings of Hive cluster might not be suitable for the Hive queries and the data the queries are processing. In this section, we discuss some parameters that users can tune so that the performance of Hive queries can be improved. Users need to add the parameter tuning queries before the queries of processing data.",
      "nodes": [
        {
          "content": "The default parameter settings of Hive cluster might not be suitable for the Hive queries and the data the queries are processing.",
          "pos": [
            0,
            130
          ]
        },
        {
          "content": "In this section, we discuss some parameters that users can tune so that the performance of Hive queries can be improved.",
          "pos": [
            131,
            251
          ]
        },
        {
          "content": "Users need to add the parameter tuning queries before the queries of processing data.",
          "pos": [
            252,
            337
          ]
        }
      ]
    },
    {
      "pos": [
        17164,
        17442
      ],
      "content": "Java heap space : For queries involving joining large datasets, or processing long records, a typical error is <bpt id=\"p25\">**</bpt>running out of heap space<ept id=\"p25\">**</ept>. This can be tuned by setting parameters <ph id=\"ph68\">`mapreduce.map.java.opts`</ph><ph id=\"ph69\"/> and <ph id=\"ph70\">`mapreduce.task.io.sort.mb`</ph><ph id=\"ph71\"/> to desired values. Here is an example:",
      "nodes": [
        {
          "content": "Java heap space : For queries involving joining large datasets, or processing long records, a typical error is <bpt id=\"p25\">**</bpt>running out of heap space<ept id=\"p25\">**</ept>.",
          "pos": [
            0,
            181
          ]
        },
        {
          "content": "This can be tuned by setting parameters <ph id=\"ph68\">`mapreduce.map.java.opts`</ph><ph id=\"ph69\"/> and <ph id=\"ph70\">`mapreduce.task.io.sort.mb`</ph><ph id=\"ph71\"/> to desired values.",
          "pos": [
            182,
            366
          ]
        },
        {
          "content": "Here is an example:",
          "pos": [
            367,
            386
          ]
        }
      ]
    },
    {
      "pos": [
        17550,
        17769
      ],
      "content": "This parameter allocates 4GB memory to Java heap space and also makes sorting more efficient by allocating more memory for it. It is a good idea to play with it if there are any job failure errors related to heap space.",
      "nodes": [
        {
          "content": "This parameter allocates 4GB memory to Java heap space and also makes sorting more efficient by allocating more memory for it.",
          "pos": [
            0,
            126
          ]
        },
        {
          "content": "It is a good idea to play with it if there are any job failure errors related to heap space.",
          "pos": [
            127,
            219
          ]
        }
      ]
    },
    {
      "pos": [
        17774,
        18295
      ],
      "content": "DFS block size : This parameter sets the smallest unit of data that the file system stores. As an example, if the DFS block size is 128MB, then any data of size less than and up to 128MB is stored in a single block, while data that is larger than 128MB is allotted extra blocks. Choosing a very small block size causes large overheads in Hadoop since the name node has to process many more requests to find the relevant block pertaining to the file. A recommended setting when dealing with gigabytes (or larger) data is :",
      "nodes": [
        {
          "content": "DFS block size : This parameter sets the smallest unit of data that the file system stores.",
          "pos": [
            0,
            91
          ]
        },
        {
          "content": "As an example, if the DFS block size is 128MB, then any data of size less than and up to 128MB is stored in a single block, while data that is larger than 128MB is allotted extra blocks.",
          "pos": [
            92,
            278
          ]
        },
        {
          "content": "Choosing a very small block size causes large overheads in Hadoop since the name node has to process many more requests to find the relevant block pertaining to the file.",
          "pos": [
            279,
            449
          ]
        },
        {
          "content": "A recommended setting when dealing with gigabytes (or larger) data is :",
          "pos": [
            450,
            521
          ]
        }
      ]
    },
    {
      "pos": [
        18334,
        18625
      ],
      "content": "Optimizing join operation in Hive : While join operations in the map/reduce framework typically take place in the reduce phase, some times, enormous gains can be achieved by scheduling joins in the map phase (also called \"mapjoins\"). To direct Hive to do this whenever possible, we can set :",
      "nodes": [
        {
          "content": "Optimizing join operation in Hive : While join operations in the map/reduce framework typically take place in the reduce phase, some times, enormous gains can be achieved by scheduling joins in the map phase (also called \"mapjoins\").",
          "pos": [
            0,
            233
          ]
        },
        {
          "content": "To direct Hive to do this whenever possible, we can set :",
          "pos": [
            234,
            291
          ]
        }
      ]
    },
    {
      "pos": [
        18672,
        19021
      ],
      "content": "Specifying the number of mappers to Hive : While Hadoop allows the user to set the number of reducers, the number of mappers may typically not be set by the user. A trick that allows some degree of control on this number is to choose the hadoop variables, mapred.min.split.size and mapred.max.split.size. The size of each map task is determined by :",
      "nodes": [
        {
          "content": "Specifying the number of mappers to Hive : While Hadoop allows the user to set the number of reducers, the number of mappers may typically not be set by the user.",
          "pos": [
            0,
            162
          ]
        },
        {
          "content": "A trick that allows some degree of control on this number is to choose the hadoop variables, mapred.min.split.size and mapred.max.split.size.",
          "pos": [
            163,
            304
          ]
        },
        {
          "content": "The size of each map task is determined by :",
          "pos": [
            305,
            349
          ]
        }
      ]
    },
    {
      "pos": [
        19118,
        19379
      ],
      "content": "Typically, the default value of mapred.min.split.size is 0, that of mapred.max.split.size is Long.MAX and that of dfs.block.size is 64MB. As we can see, given the data size, tuning these parameters by \"setting\" them allows us to tune the number of mappers used.",
      "nodes": [
        {
          "content": "Typically, the default value of mapred.min.split.size is 0, that of mapred.max.split.size is Long.MAX and that of dfs.block.size is 64MB.",
          "pos": [
            0,
            137
          ]
        },
        {
          "content": "As we can see, given the data size, tuning these parameters by \"setting\" them allows us to tune the number of mappers used.",
          "pos": [
            138,
            261
          ]
        }
      ]
    },
    {
      "pos": [
        19385,
        19736
      ],
      "content": "A few other more advanced options for optimizing Hive performance are mentioned below. These allow for the setting of memory allocated to map and reduce tasks, and can be useful in tweaking performance. Please keep in mind that the <ph id=\"ph72\">`mapreduce.reduce.memory.mb`</ph><ph id=\"ph73\"/> cannot be greater than the physical memory size of each worker node in the Hadoop cluster.",
      "nodes": [
        {
          "content": "A few other more advanced options for optimizing Hive performance are mentioned below.",
          "pos": [
            0,
            86
          ]
        },
        {
          "content": "These allow for the setting of memory allocated to map and reduce tasks, and can be useful in tweaking performance.",
          "pos": [
            87,
            202
          ]
        },
        {
          "content": "Please keep in mind that the <ph id=\"ph72\">`mapreduce.reduce.memory.mb`</ph><ph id=\"ph73\"/> cannot be greater than the physical memory size of each worker node in the Hadoop cluster.",
          "pos": [
            203,
            385
          ]
        }
      ]
    }
  ],
  "content": "<properties \n    pageTitle=\"Submit Hive Queries to Hadoop clusters in the Cortana Analytics Process  | Microsoft Azure\" \n    description=\"Process Data from Hive Tables\" \n    services=\"machine-learning\" \n    documentationCenter=\"\" \n    authors=\"hangzh-msft\" \n    manager=\"paulettm\" \n    editor=\"cgronlun\"  />\n\n<tags \n    ms.service=\"machine-learning\" \n    ms.workload=\"data-services\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"02/08/2016\" \n    ms.author=\"hangzh;bradsev\" /> \n\n#<a name=\"heading\"></a> Submit Hive Queries to HDInsight Hadoop clusters in the Cortana Analytics Process\n\nThis document describes various ways of submitting Hive queries to Hadoop clusters that are managed by an HDInsight service in Azure. Hive queries can be submitted by using: \n\n* the Hadoop Command Line on the headnode of the cluster\n* the IPython Notebook \n* the Hive Editor\n* Azure PowerShell scripts \n\nGeneric Hive queries that can be used to explore the data or to generate features that use embedded Hive User Defined Functions (UDFs) are provided. \n\nExamples of queries the specific to [NYC Taxi Trip Data](http://chriswhong.com/open-data/foil_nyc_taxi/) scenarios are also provided in [Github repository](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/DataScienceProcess/DataScienceScripts). These queries already have data schema specified and are ready to be submitted to run for this scenario. \n\nIn the final section, parameters that users can tune to improve the performance of Hive queries are discussed.\n\n## Prerequisites\nThis article assumes that you have:\n \n* Created an Azure storage account. If you need instructions for this task, see [Create an Azure Storage account](../hdinsight-get-started.md#storage) \n* Provisioned an Hadoop cluster with the HDInsight service.  If you need instructions, see [Provision an HDInsight cluster](../hdinsight-get-started.md#provision).\n* Uploaded the data to Hive tables in Azure HDInsight Hadoop clusters. If it has not, please follow the instructions provided at [Create and load data to Hive tables](machine-learning-data-science-hive-tables.md) to upload data to Hive tables first.\n* Enabled remote access to the cluster. If you need instructions, see [Access the Head Node of Hadoop Cluster](machine-learning-data-science-customize-hadoop-cluster.md#remoteaccess). \n\n\n## <a name=\"submit\"></a>How to submit Hive queries\n\n1. [Submit Hive queries through Hadoop Command Line in headnode of Hadoop cluster](#headnode)\n2. [Submit Hive queries with the Hive Editor](#hive-editor)\n3. [Submit Hive queries with Azure PowerShell Commands](#ps)\n \n###<a name=\"headnode\"></a> 1. Submit Hive queries through Hadoop Command Line in headnode of Hadoop cluster\n\nIf the Hive query is complex, submitting it directly in the head node of the Hadoop cluster typically leads to faster turn around than submitting it with a Hive Editor or Azure PowerShell scripts. \n\nLog in to the head node of the Hadoop cluster, open the Hadoop Command Line on the desktop of the head node, and enter command `cd %hive_home%\\bin`.\n\nUsers have three ways to submit Hive queries in the Hadoop Command Line: \n\n* directly\n* using .hql files\n* with the Hive command console\n\n#### Submit Hive queries directly in Hadoop Command Line. \n\nUsers can run command like `hive -e \"<your hive query>;` to submit simple Hive queries directly in Hadoop Command Line. Here is an example, where the red box outlines the command that submits the Hive query, and the green box outlines the output from the Hive query.\n\n![Create workspace][10]\n\n#### Submit Hive queries in .hql files\n\nWhen the Hive query is more complicated and has multiple lines, editing queries in command line or Hive command console is not practical. An alternative is to use a text editor in the head node of the Hadoop cluster to save the Hive queries in a .hql file in a local directory of the head node. Then the Hive query in the .hql file can be submitted by using the `-f` argument as follows:\n    \n    `hive -f \"<path to the .hql file>\"`\n\n![Create workspace][15]\n\n\n**Suppress progress status screen print of Hive queries**\n\nBy default, after Hive query is submitted in Hadoop Command Line, the progress of the Map/Reduce job will be printed out on screen. To suppress the screen print of the Map/Reduce job progress, you can use an argument `-S` (\"S\" in upper case) in the command line as follows:\n\n    hive -S -f \"<path to the .hql file>\"\n    hive -S -e \"<Hive queries>\"\n\n#### Submit Hive queries in Hive command console.\n\nUsers can also first enter the Hive command console by running command `hive` in Hadoop Command Line, and then submit Hive queries in Hive command console. Here is an example. In this example, the two red boxes highlight the commands used to enter the Hive command console, and the Hive query submitted in Hive command console, respectively. The green box highlights the output from the Hive query. \n\n![Create workspace][11]\n\nThe previous examples directly output the Hive query results on screen. Users can also write the output to a local file on the head node, or to an Azure blob. Then, users can use other tools to further analyze the output of Hive queries.\n\n**Output Hive query results to a local file.** \n\nTo output Hive query results to a local directory on the head node, users have to submit the Hive query in the Hadoop Command Line as follows:\n\n    `hive -e \"<hive query>\" > <local path in the head node>`\n\nIn the following example, the output of Hive query is written into a file `hivequeryoutput.txt` in directory `C:\\apps\\temp`.\n\n![Create workspace][12]\n\n**Output Hive query results to an Azure blob**\n\nUsers can also output the Hive query results to an Azure blob, within the default container of the Hadoop cluster. The Hive query has to be like this:\n\n    `insert overwrite directory wasb:///<directory within the default container> <select clause from ...>`\n\nIn the following example, the output of Hive query is written to a blob directory `queryoutputdir` within the default container of the Hadoop cluster. Here, you only need to provide the directory name, without the blob name. An error will be thrown out if you provide both directory and blob names, such as `wasb:///queryoutputdir/queryoutput.txt`. \n\n![Create workspace][13]\n\nIf you open the default container of the Hadoop cluster using tools like Azure Storage Explorer, you will see the output of the Hive query as follows. You can apply the filter (highlighted by red box) to only retrieve the blob with specified letters in names.\n\n![Create workspace][14]\n\n###<a name=\"hive-editor\"></a> 2. Submit Hive queries with the Hive Editor\n\nUsers can also use Query Console (Hive Editor) by entering the URL in a web browser `https://<Hadoop cluster name>.azurehdinsight.net/Home/HiveEditor` (you will be asked to input the Hadoop cluster credentials to log in),\n\n###<a name=\"ps\"></a> 3. Submit Hive queries with Azure PowerShell Commands\n\nUsers can also us PowerShell to submit Hive queries. For instructions, see [Submit Hive jobs using PowerShell](../hdinsight/hdinsight-submit-hadoop-jobs-programmatically.md#hive-powershell). \n\n## <a name=\"explore\"></a>Data Exploration, Feature Engineering and Hive Parameter Tuning\n\nWe describe the following data wrangling tasks in this section using Hive in Azure HDInsight Hadoop clusters:\n\n1. [Data Exploration](#hive-dataexploration)\n2. [Feature Generation](#hive-featureengineering)\n\n> [AZURE.NOTE] The sample Hive queries assume that the data has been uploaded to Hive tables in Azure HDInsight Hadoop clusters. If it has not, please follow [Create and load data to Hive tables](machine-learning-data-science-hive-tables.md) to upload data to Hive tables first.\n\n###<a name=\"hive-dataexploration\"></a>Data Exploration\nHere are a few sample Hive scripts that can be used to explore data in Hive tables.\n\n1. Get the count of observations per partition\n    `SELECT <partitionfieldname>, count(*) from <databasename>.<tablename> group by <partitionfieldname>;`\n\n2. Get the count of observations per day\n    `SELECT to_date(<date_columnname>), count(*) from <databasename>.<tablename> group by to_date(<date_columnname>);` \n\n3. Get the levels in a categorical column  \n    `SELECT  distinct <column_name> from <databasename>.<tablename>`\n\n4. Get the number of levels in combination of two categorical columns \n    `SELECT <column_a>, <column_b>, count(*) from <databasename>.<tablename> group by <column_a>, <column_b>`\n\n5. Get the distribution for numerical columns  \n    `SELECT <column_name>, count(*) from <databasename>.<tablename> group by <column_name>`\n\n6. Extract records from joining two tables \n\n        SELECT \n            a.<common_columnname1> as <new_name1>,\n            a.<common_columnname2> as <new_name2>,\n            a.<a_column_name1> as <new_name3>,\n            a.<a_column_name2> as <new_name4>,\n            b.<b_column_name1> as <new_name5>,\n            b.<b_column_name2> as <new_name6>\n        FROM\n            (\n            SELECT <common_columnname1>, \n                <common_columnname2>,\n                <a_column_name1>,\n                <a_column_name2>,\n            FROM <databasename>.<tablename1>\n            ) a\n            join\n            (\n            SELECT <common_columnname1>, \n                <common_columnname2>,\n                <b_column_name1>,\n                <b_column_name2>,\n            FROM <databasename>.<tablename2>\n            ) b\n            ON a.<common_columnname1>=b.<common_columnname1> and a.<common_columnname2>=b.<common_columnname2>\n\n###<a name=\"hive-featureengineering\"></a>Feature Generation\n\nIn this section, we describe ways of generating features using Hive queries: \n  \n1. [Frequency based Feature Generation](#hive-frequencyfeature)\n2. [Risks of Categorical Variables in Binary Classification](#hive-riskfeature)\n3. [Extract features from Datetime Field](#hive-datefeature)\n4. [Extract features from Text Field](#hive-textfeature)\n5. [Calculate distance between GPS coordinates](#hive-gpsdistance)\n\n> [AZURE.NOTE] Once you generate additional features, you can either add them as columns to the existing table or create a new table with the additional features and primary key, which can then be joined with the original table.\n\n####<a name=\"hive-frequencyfeature\"></a> Frequency based Feature Generation\n\nSometimes, it is valuable to calculate the frequencies of the levels of a categorical variable, or the frequencies of level combinations of multiple categorical variables. Users can use the following script to calculate the frequencies:\n\n        select \n            a.<column_name1>, a.<column_name2>, a.sub_count/sum(a.sub_count) over () as frequency\n        from\n        (\n            select \n                <column_name1>,<column_name2>, count(*) as sub_count \n            from <databasename>.<tablename> group by <column_name1>, <column_name2>\n        )a\n        order by frequency desc;\n    \n\n####<a name=\"hive-riskfeature\"></a> Risks of Categorical Variables in Binary Classification\n\nIn binary classification, sometimes we need to convert non-numeric categorical variables into numeric features by replacing the non-numeric levels with numeric risks, since some models might only take numeric features. In this section, we show some generic Hive queries of calculating the risk values (log odds) of a categorical variable. \n\n\n        set smooth_param1=1;\n        set smooth_param2=20;\n        select \n            <column_name1>,<column_name2>, \n            ln((sum_target+${hiveconf:smooth_param1})/(record_count-sum_target+${hiveconf:smooth_param2}-${hiveconf:smooth_param1})) as risk\n        from\n            (\n            select \n                <column_nam1>, <column_name2>, sum(binary_target) as sum_target, sum(1) as record_count\n            from\n                (\n                select \n                    <column_name1>, <column_name2>, if(target_column>0,1,0) as binary_target\n                from <databasename>.<tablename> \n                )a\n            group by <column_name1>, <column_name2>\n            )b \n\nIn this example, variables `smooth_param1` and `smooth_param2` are set to smooth the risk values calculated from data. Risks are ranged between -Inf and Inf. Risks>0 stands for the probability that the target equals 1 is greater than 0.5. \n\nAfter the risk table is calculated, users can assign risk values to a table by joining it with the risk table. The Hive joining query has been given in previous section.\n\n####<a name=\"hive-datefeature\"></a> Extract features from Datetime Fields\n\nHive comes along with a set of UDFs for processing datetime fields. In Hive, the default datetime format is 'yyyy-MM-dd 00:00:00' (like '1970-01-01 12:21:32'). In this section, we show examples of extracting the day of a month, and the month from a datetime field, and examples of converting a datetime string in a format other than the default format to a datetime string in default format. \n\n        select day(<datetime field>), month(<datetime field>) \n        from <databasename>.<tablename>;\n\nThis Hive query assumes that the `<datetime field>` is in the default datetime format.\n\nIf a datetime field is not in the default format, we need to first convert the datetile field into Unix time stamp, and then convert the Unix time stamp to a datetime string in the default format. After the datetime is in default format, users can apply the embedded datetime UDFs to extract features.\n\n        select from_unixtime(unix_timestamp(<datetime field>,'<pattern of the datetime field>'))\n        from <databasename>.<tablename>;\n\nIn this query, if the `<datetime field>` has the pattern like `03/26/2015 12:04:39`, the `'<pattern of the datetime field>'` should be `'MM/dd/yyyy HH:mm:ss'`. To test it, users can run\n\n        select from_unixtime(unix_timestamp('05/15/2015 09:32:10','MM/dd/yyyy HH:mm:ss'))\n        from hivesampletable limit 1;\n\nIn this query, `hivesampletable` comes with all Azure HDInsight Hadoop clusters by default when the clusters are provisioned. \n\n\n####<a name=\"hive-textfeature\"></a> Extract features from Text Fields\n\nAssume that the Hive table has a text field, which is a string of words separated by space, the following query extract the length of the string, and the number of words in the string.\n\n        select length(<text field>) as str_len, size(split(<text field>,' ')) as word_num \n        from <databasename>.<tablename>;\n\n####<a name=\"hive-gpsdistance\"></a> Calculate distance between GPS coordinates\n\nThe query given in this section can be directly applied on the NYC Taxi Trip Data. The purpose of this query is to show how to apply the embedded mathematical functions in Hive to generate features. \n\nThe fields that are used in this query are GPS coordinates of pickup and dropoff locations, named pickup\\_longitude, pickup\\_latitude, dropoff\\_longitude, and dropoff\\_latitude. The queries to calculate the direct distance between the pickup and dropoff coordinates are:\n\n        set R=3959;\n        set pi=radians(180);\n        select pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude, \n            ${hiveconf:R}*2*2*atan((1-sqrt(1-pow(sin((dropoff_latitude-pickup_latitude)\n            *${hiveconf:pi}/180/2),2)-cos(pickup_latitude*${hiveconf:pi}/180)\n            *cos(dropoff_latitude*${hiveconf:pi}/180)*pow(sin((dropoff_longitude-pickup_longitude)*${hiveconf:pi}/180/2),2)))\n            /sqrt(pow(sin((dropoff_latitude-pickup_latitude)*${hiveconf:pi}/180/2),2)\n            +cos(pickup_latitude*${hiveconf:pi}/180)*cos(dropoff_latitude*${hiveconf:pi}/180)*\n            pow(sin((dropoff_longitude-pickup_longitude)*${hiveconf:pi}/180/2),2))) as direct_distance \n        from nyctaxi.trip \n        where pickup_longitude between -90 and 0\n        and pickup_latitude between 30 and 90\n        and dropoff_longitude between -90 and 0\n        and dropoff_latitude between 30 and 90\n        limit 10; \n\nThe mathematical equations of calculating distance between two GPS coordinates can be found at [Movable Type Scripts](http://www.movable-type.co.uk/scripts/latlong.html), authored by Peter Lapisu. In his Javascript, the function toRad() is just `lat_or_lon*pi/180`, which converts degrees to radians. Here, `lat_or_lon` is the latitude or longitude. Since Hive does not provide function `atan2`, but provides function `atan`, the `atan2` function is implemented by `atan` function in the above Hive query, based on its definition in [Wikipedia](http://en.wikipedia.org/wiki/Atan2). \n\n![Create workspace][1]\n\nA full list of Hive embedded UDFs can be found in the [UDF Language Manual](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-MathematicalFunctions). \n\n## <a name=\"tuning\"></a> Advanced topics: Tune Hive Parameters to Improve Query Speed\n\nThe default parameter settings of Hive cluster might not be suitable for the Hive queries and the data the queries are processing. In this section, we discuss some parameters that users can tune so that the performance of Hive queries can be improved. Users need to add the parameter tuning queries before the queries of processing data. \n\n1. Java heap space : For queries involving joining large datasets, or processing long records, a typical error is **running out of heap space**. This can be tuned by setting parameters `mapreduce.map.java.opts` and `mapreduce.task.io.sort.mb` to desired values. Here is an example:\n\n        set mapreduce.map.java.opts=-Xmx4096m;\n        set mapreduce.task.io.sort.mb=-Xmx1024m;\n    \n\n    This parameter allocates 4GB memory to Java heap space and also makes sorting more efficient by allocating more memory for it. It is a good idea to play with it if there are any job failure errors related to heap space.\n\n2. DFS block size : This parameter sets the smallest unit of data that the file system stores. As an example, if the DFS block size is 128MB, then any data of size less than and up to 128MB is stored in a single block, while data that is larger than 128MB is allotted extra blocks. Choosing a very small block size causes large overheads in Hadoop since the name node has to process many more requests to find the relevant block pertaining to the file. A recommended setting when dealing with gigabytes (or larger) data is :\n\n        set dfs.block.size=128m;\n\n3. Optimizing join operation in Hive : While join operations in the map/reduce framework typically take place in the reduce phase, some times, enormous gains can be achieved by scheduling joins in the map phase (also called \"mapjoins\"). To direct Hive to do this whenever possible, we can set :\n\n        set hive.auto.convert.join=true;\n\n4. Specifying the number of mappers to Hive : While Hadoop allows the user to set the number of reducers, the number of mappers may typically not be set by the user. A trick that allows some degree of control on this number is to choose the hadoop variables, mapred.min.split.size and mapred.max.split.size. The size of each map task is determined by :\n\n        num_maps = max(mapred.min.split.size, min(mapred.max.split.size, dfs.block.size))\n\n    Typically, the default value of mapred.min.split.size is 0, that of mapred.max.split.size is Long.MAX and that of dfs.block.size is 64MB. As we can see, given the data size, tuning these parameters by \"setting\" them allows us to tune the number of mappers used. \n\n5. A few other more advanced options for optimizing Hive performance are mentioned below. These allow for the setting of memory allocated to map and reduce tasks, and can be useful in tweaking performance. Please keep in mind that the `mapreduce.reduce.memory.mb` cannot be greater than the physical memory size of each worker node in the Hadoop cluster.\n\n        set mapreduce.map.memory.mb = 2048;\n        set mapreduce.reduce.memory.mb=6144;\n        set mapreduce.reduce.java.opts=-Xmx8192m;\n        set mapred.reduce.tasks=128;\n        set mapred.tasktracker.reduce.tasks.maximum=128;\n\n[1]: ./media/machine-learning-data-science-hive-queries/atan2new.png\n[10]: ./media/machine-learning-data-science-hive-queries/run-hive-queries-1.png\n[11]: ./media/machine-learning-data-science-hive-queries/run-hive-queries-2.png\n[12]: ./media/machine-learning-data-science-hive-queries/output-hive-results-1.png\n[13]: ./media/machine-learning-data-science-hive-queries/output-hive-results-2.png\n[14]: ./media/machine-learning-data-science-hive-queries/output-hive-results-3.png\n[15]: ./media/machine-learning-data-science-hive-queries/run-hive-queries-3.png\n\n\n "
}