{
  "nodes": [
    {
      "pos": [
        27,
        110
      ],
      "content": "Import data into Machine Learning Studio from online data sources | Microsoft Azure"
    },
    {
      "pos": [
        129,
        220
      ],
      "content": "How to import your training data Azure Machine Learning Studio from various online sources."
    },
    {
      "pos": [
        635,
        737
      ],
      "content": "Import data into Azure Machine Learning Studio from various online data sources with the Reader module"
    },
    {
      "pos": [
        739,
        923
      ],
      "content": "This document describes the support for importing online data from various sources and the information needed to move data from these sources into an Azure Machine Learning experiment."
    },
    {
      "pos": [
        1048,
        1060
      ],
      "content": "Introduction"
    },
    {
      "pos": [
        1062,
        1238
      ],
      "content": "You can access data from within the Azure Machine Learning Studio from one of several online data sources while your experiment is running by using the [Reader][reader] module:"
    },
    {
      "pos": [
        1242,
        1262
      ],
      "content": "A Web URL using HTTP"
    },
    {
      "pos": [
        1265,
        1284
      ],
      "content": "Hadoop using HiveQL"
    },
    {
      "pos": [
        1287,
        1305
      ],
      "content": "Azure blob storage"
    },
    {
      "pos": [
        1308,
        1319
      ],
      "content": "Azure table"
    },
    {
      "pos": [
        1322,
        1366
      ],
      "content": "Azure SQL database or SQL Server on Azure VM"
    },
    {
      "pos": [
        1369,
        1406
      ],
      "content": "A data feed provider, OData currently"
    },
    {
      "pos": [
        1410,
        1902
      ],
      "content": "The workflow for conducting experiments in Azure Machine Learning Studio consists of dragging-and-dropping components onto the canvas. To access online data sources, add the [Reader][reader] module to your experiment, select the <bpt id=\"p1\">**</bpt>Data source<ept id=\"p1\">**</ept>, and then provide the parameters needed to access the data. The online data sources that are supported are itemized in the table below. This table also summarizes the file formats that are supported and parameters that are used to access the data.",
      "nodes": [
        {
          "content": "The workflow for conducting experiments in Azure Machine Learning Studio consists of dragging-and-dropping components onto the canvas.",
          "pos": [
            0,
            134
          ]
        },
        {
          "content": "To access online data sources, add the [Reader][reader] module to your experiment, select the <bpt id=\"p1\">**</bpt>Data source<ept id=\"p1\">**</ept>, and then provide the parameters needed to access the data.",
          "pos": [
            135,
            342
          ]
        },
        {
          "content": "The online data sources that are supported are itemized in the table below.",
          "pos": [
            343,
            418
          ]
        },
        {
          "content": "This table also summarizes the file formats that are supported and parameters that are used to access the data.",
          "pos": [
            419,
            530
          ]
        }
      ]
    },
    {
      "pos": [
        1906,
        2183
      ],
      "content": "<ph id=\"ph3\">[AZURE.NOTE]</ph><ph id=\"ph4\"/> This article provides general information about the [Reader][reader] module. For more detailed information about the types of data you can access, formats, parameters, and answers to common questions, see the module reference topic for the [Reader][reader] module.",
      "nodes": [
        {
          "content": "<ph id=\"ph3\">[AZURE.NOTE]</ph><ph id=\"ph4\"/> This article provides general information about the [Reader][reader] module.",
          "pos": [
            0,
            121
          ]
        },
        {
          "content": "For more detailed information about the types of data you can access, formats, parameters, and answers to common questions, see the module reference topic for the [Reader][reader] module.",
          "pos": [
            122,
            309
          ]
        }
      ]
    },
    {
      "pos": [
        2187,
        2426
      ],
      "content": "<ph id=\"ph5\">[AZURE.NOTE]</ph><ph id=\"ph6\"/> Because this training data is accessed while your experiment is running, it is only available in that experiment. By comparison, data that has been stored in a dataset modules are available to any experiment in your workspace.",
      "nodes": [
        {
          "content": "<ph id=\"ph5\">[AZURE.NOTE]</ph><ph id=\"ph6\"/> Because this training data is accessed while your experiment is running, it is only available in that experiment.",
          "pos": [
            0,
            158
          ]
        },
        {
          "content": "By comparison, data that has been stored in a dataset modules are available to any experiment in your workspace.",
          "pos": [
            159,
            271
          ]
        }
      ]
    },
    {
      "pos": [
        2432,
        2461
      ],
      "content": "Supported online data sources"
    },
    {
      "pos": [
        2462,
        2539
      ],
      "content": "Azure Machine Learning <bpt id=\"p2\">**</bpt>Reader<ept id=\"p2\">**</ept><ph id=\"ph7\"/> module supports the following data sources:"
    },
    {
      "pos": [
        2541,
        2552
      ],
      "content": "Data Source"
    },
    {
      "pos": [
        2555,
        2566
      ],
      "content": "Description"
    },
    {
      "pos": [
        2569,
        2579
      ],
      "content": "Parameters"
    },
    {
      "pos": [
        2595,
        2611
      ],
      "content": "Web URL via HTTP"
    },
    {
      "pos": [
        2613,
        2804
      ],
      "content": "Reads data in comma-separated values (CSV), tab-separated values (TSV), attribute-relation file format (ARFF), and Support Vector Machines (SVM-light) formats, from any web URL that uses HTTP"
    },
    {
      "pos": [
        2810,
        2813
      ],
      "content": "URL"
    },
    {
      "pos": [
        2817,
        2917
      ],
      "content": ": Specifies the full name of the file, including the site URL and the file name, with any extension."
    },
    {
      "pos": [
        2931,
        2942
      ],
      "content": "Data format"
    },
    {
      "pos": [
        2946,
        3087
      ],
      "content": ": Specifies one of the supported data formats: CSV, TSV, ARFF, or SVM-light. If the data has a header row, it is used to assign column names.",
      "nodes": [
        {
          "content": ": Specifies one of the supported data formats: CSV, TSV, ARFF, or SVM-light.",
          "pos": [
            0,
            76
          ]
        },
        {
          "content": "If the data has a header row, it is used to assign column names.",
          "pos": [
            77,
            141
          ]
        }
      ]
    },
    {
      "pos": [
        3089,
        3100
      ],
      "content": "Hadoop/HDFS"
    },
    {
      "pos": [
        3101,
        3342
      ],
      "content": "Reads data from distributed storage in Hadoop. You specify the data you want by using HiveQL, a SQL-like query language. HiveQL can also be used to aggregate data and perform data filtering before you add the data to Machine Learning Studio.",
      "nodes": [
        {
          "content": "Reads data from distributed storage in Hadoop.",
          "pos": [
            0,
            46
          ]
        },
        {
          "content": "You specify the data you want by using HiveQL, a SQL-like query language.",
          "pos": [
            47,
            120
          ]
        },
        {
          "content": "HiveQL can also be used to aggregate data and perform data filtering before you add the data to Machine Learning Studio.",
          "pos": [
            121,
            241
          ]
        }
      ]
    },
    {
      "pos": [
        3343,
        4315
      ],
      "content": "<ph id=\"ph8\">&lt;b&gt;</ph>Hive database query<ph id=\"ph9\">&lt;/b&gt;</ph>: Specifies the Hive query used to generate the data.<ph id=\"ph10\">&lt;br/&gt;</ph><ph id=\"ph11\">&lt;br/&gt;</ph><ph id=\"ph12\">&lt;b&gt;</ph>HCatalog server URI <ph id=\"ph13\">&lt;/b&gt;</ph><ph id=\"ph14\"/> : Specified the name of your cluster using the format <bpt id=\"p3\">*</bpt>&amp;lt;your cluster name&amp;gt;.azurehdinsight.net.<ept id=\"p3\">*</ept><ph id=\"ph15\">&lt;br/&gt;</ph><ph id=\"ph16\">&lt;br/&gt;</ph><ph id=\"ph17\">&lt;b&gt;</ph>Hadoop user account name<ph id=\"ph18\">&lt;/b&gt;</ph>: Specifies the Hadoop user account name used to provision the cluster.<ph id=\"ph19\">&lt;br/&gt;</ph><ph id=\"ph20\">&lt;br/&gt;</ph><ph id=\"ph21\">&lt;b&gt;</ph>Hadoop user account password<ph id=\"ph22\">&lt;/b&gt;</ph><ph id=\"ph23\"/> : Specifies the credentials used when provisioning the cluster. For more information, see <bpt id=\"p4\">[</bpt>Create Hadoop clusters in HDInsight<ept id=\"p4\">](article-hdinsight-provision-clusters)</ept>.<ph id=\"ph24\">&lt;br/&gt;</ph><ph id=\"ph25\">&lt;br/&gt;</ph><ph id=\"ph26\">&lt;b&gt;</ph>Location of output data<ph id=\"ph27\">&lt;/b&gt;</ph>: Specifies whether the data is stored in a Hadoop distributed file system (HDFS) or in Azure. <ph id=\"ph28\">&lt;br/&gt;</ph><ph id=\"ph29\">&lt;ul&gt;</ph>If you store output data in HDFS, specify the HDFS server URI. (Be sure to use the HDInsight cluster name without the HTTPS:// prefix). <ph id=\"ph30\">&lt;br/&gt;</ph><ph id=\"ph31\">&lt;br/&gt;</ph>If you store your output data in Azure, you must specify the Azure storage account name, Storage access key and Storage container name.<ph id=\"ph32\">&lt;/ul&gt;</ph>",
      "nodes": [
        {
          "content": "<ph id=\"ph8\">&lt;b&gt;</ph>Hive database query<ph id=\"ph9\">&lt;/b&gt;</ph>: Specifies the Hive query used to generate the data.<ph id=\"ph10\">&lt;br/&gt;</ph><ph id=\"ph11\">&lt;br/&gt;</ph><ph id=\"ph12\">&lt;b&gt;</ph>HCatalog server URI <ph id=\"ph13\">&lt;/b&gt;</ph><ph id=\"ph14\"/> : Specified the name of your cluster using the format <bpt id=\"p3\">*</bpt>&amp;lt;your cluster name&amp;gt;.azurehdinsight.net.<ept id=\"p3\">*</ept><ph id=\"ph15\">&lt;br/&gt;</ph><ph id=\"ph16\">&lt;br/&gt;</ph><ph id=\"ph17\">&lt;b&gt;</ph>Hadoop user account name<ph id=\"ph18\">&lt;/b&gt;</ph>: Specifies the Hadoop user account name used to provision the cluster.<ph id=\"ph19\">&lt;br/&gt;</ph><ph id=\"ph20\">&lt;br/&gt;</ph><ph id=\"ph21\">&lt;b&gt;</ph>Hadoop user account password<ph id=\"ph22\">&lt;/b&gt;</ph><ph id=\"ph23\"/> : Specifies the credentials used when provisioning the cluster.",
          "pos": [
            0,
            863
          ]
        },
        {
          "content": "For more information, see <bpt id=\"p4\">[</bpt>Create Hadoop clusters in HDInsight<ept id=\"p4\">](article-hdinsight-provision-clusters)</ept>.<ph id=\"ph24\">&lt;br/&gt;</ph><ph id=\"ph25\">&lt;br/&gt;</ph><ph id=\"ph26\">&lt;b&gt;</ph>Location of output data<ph id=\"ph27\">&lt;/b&gt;</ph>: Specifies whether the data is stored in a Hadoop distributed file system (HDFS) or in Azure.",
          "pos": [
            864,
            1238
          ]
        },
        {
          "content": "<ph id=\"ph28\">&lt;br/&gt;</ph><ph id=\"ph29\">&lt;ul&gt;</ph>If you store output data in HDFS, specify the HDFS server URI.",
          "pos": [
            1239,
            1360
          ]
        },
        {
          "content": "(Be sure to use the HDInsight cluster name without the HTTPS:// prefix).",
          "pos": [
            1361,
            1433
          ]
        },
        {
          "content": "<ph id=\"ph30\">&lt;br/&gt;</ph><ph id=\"ph31\">&lt;br/&gt;</ph>If you store your output data in Azure, you must specify the Azure storage account name, Storage access key and Storage container name.<ph id=\"ph32\">&lt;/ul&gt;</ph>",
          "pos": [
            1434,
            1659
          ]
        }
      ]
    },
    {
      "pos": [
        4317,
        4329
      ],
      "content": "SQL database"
    },
    {
      "pos": [
        4331,
        4446
      ],
      "content": "Reads data that is stored in an Azure SQL database or in a SQL Server database running on an Azure virtual machine."
    },
    {
      "pos": [
        4449,
        5372
      ],
      "content": "<ph id=\"ph33\">&lt;b&gt;</ph>Database server name<ph id=\"ph34\">&lt;/b&gt;</ph>: Specifies the name of the server on which the database is running.<ph id=\"ph35\">&lt;br/&gt;</ph><ph id=\"ph36\">&lt;ul&gt;</ph>In case of Azure SQL Database enter the server name that is generated. Typically it has the form <bpt id=\"p5\">*</bpt>&amp;lt;generated_identifier&amp;gt;.database.windows.net.<ept id=\"p5\">*</ept> <ph id=\"ph37\">&lt;br/&gt;</ph><ph id=\"ph38\">&lt;br/&gt;</ph>In case of a SQL server hosted on a Azure Virtual machine enter <bpt id=\"p6\">*</bpt>tcp:&amp;lt;Virtual Machine DNS Name&amp;gt;, 1433<ept id=\"p6\">*</ept><ph id=\"ph39\">&lt;/ul&gt;</ph><ph id=\"ph40\">&lt;br/&gt;</ph><ph id=\"ph41\">&lt;b&gt;</ph>Database name <ph id=\"ph42\">&lt;/b&gt;</ph><ph id=\"ph43\"/> : Specifies the name of the database on the server. <ph id=\"ph44\">&lt;br/&gt;</ph><ph id=\"ph45\">&lt;br/&gt;</ph><ph id=\"ph46\">&lt;b&gt;</ph>Server user account name<ph id=\"ph47\">&lt;/b&gt;</ph>: Specifies a user name for an account that has access permissions for the database. <ph id=\"ph48\">&lt;br/&gt;</ph><ph id=\"ph49\">&lt;br/&gt;</ph><ph id=\"ph50\">&lt;b&gt;</ph>Server user account password<ph id=\"ph51\">&lt;/b&gt;</ph>: Specifies the password for the user account.<ph id=\"ph52\">&lt;br/&gt;</ph><ph id=\"ph53\">&lt;br/&gt;</ph><ph id=\"ph54\">&lt;b&gt;</ph>Accept any server certificate<ph id=\"ph55\">&lt;/b&gt;</ph>: Use this option (less secure) if you want to skip reviewing the site certificate before you read your data.<ph id=\"ph56\">&lt;br/&gt;</ph><ph id=\"ph57\">&lt;br/&gt;</ph><ph id=\"ph58\">&lt;b&gt;</ph>Database query<ph id=\"ph59\">&lt;/b&gt;</ph>:Enter a SQL statement that describes the data you want to read.",
      "nodes": [
        {
          "content": "<ph id=\"ph33\">&lt;b&gt;</ph>Database server name<ph id=\"ph34\">&lt;/b&gt;</ph>: Specifies the name of the server on which the database is running.<ph id=\"ph35\">&lt;br/&gt;</ph><ph id=\"ph36\">&lt;ul&gt;</ph>In case of Azure SQL Database enter the server name that is generated.",
          "pos": [
            0,
            274
          ]
        },
        {
          "content": "Typically it has the form <bpt id=\"p5\">*</bpt>&amp;lt;generated_identifier&amp;gt;.database.windows.net.<ept id=\"p5\">*</ept> <ph id=\"ph37\">&lt;br/&gt;</ph><ph id=\"ph38\">&lt;br/&gt;</ph>In case of a SQL server hosted on a Azure Virtual machine enter <bpt id=\"p6\">*</bpt>tcp:&amp;lt;Virtual Machine DNS Name&amp;gt;, 1433<ept id=\"p6\">*</ept><ph id=\"ph39\">&lt;/ul&gt;</ph><ph id=\"ph40\">&lt;br/&gt;</ph><ph id=\"ph41\">&lt;b&gt;</ph>Database name <ph id=\"ph42\">&lt;/b&gt;</ph><ph id=\"ph43\"/> : Specifies the name of the database on the server.",
          "pos": [
            275,
            812
          ]
        },
        {
          "content": "<ph id=\"ph44\">&lt;br/&gt;</ph><ph id=\"ph45\">&lt;br/&gt;</ph><ph id=\"ph46\">&lt;b&gt;</ph>Server user account name<ph id=\"ph47\">&lt;/b&gt;</ph>: Specifies a user name for an account that has access permissions for the database.",
          "pos": [
            813,
            1038
          ]
        },
        {
          "content": "<ph id=\"ph48\">&lt;br/&gt;</ph><ph id=\"ph49\">&lt;br/&gt;</ph><ph id=\"ph50\">&lt;b&gt;</ph>Server user account password<ph id=\"ph51\">&lt;/b&gt;</ph>: Specifies the password for the user account.<ph id=\"ph52\">&lt;br/&gt;</ph><ph id=\"ph53\">&lt;br/&gt;</ph><ph id=\"ph54\">&lt;b&gt;</ph>Accept any server certificate<ph id=\"ph55\">&lt;/b&gt;</ph>: Use this option (less secure) if you want to skip reviewing the site certificate before you read your data.<ph id=\"ph56\">&lt;br/&gt;</ph><ph id=\"ph57\">&lt;br/&gt;</ph><ph id=\"ph58\">&lt;b&gt;</ph>Database query<ph id=\"ph59\">&lt;/b&gt;</ph>:Enter a SQL statement that describes the data you want to read.",
          "pos": [
            1039,
            1680
          ]
        }
      ]
    },
    {
      "pos": [
        5374,
        5385
      ],
      "content": "Azure Table"
    },
    {
      "pos": [
        5386,
        5437
      ],
      "content": "Reads data from the Table service in Azure Storage."
    },
    {
      "pos": [
        5447,
        5643
      ],
      "content": "If you read large amounts of data infrequently, use the Azure Table Service. It provides a flexible, non-relational (NoSQL), massively scalable, inexpensive, and highly available storage solution.",
      "nodes": [
        {
          "content": "If you read large amounts of data infrequently, use the Azure Table Service.",
          "pos": [
            0,
            76
          ]
        },
        {
          "content": "It provides a flexible, non-relational (NoSQL), massively scalable, inexpensive, and highly available storage solution.",
          "pos": [
            77,
            196
          ]
        }
      ]
    },
    {
      "pos": [
        5645,
        7640
      ],
      "content": "The options in the <bpt id=\"p7\">**</bpt>Reader<ept id=\"p7\">**</ept><ph id=\"ph60\"/> change depending on whether you are accessing public information or a private storage account that requires login credentials. This is determined by the <ph id=\"ph61\">&lt;b&gt;</ph>Authentication Type<ph id=\"ph62\">&lt;/b&gt;</ph><ph id=\"ph63\"/> which can have value of \"PublicOrSAS\" or \"Account\", each of which has its own set of parameters. <ph id=\"ph64\">&lt;br/&gt;</ph><ph id=\"ph65\">&lt;br/&gt;</ph><ph id=\"ph66\">&lt;b&gt;</ph>Public or Shared Access Signature (SAS) URI<ph id=\"ph67\">&lt;/b&gt;</ph>: The parameters are:<ph id=\"ph68\">&lt;br/&gt;</ph><ph id=\"ph69\">&lt;br/&gt;</ph><ph id=\"ph70\">&lt;ul&gt;</ph><ph id=\"ph71\">&lt;b&gt;</ph>Table URI<ph id=\"ph72\">&lt;/b&gt;</ph>: Specifies the Public or SAS URL for the table.<ph id=\"ph73\">&lt;br/&gt;</ph><ph id=\"ph74\">&lt;br/&gt;</ph><ph id=\"ph75\">&lt;b&gt;</ph>Specifies the rows to scan for property names<ph id=\"ph76\">&lt;/b&gt;</ph>: The values are <ph id=\"ph77\">&lt;i&gt;</ph>TopN<ph id=\"ph78\">&lt;/i&gt;</ph><ph id=\"ph79\"/> to scan the specified number of rows, or <ph id=\"ph80\">&lt;i&gt;</ph>ScanAll<ph id=\"ph81\">&lt;/i&gt;</ph><ph id=\"ph82\"/> to get all rows in the table. <ph id=\"ph83\">&lt;br/&gt;</ph><ph id=\"ph84\">&lt;br/&gt;</ph>If the data is homogeneous and predictable, it is recommended that you select <bpt id=\"p8\">*</bpt>TopN<ept id=\"p8\">*</ept><ph id=\"ph85\"/> and enter a number for N. For large tables, this can result in quicker reading times.<ph id=\"ph86\">&lt;br/&gt;</ph><ph id=\"ph87\">&lt;br/&gt;</ph>If the data is structured with sets of properties that vary based on the depth and position of the table, choose the <bpt id=\"p9\">*</bpt>ScanAll<ept id=\"p9\">*</ept><ph id=\"ph88\"/> option to scan all rows. This ensures the integrity of your resulting property and metadata conversion.<ph id=\"ph89\">&lt;br/&gt;</ph><ph id=\"ph90\">&lt;br/&gt;</ph><ph id=\"ph91\">&lt;/ul&gt;</ph><ph id=\"ph92\">&lt;b&gt;</ph>Private Storage Account<ph id=\"ph93\">&lt;/b&gt;</ph>: The parameters are: <ph id=\"ph94\">&lt;br/&gt;</ph><ph id=\"ph95\">&lt;br/&gt;</ph><ph id=\"ph96\">&lt;ul&gt;</ph><ph id=\"ph97\">&lt;b&gt;</ph>Account name<ph id=\"ph98\">&lt;/b&gt;</ph>: Specifies the name of the account that contains the table to read.<ph id=\"ph99\">&lt;br/&gt;</ph><ph id=\"ph100\">&lt;br/&gt;</ph><ph id=\"ph101\">&lt;b&gt;</ph>Account key<ph id=\"ph102\">&lt;/b&gt;</ph>: Specifies the storage key associated with the account.<ph id=\"ph103\">&lt;br/&gt;</ph><ph id=\"ph104\">&lt;br/&gt;</ph><ph id=\"ph105\">&lt;b&gt;</ph>Table name<ph id=\"ph106\">&lt;/b&gt;</ph><ph id=\"ph107\"/> : Specifies the name of the table that contains the data to read.<ph id=\"ph108\">&lt;br/&gt;</ph><ph id=\"ph109\">&lt;br/&gt;</ph><ph id=\"ph110\">&lt;b&gt;</ph>Rows to scan for property names<ph id=\"ph111\">&lt;/b&gt;</ph>: The values are <ph id=\"ph112\">&lt;i&gt;</ph>TopN<ph id=\"ph113\">&lt;/i&gt;</ph><ph id=\"ph114\"/> to scan the specified number of rows, or <ph id=\"ph115\">&lt;i&gt;</ph>ScanAll<ph id=\"ph116\">&lt;/i&gt;</ph><ph id=\"ph117\"/> to get all rows in the table.<ph id=\"ph118\">&lt;br/&gt;</ph><ph id=\"ph119\">&lt;br/&gt;</ph>If the data is homogeneous and predictable, we recommend that you select <bpt id=\"p10\">*</bpt>TopN<ept id=\"p10\">*</ept><ph id=\"ph120\"/> and enter a number for N. For large tables, this can result in quicker reading times.<ph id=\"ph121\">&lt;br/&gt;</ph><ph id=\"ph122\">&lt;br/&gt;</ph>If the data is structured with sets of properties that vary based on the depth and position of the table, choose the <bpt id=\"p11\">*</bpt>ScanAll<ept id=\"p11\">*</ept><ph id=\"ph123\"/> option to scan all rows. This ensures the integrity of your resulting property and metadata conversion.<ph id=\"ph124\">&lt;br/&gt;</ph><ph id=\"ph125\">&lt;br/&gt;</ph>",
      "nodes": [
        {
          "content": "The options in the <bpt id=\"p7\">**</bpt>Reader<ept id=\"p7\">**</ept><ph id=\"ph60\"/> change depending on whether you are accessing public information or a private storage account that requires login credentials.",
          "pos": [
            0,
            209
          ]
        },
        {
          "content": "This is determined by the <ph id=\"ph61\">&lt;b&gt;</ph>Authentication Type<ph id=\"ph62\">&lt;/b&gt;</ph><ph id=\"ph63\"/> which can have value of \"PublicOrSAS\" or \"Account\", each of which has its own set of parameters.",
          "pos": [
            210,
            424
          ]
        },
        {
          "content": "<ph id=\"ph64\">&lt;br/&gt;</ph><ph id=\"ph65\">&lt;br/&gt;</ph><ph id=\"ph66\">&lt;b&gt;</ph>Public or Shared Access Signature (SAS) URI<ph id=\"ph67\">&lt;/b&gt;</ph>: The parameters are:<ph id=\"ph68\">&lt;br/&gt;</ph><ph id=\"ph69\">&lt;br/&gt;</ph><ph id=\"ph70\">&lt;ul&gt;</ph><ph id=\"ph71\">&lt;b&gt;</ph>Table URI<ph id=\"ph72\">&lt;/b&gt;</ph>: Specifies the Public or SAS URL for the table.<ph id=\"ph73\">&lt;br/&gt;</ph><ph id=\"ph74\">&lt;br/&gt;</ph><ph id=\"ph75\">&lt;b&gt;</ph>Specifies the rows to scan for property names<ph id=\"ph76\">&lt;/b&gt;</ph>: The values are <ph id=\"ph77\">&lt;i&gt;</ph>TopN<ph id=\"ph78\">&lt;/i&gt;</ph><ph id=\"ph79\"/> to scan the specified number of rows, or <ph id=\"ph80\">&lt;i&gt;</ph>ScanAll<ph id=\"ph81\">&lt;/i&gt;</ph><ph id=\"ph82\"/> to get all rows in the table.",
          "pos": [
            425,
            1215
          ]
        },
        {
          "content": "<ph id=\"ph83\">&lt;br/&gt;</ph><ph id=\"ph84\">&lt;br/&gt;</ph>If the data is homogeneous and predictable, it is recommended that you select <bpt id=\"p8\">*</bpt>TopN<ept id=\"p8\">*</ept><ph id=\"ph85\"/> and enter a number for N. For large tables, this can result in quicker reading times.<ph id=\"ph86\">&lt;br/&gt;</ph><ph id=\"ph87\">&lt;br/&gt;</ph>If the data is structured with sets of properties that vary based on the depth and position of the table, choose the <bpt id=\"p9\">*</bpt>ScanAll<ept id=\"p9\">*</ept><ph id=\"ph88\"/> option to scan all rows.",
          "pos": [
            1216,
            1763
          ]
        },
        {
          "content": "This ensures the integrity of your resulting property and metadata conversion.<ph id=\"ph89\">&lt;br/&gt;</ph><ph id=\"ph90\">&lt;br/&gt;</ph><ph id=\"ph91\">&lt;/ul&gt;</ph><ph id=\"ph92\">&lt;b&gt;</ph>Private Storage Account<ph id=\"ph93\">&lt;/b&gt;</ph>: The parameters are: <ph id=\"ph94\">&lt;br/&gt;</ph><ph id=\"ph95\">&lt;br/&gt;</ph><ph id=\"ph96\">&lt;ul&gt;</ph><ph id=\"ph97\">&lt;b&gt;</ph>Account name<ph id=\"ph98\">&lt;/b&gt;</ph>: Specifies the name of the account that contains the table to read.<ph id=\"ph99\">&lt;br/&gt;</ph><ph id=\"ph100\">&lt;br/&gt;</ph><ph id=\"ph101\">&lt;b&gt;</ph>Account key<ph id=\"ph102\">&lt;/b&gt;</ph>: Specifies the storage key associated with the account.<ph id=\"ph103\">&lt;br/&gt;</ph><ph id=\"ph104\">&lt;br/&gt;</ph><ph id=\"ph105\">&lt;b&gt;</ph>Table name<ph id=\"ph106\">&lt;/b&gt;</ph><ph id=\"ph107\"/> : Specifies the name of the table that contains the data to read.<ph id=\"ph108\">&lt;br/&gt;</ph><ph id=\"ph109\">&lt;br/&gt;</ph><ph id=\"ph110\">&lt;b&gt;</ph>Rows to scan for property names<ph id=\"ph111\">&lt;/b&gt;</ph>: The values are <ph id=\"ph112\">&lt;i&gt;</ph>TopN<ph id=\"ph113\">&lt;/i&gt;</ph><ph id=\"ph114\"/> to scan the specified number of rows, or <ph id=\"ph115\">&lt;i&gt;</ph>ScanAll<ph id=\"ph116\">&lt;/i&gt;</ph><ph id=\"ph117\"/> to get all rows in the table.<ph id=\"ph118\">&lt;br/&gt;</ph><ph id=\"ph119\">&lt;br/&gt;</ph>If the data is homogeneous and predictable, we recommend that you select <bpt id=\"p10\">*</bpt>TopN<ept id=\"p10\">*</ept><ph id=\"ph120\"/> and enter a number for N. For large tables, this can result in quicker reading times.<ph id=\"ph121\">&lt;br/&gt;</ph><ph id=\"ph122\">&lt;br/&gt;</ph>If the data is structured with sets of properties that vary based on the depth and position of the table, choose the <bpt id=\"p11\">*</bpt>ScanAll<ept id=\"p11\">*</ept><ph id=\"ph123\"/> option to scan all rows.",
          "pos": [
            1764,
            3614
          ]
        },
        {
          "content": "This ensures the integrity of your resulting property and metadata conversion.<ph id=\"ph124\">&lt;br/&gt;</ph><ph id=\"ph125\">&lt;br/&gt;</ph>",
          "pos": [
            3615,
            3755
          ]
        }
      ]
    },
    {
      "pos": [
        7642,
        7660
      ],
      "content": "Azure Blob Storage"
    },
    {
      "pos": [
        7663,
        7771
      ],
      "content": "Reads data stored in the Blob service in Azure Storage, including images, unstructured text, or binary data."
    },
    {
      "pos": [
        7781,
        7949
      ],
      "content": "You can use the Blob service to publicly expose data, or to privately store application data. You can access your data from anywhere by using HTTP or HTTPS connections.",
      "nodes": [
        {
          "content": "You can use the Blob service to publicly expose data, or to privately store application data.",
          "pos": [
            0,
            93
          ]
        },
        {
          "content": "You can access your data from anywhere by using HTTP or HTTPS connections.",
          "pos": [
            94,
            168
          ]
        }
      ]
    },
    {
      "pos": [
        7952,
        9503
      ],
      "content": "The options in the <bpt id=\"p12\">**</bpt>Reader<ept id=\"p12\">**</ept><ph id=\"ph126\"/> module change depending on whether you are accessing public information or a private storage account that requires login credentials. This is determined by the <ph id=\"ph127\">&lt;b&gt;</ph>Authentication Type<ph id=\"ph128\">&lt;/b&gt;</ph><ph id=\"ph129\"/> which can have a value either of \"PublicOrSAS\" or of \"Account\".<ph id=\"ph130\">&lt;br/&gt;</ph><ph id=\"ph131\">&lt;br/&gt;</ph><ph id=\"ph132\">&lt;b&gt;</ph>Public or Shared Access Signature (SAS) URI<ph id=\"ph133\">&lt;/b&gt;</ph>: The parameters are:<ph id=\"ph134\">&lt;br/&gt;</ph><ph id=\"ph135\">&lt;br/&gt;</ph><ph id=\"ph136\">&lt;ul&gt;</ph><ph id=\"ph137\">&lt;b&gt;</ph>URI<ph id=\"ph138\">&lt;/b&gt;</ph>: Specifies the Public or SAS URL for the storage blob.<ph id=\"ph139\">&lt;br/&gt;</ph><ph id=\"ph140\">&lt;br/&gt;</ph><ph id=\"ph141\">&lt;b&gt;</ph>File Format<ph id=\"ph142\">&lt;/b&gt;</ph>: Specifies the format of the data in the Blob service. The supported formats are CSV, TSV, and ARFF.<ph id=\"ph143\">&lt;br/&gt;</ph><ph id=\"ph144\">&lt;br/&gt;</ph><ph id=\"ph145\">&lt;/ul&gt;</ph><ph id=\"ph146\">&lt;b&gt;</ph>Private Storage Account<ph id=\"ph147\">&lt;/b&gt;</ph>: The parameters are: <ph id=\"ph148\">&lt;br/&gt;</ph><ph id=\"ph149\">&lt;br/&gt;</ph><ph id=\"ph150\">&lt;ul&gt;</ph><ph id=\"ph151\">&lt;b&gt;</ph>Account name<ph id=\"ph152\">&lt;/b&gt;</ph>: Specifies the name of the account that contains the blob you want to read.<ph id=\"ph153\">&lt;br/&gt;</ph><ph id=\"ph154\">&lt;br/&gt;</ph><ph id=\"ph155\">&lt;b&gt;</ph>Account key<ph id=\"ph156\">&lt;/b&gt;</ph>: Specifies the storage key associated with the account.<ph id=\"ph157\">&lt;br/&gt;</ph><ph id=\"ph158\">&lt;br/&gt;</ph><ph id=\"ph159\">&lt;b&gt;</ph>Path to container, directory, or blob <ph id=\"ph160\">&lt;/b&gt;</ph><ph id=\"ph161\"/> : Specifies the name of the blob that contains the data to read.<ph id=\"ph162\">&lt;br/&gt;</ph><ph id=\"ph163\">&lt;br/&gt;</ph><ph id=\"ph164\">&lt;b&gt;</ph>Blob file format<ph id=\"ph165\">&lt;/b&gt;</ph>: Specifies the format of the data in the blob service. The supported data formats are CSV, TSV, ARFF, CSV with a specified encoding, and Excel. <ph id=\"ph166\">&lt;br/&gt;</ph><ph id=\"ph167\">&lt;br/&gt;</ph><ph id=\"ph168\">&lt;ul&gt;</ph>If the format is CSV or TSV, be sure to indicate whether the file contains a header row.<ph id=\"ph169\">&lt;br/&gt;</ph><ph id=\"ph170\">&lt;br/&gt;</ph>You can use the Excel option to read data from Excel workbooks. In the <ph id=\"ph171\">&lt;i&gt;</ph>Excel data format<ph id=\"ph172\">&lt;/i&gt;</ph><ph id=\"ph173\"/> option, indicate whether the data is in an Excel worksheet range, or in an Excel table. In the <ph id=\"ph174\">&lt;i&gt;</ph>Excel sheet or embedded table <ph id=\"ph175\">&lt;/i&gt;</ph>option, specify the name of the sheet or table that you want to read from.<ph id=\"ph176\">&lt;/ul&gt;</ph><ph id=\"ph177\">&lt;br/&gt;</ph>",
      "nodes": [
        {
          "content": "The options in the <bpt id=\"p12\">**</bpt>Reader<ept id=\"p12\">**</ept><ph id=\"ph126\"/> module change depending on whether you are accessing public information or a private storage account that requires login credentials.",
          "pos": [
            0,
            219
          ]
        },
        {
          "content": "This is determined by the <ph id=\"ph127\">&lt;b&gt;</ph>Authentication Type<ph id=\"ph128\">&lt;/b&gt;</ph><ph id=\"ph129\"/> which can have a value either of \"PublicOrSAS\" or of \"Account\".<ph id=\"ph130\">&lt;br/&gt;</ph><ph id=\"ph131\">&lt;br/&gt;</ph><ph id=\"ph132\">&lt;b&gt;</ph>Public or Shared Access Signature (SAS) URI<ph id=\"ph133\">&lt;/b&gt;</ph>: The parameters are:<ph id=\"ph134\">&lt;br/&gt;</ph><ph id=\"ph135\">&lt;br/&gt;</ph><ph id=\"ph136\">&lt;ul&gt;</ph><ph id=\"ph137\">&lt;b&gt;</ph>URI<ph id=\"ph138\">&lt;/b&gt;</ph>: Specifies the Public or SAS URL for the storage blob.<ph id=\"ph139\">&lt;br/&gt;</ph><ph id=\"ph140\">&lt;br/&gt;</ph><ph id=\"ph141\">&lt;b&gt;</ph>File Format<ph id=\"ph142\">&lt;/b&gt;</ph>: Specifies the format of the data in the Blob service.",
          "pos": [
            220,
            985
          ]
        },
        {
          "content": "The supported formats are CSV, TSV, and ARFF.<ph id=\"ph143\">&lt;br/&gt;</ph><ph id=\"ph144\">&lt;br/&gt;</ph><ph id=\"ph145\">&lt;/ul&gt;</ph><ph id=\"ph146\">&lt;b&gt;</ph>Private Storage Account<ph id=\"ph147\">&lt;/b&gt;</ph>: The parameters are: <ph id=\"ph148\">&lt;br/&gt;</ph><ph id=\"ph149\">&lt;br/&gt;</ph><ph id=\"ph150\">&lt;ul&gt;</ph><ph id=\"ph151\">&lt;b&gt;</ph>Account name<ph id=\"ph152\">&lt;/b&gt;</ph>: Specifies the name of the account that contains the blob you want to read.<ph id=\"ph153\">&lt;br/&gt;</ph><ph id=\"ph154\">&lt;br/&gt;</ph><ph id=\"ph155\">&lt;b&gt;</ph>Account key<ph id=\"ph156\">&lt;/b&gt;</ph>: Specifies the storage key associated with the account.<ph id=\"ph157\">&lt;br/&gt;</ph><ph id=\"ph158\">&lt;br/&gt;</ph><ph id=\"ph159\">&lt;b&gt;</ph>Path to container, directory, or blob <ph id=\"ph160\">&lt;/b&gt;</ph><ph id=\"ph161\"/> : Specifies the name of the blob that contains the data to read.<ph id=\"ph162\">&lt;br/&gt;</ph><ph id=\"ph163\">&lt;br/&gt;</ph><ph id=\"ph164\">&lt;b&gt;</ph>Blob file format<ph id=\"ph165\">&lt;/b&gt;</ph>: Specifies the format of the data in the blob service.",
          "pos": [
            986,
            2087
          ]
        },
        {
          "content": "The supported data formats are CSV, TSV, ARFF, CSV with a specified encoding, and Excel.",
          "pos": [
            2088,
            2176
          ]
        },
        {
          "content": "<ph id=\"ph166\">&lt;br/&gt;</ph><ph id=\"ph167\">&lt;br/&gt;</ph><ph id=\"ph168\">&lt;ul&gt;</ph>If the format is CSV or TSV, be sure to indicate whether the file contains a header row.<ph id=\"ph169\">&lt;br/&gt;</ph><ph id=\"ph170\">&lt;br/&gt;</ph>You can use the Excel option to read data from Excel workbooks.",
          "pos": [
            2177,
            2482
          ]
        },
        {
          "content": "In the <ph id=\"ph171\">&lt;i&gt;</ph>Excel data format<ph id=\"ph172\">&lt;/i&gt;</ph><ph id=\"ph173\"/> option, indicate whether the data is in an Excel worksheet range, or in an Excel table.",
          "pos": [
            2483,
            2670
          ]
        },
        {
          "content": "In the <ph id=\"ph174\">&lt;i&gt;</ph>Excel sheet or embedded table <ph id=\"ph175\">&lt;/i&gt;</ph>option, specify the name of the sheet or table that you want to read from.<ph id=\"ph176\">&lt;/ul&gt;</ph><ph id=\"ph177\">&lt;br/&gt;</ph>",
          "pos": [
            2671,
            2903
          ]
        }
      ]
    },
    {
      "pos": [
        9505,
        9523
      ],
      "content": "Data Feed Provider"
    },
    {
      "pos": [
        9526,
        9635
      ],
      "content": "Reads data from a supported feed provider. Currently only the Open Data Protocol (OData) format is supported.",
      "nodes": [
        {
          "content": "Reads data from a supported feed provider.",
          "pos": [
            0,
            42
          ]
        },
        {
          "content": "Currently only the Open Data Protocol (OData) format is supported.",
          "pos": [
            43,
            109
          ]
        }
      ]
    },
    {
      "pos": [
        9641,
        9658
      ],
      "content": "Data content type"
    },
    {
      "pos": [
        9662,
        9691
      ],
      "content": ": Specifies the OData format."
    },
    {
      "pos": [
        9704,
        9714
      ],
      "content": "Source URL"
    },
    {
      "pos": [
        9718,
        9761
      ],
      "content": ": Specifies the full URL for the data feed."
    },
    {
      "pos": [
        9767,
        9890
      ],
      "content": "For example, the following URL reads from the Northwind sample database: http://services.odata.org/northwind/northwind.svc/"
    },
    {
      "pos": [
        9921,
        10009
      ],
      "content": "[reader]: https://msdn.microsoft.com/library/azure/4e1b0fe6-aded-4b3f-a36f-39b8862b9004/"
    }
  ],
  "content": "<properties\n    pageTitle=\"Import data into Machine Learning Studio from online data sources | Microsoft Azure\"\n    description=\"How to import your training data Azure Machine Learning Studio from various online sources.\"\n    keywords=\"import data,data format,data types,data sources,training data\"\n    services=\"machine-learning\"\n    documentationCenter=\"\"\n    authors=\"bradsev\"\n    manager=\"paulettm\"\n    editor=\"cgronlun\"/>\n\n<tags\n    ms.service=\"machine-learning\"\n    ms.workload=\"data-services\"\n    ms.tgt_pltfrm=\"na\"\n    ms.devlang=\"na\"\n    ms.topic=\"article\"\n    ms.date=\"02/09/2016\"\n    ms.author=\"bradsev;garye;gopitk\" />\n\n\n# Import data into Azure Machine Learning Studio from various online data sources with the Reader module\n\nThis document describes the support for importing online data from various sources and the information needed to move data from these sources into an Azure Machine Learning experiment.\n\n[AZURE.INCLUDE [import-data-into-aml-studio-selector](../../includes/machine-learning-import-data-into-aml-studio.md)]\n\n## Introduction\n\nYou can access data from within the Azure Machine Learning Studio from one of several online data sources while your experiment is running by using the [Reader][reader] module:\n\n- A Web URL using HTTP\n- Hadoop using HiveQL\n- Azure blob storage\n- Azure table\n- Azure SQL database or SQL Server on Azure VM\n- A data feed provider, OData currently\n \n\nThe workflow for conducting experiments in Azure Machine Learning Studio consists of dragging-and-dropping components onto the canvas. To access online data sources, add the [Reader][reader] module to your experiment, select the **Data source**, and then provide the parameters needed to access the data. The online data sources that are supported are itemized in the table below. This table also summarizes the file formats that are supported and parameters that are used to access the data.\n\n> [AZURE.NOTE] This article provides general information about the [Reader][reader] module. For more detailed information about the types of data you can access, formats, parameters, and answers to common questions, see the module reference topic for the [Reader][reader] module.\n\n> [AZURE.NOTE] Because this training data is accessed while your experiment is running, it is only available in that experiment. By comparison, data that has been stored in a dataset modules are available to any experiment in your workspace.\n\n\n## Supported online data sources\nAzure Machine Learning **Reader** module supports the following data sources:\n\nData Source | Description | Parameters |\n---|---|---|\nWeb URL via HTTP |Reads data in comma-separated values (CSV), tab-separated values (TSV), attribute-relation file format (ARFF), and Support Vector Machines (SVM-light) formats, from any web URL that uses HTTP | <b>URL</b>: Specifies the full name of the file, including the site URL and the file name, with any extension. <br/><br/><b>Data format</b>: Specifies one of the supported data formats: CSV, TSV, ARFF, or SVM-light. If the data has a header row, it is used to assign column names.|\nHadoop/HDFS|Reads data from distributed storage in Hadoop. You specify the data you want by using HiveQL, a SQL-like query language. HiveQL can also be used to aggregate data and perform data filtering before you add the data to Machine Learning Studio.|<b>Hive database query</b>: Specifies the Hive query used to generate the data.<br/><br/><b>HCatalog server URI </b> : Specified the name of your cluster using the format *&lt;your cluster name&gt;.azurehdinsight.net.*<br/><br/><b>Hadoop user account name</b>: Specifies the Hadoop user account name used to provision the cluster.<br/><br/><b>Hadoop user account password</b> : Specifies the credentials used when provisioning the cluster. For more information, see [Create Hadoop clusters in HDInsight](article-hdinsight-provision-clusters).<br/><br/><b>Location of output data</b>: Specifies whether the data is stored in a Hadoop distributed file system (HDFS) or in Azure. <br/><ul>If you store output data in HDFS, specify the HDFS server URI. (Be sure to use the HDInsight cluster name without the HTTPS:// prefix). <br/><br/>If you store your output data in Azure, you must specify the Azure storage account name, Storage access key and Storage container name.</ul>|\nSQL database |Reads data that is stored in an Azure SQL database or in a SQL Server database running on an Azure virtual machine. | <b>Database server name</b>: Specifies the name of the server on which the database is running.<br/><ul>In case of Azure SQL Database enter the server name that is generated. Typically it has the form *&lt;generated_identifier&gt;.database.windows.net.* <br/><br/>In case of a SQL server hosted on a Azure Virtual machine enter *tcp:&lt;Virtual Machine DNS Name&gt;, 1433*</ul><br/><b>Database name </b> : Specifies the name of the database on the server. <br/><br/><b>Server user account name</b>: Specifies a user name for an account that has access permissions for the database. <br/><br/><b>Server user account password</b>: Specifies the password for the user account.<br/><br/><b>Accept any server certificate</b>: Use this option (less secure) if you want to skip reviewing the site certificate before you read your data.<br/><br/><b>Database query</b>:Enter a SQL statement that describes the data you want to read.|\nAzure Table|Reads data from the Table service in Azure Storage.<br/><br/>If you read large amounts of data infrequently, use the Azure Table Service. It provides a flexible, non-relational (NoSQL), massively scalable, inexpensive, and highly available storage solution.| The options in the **Reader** change depending on whether you are accessing public information or a private storage account that requires login credentials. This is determined by the <b>Authentication Type</b> which can have value of \"PublicOrSAS\" or \"Account\", each of which has its own set of parameters. <br/><br/><b>Public or Shared Access Signature (SAS) URI</b>: The parameters are:<br/><br/><ul><b>Table URI</b>: Specifies the Public or SAS URL for the table.<br/><br/><b>Specifies the rows to scan for property names</b>: The values are <i>TopN</i> to scan the specified number of rows, or <i>ScanAll</i> to get all rows in the table. <br/><br/>If the data is homogeneous and predictable, it is recommended that you select *TopN* and enter a number for N. For large tables, this can result in quicker reading times.<br/><br/>If the data is structured with sets of properties that vary based on the depth and position of the table, choose the *ScanAll* option to scan all rows. This ensures the integrity of your resulting property and metadata conversion.<br/><br/></ul><b>Private Storage Account</b>: The parameters are: <br/><br/><ul><b>Account name</b>: Specifies the name of the account that contains the table to read.<br/><br/><b>Account key</b>: Specifies the storage key associated with the account.<br/><br/><b>Table name</b> : Specifies the name of the table that contains the data to read.<br/><br/><b>Rows to scan for property names</b>: The values are <i>TopN</i> to scan the specified number of rows, or <i>ScanAll</i> to get all rows in the table.<br/><br/>If the data is homogeneous and predictable, we recommend that you select *TopN* and enter a number for N. For large tables, this can result in quicker reading times.<br/><br/>If the data is structured with sets of properties that vary based on the depth and position of the table, choose the *ScanAll* option to scan all rows. This ensures the integrity of your resulting property and metadata conversion.<br/><br/>|\nAzure Blob Storage | Reads data stored in the Blob service in Azure Storage, including images, unstructured text, or binary data.<br/><br/>You can use the Blob service to publicly expose data, or to privately store application data. You can access your data from anywhere by using HTTP or HTTPS connections. | The options in the **Reader** module change depending on whether you are accessing public information or a private storage account that requires login credentials. This is determined by the <b>Authentication Type</b> which can have a value either of \"PublicOrSAS\" or of \"Account\".<br/><br/><b>Public or Shared Access Signature (SAS) URI</b>: The parameters are:<br/><br/><ul><b>URI</b>: Specifies the Public or SAS URL for the storage blob.<br/><br/><b>File Format</b>: Specifies the format of the data in the Blob service. The supported formats are CSV, TSV, and ARFF.<br/><br/></ul><b>Private Storage Account</b>: The parameters are: <br/><br/><ul><b>Account name</b>: Specifies the name of the account that contains the blob you want to read.<br/><br/><b>Account key</b>: Specifies the storage key associated with the account.<br/><br/><b>Path to container, directory, or blob </b> : Specifies the name of the blob that contains the data to read.<br/><br/><b>Blob file format</b>: Specifies the format of the data in the blob service. The supported data formats are CSV, TSV, ARFF, CSV with a specified encoding, and Excel. <br/><br/><ul>If the format is CSV or TSV, be sure to indicate whether the file contains a header row.<br/><br/>You can use the Excel option to read data from Excel workbooks. In the <i>Excel data format</i> option, indicate whether the data is in an Excel worksheet range, or in an Excel table. In the <i>Excel sheet or embedded table </i>option, specify the name of the sheet or table that you want to read from.</ul><br/>|\nData Feed Provider | Reads data from a supported feed provider. Currently only the Open Data Protocol (OData) format is supported. | <b>Data content type</b>: Specifies the OData format.<br/><br/><b>Source URL</b>: Specifies the full URL for the data feed. <br/>For example, the following URL reads from the Northwind sample database: http://services.odata.org/northwind/northwind.svc/|\n\n\n<!-- Module References -->\n[reader]: https://msdn.microsoft.com/library/azure/4e1b0fe6-aded-4b3f-a36f-39b8862b9004/\n"
}