{
  "nodes": [
    {
      "pos": [
        28,
        87
      ],
      "content": "Known issues of Apache Spark in HDInsight | Microsoft Azure"
    },
    {
      "pos": [
        107,
        149
      ],
      "content": "Known issues of Apache Spark in HDInsight."
    },
    {
      "pos": [
        488,
        543
      ],
      "content": "Known issues of Apache Spark in Azure HDInsight (Linux)"
    },
    {
      "pos": [
        545,
        634
      ],
      "content": "This document keeps track of all the known issues for the HDInsight Spark public preview."
    },
    {
      "pos": [
        640,
        670
      ],
      "content": "Livy leaks interactive session"
    },
    {
      "pos": [
        673,
        685
      ],
      "content": "<bpt id=\"p1\">**</bpt>Symptom:<ept id=\"p1\">**</ept>"
    },
    {
      "pos": [
        689,
        935
      ],
      "content": "When Livy is restarted with an interactive session (from Ambari or due to headnode 0 virtual machine reboot) still alive, an interactive job session will be leaked. Because of this, new jobs can stuck in the Accepted state, and cannot be started.",
      "nodes": [
        {
          "content": "When Livy is restarted with an interactive session (from Ambari or due to headnode 0 virtual machine reboot) still alive, an interactive job session will be leaked.",
          "pos": [
            0,
            164
          ]
        },
        {
          "content": "Because of this, new jobs can stuck in the Accepted state, and cannot be started.",
          "pos": [
            165,
            246
          ]
        }
      ]
    },
    {
      "pos": [
        937,
        952
      ],
      "content": "<bpt id=\"p2\">**</bpt>Mitigation:<ept id=\"p2\">**</ept>"
    },
    {
      "pos": [
        954,
        1006
      ],
      "content": "Use the following procedure to workaround the issue:"
    },
    {
      "pos": [
        1011,
        1029
      ],
      "content": "Ssh into headnode."
    },
    {
      "pos": [
        1034,
        1133
      ],
      "content": "Run the following command to find the application IDs of the interactive jobs started through Livy."
    },
    {
      "pos": [
        1172,
        1397
      ],
      "content": "The default job names will be Livy if the jobs were started with a Livy interactive session with no explicit names specified, For the Livy session started by Jupyter notebook, the job name will start with remotesparkmagics_*."
    },
    {
      "pos": [
        1403,
        1448
      ],
      "content": "Run the following command to kill those jobs."
    },
    {
      "pos": [
        1500,
        1528
      ],
      "content": "New jobs will start running."
    },
    {
      "pos": [
        1533,
        1565
      ],
      "content": "Spark History Server not started"
    },
    {
      "pos": [
        1568,
        1580
      ],
      "content": "<bpt id=\"p3\">**</bpt>Symptom:<ept id=\"p3\">**</ept>"
    },
    {
      "pos": [
        1583,
        1660
      ],
      "content": "Spark History Server is not started automatically after a cluster is created."
    },
    {
      "pos": [
        1664,
        1679
      ],
      "content": "<bpt id=\"p4\">**</bpt>Mitigation:<ept id=\"p4\">**</ept>"
    },
    {
      "pos": [
        1682,
        1728
      ],
      "content": "Manually start the history server from Ambari."
    },
    {
      "pos": [
        1733,
        1780
      ],
      "content": "Error while loading a Notebooks larger than 2MB"
    },
    {
      "pos": [
        1782,
        1794
      ],
      "content": "<bpt id=\"p5\">**</bpt>Symptom:<ept id=\"p5\">**</ept>"
    },
    {
      "pos": [
        1796,
        1897
      ],
      "content": "You might see an error <bpt id=\"p6\">**</bpt><ph id=\"ph2\">`Error loading notebook`</ph><ept id=\"p6\">**</ept><ph id=\"ph3\"/> when you load notebooks that are larger than 2MB."
    },
    {
      "pos": [
        1901,
        1916
      ],
      "content": "<bpt id=\"p7\">**</bpt>Mitigation:<ept id=\"p7\">**</ept>"
    },
    {
      "pos": [
        1918,
        2466
      ],
      "content": "If you get this error, it does not mean your data is corrupt or lost.  Your notebooks are still on disk in <ph id=\"ph4\">`/var/lib/jupyter`</ph>, and you can SSH into the cluster to access them. You can copy your notebooks from your cluster to your local machine (using SCP or WinSCP) as a backup to prevent the loss of any important data in the notebook. You can then SSH tunnel into your headnode at port 8001 to access Jupyter without going through the gateway.  From there, you can clear the output of your notebook and re-save it to minimize the notebook’s size.",
      "nodes": [
        {
          "content": "If you get this error, it does not mean your data is corrupt or lost.",
          "pos": [
            0,
            69
          ]
        },
        {
          "content": "Your notebooks are still on disk in <ph id=\"ph4\">`/var/lib/jupyter`</ph>, and you can SSH into the cluster to access them.",
          "pos": [
            71,
            193
          ]
        },
        {
          "content": "You can copy your notebooks from your cluster to your local machine (using SCP or WinSCP) as a backup to prevent the loss of any important data in the notebook.",
          "pos": [
            194,
            354
          ]
        },
        {
          "content": "You can then SSH tunnel into your headnode at port 8001 to access Jupyter without going through the gateway.",
          "pos": [
            355,
            463
          ]
        },
        {
          "content": "From there, you can clear the output of your notebook and re-save it to minimize the notebook’s size.",
          "pos": [
            465,
            566
          ]
        }
      ]
    },
    {
      "pos": [
        2468,
        2556
      ],
      "content": "To prevent this error from happening in the future, you must follow some best practices:"
    },
    {
      "pos": [
        2560,
        2940
      ],
      "content": "It is important to keep the notebook size small. Any output from your Spark jobs that is sent back to Jupyter is persisted in the notebook.  It is a best practice with Jupyter in general to avoid running <ph id=\"ph5\">`.collect()`</ph><ph id=\"ph6\"/> on large RDD’s or dataframes; instead, if you want to peek at an RDD’s contents, consider running <ph id=\"ph7\">`.take()`</ph><ph id=\"ph8\"/> or <ph id=\"ph9\">`.sample()`</ph><ph id=\"ph10\"/> so that your output doesn’t get too big.",
      "nodes": [
        {
          "content": "It is important to keep the notebook size small.",
          "pos": [
            0,
            48
          ]
        },
        {
          "content": "Any output from your Spark jobs that is sent back to Jupyter is persisted in the notebook.",
          "pos": [
            49,
            139
          ]
        },
        {
          "content": "It is a best practice with Jupyter in general to avoid running <ph id=\"ph5\">`.collect()`</ph><ph id=\"ph6\"/> on large RDD’s or dataframes; instead, if you want to peek at an RDD’s contents, consider running <ph id=\"ph7\">`.take()`</ph><ph id=\"ph8\"/> or <ph id=\"ph9\">`.sample()`</ph><ph id=\"ph10\"/> so that your output doesn’t get too big.",
          "pos": [
            141,
            477
          ]
        }
      ]
    },
    {
      "pos": [
        2943,
        3017
      ],
      "content": "Also, when you save a notebook, clear all output cells to reduce the size."
    },
    {
      "pos": [
        3023,
        3074
      ],
      "content": "Notebook initial startup takes longer than expected"
    },
    {
      "pos": [
        3077,
        3089
      ],
      "content": "<bpt id=\"p8\">**</bpt>Symptom:<ept id=\"p8\">**</ept>"
    },
    {
      "pos": [
        3092,
        3176
      ],
      "content": "First statement in Jupyter notebook using Spark Magic could take more than a minute."
    },
    {
      "pos": [
        3180,
        3195
      ],
      "content": "<bpt id=\"p9\">**</bpt>Mitigation:<ept id=\"p9\">**</ept>"
    },
    {
      "pos": [
        3198,
        3241
      ],
      "content": "No workaround. It takes a minute sometimes.",
      "nodes": [
        {
          "content": "No workaround.",
          "pos": [
            0,
            14
          ]
        },
        {
          "content": "It takes a minute sometimes.",
          "pos": [
            15,
            43
          ]
        }
      ]
    },
    {
      "pos": [
        3246,
        3294
      ],
      "content": "Jupyter notebook timeout in creating the session"
    },
    {
      "pos": [
        3296,
        3308
      ],
      "content": "<bpt id=\"p10\">**</bpt>Symptom:<ept id=\"p10\">**</ept>"
    },
    {
      "pos": [
        3311,
        3448
      ],
      "content": "When Spark cluster is out of resources, the Spark and Pyspark kernels in the Jupyter notebook will timeout trying to create the session. ",
      "nodes": [
        {
          "content": "When Spark cluster is out of resources, the Spark and Pyspark kernels in the Jupyter notebook will timeout trying to create the session.",
          "pos": [
            0,
            136
          ]
        },
        {
          "content": " ",
          "pos": [
            136,
            137
          ]
        }
      ]
    },
    {
      "pos": [
        3449,
        3461
      ],
      "content": "Mitigations:"
    },
    {
      "pos": [
        3467,
        3515
      ],
      "content": "Free up some resources in your Spark cluster by:"
    },
    {
      "pos": [
        3523,
        3632
      ],
      "content": "Stop other Spark notebooks by going to the Close and Halt menu or clicking Shutdown in the notebook explorer."
    },
    {
      "pos": [
        3639,
        3679
      ],
      "content": "Stop other Spark applications from YARN."
    },
    {
      "pos": [
        3684,
        3803
      ],
      "content": "Restart the notebook you were trying to start up. Enough resources should be available for you to create a session now.",
      "nodes": [
        {
          "content": "Restart the notebook you were trying to start up.",
          "pos": [
            0,
            49
          ]
        },
        {
          "content": "Enough resources should be available for you to create a session now.",
          "pos": [
            50,
            119
          ]
        }
      ]
    },
    {
      "pos": [
        3807,
        3847
      ],
      "content": "Notebook output results formatting issue"
    },
    {
      "pos": [
        3849,
        3861
      ],
      "content": "<bpt id=\"p11\">**</bpt>Symptom:<ept id=\"p11\">**</ept>"
    },
    {
      "pos": [
        3864,
        4074
      ],
      "content": "Notebook output results are badly formatted after executing a cell from the Spark and Pyspark Jupyter kernels. This includes successful results from cell executions as well as Spark stacktraces or other errors.",
      "nodes": [
        {
          "content": "Notebook output results are badly formatted after executing a cell from the Spark and Pyspark Jupyter kernels.",
          "pos": [
            0,
            110
          ]
        },
        {
          "content": "This includes successful results from cell executions as well as Spark stacktraces or other errors.",
          "pos": [
            111,
            210
          ]
        }
      ]
    },
    {
      "pos": [
        4077,
        4092
      ],
      "content": "<bpt id=\"p12\">**</bpt>Mitigation:<ept id=\"p12\">**</ept>"
    },
    {
      "pos": [
        4095,
        4144
      ],
      "content": "This issue will be addressed in a future release."
    },
    {
      "pos": [
        4148,
        4173
      ],
      "content": "Typos in sample notebooks"
    },
    {
      "pos": [
        4178,
        4248
      ],
      "content": "<bpt id=\"p13\">**</bpt>Python notebook 4 (Analyze logs with Spark using a custom library)<ept id=\"p13\">**</ept>"
    },
    {
      "pos": [
        4254,
        4440
      ],
      "content": "\"Let us assume you copy it over to wasb:///example/data/iislogparser.py\" should be \"Let us assume you copy it over to wasb:///HdiSamples/HdiSamples/WebsiteLogSampleData/iislogparser.py\"."
    },
    {
      "pos": [
        4445,
        4549
      ],
      "content": "<bpt id=\"p14\">**</bpt>Python notebook 5 (Spark Machine Learning - Predictive analysis on food inspection data using MLLib)<ept id=\"p14\">**</ept>"
    },
    {
      "pos": [
        4555,
        4725
      ],
      "content": "\"A quick visualization can help us reason about the distribution of these outcomes\" contains some incorrect code that will not run.  It should be edited to the following:",
      "nodes": [
        {
          "content": "\"A quick visualization can help us reason about the distribution of these outcomes\" contains some incorrect code that will not run.",
          "pos": [
            0,
            131
          ]
        },
        {
          "content": "It should be edited to the following:",
          "pos": [
            133,
            170
          ]
        }
      ]
    },
    {
      "pos": [
        5120,
        5224
      ],
      "content": "<bpt id=\"p15\">**</bpt>Python notebook 5 (Spark Machine Learning - Predictive analysis on food inspection data using MLLib)<ept id=\"p15\">**</ept>"
    },
    {
      "pos": [
        5230,
        5436
      ],
      "content": "The final comment notes that the false negative rate and false positive rate are 12.6% and 16.0% respectively.  These numbers are inaccurate; run the code to display the pie graph with the true percentages.",
      "nodes": [
        {
          "content": "The final comment notes that the false negative rate and false positive rate are 12.6% and 16.0% respectively.",
          "pos": [
            0,
            110
          ]
        },
        {
          "content": "These numbers are inaccurate; run the code to display the pie graph with the true percentages.",
          "pos": [
            112,
            206
          ]
        }
      ]
    },
    {
      "pos": [
        5441,
        5469
      ],
      "content": "<bpt id=\"p16\">**</bpt>Python notebooks 6 and 7<ept id=\"p16\">**</ept>"
    },
    {
      "pos": [
        5475,
        5883
      ],
      "content": "The first cell fails to register the sc.stop() method to be called when the notebook exits.  Under certain circumstances this could cause Spark resources to leak.  You can avoid this by making sure to run import atexit; atexit.register(lambda: sc.stop()) in those notebooks before stopping them.  If you have accidentally leaked resources, then follow the instructions above to kill leaked YARN applications.",
      "nodes": [
        {
          "content": "The first cell fails to register the sc.stop() method to be called when the notebook exits.",
          "pos": [
            0,
            91
          ]
        },
        {
          "content": "Under certain circumstances this could cause Spark resources to leak.",
          "pos": [
            93,
            162
          ]
        },
        {
          "content": "You can avoid this by making sure to run import atexit; atexit.register(lambda: sc.stop()) in those notebooks before stopping them.",
          "pos": [
            164,
            295
          ]
        },
        {
          "content": "If you have accidentally leaked resources, then follow the instructions above to kill leaked YARN applications.",
          "pos": [
            297,
            408
          ]
        }
      ]
    },
    {
      "pos": [
        5892,
        5935
      ],
      "content": "Cannot customize core/memory configurations"
    },
    {
      "pos": [
        5937,
        5949
      ],
      "content": "<bpt id=\"p17\">**</bpt>Symptom:<ept id=\"p17\">**</ept>"
    },
    {
      "pos": [
        5952,
        6056
      ],
      "content": "You cannot specify different core/memory configurations than the default from the Spark/Pyspark kernels."
    },
    {
      "pos": [
        6059,
        6074
      ],
      "content": "<bpt id=\"p18\">**</bpt>Mitigation:<ept id=\"p18\">**</ept>"
    },
    {
      "pos": [
        6077,
        6100
      ],
      "content": "This feature is coming."
    },
    {
      "pos": [
        6106,
        6145
      ],
      "content": "Permission issue in Spark log directory"
    },
    {
      "pos": [
        6148,
        6160
      ],
      "content": "<bpt id=\"p19\">**</bpt>Symptom:<ept id=\"p19\">**</ept>"
    },
    {
      "pos": [
        6163,
        6351
      ],
      "content": "When hdiuser submits a job with spark-submit, there is an error java.io.FileNotFoundException: /var/log/spark/sparkdriver_hdiuser.log (Permission denied) and the driver log is not written."
    },
    {
      "pos": [
        6354,
        6369
      ],
      "content": "<bpt id=\"p20\">**</bpt>Mitigation:<ept id=\"p20\">**</ept>"
    },
    {
      "pos": [
        6375,
        6407
      ],
      "content": "Add hdiuser to the Hadoop group."
    },
    {
      "pos": [
        6412,
        6477
      ],
      "content": "Provide 777 permissions on /var/log/spark after cluster creation."
    },
    {
      "pos": [
        6482,
        6564
      ],
      "content": "Update the spark log location using Ambari to be a directory with 777 permissions."
    },
    {
      "pos": [
        6570,
        6595
      ],
      "content": "Run spark-submit as sudo."
    },
    {
      "pos": [
        6600,
        6608
      ],
      "content": "See also"
    },
    {
      "pos": [
        6612,
        6699
      ],
      "content": "<bpt id=\"p21\">[</bpt>Overview: Apache Spark on Azure HDInsight (Linux)<ept id=\"p21\">](hdinsight-apache-spark-overview.md)</ept>"
    },
    {
      "pos": [
        6702,
        6855
      ],
      "content": "<bpt id=\"p22\">[</bpt>Get started: Provision Apache Spark on Azure HDInsight (Linux) and run interactive queries using Spark SQL<ept id=\"p22\">](hdinsight-apache-spark-jupyter-spark-sql.md)</ept>"
    }
  ],
  "content": "<properties \n    pageTitle=\"Known issues of Apache Spark in HDInsight | Microsoft Azure\" \n    description=\"Known issues of Apache Spark in HDInsight.\" \n    services=\"hdinsight\" \n    documentationCenter=\"\" \n    authors=\"mumian\" \n    manager=\"paulettm\" \n    editor=\"cgronlun\"\n    tags=\"azure-portal\"/>\n\n<tags \n    ms.service=\"hdinsight\" \n    ms.workload=\"big-data\" \n    ms.tgt_pltfrm=\"na\" \n    ms.devlang=\"na\" \n    ms.topic=\"article\" \n    ms.date=\"02/01/2016\" \n    ms.author=\"nitinme\"/>\n\n# Known issues of Apache Spark in Azure HDInsight (Linux)\n\nThis document keeps track of all the known issues for the HDInsight Spark public preview.  \n\n##Livy leaks interactive session\n \n**Symptom:**  \n\nWhen Livy is restarted with an interactive session (from Ambari or due to headnode 0 virtual machine reboot) still alive, an interactive job session will be leaked. Because of this, new jobs can stuck in the Accepted state, and cannot be started.\n\n**Mitigation:**\n\nUse the following procedure to workaround the issue:\n\n1. Ssh into headnode. \n2. Run the following command to find the application IDs of the interactive jobs started through Livy. \n\n        yarn application –list\n\n    The default job names will be Livy if the jobs were started with a Livy interactive session with no explicit names specified, For the Livy session started by Jupyter notebook, the job name will start with remotesparkmagics_*. \n\n3. Run the following command to kill those jobs. \n\n        yarn application –kill <Application ID>\n\nNew jobs will start running. \n\n##Spark History Server not started \n\n**Symptom:**\n \nSpark History Server is not started automatically after a cluster is created.  \n\n**Mitigation:** \n\nManually start the history server from Ambari. \n\n##Error while loading a Notebooks larger than 2MB\n\n**Symptom:**\n\nYou might see an error **`Error loading notebook`** when you load notebooks that are larger than 2MB.  \n\n**Mitigation:**\n\nIf you get this error, it does not mean your data is corrupt or lost.  Your notebooks are still on disk in `/var/lib/jupyter`, and you can SSH into the cluster to access them. You can copy your notebooks from your cluster to your local machine (using SCP or WinSCP) as a backup to prevent the loss of any important data in the notebook. You can then SSH tunnel into your headnode at port 8001 to access Jupyter without going through the gateway.  From there, you can clear the output of your notebook and re-save it to minimize the notebook’s size.\n\nTo prevent this error from happening in the future, you must follow some best practices:\n\n* It is important to keep the notebook size small. Any output from your Spark jobs that is sent back to Jupyter is persisted in the notebook.  It is a best practice with Jupyter in general to avoid running `.collect()` on large RDD’s or dataframes; instead, if you want to peek at an RDD’s contents, consider running `.take()` or `.sample()` so that your output doesn’t get too big.\n* Also, when you save a notebook, clear all output cells to reduce the size.\n\n\n\n##Notebook initial startup takes longer than expected \n\n**Symptom:** \n\nFirst statement in Jupyter notebook using Spark Magic could take more than a minute.  \n\n**Mitigation:**\n \nNo workaround. It takes a minute sometimes. \n\n##Jupyter notebook timeout in creating the session\n\n**Symptom:** \n\nWhen Spark cluster is out of resources, the Spark and Pyspark kernels in the Jupyter notebook will timeout trying to create the session. \nMitigations: \n\n1. Free up some resources in your Spark cluster by:\n\n    - Stop other Spark notebooks by going to the Close and Halt menu or clicking Shutdown in the notebook explorer.\n    - Stop other Spark applications from YARN.\n\n2. Restart the notebook you were trying to start up. Enough resources should be available for you to create a session now.\n\n##Notebook output results formatting issue\n\n**Symptom:**\n \nNotebook output results are badly formatted after executing a cell from the Spark and Pyspark Jupyter kernels. This includes successful results from cell executions as well as Spark stacktraces or other errors. \n\n**Mitigation:**\n \nThis issue will be addressed in a future release.\n\n##Typos in sample notebooks\n \n- **Python notebook 4 (Analyze logs with Spark using a custom library)**\n\n    \"Let us assume you copy it over to wasb:///example/data/iislogparser.py\" should be \"Let us assume you copy it over to wasb:///HdiSamples/HdiSamples/WebsiteLogSampleData/iislogparser.py\". \n\n- **Python notebook 5 (Spark Machine Learning - Predictive analysis on food inspection data using MLLib)**\n\n    \"A quick visualization can help us reason about the distribution of these outcomes\" contains some incorrect code that will not run.  It should be edited to the following: \n\n        countResults = df.groupBy('results').count().withColumnRenamed('count', 'cnt').collect() \n        labels = [row.results for row in countResults] \n        sizes = [row.cnt for row in countResults] \n        colors = ['turquoise', 'seagreen', 'mediumslateblue', 'palegreen', 'coral'] \n        plt.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors) plt.axis('equal') \n        \n- **Python notebook 5 (Spark Machine Learning - Predictive analysis on food inspection data using MLLib)**\n\n    The final comment notes that the false negative rate and false positive rate are 12.6% and 16.0% respectively.  These numbers are inaccurate; run the code to display the pie graph with the true percentages. \n\n- **Python notebooks 6 and 7**\n\n    The first cell fails to register the sc.stop() method to be called when the notebook exits.  Under certain circumstances this could cause Spark resources to leak.  You can avoid this by making sure to run import atexit; atexit.register(lambda: sc.stop()) in those notebooks before stopping them.  If you have accidentally leaked resources, then follow the instructions above to kill leaked YARN applications.\n     \n##Cannot customize core/memory configurations\n\n**Symptom:**\n \nYou cannot specify different core/memory configurations than the default from the Spark/Pyspark kernels. \n\n**Mitigation:**\n \nThis feature is coming. \n\n## Permission issue in Spark log directory \n\n**Symptom:**\n \nWhen hdiuser submits a job with spark-submit, there is an error java.io.FileNotFoundException: /var/log/spark/sparkdriver_hdiuser.log (Permission denied) and the driver log is not written. \n\n**Mitigation:**\n \n1. Add hdiuser to the Hadoop group. \n2. Provide 777 permissions on /var/log/spark after cluster creation. \n3. Update the spark log location using Ambari to be a directory with 777 permissions.  \n4. Run spark-submit as sudo. \n\n##See also\n\n- [Overview: Apache Spark on Azure HDInsight (Linux)](hdinsight-apache-spark-overview.md)\n- [Get started: Provision Apache Spark on Azure HDInsight (Linux) and run interactive queries using Spark SQL](hdinsight-apache-spark-jupyter-spark-sql.md)"
}