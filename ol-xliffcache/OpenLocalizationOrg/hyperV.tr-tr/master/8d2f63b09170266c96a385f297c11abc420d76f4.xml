{"nodes":[{"content":"Tutorial - Get started with the Azure Batch .NET library | Microsoft Azure","pos":[27,101]},{"content":"Learn the basic concepts of Azure Batch and how to develop for the Batch service with a simple scenario","pos":[120,223]},{"content":"Get started with the Azure Batch library for .NET","pos":[521,570]},{"content":"Learn the basics of <bpt id=\"p1\">[</bpt>Azure Batch<ph id=\"ph1\">][</ph>azure_batch<ept id=\"p1\">]</ept> and the <bpt id=\"p2\">[</bpt>Batch .NET<ph id=\"ph2\">][</ph>net_api<ept id=\"p2\">]</ept> library in this article as we discuss a C# sample application step by step.","pos":[572,724]},{"content":"We'll look at how this sample application leverages the Batch service to process a parallel workload in the cloud, as well as how it interacts with <bpt id=\"p1\">[</bpt>Azure Storage<ept id=\"p1\">](../storage/storage-introduction.md)</ept> for file staging and retrieval.","pos":[725,956]},{"content":"You'll learn common Batch application workflow techniques.","pos":[957,1015]},{"content":"You'll also gain a base understanding of the major components of Batch, such as jobs, tasks, pools, and compute nodes.","pos":[1016,1134]},{"content":"Batch solution workflow (basic)","pos":[1138,1169]},{"content":"Prerequisites","pos":[1184,1197]},{"content":"This article assumes that you have a working knowledge of C# and Visual Studio.","pos":[1199,1278]},{"content":"It also assumes that you're able to satisfy the account creation requirements that are specified below for Azure and the Batch and Storage services.","pos":[1279,1427]},{"content":"Accounts","pos":[1433,1441]},{"pos":[1445,1562],"content":"<bpt id=\"p1\">**</bpt>Azure account<ept id=\"p1\">**</ept>: If you don't already have an Azure subscription, <bpt id=\"p2\">[</bpt>create a free Azure account<ph id=\"ph1\">][</ph>azure_free_account<ept id=\"p2\">]</ept>"},{"pos":[1566,1685],"content":"<bpt id=\"p1\">**</bpt>Batch account<ept id=\"p1\">**</ept>: Once you have an Azure subscription, <bpt id=\"p2\">[</bpt>create an Azure Batch account<ept id=\"p2\">](batch-account-create-portal.md)</ept>"},{"pos":[1689,1891],"content":"<bpt id=\"p1\">**</bpt>Storage account<ept id=\"p1\">**</ept>: See <bpt id=\"p2\">[</bpt>Create a storage account<ept id=\"p2\">](../storage/storage-create-storage-account.md#create-a-storage-account)</ept> in <bpt id=\"p3\">[</bpt>About Azure storage accounts<ept id=\"p3\">](../storage/storage-create-storage-account.md)</ept>"},{"content":"Visual Studio","pos":[1898,1911]},{"content":"You must have <bpt id=\"p1\">**</bpt>Visual Studio 2013<ept id=\"p1\">**</ept> or <bpt id=\"p2\">**</bpt>Visual Studio 2015<ept id=\"p2\">**</ept> to build the sample project.","pos":[1913,2004]},{"content":"You can find free and trial versions of Visual Studio in the <bpt id=\"p1\">[</bpt>overview of Visual Studio 2015 products<ph id=\"ph1\">][</ph>visual_studio<ept id=\"p1\">]</ept>","pos":[2005,2122]},{"pos":[2129,2157],"content":"<bpt id=\"p1\">*</bpt>DotNetTutorial<ept id=\"p1\">*</ept> code sample"},{"content":"The <bpt id=\"p1\">[</bpt>DotNetTutorial<ph id=\"ph1\">][</ph>github_dotnettutorial<ept id=\"p1\">]</ept> sample is one of the many code samples found in the <bpt id=\"p2\">[</bpt>azure-batch-samples<ph id=\"ph2\">][</ph>github_samples<ept id=\"p2\">]</ept> repository on GitHub.","pos":[2159,2314]},{"content":"You can download the sample by clicking the <bpt id=\"p1\">**</bpt>Download ZIP<ept id=\"p1\">**</ept> button on the repository home page, or by clicking the <bpt id=\"p2\">[</bpt>azure-batch-samples-master.zip<ph id=\"ph1\">][</ph>github_samples_zip<ept id=\"p2\">]</ept> direct download link.","pos":[2315,2505]},{"content":"Once you've extracted the contents of the ZIP file, you will find the solution in the following folder:","pos":[2506,2609]},{"content":"Azure Batch Explorer (optional)","pos":[2677,2708]},{"content":"The <bpt id=\"p1\">[</bpt>Azure Batch Explorer<ph id=\"ph1\">][</ph>github_batchexplorer<ept id=\"p1\">]</ept> is a free utility that is included in the <bpt id=\"p2\">[</bpt>azure-batch-samples<ph id=\"ph2\">][</ph>github_samples<ept id=\"p2\">]</ept> repository on GitHub.","pos":[2710,2860]},{"content":"While the Batch Explorer is not required to complete this tutorial, we highly recommend it for use in the debugging and administration of entities in your Batch account.","pos":[2861,3030]},{"content":"You can read about an older version of the Batch Explorer in the <bpt id=\"p1\">[</bpt>Azure Batch Explorer sample walkthrough<ph id=\"ph1\">][</ph>batch_explorer_blog<ept id=\"p1\">]</ept> blog post.","pos":[3031,3169]},{"content":"DotNetTutorial sample project overview","pos":[3174,3212]},{"pos":[3214,3353],"content":"The <bpt id=\"p1\">*</bpt>DotNetTutorial<ept id=\"p1\">*</ept> code sample is a Visual Studio 2013 solution that consists of two projects: <bpt id=\"p2\">**</bpt>DotNetTutorial<ept id=\"p2\">**</ept> and <bpt id=\"p3\">**</bpt>TaskApplication<ept id=\"p3\">**</ept>"},{"content":"<bpt id=\"p1\">**</bpt>DotNetTutorial<ept id=\"p1\">**</ept> is the client application that interacts with the Batch and Storage services to execute a parallel workload on compute nodes (virtual machines).","pos":[3358,3521]},{"content":"DotNetTutorial runs on your local workstation.","pos":[3522,3568]},{"content":"<bpt id=\"p1\">**</bpt>TaskApplication<ept id=\"p1\">**</ept> is the program that runs on compute nodes in Azure to perform the actual work.","pos":[3572,3670]},{"content":"In the sample, <ph id=\"ph1\">`TaskApplication.exe`</ph> parses the text in a file downloaded from Azure Storage (the input file).","pos":[3671,3781]},{"content":"Then it produces a text file (the output file) that contains a list of the top three words that appear in the input file.","pos":[3782,3903]},{"content":"After it creates the output file, TaskApplication uploads the file to Azure Storage.","pos":[3904,3988]},{"content":"This makes it available to the client application for download.","pos":[3989,4052]},{"content":"TaskApplication runs in parallel on multiple compute nodes in the Batch service.","pos":[4053,4133]},{"content":"The following diagram illustrates the primary operations that are performed by the client application, <bpt id=\"p1\">*</bpt>DotNetTutorial<ept id=\"p1\">*</ept>, and the application that is executed by the tasks, <bpt id=\"p2\">*</bpt>TaskApplication<ept id=\"p2\">*</ept>.","pos":[4135,4325]},{"content":"This basic workflow is typical of many compute solutions that are created with Batch.","pos":[4326,4411]},{"content":"While it does not demonstrate every feature available in the Batch service, nearly every Batch scenario will include similar processes.","pos":[4412,4547]},{"content":"Batch example workflow","pos":[4551,4573]},{"content":"Step 1.","pos":[4587,4594]},{"content":"Create <bpt id=\"p1\">**</bpt>containers<ept id=\"p1\">**</ept> in Azure Blob Storage.","pos":[4633,4677]},{"content":"Step 2.","pos":[4686,4693]},{"content":"Upload task application files and input files to containers.","pos":[4745,4805]},{"content":"Step 3.","pos":[4814,4821]},{"content":"Create a Batch <bpt id=\"p1\">**</bpt>pool<ept id=\"p1\">**</ept>","pos":[4852,4875]},{"content":"3a.","pos":[4910,4913]},{"content":"The pool <bpt id=\"p1\">**</bpt>StartTask<ept id=\"p1\">**</ept> downloads the task binary files (TaskApplication) to nodes as they join the pool.","pos":[4916,5020]},{"content":"Step 4.","pos":[5029,5036]},{"content":"Create a Batch <bpt id=\"p1\">**</bpt>job<ept id=\"p1\">**</ept>","pos":[5066,5088]},{"content":"Step 5.","pos":[5098,5105]},{"content":"Add <bpt id=\"p1\">**</bpt>tasks<ept id=\"p1\">**</ept> to the job.","pos":[5135,5160]},{"content":"5a.","pos":[5194,5197]},{"content":"The tasks are scheduled to execute on nodes.","pos":[5200,5244]},{"content":"5b.","pos":[5280,5283]},{"content":"Each task downloads its input data from Azure Storage, then begins execution.","pos":[5286,5363]},{"content":"Step 6.","pos":[5372,5379]},{"content":"Monitor tasks.","pos":[5406,5420]},{"content":"6a.","pos":[5454,5457]},{"content":"As tasks are completed, they upload their output data to Azure Storage.","pos":[5460,5531]},{"content":"Step 7.","pos":[5540,5547]},{"content":"Download task output from Storage.","pos":[5581,5615]},{"pos":[5617,5815],"content":"As mentioned, not every Batch solution will perform these exact steps, and may include many more, but the <bpt id=\"p1\">*</bpt>DotNetTutorial<ept id=\"p1\">*</ept> sample application demonstrates common processes found in a Batch solution."},{"pos":[5820,5861],"content":"Build the <bpt id=\"p1\">*</bpt>DotNetTutorial<ept id=\"p1\">*</ept> sample project"},{"content":"Before you can successfully run the sample, you must specify both Batch and Storage account credentials in the <bpt id=\"p1\">*</bpt>DotNetTutorial<ept id=\"p1\">*</ept> project's <ph id=\"ph1\">`Program.cs`</ph> file.","pos":[5863,6019]},{"content":"If you have not done so already, open the solution in Visual Studio by double-clicking on the <ph id=\"ph1\">`DotNetTutorial.sln`</ph> solution file.","pos":[6020,6149]},{"content":"Or open it from within Visual Studio by using the <bpt id=\"p1\">**</bpt>File &gt; Open &gt; Project/Solution<ept id=\"p1\">**</ept> menu.","pos":[6150,6240]},{"content":"Open <ph id=\"ph1\">`Program.cs`</ph> within the <bpt id=\"p1\">*</bpt>DotNetTutorial<ept id=\"p1\">*</ept> project.","pos":[6242,6296]},{"content":"Then add your credentials as specified near the top of the file:","pos":[6297,6361]},{"pos":[6864,6997],"content":"You can find your Batch and Storage account credentials within the account blade of each service in the <bpt id=\"p1\">[</bpt>Azure portal<ph id=\"ph1\">][</ph>azure_portal<ept id=\"p1\">]</ept>:"},{"content":"Batch credentials in the portal","pos":[7001,7032]},{"content":"Storage credentials in the portal","pos":[7039,7072]},{"content":"Now that you've updated the project with your credentials, right-click the solution in Solution Explorer and click <bpt id=\"p1\">**</bpt>Build Solution<ept id=\"p1\">**</ept>.","pos":[7084,7218]},{"content":"Confirm the restoration of any NuGet packages, if you're prompted.","pos":[7219,7285]},{"content":"If the NuGet packages are not automatically restored, or if you see errors about a failure to restore the packages, ensure that you have the <bpt id=\"p1\">[</bpt>NuGet Package Manager<ph id=\"ph1\">][</ph>nuget_packagemgr<ept id=\"p1\">]</ept> installed.","pos":[7301,7494]},{"content":"Then enable the download of missing packages.","pos":[7495,7540]},{"content":"See <bpt id=\"p1\">[</bpt>Enabling Package Restore During Build<ph id=\"ph1\">][</ph>nuget_restore<ept id=\"p1\">]</ept> to enable package download.","pos":[7541,7627]},{"content":"In the following sections, we break down the sample application into the steps that it performs to process a workload in the Batch service, and discuss those steps in detail.","pos":[7629,7803]},{"content":"We encourage you to refer to the open solution in Visual Studio while you work your way through the rest of this article, since not every line of code in the sample is discussed.","pos":[7804,7982]},{"content":"Navigate to the top of the <ph id=\"ph1\">`MainAsync`</ph> method in the <bpt id=\"p1\">*</bpt>DotNetTutorial<ept id=\"p1\">*</ept> project's <ph id=\"ph2\">`Program.cs`</ph> file to start with Step 1.","pos":[7984,8103]},{"content":"Each step below then roughly follows the progression of method calls in","pos":[8104,8175]},{"content":"Step 1: Create Storage containers","pos":[8193,8226]},{"content":"Create containers in Azure Storage","pos":[8230,8264]},{"content":"Batch includes built-in support for interacting with Azure Storage.","pos":[8276,8343]},{"content":"Containers in your Storage account will provide the files needed by the tasks that run in your Batch account.","pos":[8344,8453]},{"content":"The containers also provide a place to store the output data that the tasks produce.","pos":[8454,8538]},{"content":"The first thing the <bpt id=\"p1\">*</bpt>DotNetTutorial<ept id=\"p1\">*</ept> client application does is create three containers in <bpt id=\"p2\">[</bpt>Azure Blob Storage<ept id=\"p2\">](../storage/storage-introduction.md)</ept>:","pos":[8539,8687]},{"pos":[8691,8817],"content":"<bpt id=\"p1\">**</bpt>application<ept id=\"p1\">**</ept>: This container will store the application run by the tasks, as well as any of its dependencies, such as DLLs."},{"pos":[8820,8904],"content":"<bpt id=\"p1\">**</bpt>input<ept id=\"p1\">**</ept>: Tasks will download the data files to process from the <bpt id=\"p2\">*</bpt>input<ept id=\"p2\">*</ept> container."},{"pos":[8907,9017],"content":"<bpt id=\"p1\">**</bpt>output<ept id=\"p1\">**</ept>: When tasks complete input file processing, they will upload the results to the <bpt id=\"p2\">*</bpt>output<ept id=\"p2\">*</ept> container."},{"content":"In order to interact with a Storage account and create containers, we use the <bpt id=\"p1\">[</bpt>Azure Storage Client Library for .NET<ph id=\"ph1\">][</ph>net_api_storage<ept id=\"p1\">]</ept>.","pos":[9019,9154]},{"content":"We create a reference to the account with <bpt id=\"p1\">[</bpt>CloudStorageAccount<ph id=\"ph1\">][</ph>net_cloudstorageaccount<ept id=\"p1\">]</ept>, and from that create a <bpt id=\"p2\">[</bpt>CloudBlobClient<ph id=\"ph2\">][</ph>net_cloudblobclient<ept id=\"p2\">]</ept>:","pos":[9155,9307]},{"content":"We use the <ph id=\"ph1\">`blobClient`</ph> reference throughout the application and pass it as a parameter to a number of methods.","pos":[9812,9923]},{"content":"An example of this is in the code block that immediately follows the above, where we call <ph id=\"ph1\">`CreateContainerIfNotExistAsync`</ph> to actually create the containers.","pos":[9924,10081]},{"content":"Once the containers have been created, the application can now upload the files that will be used by the tasks.","pos":[11051,11162]},{"content":"<bpt id=\"p1\">[</bpt>How to use Blob Storage from .NET<ept id=\"p1\">](../storage/storage-dotnet-how-to-use-blobs.md)</ept> provides a good overview of working with Azure Storage containers and blobs.","pos":[11178,11337]},{"content":"It should be near the top of your reading list as you start working with Batch.","pos":[11338,11417]},{"content":"Step 2: Upload task application and data files","pos":[11422,11468]},{"content":"Upload task application and input (data) files to containers","pos":[11472,11532]},{"content":"In the file upload operation, <bpt id=\"p1\">*</bpt>DotNetTutorial<ept id=\"p1\">*</ept> first defines collections of <bpt id=\"p2\">**</bpt>application<ept id=\"p2\">**</ept> and <bpt id=\"p3\">**</bpt>input<ept id=\"p3\">**</ept> file paths as they exist on the local machine.","pos":[11544,11696]},{"content":"Then it uploads these files to the containers that you created in the previous step.","pos":[11697,11781]},{"pos":[13009,13087],"content":"There are two methods in <ph id=\"ph1\">`Program.cs`</ph> that are involved in the upload process:"},{"pos":[13120,13331],"content":": This method returns a collection of <bpt id=\"p1\">[</bpt>ResourceFile<ph id=\"ph1\">][</ph>net_resourcefile<ept id=\"p1\">]</ept> objects (discussed below) and internally calls <ph id=\"ph2\">`UploadFileToContainerAsync`</ph> to upload each file that is passed in the <bpt id=\"p2\">*</bpt>filePaths<ept id=\"p2\">*</ept> parameter."},{"content":": This is the method that actually performs the file upload and creates the <bpt id=\"p1\">[</bpt>ResourceFile<ph id=\"ph1\">][</ph>net_resourcefile<ept id=\"p1\">]</ept> objects.","pos":[13362,13479]},{"content":"After uploading the file, it obtains a shared access signature (SAS) for the file and returns a ResourceFile object that represents it.","pos":[13480,13615]},{"content":"Shared access signatures are also discussed below.","pos":[13616,13666]},{"content":"ResourceFiles","pos":[14928,14941]},{"content":"A <bpt id=\"p1\">[</bpt>ResourceFile<ph id=\"ph1\">][</ph>net_resourcefile<ept id=\"p1\">]</ept> provides tasks in Batch with the URL to a file in Azure Storage that will be downloaded to a compute node before that task is run.","pos":[14943,15108]},{"content":"The <bpt id=\"p1\">[</bpt>ResourceFile.BlobSource<ph id=\"ph1\">][</ph>net_resourcefile_blobsource<ept id=\"p1\">]</ept> property specifies the full URL of the file as it exists in Azure Storage.","pos":[15109,15242]},{"content":"The URL may also include a shared access signature (SAS) that provides secure access to the file.","pos":[15243,15340]},{"content":"Most tasks types within Batch .NET include a <bpt id=\"p1\">*</bpt>ResourceFiles<ept id=\"p1\">*</ept> property, including:","pos":[15341,15422]},{"pos":[15427,15446],"content":"CloudTask<ph id=\"ph1\">][</ph>net_task"},{"pos":[15451,15480],"content":"StartTask<ph id=\"ph1\">][</ph>net_pool_starttask"},{"pos":[15485,15520],"content":"JobPreparationTask<ph id=\"ph1\">][</ph>net_jobpreptask"},{"pos":[15525,15555],"content":"JobReleaseTask<ph id=\"ph1\">][</ph>net_jobreltask"},{"pos":[15558,15796],"content":"The DotNetTutorial sample application does not use the JobPreparationTask or JobReleaseTask task types, but you can read more about them in <bpt id=\"p1\">[</bpt>Run job preparation and completion tasks on Azure Batch compute nodes<ept id=\"p1\">](batch-job-prep-release.md)</ept>"},{"content":"Shared access signature (SAS)","pos":[15803,15832]},{"content":"Shared access signatures are strings which--when included as part of a URL--provide secure access to containers and blobs in Azure Storage.","pos":[15834,15973]},{"content":"The DotNetTutorial application uses both blob and container shared access signature URLs, and demonstrates how to obtain these shared access signature strings from the Storage service.","pos":[15974,16158]},{"content":"<bpt id=\"p1\">**</bpt>Blob shared access signatures<ept id=\"p1\">**</ept>: The pool's StartTask in DotNetTutorial uses blob shared access signatures when it downloads the application binaries and input data files from Storage (see Step #3 below).","pos":[16162,16368]},{"content":"The <ph id=\"ph1\">`UploadFileToContainerAsync`</ph> method in DotNetTutorial's <ph id=\"ph2\">`Program.cs`</ph> contains the code that obtains each blob's shared access signature.","pos":[16369,16509]},{"content":"It does so by calling <bpt id=\"p1\">[</bpt>CloudBlob.GetSharedAccessSignature<ph id=\"ph1\">][</ph>net_sas_blob<ept id=\"p1\">]</ept>","pos":[16510,16582]},{"content":"<bpt id=\"p1\">**</bpt>Container shared access signatures<ept id=\"p1\">**</ept>: As each task finishes its work on the compute node, it uploads its output file to the <bpt id=\"p2\">*</bpt>output<ept id=\"p2\">*</ept> container in Azure Storage.","pos":[16587,16749]},{"content":"To do so, TaskApplication uses a container shared access signature that provides write access to the container as part of the path when it uploads the file.","pos":[16750,16906]},{"content":"Obtaining the container shared access signature is done in a similar fashion as when obtaining the blob shared access signature.","pos":[16907,17035]},{"content":"In DotNetTutorial, you will find that the <ph id=\"ph1\">`GetContainerSasUrl`</ph> helper method calls <bpt id=\"p1\">[</bpt>CloudBlobContainer.GetSharedAccessSignature<ph id=\"ph2\">][</ph>net_sas_container<ept id=\"p1\">]</ept> to do so.","pos":[17036,17193]},{"content":"You'll read more about how TaskApplication uses the container shared access signature in \"Step 6: Monitor Tasks.\"","pos":[17194,17307]},{"pos":[17323,17728],"content":"Check out the two-part series on shared access signatures, <bpt id=\"p1\">[</bpt>Part 1: Understanding the shared access signature (SAS) model<ept id=\"p1\">](../storage/storage-dotnet-shared-access-signature-part-1.md)</ept> and <bpt id=\"p2\">[</bpt>Part 2: Create and use a shared access signature (SAS) with the Blob service<ept id=\"p2\">](../storage/storage-dotnet-shared-access-signature-part-2.md)</ept>, to learn more about providing secure access to data in your Storage account."},{"content":"Step 3: Create Batch pool","pos":[17733,17758]},{"content":"Create a Batch pool","pos":[17762,17781]},{"content":"After it uploads the application and data files to the Storage account, <bpt id=\"p1\">*</bpt>DotNetTutorial<ept id=\"p1\">*</ept> starts its interaction with the Batch service by using the Batch .NET library.","pos":[17793,17960]},{"content":"To do so, a <bpt id=\"p1\">[</bpt>BatchClient<ph id=\"ph1\">][</ph>net_batchclient<ept id=\"p1\">]</ept> is first created:","pos":[17961,18021]},{"pos":[18229,18463],"content":"Next, a pool of compute nodes is created in the Batch account with a call to <ph id=\"ph1\">`CreatePoolAsync`. `CreatePoolAsync`</ph> uses the <bpt id=\"p1\">[</bpt>BatchClient.PoolOperations.CreatePool<ph id=\"ph2\">][</ph>net_pool_create<ept id=\"p1\">]</ept> method to actually create a pool in the Batch service."},{"content":"When you create a pool with <bpt id=\"p1\">[</bpt>CreatePool<ph id=\"ph1\">][</ph>net_pool_create<ept id=\"p1\">]</ept>, you specify a number of parameters such as the number of compute nodes, the <bpt id=\"p2\">[</bpt>size of the nodes<ept id=\"p2\">](../cloud-services/cloud-services-sizes-specs.md)</ept>, and the nodes' operating system.","pos":[20417,20654]},{"content":"In <bpt id=\"p1\">*</bpt>DotNetTutorial<ept id=\"p1\">*</ept>, we use <bpt id=\"p2\">[</bpt>CloudServiceConfiguration<ph id=\"ph1\">][</ph>net_cloudserviceconfiguration<ept id=\"p2\">]</ept> to specify Windows Server 2012 R2 from <bpt id=\"p3\">[</bpt>Cloud Services<ept id=\"p3\">](../cloud-services/cloud-services-guestos-update-matrix.md)</ept>.","pos":[20655,20857]},{"content":"However, by specifying a <bpt id=\"p1\">[</bpt>VirtualMachineConfiguration<ph id=\"ph1\">][</ph>net_virtualmachineconfiguration<ept id=\"p1\">]</ept> instead, you can create pools of nodes created from Marketplace images, which includes both Windows and Linux images--see <bpt id=\"p2\">[</bpt>Introducing Linux support on Azure Batch<ph id=\"ph2\">][</ph>blog_linux<ept id=\"p2\">]</ept> for more information.","pos":[20858,21144]},{"content":"You are charged for compute resources in Batch.","pos":[21166,21213]},{"content":"To minimize costs, you can lower <ph id=\"ph1\">`targetDedicated`</ph> to 1 before you run the sample.","pos":[21214,21296]},{"content":"Along with these physical node properties, you may also specify a <bpt id=\"p1\">[</bpt>StartTask<ph id=\"ph1\">][</ph>net_pool_starttask<ept id=\"p1\">]</ept> for the pool.","pos":[21298,21409]},{"content":"The StartTask will execute on each node as that node joins the pool, as well as each time a node is restarted.","pos":[21410,21520]},{"content":"The StartTask is especially useful for installing applications on compute nodes prior to the execution of tasks.","pos":[21521,21633]},{"content":"For example, if your tasks process data by using Python scripts, you could use a StartTask to install Python on the compute nodes.","pos":[21634,21764]},{"content":"In this sample application, the StartTask copies the files that it downloads from Storage (which are specified by using the <bpt id=\"p1\">[</bpt>StartTask<ph id=\"ph1\">][</ph>net_starttask<ept id=\"p1\">]</ept><ph id=\"ph2\">.</ph><bpt id=\"p2\">[</bpt>ResourceFiles<ph id=\"ph3\">][</ph>net_starttask_resourcefiles<ept id=\"p2\">]</ept> property) from the StartTask working directory to the shared directory that <bpt id=\"p3\">*</bpt>all<ept id=\"p3\">*</ept> tasks running on the node can access.","pos":[21766,22081]},{"content":"Essentially, this copies <ph id=\"ph1\">`TaskApplication.exe`</ph> and its dependencies to the shared directory on each node as the node joins the pool, so that any tasks that run on the node can access it.","pos":[22082,22268]},{"content":"The <bpt id=\"p1\">**</bpt>application packages<ept id=\"p1\">**</ept> feature of Azure Batch provides another way to get your application onto the compute nodes in a pool.","pos":[22284,22414]},{"content":"See <bpt id=\"p1\">[</bpt>Application deployment with Azure Batch application packages<ept id=\"p1\">](batch-application-packages.md)</ept> for details.","pos":[22415,22525]},{"content":"Also notable in the code snippet above is the use of two environment variables in the <bpt id=\"p1\">*</bpt>CommandLine<ept id=\"p1\">*</ept> property of the StartTask: <ph id=\"ph1\">`%AZ_BATCH_TASK_WORKING_DIR%`</ph> and <ph id=\"ph2\">`%AZ_BATCH_NODE_SHARED_DIR%`</ph>.","pos":[22527,22717]},{"content":"Each compute node within a Batch pool is automatically configured with a number of environment variables that are specific to Batch.","pos":[22718,22850]},{"content":"Any process that is executed by a task has access to these environment variables.","pos":[22851,22932]},{"pos":[22948,23242],"content":"To find out more about the environment variables that are available on compute nodes in a Batch pool, as well as information on task working directories, see the \"Environment settings for tasks\" and \"Files and directories\" sections in the <bpt id=\"p1\">[</bpt>overview of Azure Batch features<ept id=\"p1\">](batch-api-basics.md)</ept>"},{"content":"Step 4: Create Batch job","pos":[23248,23272]},{"content":"Create Batch job","pos":[23276,23292]},{"content":"A Batch job is essentially a collection of tasks that are associated with a pool of compute nodes.","pos":[23303,23401]},{"content":"You can use it not only for organizing and tracking tasks in related workloads, but also for imposing certain constraints--such as the maximum runtime for the job (and by extension, its tasks), as well as job priority in relation to other jobs in the Batch account.","pos":[23402,23667]},{"content":"In this example, however, the job is associated only with the pool that was created in step #3.","pos":[23668,23763]},{"content":"No additional properties are configured.","pos":[23764,23804]},{"content":"All Batch jobs are associated with a specific pool.","pos":[23806,23857]},{"content":"This association indicates which nodes the job's tasks will execute on.","pos":[23858,23929]},{"content":"You specify this by using the <bpt id=\"p1\">[</bpt>CloudJob.PoolInformation<ph id=\"ph1\">][</ph>net_job_poolinfo<ept id=\"p1\">]</ept> property, as shown in the code snippet below.","pos":[23931,24051]},{"content":"Now that a job has been created, tasks are added to perform the work.","pos":[24405,24474]},{"content":"Step 5: Add tasks to job","pos":[24479,24503]},{"content":"Add tasks to job","pos":[24507,24523]},{"content":"(1) Tasks are added to the job, (2) the tasks are scheduled to run on nodes, and (3) the tasks download the data files to process","pos":[24534,24663]},{"content":"To actually perform work, tasks must be added to a job.","pos":[24666,24721]},{"content":"Each <bpt id=\"p1\">[</bpt>CloudTask<ph id=\"ph1\">][</ph>net_task<ept id=\"p1\">]</ept> is configured by using a command-line property and <bpt id=\"p2\">[</bpt>ResourceFiles<ph id=\"ph2\">][</ph>net_task_resourcefiles<ept id=\"p2\">]</ept> (as with the pool's StartTask) that the task downloads to the node before its command line is automatically executed.","pos":[24722,24957]},{"content":"In the <bpt id=\"p1\">*</bpt>DotNetTutorial<ept id=\"p1\">*</ept> sample project, each task processes only one file.","pos":[24958,25032]},{"content":"Thus, its ResourceFiles collection contains a single element.","pos":[25033,25094]},{"content":"When they access environment variables such as <ph id=\"ph1\">`%AZ_BATCH_NODE_SHARED_DIR%`</ph> or execute an application not found in the node's <ph id=\"ph2\">`PATH`</ph>, task command lines must be prefixed with <ph id=\"ph3\">`cmd /c`</ph>.","pos":[26509,26693]},{"content":"This will explicitly execute the command interpreter and instruct it to terminate after carrying out your command.","pos":[26694,26808]},{"content":"This requirement is unnecessary if your tasks execute an application in the node's <ph id=\"ph1\">`PATH`</ph> (such as <bpt id=\"p1\">*</bpt>robocopy.exe<ept id=\"p1\">*</ept> or <bpt id=\"p2\">*</bpt>powershell.exe<ept id=\"p2\">*</ept>) and no environment variables are used.","pos":[26809,26982]},{"pos":[26984,27174],"content":"Within the <ph id=\"ph1\">`foreach`</ph> loop in the code snippet above, you can see that the command line for the task is constructed such that three command-line arguments are passed to <bpt id=\"p1\">*</bpt>TaskApplication.exe<ept id=\"p1\">*</ept>:"},{"content":"The <bpt id=\"p1\">**</bpt>first argument<ept id=\"p1\">**</ept> is the path of the file to process.","pos":[27179,27237]},{"content":"This is the local path to the file as it exists on the node.","pos":[27238,27298]},{"content":"When the ResourceFile object in <ph id=\"ph1\">`UploadFileToContainerAsync`</ph> was first created above, the file name was used for this property (as a parameter to the ResourceFile constructor).","pos":[27299,27475]},{"content":"This indicates that the file can be found in the same directory as <bpt id=\"p1\">*</bpt>TaskApplication.exe<ept id=\"p1\">*</ept>","pos":[27476,27564]},{"content":"The <bpt id=\"p1\">**</bpt>second argument<ept id=\"p1\">**</ept> specifies that the top <bpt id=\"p2\">*</bpt>N<ept id=\"p2\">*</ept> words should be written to the output file.","pos":[27570,27664]},{"content":"In the sample, this is hard-coded so that the top three words will be written to the output file.","pos":[27665,27762]},{"content":"The <bpt id=\"p1\">**</bpt>third argument<ept id=\"p1\">**</ept> is the shared access signature (SAS) that provides write access to the <bpt id=\"p2\">**</bpt>output<ept id=\"p2\">**</ept> container in Azure Storage.","pos":[27767,27899]},{"content":"<bpt id=\"p1\">*</bpt>TaskApplication.exe<ept id=\"p1\">*</ept> uses this shared access signature URL when it uploads the output file to Azure Storage.","pos":[27900,28009]},{"content":"You can find the code for this in the <ph id=\"ph1\">`UploadFileToContainer`</ph> method in the TaskApplication project's <ph id=\"ph2\">`Program.cs`</ph> file:","pos":[28010,28130]},{"content":"Step 6: Monitor tasks","pos":[29458,29479]},{"content":"Monitor tasks","pos":[29483,29496]},{"content":"The client application (1) monitors the tasks for completion and success status, and (2) the tasks upload result data to Azure Storage","pos":[29507,29641]},{"content":"When tasks are added to a job, they are automatically queued and scheduled for execution on compute nodes within the pool associated with the job.","pos":[29644,29790]},{"content":"Based on the settings you specify, Batch handles all task queuing, scheduling, retrying, and other task administration duties for you.","pos":[29791,29925]},{"content":"There are many approaches to monitoring task execution.","pos":[29926,29981]},{"content":"DotNetTutorial shows a simple example that reports only on completion and task failure or success states.","pos":[29982,30087]},{"content":"Within the <ph id=\"ph1\">`MonitorTasks`</ph> method in DotNetTutorial's <ph id=\"ph2\">`Program.cs`</ph>, there are three Batch .NET concepts that warrant discussion.","pos":[30089,30216]},{"content":"They are listed below in their order of appearance:","pos":[30217,30268]},{"content":"<bpt id=\"p1\">**</bpt>ODATADetailLevel<ept id=\"p1\">**</ept>: Specifying <bpt id=\"p2\">[</bpt>ODATADetailLevel<ph id=\"ph1\">][</ph>net_odatadetaillevel<ept id=\"p2\">]</ept> in list operations (such as obtaining a list of a job's tasks) is essential in ensuring Batch application performance.","pos":[30273,30465]},{"content":"Add <bpt id=\"p1\">[</bpt>Query the Azure Batch service efficiently<ept id=\"p1\">](batch-efficient-list-queries.md)</ept> to your reading list if you plan on doing any sort of status monitoring within your Batch applications.","pos":[30466,30650]},{"content":"<bpt id=\"p1\">**</bpt>TaskStateMonitor<ept id=\"p1\">**</ept>: <bpt id=\"p2\">[</bpt>TaskStateMonitor<ph id=\"ph1\">][</ph>net_taskstatemonitor<ept id=\"p2\">]</ept> provides Batch .NET applications with helper utilities for monitoring task states.","pos":[30655,30800]},{"content":"In <ph id=\"ph1\">`MonitorTasks`</ph>, <bpt id=\"p1\">*</bpt>DotNetTutorial<ept id=\"p1\">*</ept> waits for all tasks to reach <bpt id=\"p2\">[</bpt>TaskState.Completed<ph id=\"ph2\">][</ph>net_taskstate<ept id=\"p2\">]</ept> within a time limit.","pos":[30801,30923]},{"content":"Then it terminates the job.","pos":[30924,30951]},{"content":"<bpt id=\"p1\">**</bpt>TerminateJobAsync<ept id=\"p1\">**</ept>: Terminating a job with <bpt id=\"p2\">[</bpt>JobOperations.TerminateJobAsync<ph id=\"ph1\">][</ph>net_joboperations_terminatejob<ept id=\"p2\">]</ept> (or the blocking JobOperations.TerminateJob) will mark that job as completed.","pos":[30956,31145]},{"content":"It is essential to do so if your Batch solution uses a <bpt id=\"p1\">[</bpt>JobReleaseTask<ph id=\"ph1\">][</ph>net_jobreltask<ept id=\"p1\">]</ept>.","pos":[31146,31234]},{"content":"This is a special type of task, which is described in <bpt id=\"p1\">[</bpt>Job preparation and completion tasks<ept id=\"p1\">](batch-job-prep-release.md)</ept>","pos":[31235,31354]},{"pos":[31357,31434],"content":"The <ph id=\"ph1\">`MonitorTasks`</ph> method from <bpt id=\"p1\">*</bpt>DotNetTutorial<ept id=\"p1\">*</ept>'s <ph id=\"ph2\">`Program.cs`</ph> appears below:"},{"content":"Step 7: Download task output","pos":[35204,35232]},{"content":"Download task output from Storage","pos":[35236,35269]},{"content":"Now that the job is completed, the output from the tasks can be downloaded from Azure Storage.","pos":[35280,35374]},{"content":"This is done with a call to <ph id=\"ph1\">`DownloadBlobsFromContainerAsync`</ph> in <bpt id=\"p1\">*</bpt>DotNetTutorial<ept id=\"p1\">*</ept>'s <ph id=\"ph2\">`Program.cs`</ph>:","pos":[35375,35472]},{"content":"The call to <ph id=\"ph1\">`DownloadBlobsFromContainerAsync`</ph> in the <bpt id=\"p1\">*</bpt>DotNetTutorial<ept id=\"p1\">*</ept> application specifies that the files should be downloaded to your <ph id=\"ph2\">`%TEMP%`</ph> folder.","pos":[36551,36703]},{"content":"Feel free to modify this output location.","pos":[36704,36745]},{"content":"Step 8: Delete containers","pos":[36750,36775]},{"content":"Because you are charged for data that resides in Azure Storage, it is always a good idea to remove any blobs that are no longer needed for your Batch jobs.","pos":[36777,36932]},{"content":"In DotNetTutorial's <ph id=\"ph1\">`Program.cs`</ph>, this is done with three calls to the helper method <ph id=\"ph2\">`DeleteContainerAsync`</ph>:","pos":[36933,37041]},{"pos":[37261,37402],"content":"The method itself merely obtains a reference to the container, and then calls <bpt id=\"p1\">[</bpt>CloudBlobContainer.DeleteIfExistsAsync<ph id=\"ph1\">][</ph>net_container_delete<ept id=\"p1\">]</ept>:"},{"content":"Step 9: Delete the job and the pool","pos":[37869,37904]},{"content":"In the final step, the user is prompted to delete the job and the pool that were created by the DotNetTutorial application.","pos":[37906,38029]},{"content":"Although you are not charged for jobs and tasks themselves, you <bpt id=\"p1\">*</bpt>are<ept id=\"p1\">*</ept> charged for compute nodes.","pos":[38030,38126]},{"content":"Thus, we recommend that you allocate nodes only as needed.","pos":[38127,38185]},{"content":"Deleting unused pools can be part of your maintenance process.","pos":[38186,38248]},{"pos":[38250,38433],"content":"The BatchClient's <bpt id=\"p1\">[</bpt>JobOperations<ph id=\"ph1\">][</ph>net_joboperations<ept id=\"p1\">]</ept> and <bpt id=\"p2\">[</bpt>PoolOperations<ph id=\"ph2\">][</ph>net_pooloperations<ept id=\"p2\">]</ept> both have corresponding deletion methods, which are called if the user confirms deletion:"},{"content":"Keep in mind that you are charged for compute resources--deleting unused pools will minimize cost.","pos":[38947,39045]},{"content":"Also, be aware that deleting a pool deletes all compute nodes within that pool, and that any data on the nodes will be unrecoverable after the pool is deleted.","pos":[39046,39205]},{"pos":[39210,39241],"content":"Run the <bpt id=\"p1\">*</bpt>DotNetTutorial<ept id=\"p1\">*</ept> sample"},{"content":"When you run the sample application, the console output will be similar to the following.","pos":[39243,39332]},{"content":"During execution, you will experience a pause at <ph id=\"ph1\">`Awaiting task completion, timeout in 00:30:00...`</ph> while the pool's compute nodes are started.","pos":[39333,39476]},{"content":"Use the <bpt id=\"p1\">[</bpt>Batch Explorer<ph id=\"ph1\">][</ph>github_batchexplorer<ept id=\"p1\">]</ept> to monitor your pool, compute nodes, job, and tasks during and after execution.","pos":[39477,39603]},{"content":"Use the <bpt id=\"p1\">[</bpt>Azure portal<ph id=\"ph1\">][</ph>azure_portal<ept id=\"p1\">]</ept> or one of the <bpt id=\"p2\">[</bpt>available Azure Storage explorers<ph id=\"ph2\">][</ph>storage_explorers<ept id=\"p2\">]</ept> to view the Storage resources (containers and blobs) that are created by the application.","pos":[39604,39799]},{"pos":[39801,39913],"content":"Typical execution time is <bpt id=\"p1\">**</bpt>approximately 5 minutes<ept id=\"p1\">**</ept> when you run the application in its default configuration."},{"content":"Next steps","pos":[41053,41063]},{"content":"Feel free to make changes to <bpt id=\"p1\">*</bpt>DotNetTutorial<ept id=\"p1\">*</ept> and <bpt id=\"p2\">*</bpt>TaskApplication<ept id=\"p2\">*</ept> to experiment with different compute scenarios.","pos":[41065,41180]},{"content":"For example, try adding an execution delay to <bpt id=\"p1\">*</bpt>TaskApplication<ept id=\"p1\">*</ept>, such as with <bpt id=\"p2\">[</bpt>Thread.Sleep<ph id=\"ph1\">][</ph>net_thread_sleep<ept id=\"p2\">]</ept>, to simulate long-running tasks and monitor them with the Batch Explorer's <bpt id=\"p3\">*</bpt>Heat Map<ept id=\"p3\">*</ept> feature.","pos":[41181,41386]},{"content":"Try adding more tasks or adjusting the number of compute nodes.","pos":[41387,41450]},{"content":"Add logic to check for and allow the use of an existing pool to speed execution time (<bpt id=\"p1\">*</bpt>hint<ept id=\"p1\">*</ept>: check out <ph id=\"ph1\">`ArticleHelpers.cs`</ph> in the <bpt id=\"p2\">[</bpt>Microsoft.Azure.Batch.Samples.Common<ph id=\"ph2\">][</ph>github_samples_common<ept id=\"p2\">]</ept> project in <bpt id=\"p3\">[</bpt>azure-batch-samples<ph id=\"ph3\">][</ph>github_samples<ept id=\"p3\">]</ept>","pos":[41451,41692]},{"content":"Now that you're familiar with the basic workflow of a Batch solution, it's time to dig in to the additional features of the Batch service.","pos":[41696,41834]},{"pos":[41838,41962],"content":"Review the <bpt id=\"p1\">[</bpt>Overview of Azure Batch features<ept id=\"p1\">](batch-api-basics.md)</ept> article, which we recommend if you're new to the service."},{"pos":[41965,42091],"content":"Start on the other Batch development articles under <bpt id=\"p1\">**</bpt>Development in-depth<ept id=\"p1\">**</ept> in the <bpt id=\"p2\">[</bpt>Batch learning path<ph id=\"ph1\">][</ph>batch_learning_path<ept id=\"p2\">]</ept>"},{"pos":[42095,42232],"content":"Check out a different implementation of processing the \"top N words\" workload by using Batch in the <bpt id=\"p1\">[</bpt>TopNWords<ph id=\"ph1\">][</ph>github_topnwords<ept id=\"p1\">]</ept> sample."},{"pos":[42235,42246],"content":"azure_batch"},{"pos":[42294,42312],"content":"azure_free_account"},{"pos":[42350,42362],"content":"azure_portal"},{"pos":[42391,42410],"content":"batch_explorer_blog"},{"pos":[42516,42535],"content":"batch_learning_path"},{"pos":[42603,42613],"content":"blog_linux"},{"pos":[42720,42740],"content":"github_batchexplorer"},{"pos":[42822,42843],"content":"github_dotnettutorial"},{"pos":[42942,42956],"content":"github_samples"},{"pos":[43005,43026],"content":"github_samples_common"},{"pos":[43101,43119],"content":"github_samples_zip"},{"pos":[43187,43203],"content":"github_topnwords"},{"pos":[43281,43288],"content":"net_api"},{"pos":[43346,43361],"content":"net_api_storage"},{"pos":[43420,43435],"content":"net_batchclient"},{"pos":[43519,43538],"content":"net_cloudblobclient"},{"pos":[43634,43656],"content":"net_cloudblobcontainer"},{"pos":[43755,43778],"content":"net_cloudstorageaccount"},{"pos":[43879,43908],"content":"net_cloudserviceconfiguration"},{"pos":[44006,44026],"content":"net_container_delete"},{"pos":[44145,44152],"content":"net_job"},{"pos":[44233,44249],"content":"net_job_poolinfo"},{"pos":[44362,44379],"content":"net_joboperations"},{"pos":[44472,44502],"content":"net_joboperations_terminatejob"},{"pos":[44606,44621],"content":"net_jobpreptask"},{"pos":[44721,44735],"content":"net_jobreltask"},{"pos":[44831,44839],"content":"net_node"},{"pos":[44923,44943],"content":"net_odatadetaillevel"},{"pos":[45032,45040],"content":"net_pool"},{"pos":[45122,45137],"content":"net_pool_create"},{"pos":[45235,45253],"content":"net_pool_starttask"},{"pos":[45345,45363],"content":"net_pooloperations"},{"pos":[45457,45473],"content":"net_resourcefile"},{"pos":[45558,45585],"content":"net_resourcefile_blobsource"},{"pos":[45681,45693],"content":"net_sas_blob"},{"pos":[45814,45831],"content":"net_sas_container"},{"pos":[45961,45974],"content":"net_starttask"},{"pos":[46056,46083],"content":"net_starttask_resourcefiles"},{"pos":[46179,46187],"content":"net_task"},{"pos":[46269,46291],"content":"net_task_resourcefiles"},{"pos":[46387,46400],"content":"net_taskstate"},{"pos":[46489,46509],"content":"net_taskstatemonitor"},{"pos":[46598,46614],"content":"net_thread_sleep"},{"pos":[46677,46708],"content":"net_virtualmachineconfiguration"},{"pos":[46808,46824],"content":"nuget_packagemgr"},{"pos":[46912,46925],"content":"nuget_restore"},{"pos":[47033,47050],"content":"storage_explorers"},{"pos":[47159,47172],"content":"visual_studio"},{"pos":[47240,47337],"content":"1<ph id=\"ph1\">]: ./media/batch-dotnet-get-started/batch_workflow_01_sm.png \"</ph>Create containers in Azure Storage"},{"pos":[47340,47463],"content":"2<ph id=\"ph1\">]: ./media/batch-dotnet-get-started/batch_workflow_02_sm.png \"</ph>Upload task application and input (data) files to containers"},{"pos":[47466,47546],"content":"3<ph id=\"ph1\">]: ./media/batch-dotnet-get-started/batch_workflow_03_sm.png \"</ph>Create Batch pool"},{"pos":[47549,47628],"content":"4<ph id=\"ph1\">]: ./media/batch-dotnet-get-started/batch_workflow_04_sm.png \"</ph>Create Batch job"},{"pos":[47631,47710],"content":"5<ph id=\"ph1\">]: ./media/batch-dotnet-get-started/batch_workflow_05_sm.png \"</ph>Add tasks to job"},{"pos":[47713,47789],"content":"6<ph id=\"ph1\">]: ./media/batch-dotnet-get-started/batch_workflow_06_sm.png \"</ph>Monitor tasks"},{"pos":[47792,47888],"content":"7<ph id=\"ph1\">]: ./media/batch-dotnet-get-started/batch_workflow_07_sm.png \"</ph>Download task output from Storage"},{"pos":[47891,47989],"content":"8<ph id=\"ph1\">]: ./media/batch-dotnet-get-started/batch_workflow_sm.png \"</ph>Batch solution workflow (full diagram)"},{"pos":[47992,48082],"content":"9<ph id=\"ph1\">]: ./media/batch-dotnet-get-started/credentials_batch_sm.png \"</ph>Batch credentials in Portal"},{"pos":[48085,48180],"content":"10<ph id=\"ph1\">]: ./media/batch-dotnet-get-started/credentials_storage_sm.png \"</ph>Storage credentials in Portal"},{"pos":[48183,48293],"content":"11<ph id=\"ph1\">]: ./media/batch-dotnet-get-started/batch_workflow_minimal_sm.png \"</ph>Batch solution workflow (minimal diagram)"}],"content":"<properties\n    pageTitle=\"Tutorial - Get started with the Azure Batch .NET library | Microsoft Azure\"\n    description=\"Learn the basic concepts of Azure Batch and how to develop for the Batch service with a simple scenario\"\n    services=\"batch\"\n    documentationCenter=\".net\"\n    authors=\"mmacy\"\n    manager=\"timlt\"\n    editor=\"\"/>\n\n<tags\n    ms.service=\"batch\"\n    ms.devlang=\"dotnet\"\n    ms.topic=\"hero-article\"\n    ms.tgt_pltfrm=\"na\"\n    ms.workload=\"big-compute\"\n    ms.date=\"04/11/2016\"\n    ms.author=\"marsma\"/>\n\n# Get started with the Azure Batch library for .NET\n\nLearn the basics of [Azure Batch][azure_batch] and the [Batch .NET][net_api] library in this article as we discuss a C# sample application step by step. We'll look at how this sample application leverages the Batch service to process a parallel workload in the cloud, as well as how it interacts with [Azure Storage](../storage/storage-introduction.md) for file staging and retrieval. You'll learn common Batch application workflow techniques. You'll also gain a base understanding of the major components of Batch, such as jobs, tasks, pools, and compute nodes.\n\n![Batch solution workflow (basic)][11]<br/>\n\n## Prerequisites\n\nThis article assumes that you have a working knowledge of C# and Visual Studio. It also assumes that you're able to satisfy the account creation requirements that are specified below for Azure and the Batch and Storage services.\n\n### Accounts\n\n- **Azure account**: If you don't already have an Azure subscription, [create a free Azure account][azure_free_account].\n- **Batch account**: Once you have an Azure subscription, [create an Azure Batch account](batch-account-create-portal.md).\n- **Storage account**: See [Create a storage account](../storage/storage-create-storage-account.md#create-a-storage-account) in [About Azure storage accounts](../storage/storage-create-storage-account.md).\n\n### Visual Studio\n\nYou must have **Visual Studio 2013** or **Visual Studio 2015** to build the sample project. You can find free and trial versions of Visual Studio in the [overview of Visual Studio 2015 products][visual_studio].\n\n### *DotNetTutorial* code sample\n\nThe [DotNetTutorial][github_dotnettutorial] sample is one of the many code samples found in the [azure-batch-samples][github_samples] repository on GitHub. You can download the sample by clicking the **Download ZIP** button on the repository home page, or by clicking the [azure-batch-samples-master.zip][github_samples_zip] direct download link. Once you've extracted the contents of the ZIP file, you will find the solution in the following folder:\n\n`\\azure-batch-samples\\CSharp\\ArticleProjects\\DotNetTutorial`\n\n### Azure Batch Explorer (optional)\n\nThe [Azure Batch Explorer][github_batchexplorer] is a free utility that is included in the [azure-batch-samples][github_samples] repository on GitHub. While the Batch Explorer is not required to complete this tutorial, we highly recommend it for use in the debugging and administration of entities in your Batch account. You can read about an older version of the Batch Explorer in the [Azure Batch Explorer sample walkthrough][batch_explorer_blog] blog post.\n\n## DotNetTutorial sample project overview\n\nThe *DotNetTutorial* code sample is a Visual Studio 2013 solution that consists of two projects: **DotNetTutorial** and **TaskApplication**.\n\n- **DotNetTutorial** is the client application that interacts with the Batch and Storage services to execute a parallel workload on compute nodes (virtual machines). DotNetTutorial runs on your local workstation.\n\n- **TaskApplication** is the program that runs on compute nodes in Azure to perform the actual work. In the sample, `TaskApplication.exe` parses the text in a file downloaded from Azure Storage (the input file). Then it produces a text file (the output file) that contains a list of the top three words that appear in the input file. After it creates the output file, TaskApplication uploads the file to Azure Storage. This makes it available to the client application for download. TaskApplication runs in parallel on multiple compute nodes in the Batch service.\n\nThe following diagram illustrates the primary operations that are performed by the client application, *DotNetTutorial*, and the application that is executed by the tasks, *TaskApplication*. This basic workflow is typical of many compute solutions that are created with Batch. While it does not demonstrate every feature available in the Batch service, nearly every Batch scenario will include similar processes.\n\n![Batch example workflow][8]<br/>\n\n[**Step 1.**](#step-1-create-storage-containers) Create **containers** in Azure Blob Storage.<br/>\n[**Step 2.**](#step-2-upload-task-application-and-data-files) Upload task application files and input files to containers.<br/>\n[**Step 3.**](#step-3-create-batch-pool) Create a Batch **pool**.<br/>\n  &nbsp;&nbsp;&nbsp;&nbsp;**3a.** The pool **StartTask** downloads the task binary files (TaskApplication) to nodes as they join the pool.<br/>\n[**Step 4.**](#step-4-create-batch-job) Create a Batch **job**.<br/>\n[**Step 5.**](#step-5-add-tasks-to-job) Add **tasks** to the job.<br/>\n  &nbsp;&nbsp;&nbsp;&nbsp;**5a.** The tasks are scheduled to execute on nodes.<br/>\n    &nbsp;&nbsp;&nbsp;&nbsp;**5b.** Each task downloads its input data from Azure Storage, then begins execution.<br/>\n[**Step 6.**](#step-6-monitor-tasks) Monitor tasks.<br/>\n  &nbsp;&nbsp;&nbsp;&nbsp;**6a.** As tasks are completed, they upload their output data to Azure Storage.<br/>\n[**Step 7.**](#step-7-download-task-output) Download task output from Storage.\n\nAs mentioned, not every Batch solution will perform these exact steps, and may include many more, but the *DotNetTutorial* sample application demonstrates common processes found in a Batch solution.\n\n## Build the *DotNetTutorial* sample project\n\nBefore you can successfully run the sample, you must specify both Batch and Storage account credentials in the *DotNetTutorial* project's `Program.cs` file. If you have not done so already, open the solution in Visual Studio by double-clicking on the `DotNetTutorial.sln` solution file. Or open it from within Visual Studio by using the **File > Open > Project/Solution** menu.\n\nOpen `Program.cs` within the *DotNetTutorial* project. Then add your credentials as specified near the top of the file:\n\n```\n// Update the Batch and Storage account credential strings below with the values\n// unique to your accounts. These are used when constructing connection strings\n// for the Batch and Storage client objects.\n\n// Batch account credentials\nprivate const string BatchAccountName = \"\";\nprivate const string BatchAccountKey  = \"\";\nprivate const string BatchAccountUrl  = \"\";\n\n// Storage account credentials\nprivate const string StorageAccountName = \"\";\nprivate const string StorageAccountKey  = \"\";\n```\n\nYou can find your Batch and Storage account credentials within the account blade of each service in the [Azure portal][azure_portal]:\n\n![Batch credentials in the portal][9]\n![Storage credentials in the portal][10]<br/>\n\nNow that you've updated the project with your credentials, right-click the solution in Solution Explorer and click **Build Solution**. Confirm the restoration of any NuGet packages, if you're prompted.\n\n> [AZURE.TIP] If the NuGet packages are not automatically restored, or if you see errors about a failure to restore the packages, ensure that you have the [NuGet Package Manager][nuget_packagemgr] installed. Then enable the download of missing packages. See [Enabling Package Restore During Build][nuget_restore] to enable package download.\n\nIn the following sections, we break down the sample application into the steps that it performs to process a workload in the Batch service, and discuss those steps in detail. We encourage you to refer to the open solution in Visual Studio while you work your way through the rest of this article, since not every line of code in the sample is discussed.\n\nNavigate to the top of the `MainAsync` method in the *DotNetTutorial* project's `Program.cs` file to start with Step 1. Each step below then roughly follows the progression of method calls in `MainAsync`.\n\n## Step 1: Create Storage containers\n\n![Create containers in Azure Storage][1]\n<br/>\n\nBatch includes built-in support for interacting with Azure Storage. Containers in your Storage account will provide the files needed by the tasks that run in your Batch account. The containers also provide a place to store the output data that the tasks produce. The first thing the *DotNetTutorial* client application does is create three containers in [Azure Blob Storage](../storage/storage-introduction.md):\n\n- **application**: This container will store the application run by the tasks, as well as any of its dependencies, such as DLLs.\n- **input**: Tasks will download the data files to process from the *input* container.\n- **output**: When tasks complete input file processing, they will upload the results to the *output* container.\n\nIn order to interact with a Storage account and create containers, we use the [Azure Storage Client Library for .NET][net_api_storage]. We create a reference to the account with [CloudStorageAccount][net_cloudstorageaccount], and from that create a [CloudBlobClient][net_cloudblobclient]:\n\n```\n// Construct the Storage account connection string\nstring storageConnectionString = String.Format(\n    \"DefaultEndpointsProtocol=https;AccountName={0};AccountKey={1}\",\n    StorageAccountName,\n    StorageAccountKey);\n\n// Retrieve the storage account\nCloudStorageAccount storageAccount = CloudStorageAccount.Parse(storageConnectionString);\n\n// Create the blob client, for use in obtaining references to blob storage containers\nCloudBlobClient blobClient = storageAccount.CreateCloudBlobClient();\n```\n\nWe use the `blobClient` reference throughout the application and pass it as a parameter to a number of methods. An example of this is in the code block that immediately follows the above, where we call `CreateContainerIfNotExistAsync` to actually create the containers.\n\n```\n// Use the blob client to create the containers in Azure Storage if they don't\n// yet exist\nconst string appContainerName    = \"application\";\nconst string inputContainerName  = \"input\";\nconst string outputContainerName = \"output\";\nawait CreateContainerIfNotExistAsync(blobClient, appContainerName);\nawait CreateContainerIfNotExistAsync(blobClient, inputContainerName);\nawait CreateContainerIfNotExistAsync(blobClient, outputContainerName);\n```\n\n```\nprivate static async Task CreateContainerIfNotExistAsync(\n    CloudBlobClient blobClient,\n    string containerName)\n{\n        CloudBlobContainer container = blobClient.GetContainerReference(containerName);\n\n        if (await container.CreateIfNotExistsAsync())\n        {\n                Console.WriteLine(\"Container [{0}] created.\", containerName);\n        }\n        else\n        {\n                Console.WriteLine(\"Container [{0}] exists, skipping creation.\",\n                    containerName);\n        }\n}\n```\n\nOnce the containers have been created, the application can now upload the files that will be used by the tasks.\n\n> [AZURE.TIP] [How to use Blob Storage from .NET](../storage/storage-dotnet-how-to-use-blobs.md) provides a good overview of working with Azure Storage containers and blobs. It should be near the top of your reading list as you start working with Batch.\n\n## Step 2: Upload task application and data files\n\n![Upload task application and input (data) files to containers][2]\n<br/>\n\nIn the file upload operation, *DotNetTutorial* first defines collections of **application** and **input** file paths as they exist on the local machine. Then it uploads these files to the containers that you created in the previous step.\n\n```\n// Paths to the executable and its dependencies that will be executed by the tasks\nList<string> applicationFilePaths = new List<string>\n{\n    // The DotNetTutorial project includes a project reference to TaskApplication,\n    // allowing us to determine the path of the task application binary dynamically\n    typeof(TaskApplication.Program).Assembly.Location,\n    \"Microsoft.WindowsAzure.Storage.dll\"\n};\n\n// The collection of data files that are to be processed by the tasks\nList<string> inputFilePaths = new List<string>\n{\n    @\"..\\..\\taskdata1.txt\",\n    @\"..\\..\\taskdata2.txt\",\n    @\"..\\..\\taskdata3.txt\"\n};\n\n// Upload the application and its dependencies to Azure Storage. This is the\n// application that will process the data files, and will be executed by each\n// of the tasks on the compute nodes.\nList<ResourceFile> applicationFiles = await UploadFilesToContainerAsync(\n    blobClient,\n    appContainerName,\n    applicationFilePaths);\n\n// Upload the data files. This is the data that will be processed by each of\n// the tasks that are executed on the compute nodes within the pool.\nList<ResourceFile> inputFiles = await UploadFilesToContainerAsync(\n    blobClient,\n    inputContainerName,\n    inputFilePaths);\n```\n\nThere are two methods in `Program.cs` that are involved in the upload process:\n\n- `UploadFilesToContainerAsync`: This method returns a collection of [ResourceFile][net_resourcefile] objects (discussed below) and internally calls `UploadFileToContainerAsync` to upload each file that is passed in the *filePaths* parameter.\n- `UploadFileToContainerAsync`: This is the method that actually performs the file upload and creates the [ResourceFile][net_resourcefile] objects. After uploading the file, it obtains a shared access signature (SAS) for the file and returns a ResourceFile object that represents it. Shared access signatures are also discussed below.\n\n```\nprivate static async Task<ResourceFile> UploadFileToContainerAsync(\n    CloudBlobClient blobClient,\n    string containerName,\n    string filePath)\n{\n        Console.WriteLine(\n            \"Uploading file {0} to container [{1}]...\", filePath, containerName);\n\n        string blobName = Path.GetFileName(filePath);\n\n        CloudBlobContainer container = blobClient.GetContainerReference(containerName);\n        CloudBlockBlob blobData = container.GetBlockBlobReference(blobName);\n        await blobData.UploadFromFileAsync(filePath, FileMode.Open);\n\n        // Set the expiry time and permissions for the blob shared access signature.\n        // In this case, no start time is specified, so the shared access signature\n        // becomes valid immediately\n        SharedAccessBlobPolicy sasConstraints = new SharedAccessBlobPolicy\n        {\n                SharedAccessExpiryTime = DateTime.UtcNow.AddHours(2),\n                Permissions = SharedAccessBlobPermissions.Read\n        };\n\n        // Construct the SAS URL for blob\n        string sasBlobToken = blobData.GetSharedAccessSignature(sasConstraints);\n        string blobSasUri = String.Format(\"{0}{1}\", blobData.Uri, sasBlobToken);\n\n        return new ResourceFile(blobSasUri, blobName);\n}\n```\n\n### ResourceFiles\n\nA [ResourceFile][net_resourcefile] provides tasks in Batch with the URL to a file in Azure Storage that will be downloaded to a compute node before that task is run. The [ResourceFile.BlobSource][net_resourcefile_blobsource] property specifies the full URL of the file as it exists in Azure Storage. The URL may also include a shared access signature (SAS) that provides secure access to the file. Most tasks types within Batch .NET include a *ResourceFiles* property, including:\n\n- [CloudTask][net_task]\n- [StartTask][net_pool_starttask]\n- [JobPreparationTask][net_jobpreptask]\n- [JobReleaseTask][net_jobreltask]\n\nThe DotNetTutorial sample application does not use the JobPreparationTask or JobReleaseTask task types, but you can read more about them in [Run job preparation and completion tasks on Azure Batch compute nodes](batch-job-prep-release.md).\n\n### Shared access signature (SAS)\n\nShared access signatures are strings which--when included as part of a URL--provide secure access to containers and blobs in Azure Storage. The DotNetTutorial application uses both blob and container shared access signature URLs, and demonstrates how to obtain these shared access signature strings from the Storage service.\n\n- **Blob shared access signatures**: The pool's StartTask in DotNetTutorial uses blob shared access signatures when it downloads the application binaries and input data files from Storage (see Step #3 below). The `UploadFileToContainerAsync` method in DotNetTutorial's `Program.cs` contains the code that obtains each blob's shared access signature. It does so by calling [CloudBlob.GetSharedAccessSignature][net_sas_blob].\n\n- **Container shared access signatures**: As each task finishes its work on the compute node, it uploads its output file to the *output* container in Azure Storage. To do so, TaskApplication uses a container shared access signature that provides write access to the container as part of the path when it uploads the file. Obtaining the container shared access signature is done in a similar fashion as when obtaining the blob shared access signature. In DotNetTutorial, you will find that the `GetContainerSasUrl` helper method calls [CloudBlobContainer.GetSharedAccessSignature][net_sas_container] to do so. You'll read more about how TaskApplication uses the container shared access signature in \"Step 6: Monitor Tasks.\"\n\n> [AZURE.TIP] Check out the two-part series on shared access signatures, [Part 1: Understanding the shared access signature (SAS) model](../storage/storage-dotnet-shared-access-signature-part-1.md) and [Part 2: Create and use a shared access signature (SAS) with the Blob service](../storage/storage-dotnet-shared-access-signature-part-2.md), to learn more about providing secure access to data in your Storage account.\n\n## Step 3: Create Batch pool\n\n![Create a Batch pool][3]\n<br/>\n\nAfter it uploads the application and data files to the Storage account, *DotNetTutorial* starts its interaction with the Batch service by using the Batch .NET library. To do so, a [BatchClient][net_batchclient] is first created:\n\n```\nBatchSharedKeyCredentials cred = new BatchSharedKeyCredentials(\n    BatchAccountUrl,\n    BatchAccountName,\n    BatchAccountKey);\n\nusing (BatchClient batchClient = BatchClient.Open(cred))\n{\n    ...\n```\n\nNext, a pool of compute nodes is created in the Batch account with a call to `CreatePoolAsync`. `CreatePoolAsync` uses the [BatchClient.PoolOperations.CreatePool][net_pool_create] method to actually create a pool in the Batch service.\n\n```\nprivate static async Task CreatePoolAsync(\n    BatchClient batchClient,\n    string poolId,\n    IList<ResourceFile> resourceFiles)\n{\n    Console.WriteLine(\"Creating pool [{0}]...\", poolId);\n\n    // Create the unbound pool. Until we call CloudPool.Commit() or CommitAsync(),\n    // no pool is actually created in the Batch service. This CloudPool instance is\n    // therefore considered \"unbound,\" and we can modify its properties.\n    CloudPool pool = batchClient.PoolOperations.CreatePool(\n            poolId: poolId,\n            targetDedicated: 3,           // 3 compute nodes\n            virtualMachineSize: \"small\",  // single-core, 1.75 GB memory, 224 GB disk\n            cloudServiceConfiguration:\n                new CloudServiceConfiguration(osFamily: \"4\")); // Win Server 2012 R2\n\n    // Create and assign the StartTask that will be executed when compute nodes join\n    // the pool. In this case, we copy the StartTask's resource files (that will be\n    // automatically downloaded to the node by the StartTask) into the shared\n    // directory that all tasks will have access to.\n    pool.StartTask = new StartTask\n    {\n        // Specify a command line for the StartTask that copies the task application\n        // files to the node's shared directory. Every compute node in a Batch pool\n        // is configured with several pre-defined environment variables that you can\n        // reference by using commands or applications run by tasks.\n\n        // Since a successful execution of robocopy can return a non-zero exit code\n        // (e.g. 1 when one or more files were successfully copied) we need to\n        // manually exit with a 0 for Batch to recognize StartTask execution success.\n        CommandLine = \"cmd /c (robocopy %AZ_BATCH_TASK_WORKING_DIR% %AZ_BATCH_NODE_SHARED_DIR%) ^& IF %ERRORLEVEL% LEQ 1 exit 0\",\n        ResourceFiles = resourceFiles,\n        WaitForSuccess = true\n    };\n\n    await pool.CommitAsync();\n}\n```\n\nWhen you create a pool with [CreatePool][net_pool_create], you specify a number of parameters such as the number of compute nodes, the [size of the nodes](../cloud-services/cloud-services-sizes-specs.md), and the nodes' operating system. In *DotNetTutorial*, we use [CloudServiceConfiguration][net_cloudserviceconfiguration] to specify Windows Server 2012 R2 from [Cloud Services](../cloud-services/cloud-services-guestos-update-matrix.md). However, by specifying a [VirtualMachineConfiguration][net_virtualmachineconfiguration] instead, you can create pools of nodes created from Marketplace images, which includes both Windows and Linux images--see [Introducing Linux support on Azure Batch][blog_linux] for more information.\n\n> [AZURE.IMPORTANT] You are charged for compute resources in Batch. To minimize costs, you can lower `targetDedicated` to 1 before you run the sample.\n\nAlong with these physical node properties, you may also specify a [StartTask][net_pool_starttask] for the pool. The StartTask will execute on each node as that node joins the pool, as well as each time a node is restarted. The StartTask is especially useful for installing applications on compute nodes prior to the execution of tasks. For example, if your tasks process data by using Python scripts, you could use a StartTask to install Python on the compute nodes.\n\nIn this sample application, the StartTask copies the files that it downloads from Storage (which are specified by using the [StartTask][net_starttask].[ResourceFiles][net_starttask_resourcefiles] property) from the StartTask working directory to the shared directory that *all* tasks running on the node can access. Essentially, this copies `TaskApplication.exe` and its dependencies to the shared directory on each node as the node joins the pool, so that any tasks that run on the node can access it.\n\n> [AZURE.TIP] The **application packages** feature of Azure Batch provides another way to get your application onto the compute nodes in a pool. See [Application deployment with Azure Batch application packages](batch-application-packages.md) for details.\n\nAlso notable in the code snippet above is the use of two environment variables in the *CommandLine* property of the StartTask: `%AZ_BATCH_TASK_WORKING_DIR%` and `%AZ_BATCH_NODE_SHARED_DIR%`. Each compute node within a Batch pool is automatically configured with a number of environment variables that are specific to Batch. Any process that is executed by a task has access to these environment variables.\n\n> [AZURE.TIP] To find out more about the environment variables that are available on compute nodes in a Batch pool, as well as information on task working directories, see the \"Environment settings for tasks\" and \"Files and directories\" sections in the [overview of Azure Batch features](batch-api-basics.md).\n\n## Step 4: Create Batch job\n\n![Create Batch job][4]<br/>\n\nA Batch job is essentially a collection of tasks that are associated with a pool of compute nodes. You can use it not only for organizing and tracking tasks in related workloads, but also for imposing certain constraints--such as the maximum runtime for the job (and by extension, its tasks), as well as job priority in relation to other jobs in the Batch account. In this example, however, the job is associated only with the pool that was created in step #3. No additional properties are configured.\n\nAll Batch jobs are associated with a specific pool. This association indicates which nodes the job's tasks will execute on.  You specify this by using the [CloudJob.PoolInformation][net_job_poolinfo] property, as shown in the code snippet below.\n\n```\nprivate static async Task CreateJobAsync(\n    BatchClient batchClient,\n    string jobId,\n    string poolId)\n{\n    Console.WriteLine(\"Creating job [{0}]...\", jobId);\n\n    CloudJob job = batchClient.JobOperations.CreateJob();\n    job.Id = jobId;\n    job.PoolInformation = new PoolInformation { PoolId = poolId };\n\n    await job.CommitAsync();\n}\n```\n\nNow that a job has been created, tasks are added to perform the work.\n\n## Step 5: Add tasks to job\n\n![Add tasks to job][5]<br/>\n*(1) Tasks are added to the job, (2) the tasks are scheduled to run on nodes, and (3) the tasks download the data files to process*\n\nTo actually perform work, tasks must be added to a job. Each [CloudTask][net_task] is configured by using a command-line property and [ResourceFiles][net_task_resourcefiles] (as with the pool's StartTask) that the task downloads to the node before its command line is automatically executed. In the *DotNetTutorial* sample project, each task processes only one file. Thus, its ResourceFiles collection contains a single element.\n\n```\nprivate static async Task<List<CloudTask>> AddTasksAsync(\n    BatchClient batchClient,\n    string jobId,\n    List<ResourceFile> inputFiles,\n    string outputContainerSasUrl)\n{\n    Console.WriteLine(\"Adding {0} tasks to job [{1}]...\", inputFiles.Count, jobId);\n\n    // Create a collection to hold the tasks that we'll be adding to the job\n    List<CloudTask> tasks = new List<CloudTask>();\n\n    // Create each of the tasks. Because we copied the task application to the\n    // node's shared directory with the pool's StartTask, we can access it via\n    // the shared directory on the node that the task runs on.\n    foreach (ResourceFile inputFile in inputFiles)\n    {\n        string taskId = \"topNtask\" + inputFiles.IndexOf(inputFile);\n        string taskCommandLine = String.Format(\n            \"cmd /c %AZ_BATCH_NODE_SHARED_DIR%\\\\TaskApplication.exe {0} 3 \\\"{1}\\\"\",\n            inputFile.FilePath,\n            outputContainerSasUrl);\n\n        CloudTask task = new CloudTask(taskId, taskCommandLine);\n        task.ResourceFiles = new List<ResourceFile> { inputFile };\n        tasks.Add(task);\n    }\n\n    // Add the tasks as a collection, as opposed to issuing a separate AddTask call\n    // for each. Bulk task submission helps to ensure efficient underlying API calls\n    // to the Batch service.\n    await batchClient.JobOperations.AddTaskAsync(jobId, tasks);\n\n    return tasks;\n}\n```\n\n> [AZURE.IMPORTANT] When they access environment variables such as `%AZ_BATCH_NODE_SHARED_DIR%` or execute an application not found in the node's `PATH`, task command lines must be prefixed with `cmd /c`. This will explicitly execute the command interpreter and instruct it to terminate after carrying out your command. This requirement is unnecessary if your tasks execute an application in the node's `PATH` (such as *robocopy.exe* or *powershell.exe*) and no environment variables are used.\n\nWithin the `foreach` loop in the code snippet above, you can see that the command line for the task is constructed such that three command-line arguments are passed to *TaskApplication.exe*:\n\n1. The **first argument** is the path of the file to process. This is the local path to the file as it exists on the node. When the ResourceFile object in `UploadFileToContainerAsync` was first created above, the file name was used for this property (as a parameter to the ResourceFile constructor). This indicates that the file can be found in the same directory as *TaskApplication.exe*.\n\n2. The **second argument** specifies that the top *N* words should be written to the output file. In the sample, this is hard-coded so that the top three words will be written to the output file.\n\n3. The **third argument** is the shared access signature (SAS) that provides write access to the **output** container in Azure Storage. *TaskApplication.exe* uses this shared access signature URL when it uploads the output file to Azure Storage. You can find the code for this in the `UploadFileToContainer` method in the TaskApplication project's `Program.cs` file:\n\n```\n// NOTE: From project TaskApplication Program.cs\n\nprivate static void UploadFileToContainer(string filePath, string containerSas)\n{\n        string blobName = Path.GetFileName(filePath);\n\n        // Obtain a reference to the container using the SAS URI.\n        CloudBlobContainer container = new CloudBlobContainer(new Uri(containerSas));\n\n        // Upload the file (as a new blob) to the container\n        try\n        {\n                CloudBlockBlob blob = container.GetBlockBlobReference(blobName);\n                blob.UploadFromFile(filePath, FileMode.Open);\n\n                Console.WriteLine(\"Write operation succeeded for SAS URL \" + containerSas);\n                Console.WriteLine();\n        }\n        catch (StorageException e)\n        {\n\n                Console.WriteLine(\"Write operation failed for SAS URL \" + containerSas);\n                Console.WriteLine(\"Additional error information: \" + e.Message);\n                Console.WriteLine();\n\n                // Indicate that a failure has occurred so that when the Batch service\n                // sets the CloudTask.ExecutionInformation.ExitCode for the task that\n                // executed this application, it properly indicates that there was a\n                // problem with the task.\n                Environment.ExitCode = -1;\n        }\n}\n```\n\n## Step 6: Monitor tasks\n\n![Monitor tasks][6]<br/>\n*The client application (1) monitors the tasks for completion and success status, and (2) the tasks upload result data to Azure Storage*\n\nWhen tasks are added to a job, they are automatically queued and scheduled for execution on compute nodes within the pool associated with the job. Based on the settings you specify, Batch handles all task queuing, scheduling, retrying, and other task administration duties for you. There are many approaches to monitoring task execution. DotNetTutorial shows a simple example that reports only on completion and task failure or success states.\n\nWithin the `MonitorTasks` method in DotNetTutorial's `Program.cs`, there are three Batch .NET concepts that warrant discussion. They are listed below in their order of appearance:\n\n1. **ODATADetailLevel**: Specifying [ODATADetailLevel][net_odatadetaillevel] in list operations (such as obtaining a list of a job's tasks) is essential in ensuring Batch application performance. Add [Query the Azure Batch service efficiently](batch-efficient-list-queries.md) to your reading list if you plan on doing any sort of status monitoring within your Batch applications.\n\n2. **TaskStateMonitor**: [TaskStateMonitor][net_taskstatemonitor] provides Batch .NET applications with helper utilities for monitoring task states. In `MonitorTasks`, *DotNetTutorial* waits for all tasks to reach [TaskState.Completed][net_taskstate] within a time limit. Then it terminates the job.\n\n3. **TerminateJobAsync**: Terminating a job with [JobOperations.TerminateJobAsync][net_joboperations_terminatejob] (or the blocking JobOperations.TerminateJob) will mark that job as completed. It is essential to do so if your Batch solution uses a [JobReleaseTask][net_jobreltask]. This is a special type of task, which is described in [Job preparation and completion tasks](batch-job-prep-release.md).\n\nThe `MonitorTasks` method from *DotNetTutorial*'s `Program.cs` appears below:\n\n```\nprivate static async Task<bool> MonitorTasks(\n    BatchClient batchClient,\n    string jobId,\n    TimeSpan timeout)\n{\n    bool allTasksSuccessful = true;\n    const string successMessage = \"All tasks reached state Completed.\";\n    const string failureMessage = \"One or more tasks failed to reach the Completed state within the timeout period.\";\n\n    // Obtain the collection of tasks currently managed by the job. Note that we use\n    // a detail level to specify that only the \"id\" property of each task should be\n    // populated. Using a detail level for all list operations helps to lower\n    // response time from the Batch service.\n    ODATADetailLevel detail = new ODATADetailLevel(selectClause: \"id\");\n    List<CloudTask> tasks =\n        await batchClient.JobOperations.ListTasks(JobId, detail).ToListAsync();\n\n    Console.WriteLine(\"Awaiting task completion, timeout in {0}...\", timeout.ToString());\n\n    // We use a TaskStateMonitor to monitor the state of our tasks. In this case, we\n    // will wait for all tasks to reach the Completed state.\n    TaskStateMonitor taskStateMonitor = batchClient.Utilities.CreateTaskStateMonitor();\n    bool timedOut = await taskStateMonitor.WaitAllAsync(\n        tasks,\n        TaskState.Completed,\n        timeout);\n\n    if (timedOut)\n    {\n        allTasksSuccessful = false;\n\n        await batchClient.JobOperations.TerminateJobAsync(jobId, failureMessage);\n\n        Console.WriteLine(failureMessage);\n    }\n    else\n    {\n        await batchClient.JobOperations.TerminateJobAsync(jobId, successMessage);\n\n        // All tasks have reached the \"Completed\" state. However, this does not\n        // guarantee that all tasks were completed successfully. Here we further\n        // check each task's ExecutionInfo property to ensure that it did not\n        // encounter a scheduling error or return a non-zero exit code.\n\n        // Update the detail level to populate only the task id and executionInfo\n        // properties. We refresh the tasks below, and need only this information\n        // for each task.\n        detail.SelectClause = \"id, executionInfo\";\n\n        foreach (CloudTask task in tasks)\n        {\n            // Populate the task's properties with the latest info from the Batch service\n            await task.RefreshAsync(detail);\n\n            if (task.ExecutionInformation.SchedulingError != null)\n            {\n                // A scheduling error indicates a problem starting the task on the\n                // node. It is important to note that the task's state can be\n                // \"Completed,\" yet the task still might have encountered a\n                // scheduling error.\n\n                allTasksSuccessful = false;\n\n                Console.WriteLine(\n                    \"WARNING: Task [{0}] encountered a scheduling error: {1}\",\n                    task.Id,\n                    task.ExecutionInformation.SchedulingError.Message);\n            }\n            else if (task.ExecutionInformation.ExitCode != 0)\n            {\n                // A non-zero exit code may indicate that the application executed by\n                // the task encountered an error during execution. As not every\n                // application returns non-zero on failure by default (e.g. robocopy),\n                // your implementation of error checking may differ from this example.\n\n                allTasksSuccessful = false;\n\n                Console.WriteLine(\"WARNING: Task [{0}] returned a non-zero exit code - this may indicate task execution or completion failure.\", task.Id);\n            }\n        }\n    }\n\n    if (allTasksSuccessful)\n    {\n        Console.WriteLine(\"Success! All tasks completed successfully within the specified timeout period.\");\n    }\n\n    return allTasksSuccessful;\n}\n```\n\n## Step 7: Download task output\n\n![Download task output from Storage][7]<br/>\n\nNow that the job is completed, the output from the tasks can be downloaded from Azure Storage. This is done with a call to `DownloadBlobsFromContainerAsync` in *DotNetTutorial*'s `Program.cs`:\n\n```\nprivate static async Task DownloadBlobsFromContainerAsync(\n    CloudBlobClient blobClient,\n    string containerName,\n    string directoryPath)\n{\n        Console.WriteLine(\"Downloading all files from container [{0}]...\", containerName);\n\n        // Retrieve a reference to a previously created container\n        CloudBlobContainer container = blobClient.GetContainerReference(containerName);\n\n        // Get a flat listing of all the block blobs in the specified container\n        foreach (IListBlobItem item in container.ListBlobs(\n                    prefix: null,\n                    useFlatBlobListing: true))\n        {\n                // Retrieve reference to the current blob\n                CloudBlob blob = (CloudBlob)item;\n\n                // Save blob contents to a file in the specified folder\n                string localOutputFile = Path.Combine(directoryPath, blob.Name);\n                await blob.DownloadToFileAsync(localOutputFile, FileMode.Create);\n        }\n\n        Console.WriteLine(\"All files downloaded to {0}\", directoryPath);\n}\n```\n\n> [AZURE.NOTE] The call to `DownloadBlobsFromContainerAsync` in the *DotNetTutorial* application specifies that the files should be downloaded to your `%TEMP%` folder. Feel free to modify this output location.\n\n## Step 8: Delete containers\n\nBecause you are charged for data that resides in Azure Storage, it is always a good idea to remove any blobs that are no longer needed for your Batch jobs. In DotNetTutorial's `Program.cs`, this is done with three calls to the helper method `DeleteContainerAsync`:\n\n```\n// Clean up Storage resources\nawait DeleteContainerAsync(blobClient, appContainerName);\nawait DeleteContainerAsync(blobClient, inputContainerName);\nawait DeleteContainerAsync(blobClient, outputContainerName);\n```\n\nThe method itself merely obtains a reference to the container, and then calls [CloudBlobContainer.DeleteIfExistsAsync][net_container_delete]:\n\n```\nprivate static async Task DeleteContainerAsync(\n    CloudBlobClient blobClient,\n    string containerName)\n{\n    CloudBlobContainer container = blobClient.GetContainerReference(containerName);\n\n    if (await container.DeleteIfExistsAsync())\n    {\n        Console.WriteLine(\"Container [{0}] deleted.\", containerName);\n    }\n    else\n    {\n        Console.WriteLine(\"Container [{0}] does not exist, skipping deletion.\",\n            containerName);\n    }\n}\n```\n\n## Step 9: Delete the job and the pool\n\nIn the final step, the user is prompted to delete the job and the pool that were created by the DotNetTutorial application. Although you are not charged for jobs and tasks themselves, you *are* charged for compute nodes. Thus, we recommend that you allocate nodes only as needed. Deleting unused pools can be part of your maintenance process.\n\nThe BatchClient's [JobOperations][net_joboperations] and [PoolOperations][net_pooloperations] both have corresponding deletion methods, which are called if the user confirms deletion:\n\n```\n// Clean up the resources we've created in the Batch account if the user so chooses\nConsole.WriteLine();\nConsole.WriteLine(\"Delete job? [yes] no\");\nstring response = Console.ReadLine().ToLower();\nif (response != \"n\" && response != \"no\")\n{\n    await batchClient.JobOperations.DeleteJobAsync(JobId);\n}\n\nConsole.WriteLine(\"Delete pool? [yes] no\");\nresponse = Console.ReadLine();\nif (response != \"n\" && response != \"no\")\n{\n    await batchClient.PoolOperations.DeletePoolAsync(PoolId);\n}\n```\n\n> [AZURE.IMPORTANT] Keep in mind that you are charged for compute resources--deleting unused pools will minimize cost. Also, be aware that deleting a pool deletes all compute nodes within that pool, and that any data on the nodes will be unrecoverable after the pool is deleted.\n\n## Run the *DotNetTutorial* sample\n\nWhen you run the sample application, the console output will be similar to the following. During execution, you will experience a pause at `Awaiting task completion, timeout in 00:30:00...` while the pool's compute nodes are started. Use the [Batch Explorer][github_batchexplorer] to monitor your pool, compute nodes, job, and tasks during and after execution. Use the [Azure portal][azure_portal] or one of the [available Azure Storage explorers][storage_explorers] to view the Storage resources (containers and blobs) that are created by the application.\n\nTypical execution time is **approximately 5 minutes** when you run the application in its default configuration.\n\n```\nSample start: 1/8/2016 09:42:58 AM\n\nContainer [application] created.\nContainer [input] created.\nContainer [output] created.\nUploading file C:\\repos\\azure-batch-samples\\CSharp\\ArticleProjects\\DotNetTutorial\\bin\\Debug\\TaskApplication.exe to container [application]...\nUploading file Microsoft.WindowsAzure.Storage.dll to container [application]...\nUploading file ..\\..\\taskdata1.txt to container [input]...\nUploading file ..\\..\\taskdata2.txt to container [input]...\nUploading file ..\\..\\taskdata3.txt to container [input]...\nCreating pool [DotNetTutorialPool]...\nCreating job [DotNetTutorialJob]...\nAdding 3 tasks to job [DotNetTutorialJob]...\nAwaiting task completion, timeout in 00:30:00...\nSuccess! All tasks completed successfully within the specified timeout period.\nDownloading all files from container [output]...\nAll files downloaded to C:\\Users\\USERNAME\\AppData\\Local\\Temp\nContainer [application] deleted.\nContainer [input] deleted.\nContainer [output] deleted.\n\nSample end: 1/8/2016 09:47:47 AM\nElapsed time: 00:04:48.5358142\n\nDelete job? [yes] no: yes\nDelete pool? [yes] no: yes\n\nSample complete, hit ENTER to exit...\n```\n\n## Next steps\n\nFeel free to make changes to *DotNetTutorial* and *TaskApplication* to experiment with different compute scenarios. For example, try adding an execution delay to *TaskApplication*, such as with [Thread.Sleep][net_thread_sleep], to simulate long-running tasks and monitor them with the Batch Explorer's *Heat Map* feature. Try adding more tasks or adjusting the number of compute nodes. Add logic to check for and allow the use of an existing pool to speed execution time (*hint*: check out `ArticleHelpers.cs` in the [Microsoft.Azure.Batch.Samples.Common][github_samples_common] project in [azure-batch-samples][github_samples]).\n\nNow that you're familiar with the basic workflow of a Batch solution, it's time to dig in to the additional features of the Batch service.\n\n- Review the [Overview of Azure Batch features](batch-api-basics.md) article, which we recommend if you're new to the service.\n- Start on the other Batch development articles under **Development in-depth** in the [Batch learning path][batch_learning_path].\n- Check out a different implementation of processing the \"top N words\" workload by using Batch in the [TopNWords][github_topnwords] sample.\n\n[azure_batch]: https://azure.microsoft.com/services/batch/\n[azure_free_account]: https://azure.microsoft.com/free/\n[azure_portal]: https://portal.azure.com\n[batch_explorer_blog]: http://blogs.technet.com/b/windowshpc/archive/2015/01/20/azure-batch-explorer-sample-walkthrough.aspx\n[batch_learning_path]: https://azure.microsoft.com/documentation/learning-paths/batch/\n[blog_linux]: http://blogs.technet.com/b/windowshpc/archive/2016/03/30/introducing-linux-support-on-azure-batch.aspx\n[github_batchexplorer]: https://github.com/Azure/azure-batch-samples/tree/master/CSharp/BatchExplorer\n[github_dotnettutorial]: https://github.com/Azure/azure-batch-samples/tree/master/CSharp/ArticleProjects/DotNetTutorial\n[github_samples]: https://github.com/Azure/azure-batch-samples\n[github_samples_common]: https://github.com/Azure/azure-batch-samples/tree/master/CSharp/Common\n[github_samples_zip]: https://github.com/Azure/azure-batch-samples/archive/master.zip\n[github_topnwords]: https://github.com/Azure/azure-batch-samples/tree/master/CSharp/TopNWords\n[net_api]: http://msdn.microsoft.com/library/azure/mt348682.aspx\n[net_api_storage]: https://msdn.microsoft.com/library/azure/mt347887.aspx\n[net_batchclient]: https://msdn.microsoft.com/library/azure/microsoft.azure.batch.batchclient.aspx\n[net_cloudblobclient]: https://msdn.microsoft.com/library/microsoft.windowsazure.storage.blob.cloudblobclient.aspx\n[net_cloudblobcontainer]: https://msdn.microsoft.com/library/microsoft.windowsazure.storage.blob.cloudblobcontainer.aspx\n[net_cloudstorageaccount]: https://msdn.microsoft.com/library/azure/microsoft.windowsazure.storage.cloudstorageaccount.aspx\n[net_cloudserviceconfiguration]: https://msdn.microsoft.com/library/azure/microsoft.azure.batch.cloudserviceconfiguration.aspx\n[net_container_delete]: https://msdn.microsoft.com/library/microsoft.windowsazure.storage.blob.cloudblobcontainer.deleteifexistsasync.aspx\n[net_job]: https://msdn.microsoft.com/library/azure/microsoft.azure.batch.cloudjob.aspx\n[net_job_poolinfo]: https://msdn.microsoft.com/library/azure/microsoft.azure.batch.protocol.models.cloudjob.poolinformation.aspx\n[net_joboperations]: https://msdn.microsoft.com/library/azure/microsoft.azure.batch.batchclient.joboperations\n[net_joboperations_terminatejob]: https://msdn.microsoft.com/library/azure/microsoft.azure.batch.joboperations.terminatejobasync.aspx\n[net_jobpreptask]: https://msdn.microsoft.com/library/azure/microsoft.azure.batch.cloudjob.jobpreparationtask.aspx\n[net_jobreltask]: https://msdn.microsoft.com/library/azure/microsoft.azure.batch.cloudjob.jobreleasetask.aspx\n[net_node]: https://msdn.microsoft.com/library/azure/microsoft.azure.batch.computenode.aspx\n[net_odatadetaillevel]: https://msdn.microsoft.com/library/azure/microsoft.azure.batch.odatadetaillevel.aspx\n[net_pool]: https://msdn.microsoft.com/library/azure/microsoft.azure.batch.cloudpool.aspx\n[net_pool_create]: https://msdn.microsoft.com/library/azure/microsoft.azure.batch.pooloperations.createpool.aspx\n[net_pool_starttask]: https://msdn.microsoft.com/library/azure/microsoft.azure.batch.cloudpool.starttask.aspx\n[net_pooloperations]: https://msdn.microsoft.com/library/azure/microsoft.azure.batch.batchclient.pooloperations\n[net_resourcefile]: https://msdn.microsoft.com/library/azure/microsoft.azure.batch.resourcefile.aspx\n[net_resourcefile_blobsource]: https://msdn.microsoft.com/library/azure/microsoft.azure.batch.resourcefile.blobsource.aspx\n[net_sas_blob]: https://msdn.microsoft.com/library/azure/microsoft.windowsazure.storage.blob.cloudblob.getsharedaccesssignature.aspx\n[net_sas_container]: https://msdn.microsoft.com/library/azure/microsoft.windowsazure.storage.blob.cloudblobcontainer.getsharedaccesssignature.aspx\n[net_starttask]: https://msdn.microsoft.com/library/azure/microsoft.azure.batch.starttask.aspx\n[net_starttask_resourcefiles]: https://msdn.microsoft.com/library/azure/microsoft.azure.batch.starttask.resourcefiles.aspx\n[net_task]: https://msdn.microsoft.com/library/azure/microsoft.azure.batch.cloudtask.aspx\n[net_task_resourcefiles]: https://msdn.microsoft.com/library/azure/microsoft.azure.batch.cloudtask.resourcefiles.aspx\n[net_taskstate]: https://msdn.microsoft.com/library/azure/microsoft.azure.batch.common.taskstate.aspx\n[net_taskstatemonitor]: https://msdn.microsoft.com/library/azure/microsoft.azure.batch.taskstatemonitor.aspx\n[net_thread_sleep]: https://msdn.microsoft.com/library/274eh01d(v=vs.110).aspx\n[net_virtualmachineconfiguration]: https://msdn.microsoft.com/library/azure/microsoft.azure.batch.virtualmachineconfiguration.aspx\n[nuget_packagemgr]: https://visualstudiogallery.msdn.microsoft.com/27077b70-9dad-4c64-adcf-c7cf6bc9970c\n[nuget_restore]: https://docs.nuget.org/consume/package-restore/msbuild-integrated#enabling-package-restore-during-build\n[storage_explorers]: http://blogs.msdn.com/b/windowsazurestorage/archive/2014/03/11/windows-azure-storage-explorers-2014.aspx\n[visual_studio]: https://www.visualstudio.com/products/vs-2015-product-editions\n\n[1]: ./media/batch-dotnet-get-started/batch_workflow_01_sm.png \"Create containers in Azure Storage\"\n[2]: ./media/batch-dotnet-get-started/batch_workflow_02_sm.png \"Upload task application and input (data) files to containers\"\n[3]: ./media/batch-dotnet-get-started/batch_workflow_03_sm.png \"Create Batch pool\"\n[4]: ./media/batch-dotnet-get-started/batch_workflow_04_sm.png \"Create Batch job\"\n[5]: ./media/batch-dotnet-get-started/batch_workflow_05_sm.png \"Add tasks to job\"\n[6]: ./media/batch-dotnet-get-started/batch_workflow_06_sm.png \"Monitor tasks\"\n[7]: ./media/batch-dotnet-get-started/batch_workflow_07_sm.png \"Download task output from Storage\"\n[8]: ./media/batch-dotnet-get-started/batch_workflow_sm.png \"Batch solution workflow (full diagram)\"\n[9]: ./media/batch-dotnet-get-started/credentials_batch_sm.png \"Batch credentials in Portal\"\n[10]: ./media/batch-dotnet-get-started/credentials_storage_sm.png \"Storage credentials in Portal\"\n[11]: ./media/batch-dotnet-get-started/batch_workflow_minimal_sm.png \"Batch solution workflow (minimal diagram)\"\n"}