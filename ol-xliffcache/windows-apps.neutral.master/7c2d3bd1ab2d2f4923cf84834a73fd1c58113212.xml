{"nodes":[{"content":"User interactions in the Universal Windows Platform (UWP) are a combination of input and output sources (such as mouse, keyboard, pen, touch, touchpad, speech, Cortana, controller, gesture, gaze, and so on), along with various modes or modifiers that enable extended experiences (including mouse wheel and buttons, pen eraser and barrel buttons, touch keyboard, and background app services).","pos":[47,438]},{"content":"Interaction primer","pos":[446,464]},{"content":"Interaction primer","pos":[568,586]},{"content":"windows input types","pos":[591,610]},{"pos":[665,1060],"content":"User interactions in the Universal Windows Platform (UWP) are a combination of input and output sources (such as mouse, keyboard, pen, touch, touchpad, speech, <bpt id=\"p1\">**</bpt>Cortana<ept id=\"p1\">**</ept>, controller, gesture, gaze, and so on), along with various modes or modifiers that enable extended experiences (including mouse wheel and buttons, pen eraser and barrel buttons, touch keyboard, and background app services)."},{"content":"The UWP uses a \"smart\" contextual interaction system that, in most cases, eliminates the need to individually handle the unique types of input received by your app.","pos":[1063,1227]},{"content":"This includes handling touch, touchpad, mouse, and pen input as a generic pointer type to support static gestures such as tap or press-and-hold, manipulation gestures such as slide for panning, or rendering digital ink.","pos":[1228,1447]},{"content":"Familiarize yourself with each input device type and its behaviors, capabilities, and limitations when paired with certain form factors.","pos":[1449,1585]},{"content":"This can help you decide whether the platform controls and affordances are sufficient for your app, or require you to provide customized interaction experiences.","pos":[1586,1747]},{"pos":[1830,1837],"content":"Cortana"},{"pos":[1840,1981],"content":"In Windows 10, <bpt id=\"p1\">**</bpt>Cortana<ept id=\"p1\">**</ept> extensibility lets you handle voice commands from a user and launch your application to carry out a single action."},{"content":"Device support","pos":[1983,1997]},{"content":"Phones and phablets","pos":[2002,2021]},{"content":"Tablet","pos":[2026,2032]},{"content":"PCs and laptops","pos":[2037,2052]},{"content":"Surface Hub","pos":[2057,2068]},{"content":"IoT","pos":[2073,2076]},{"content":"Xbox","pos":[2081,2085]},{"content":"HoloLens","pos":[2090,2098]},{"content":"cortana","pos":[2102,2109]},{"content":"Typical usage A voice command is a single utterance, defined in a Voice Command Definition (VCD) file, directed at an installed app through <bpt id=\"p1\">**</bpt>Cortana<ept id=\"p1\">**</ept>.","pos":[2159,2311]},{"content":"The app can be launched in the foreground or background, depending on the level and complexity of the interaction.","pos":[2312,2426]},{"content":"For instance, voice commands that require additional context or user input are best handled in the foreground, while basic commands can be handled in the background.","pos":[2427,2592]},{"content":"Integrating the basic functionality of your app, and providing a central entry point for the user to accomplish most of the tasks without opening your app directly, lets <bpt id=\"p1\">**</bpt>Cortana<ept id=\"p1\">**</ept> become a liaison between your app and the user.","pos":[2594,2823]},{"content":"In many cases, this can save the user significant time and effort.","pos":[2824,2890]},{"content":"For more info, see <bpt id=\"p1\">[</bpt>Cortana design guidelines<ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn974233)</ept>.","pos":[2891,2996]},{"pos":[2998,3093],"content":"More info <bpt id=\"p1\">[</bpt>Cortana design guidelines<ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn974233)</ept>"},{"pos":[3175,3181],"content":"Speech"},{"content":"Speech is an effective and natural way for people to interact with applications.","pos":[3184,3264]},{"content":"It's an easy and accurate way to communicate with applications, and lets people be productive and stay informed in a variety of situations.","pos":[3265,3404]},{"content":"Speech can complement or, in many cases, be the primary input type, depending on the user's device.","pos":[3406,3505]},{"content":"For example, devices such as HoloLens and Xbox do not support traditional input types (aside from a software keyboard in specifc scenarios).","pos":[3506,3646]},{"content":"Instead, they rely on speech input and output (often combined with other non-traditional input types such as gaze and gesture) for most user interactions.","pos":[3647,3801]},{"content":"Text-to-speech (also known as TTS, or speech synthesis) is used to inform or direct the user.","pos":[3803,3896]},{"content":"Device support","pos":[3898,3912]},{"content":"Phones and phablets","pos":[3917,3936]},{"content":"Tablet","pos":[3941,3947]},{"content":"PCs and laptops","pos":[3952,3967]},{"content":"Surface Hub","pos":[3972,3983]},{"content":"IoT","pos":[3988,3991]},{"content":"Xbox","pos":[3996,4000]},{"content":"HoloLens","pos":[4005,4013]},{"content":"speech","pos":[4017,4023]},{"content":"Typical usage","pos":[4072,4085]},{"content":"There are three modes of Speech interaction:","pos":[4087,4131]},{"content":"Natural language","pos":[4238,4254]},{"content":"Natural language is how we verbally interact with people on a regular basis.","pos":[4257,4333]},{"content":"Our speech varies from person to person and situation to situation, and is generally understood.","pos":[4334,4430]},{"content":"When it's not, we often use different words and word order to get the same idea across.","pos":[4431,4518]},{"content":"Natural language interactions with an app are similar: we speak to the app through our device as if it were a person and expect it to understand and react accordingly.","pos":[4520,4687]},{"pos":[4689,4810],"content":"Natural language is the most advanced mode of speech interaction, and can be implemented and exposed through <bpt id=\"p1\">**</bpt>Cortana<ept id=\"p1\">**</ept>."},{"content":"Command and control","pos":[4926,4945]},{"content":"Command and control is the use of verbal commands to activate controls and functionality such as clicking a button or selecting a menu item.","pos":[4948,5088]},{"content":"As command and control is critical to a successful user experience, a single input type is generally not recommended.","pos":[5090,5207]},{"content":"Speech is typically one of several input options for a user based on their preferences or hardware capabilities.","pos":[5208,5320]},{"content":"Dictation","pos":[5406,5415]},{"content":"The most basic speech input method.","pos":[5418,5453]},{"content":"Each utterance is converted to text.","pos":[5454,5490]},{"content":"Dictation is typically used when an app doesn’t need to understand meaning or intent.","pos":[5492,5577]},{"pos":[5579,5673],"content":"More info <bpt id=\"p1\">[</bpt>Speech design guidelines<ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn596121)</ept>"},{"pos":[5746,5749],"content":"Pen"},{"content":"A pen (or stylus) can serve as a pixel precise pointing device, like a mouse, and is the optimal device for digital ink input.","pos":[5752,5878]},{"pos":[5880,5945],"content":"<bpt id=\"p1\">**</bpt>Note<ept id=\"p1\">**</ept>  There are two types of pen devices: active and passive."},{"content":"Passive pens do not contain electronics, and effectively emulate touch input from a finger.","pos":[5950,6041]},{"content":"They require a basic device display that recognizes input based on contact pressure.","pos":[6042,6126]},{"content":"Because users often rest their hand as they write on the input surface, input data can become polluted due to unsuccessful palm rejection.","pos":[6127,6265]},{"content":"Active pens contain electronics and can work with complex device displays to provide much more extensive input data (including hover, or proximity data) to the system and your app.","pos":[6270,6450]},{"content":"Palm rejection is much more robust.","pos":[6451,6486]},{"content":"When we refer to pen devices here, we are referring to active pens that provide rich input data and are used primarily for precise ink and pointing interactions.","pos":[6491,6652]},{"content":"Device support","pos":[6654,6668]},{"content":"Phones and phablets","pos":[6673,6692]},{"content":"Tablet","pos":[6697,6703]},{"content":"PCs and laptops","pos":[6708,6723]},{"content":"Surface Hub","pos":[6728,6739]},{"content":"IoT","pos":[6744,6747]},{"content":"pen","pos":[6751,6754]},{"content":"Typical usage The Windows ink platform, together with a pen, provides a natural way to create handwritten notes, drawings, and annotations.","pos":[6800,6939]},{"content":"The platform supports capturing ink data from digitizer input, generating ink data, rendering that data as ink strokes on the output device, managing the ink data, and performing handwriting recognition.","pos":[6940,7143]},{"content":"In addition to capturing the spatial movements of the pen as the user writes or draws, your app can also collect info such as pressure, shape, color, and opacity, to offer user experiences that closely resemble drawing on paper with a pen, pencil, or brush.","pos":[7144,7401]},{"content":"Where pen and touch input diverge is the ability for touch to emulate direct manipulation of UI elements on the screen through physical gestures performed on those objects (such as swiping, sliding, dragging, rotating, and so on).","pos":[7403,7633]},{"content":"You should provide pen-specific UI commands, or affordances, to support these interactions.","pos":[7635,7726]},{"content":"For example, use previous and next (or + and -) buttons to let users flip through pages of content, or rotate, resize, and zoom objects.","pos":[7727,7863]},{"pos":[7865,7956],"content":"More info <bpt id=\"p1\">[</bpt>Pen design guidelines<ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn456352)</ept>"},{"pos":[8035,8040],"content":"Touch"},{"content":"With touch, physical gestures from one or more fingers can be used to either emulate the direct manipulation of UI elements (such as panning, rotating, resizing, or moving), as an alternative input method (similar to mouse or pen), or as a complementary input method (to modify aspects of other input, such as smudging an ink stroke drawn with a pen).","pos":[8043,8394]},{"content":"Tactile experiences such as this can provide more natural, real-world sensations for users as they interact with elements on a screen.","pos":[8395,8529]},{"content":"Device support","pos":[8531,8545]},{"content":"Phones and phablets","pos":[8550,8569]},{"content":"Tablet","pos":[8574,8580]},{"content":"PCs and laptops","pos":[8585,8600]},{"content":"Surface Hub","pos":[8605,8616]},{"content":"IoT","pos":[8621,8624]},{"content":"touch","pos":[8628,8633]},{"pos":[8681,8767],"content":"Typical usage Support for touch input can vary significantly, depending on the device."},{"content":"Some devices don't support touch at all, some devices support a single touch contact, while others support multi-touch (two or more contacts).","pos":[8769,8911]},{"content":"Most devices that support multi-touch input, typically recognize ten unique, concurrent contacts.","pos":[8913,9010]},{"content":"Surface Hub devices recognize 100 unique, concurrent touch contacts.","pos":[9012,9080]},{"content":"In general, touch is:","pos":[9082,9103]},{"content":"Single user, unless being used with a Microsoft Team device like Surface Hub, where collaboration is emphasized.","pos":[9109,9221]},{"content":"Not constrained to device orientation.","pos":[9226,9264]},{"content":"Used for all interactions, including text input (touch keyboard) and inking (app-configured).","pos":[9269,9362]},{"pos":[9364,9457],"content":"More info <bpt id=\"p1\">[</bpt>Touch design guidelines<ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/hh465370)</ept>"},{"pos":[9545,9553],"content":"Touchpad"},{"content":"A touchpad combines both indirect multi-touch input with the precision input of a pointing device, such as a mouse.","pos":[9556,9671]},{"content":"This combination makes the touchpad suited to both a touch-optimized UI and the smaller targets of productivity apps.","pos":[9672,9789]},{"content":"Device support","pos":[9791,9805]},{"content":"PCs and laptops","pos":[9810,9825]},{"content":"IoT","pos":[9830,9833]},{"content":"touchpad","pos":[9837,9845]},{"pos":[9896,10042],"content":"Typical usage Touchpads typically support a set of touch gestures that provide support similar to touch for direct manipulation of objects and UI."},{"content":"Because of this convergence of interaction experiences supported by touchpads, we recommend also providing mouse-style UI commands or affordances rather than relying solely on support for touch input.","pos":[10044,10244]},{"content":"Provide touchpad-specific UI commands, or affordances, to support these interactions.","pos":[10245,10330]},{"content":"You should provide mouse-specific UI commands, or affordances, to support these interactions.","pos":[10332,10425]},{"content":"For example, use previous and next (or + and -) buttons to let users flip through pages of content, or rotate, resize, and zoom objects.","pos":[10426,10562]},{"pos":[10564,10660],"content":"More info <bpt id=\"p1\">[</bpt>Touchpad design guidelines<ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn456353)</ept>"},{"pos":[10748,10756],"content":"Keyboard"},{"content":"A keyboard is the primary input device for text, and is often indispensable to people with certain disabilities or users who consider it a faster and more efficient way to interact with an app.","pos":[10759,10952]},{"pos":[10954,11185],"content":"With <bpt id=\"p1\">[</bpt>Continuum for Phone<ept id=\"p1\">](http://go.microsoft.com/fwlink/p/?LinkID=699431)</ept>, a new experience for compatible Windows 10 mobile devices, users can connect their phones to a mouse and keyboard to make their phones work like a laptop."},{"content":"Device support","pos":[11187,11201]},{"content":"Phones and phablets","pos":[11206,11225]},{"content":"Tablet","pos":[11230,11236]},{"content":"PCs and laptops","pos":[11241,11256]},{"content":"Surface Hub","pos":[11261,11272]},{"content":"IoT","pos":[11277,11280]},{"content":"Xbox","pos":[11285,11289]},{"content":"HoloLens","pos":[11294,11302]},{"content":"keyboard","pos":[11306,11314]},{"pos":[11365,11534],"content":"Typical usage Users can interact with Universal Windows apps through a hardware keyboard and two software keyboards: the On-Screen Keyboard (OSK) and the touch keyboard."},{"content":"The OSK is a visual, software keyboard that you can use instead of the physical keyboard to type and enter data using touch, mouse, pen/stylus or other pointing device (a touch screen is not required).","pos":[11536,11737]},{"content":"The OSK is provided for systems that don't have a physical keyboard, or for users whose mobility impairments prevent them from using traditional physical input devices.","pos":[11738,11906]},{"content":"The OSK emulates most, if not all, the functionality of a hardware keyboard.","pos":[11907,11983]},{"content":"The touch keyboard is a visual, software keyboard used for text entry with touch input.","pos":[11985,12072]},{"content":"The touch keyboard is not a replacement for the OSK as it is used for text input only (it doesn't emulate the hardware keyboard) and appears only when a text field or other editable text control gets focus.","pos":[12073,12279]},{"content":"The touch keyboard does not support app or system commands.","pos":[12280,12339]},{"pos":[12341,12440],"content":"<bpt id=\"p1\">**</bpt>Note<ept id=\"p1\">**</ept>  The OSK has priority over the touch keyboard, which won't be shown if the OSK is present."},{"content":"In general, a keyboard is:","pos":[12445,12471]},{"content":"Single user.","pos":[12477,12489]},{"content":"Not constrained to device orientation.","pos":[12494,12532]},{"content":"Used for text input, navigation, gameplay, and accessibility.","pos":[12537,12598]},{"content":"Always available, either proactively or reactively.","pos":[12603,12654]},{"pos":[12656,12752],"content":"More info <bpt id=\"p1\">[</bpt>Keyboard design guidelines<ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/hh972345)</ept>"},{"pos":[12831,12836],"content":"Mouse"},{"content":"A mouse is best suited for productivity apps and high-density UI where user interactions require pixel-level precision for targeting and commanding.","pos":[12839,12987]},{"content":"Device support","pos":[12989,13003]},{"content":"Phones and phablets","pos":[13008,13027]},{"content":"Tablet","pos":[13032,13038]},{"content":"PCs and laptops","pos":[13043,13058]},{"content":"Surface Hub","pos":[13063,13074]},{"content":"IoT","pos":[13079,13082]},{"content":"mouse","pos":[13086,13091]},{"content":"Typical usage Mouse input can be modified with the addition of various keyboard keys (Ctrl, Shift, Alt, and so on).","pos":[13139,13254]},{"content":"These keys can be combined with the left mouse button, the right mouse button, the wheel button, and the X buttons for an expanded mouse-optimized command set.","pos":[13255,13414]},{"content":"(Some Microsoft mouse devices have two additional buttons, referred to as X buttons, typically used to navigate back and forward in Web browsers).","pos":[13415,13561]},{"content":"Similar to pen, where mouse and touch input diverge is the ability for touch to emulate direct manipulation of UI elements on the screen through physical gestures performed on those objects (such as swiping, sliding, dragging, rotating, and so on).","pos":[13563,13811]},{"content":"You should provide mouse-specific UI commands, or affordances, to support these interactions.","pos":[13813,13906]},{"content":"For example, use previous and next (or + and -) buttons to let users flip through pages of content, or rotate, resize, and zoom objects.","pos":[13907,14043]},{"pos":[14045,14138],"content":"More info <bpt id=\"p1\">[</bpt>Mouse design guidelines<ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn456351)</ept>"},{"pos":[14223,14230],"content":"Gesture"},{"content":"A gesture is any form of user movement that is recognized as input for controlling or interacting with an application.","pos":[14233,14351]},{"content":"Gestures take many forms, from simply using a hand to target something on the screen, to specific, learned patterns of movement, to long stretches of continuous movement using the entire body.","pos":[14352,14544]},{"content":"Be careful when designing custom gestures, as their meaning can vary depending on locale and culture.","pos":[14545,14646]},{"content":"Device support","pos":[14648,14662]},{"content":"PCs and laptops","pos":[14667,14682]},{"content":"IoT","pos":[14687,14690]},{"content":"Xbox","pos":[14695,14699]},{"content":"HoloLens","pos":[14704,14712]},{"content":"gesture","pos":[14716,14723]},{"pos":[14773,14919],"content":"Typical usage Touchpads typically support a set of touch gestures that provide support similar to touch for direct manipulation of objects and UI."},{"content":"Because of this convergence of interaction experiences supported by touchpads, we recommend also providing mouse-style UI commands or affordances rather than relying solely on support for touch input.","pos":[14921,15121]},{"content":"Provide touchpad-specific UI commands, or affordances, to support these interactions.","pos":[15122,15207]},{"content":"You should provide mouse-specific UI commands, or affordances, to support these interactions.","pos":[15209,15302]},{"content":"For example, use previous and next (or + and -) buttons to let users flip through pages of content, or rotate, resize, and zoom objects.","pos":[15303,15439]},{"pos":[15558,15576],"content":"Gamepad/Controller"},{"content":"The gamepad/controller is a highly specialized device typically dedicated to playing games.","pos":[15579,15670]},{"content":"However, it is also used for to emulate basic keyboard input and provides a UI navigation experience very similar to the keyboard.","pos":[15671,15801]},{"content":"Device support","pos":[15803,15817]},{"content":"PCs and laptops","pos":[15822,15837]},{"content":"IoT","pos":[15842,15845]},{"content":"Xbox","pos":[15850,15854]},{"content":"controller","pos":[15858,15868]},{"pos":[15921,16067],"content":"Typical usage Touchpads typically support a set of touch gestures that provide support similar to touch for direct manipulation of objects and UI."},{"content":"Because of this convergence of interaction experiences supported by touchpads, we recommend also providing mouse-style UI commands or affordances rather than relying solely on support for touch input.","pos":[16069,16269]},{"content":"Provide touchpad-specific UI commands, or affordances, to support these interactions.","pos":[16270,16355]},{"content":"You should provide mouse-specific UI commands, or affordances, to support these interactions.","pos":[16357,16450]},{"content":"For example, use previous and next (or + and -) buttons to let users flip through pages of content, or rotate, resize, and zoom objects.","pos":[16451,16587]},{"pos":[16697,16712],"content":"Multiple inputs"},{"content":"Accommodating as many users and devices as possible and designing your apps to work with as many input types (gesture, speech, touch, touchpad, mouse, and keyboard) as possible maximizes flexibility, usability, and accessibility.","pos":[16715,16944]},{"content":"Device support","pos":[16946,16960]},{"content":"Phones and phablets","pos":[16965,16984]},{"content":"Tablet","pos":[16989,16995]},{"content":"PCs and laptops","pos":[17000,17015]},{"content":"Surface Hub","pos":[17020,17031]},{"content":"IoT","pos":[17036,17039]},{"content":"Xbox","pos":[17044,17048]},{"content":"HoloLens","pos":[17053,17061]},{"content":"multiple inputs","pos":[17065,17080]},{"content":"Typical usage Just as people use a combination of voice and gesture when communicating with each other, multiple types and modes of input can also be useful when interacting with an app.","pos":[17144,17330]},{"content":"However, these combined interactions need to be as intuitive and natural as possible as they can also create a very confusing experience.","pos":[17331,17468]}],"content":"---\nauthor: Karl-Bridge-Microsoft\nDescription: User interactions in the Universal Windows Platform (UWP) are a combination of input and output sources (such as mouse, keyboard, pen, touch, touchpad, speech, Cortana, controller, gesture, gaze, and so on), along with various modes or modifiers that enable extended experiences (including mouse wheel and buttons, pen eraser and barrel buttons, touch keyboard, and background app services).\ntitle: Interaction primer\nms.assetid: 73008F80-FE62-457D-BAEC-412ED6BAB0C8\nlabel: Interaction primer\ntemplate: detail.hbs\n---\n\n# Interaction primer\n\n\n![windows input types](images/input-interactions/icons-inputdevices03.png)\n\nUser interactions in the Universal Windows Platform (UWP) are a combination of input and output sources (such as mouse, keyboard, pen, touch, touchpad, speech, **Cortana**, controller, gesture, gaze, and so on), along with various modes or modifiers that enable extended experiences (including mouse wheel and buttons, pen eraser and barrel buttons, touch keyboard, and background app services). \n\nThe UWP uses a \"smart\" contextual interaction system that, in most cases, eliminates the need to individually handle the unique types of input received by your app. This includes handling touch, touchpad, mouse, and pen input as a generic pointer type to support static gestures such as tap or press-and-hold, manipulation gestures such as slide for panning, or rendering digital ink.\n\nFamiliarize yourself with each input device type and its behaviors, capabilities, and limitations when paired with certain form factors. This can help you decide whether the platform controls and affordances are sufficient for your app, or require you to provide customized interaction experiences.\n\n## <span id=\"Cortana\"></span><span id=\"cortana\"></span><span id=\"CORTANA\"></span>Cortana\n\n\nIn Windows 10, **Cortana** extensibility lets you handle voice commands from a user and launch your application to carry out a single action.\n\nDevice support\n-   Phones and phablets\n-   Tablet\n-   PCs and laptops\n-   Surface Hub\n-   IoT\n-   Xbox\n-   HoloLens\n\n![cortana](images/input-interactions/icons-cortana01.png)\n\nTypical usage\nA voice command is a single utterance, defined in a Voice Command Definition (VCD) file, directed at an installed app through **Cortana**. The app can be launched in the foreground or background, depending on the level and complexity of the interaction. For instance, voice commands that require additional context or user input are best handled in the foreground, while basic commands can be handled in the background.\n\nIntegrating the basic functionality of your app, and providing a central entry point for the user to accomplish most of the tasks without opening your app directly, lets **Cortana** become a liaison between your app and the user. In many cases, this can save the user significant time and effort. For more info, see [Cortana design guidelines](https://msdn.microsoft.com/library/windows/apps/dn974233).\n\nMore info\n[Cortana design guidelines](https://msdn.microsoft.com/library/windows/apps/dn974233)\n \n\n## <span id=\"Speech\"></span><span id=\"speech\"></span><span id=\"SPEECH\"></span>Speech\n\n\nSpeech is an effective and natural way for people to interact with applications. It's an easy and accurate way to communicate with applications, and lets people be productive and stay informed in a variety of situations.\n\nSpeech can complement or, in many cases, be the primary input type, depending on the user's device. For example, devices such as HoloLens and Xbox do not support traditional input types (aside from a software keyboard in specifc scenarios). Instead, they rely on speech input and output (often combined with other non-traditional input types such as gaze and gesture) for most user interactions.\n\nText-to-speech (also known as TTS, or speech synthesis) is used to inform or direct the user.\n\nDevice support\n-   Phones and phablets\n-   Tablet\n-   PCs and laptops\n-   Surface Hub\n-   IoT\n-   Xbox\n-   HoloLens\n\n![speech](images/input-interactions/icons-speech01.png)\n\nTypical usage\n\nThere are three modes of Speech interaction:\n\n<span id=\"Natural_language\"></span><span id=\"natural_language\"></span><span id=\"NATURAL_LANGUAGE\"></span>Natural language  \nNatural language is how we verbally interact with people on a regular basis. Our speech varies from person to person and situation to situation, and is generally understood. When it's not, we often use different words and word order to get the same idea across.\n\nNatural language interactions with an app are similar: we speak to the app through our device as if it were a person and expect it to understand and react accordingly.\n\nNatural language is the most advanced mode of speech interaction, and can be implemented and exposed through **Cortana**.\n\n<span id=\"Command_and_control\"></span><span id=\"command_and_control\"></span><span id=\"COMMAND_AND_CONTROL\"></span>Command and control  \nCommand and control is the use of verbal commands to activate controls and functionality such as clicking a button or selecting a menu item.\n\nAs command and control is critical to a successful user experience, a single input type is generally not recommended. Speech is typically one of several input options for a user based on their preferences or hardware capabilities.\n\n<span id=\"Dictation\"></span><span id=\"dictation\"></span><span id=\"DICTATION\"></span>Dictation  \nThe most basic speech input method. Each utterance is converted to text.\n\nDictation is typically used when an app doesn’t need to understand meaning or intent.\n\nMore info\n[Speech design guidelines](https://msdn.microsoft.com/library/windows/apps/dn596121)\n \n\n## <span id=\"Pen\"></span><span id=\"pen\"></span><span id=\"PEN\"></span>Pen\n\n\nA pen (or stylus) can serve as a pixel precise pointing device, like a mouse, and is the optimal device for digital ink input.\n\n**Note**  There are two types of pen devices: active and passive.\n-   Passive pens do not contain electronics, and effectively emulate touch input from a finger. They require a basic device display that recognizes input based on contact pressure. Because users often rest their hand as they write on the input surface, input data can become polluted due to unsuccessful palm rejection.\n-   Active pens contain electronics and can work with complex device displays to provide much more extensive input data (including hover, or proximity data) to the system and your app. Palm rejection is much more robust.\n\n \n\nWhen we refer to pen devices here, we are referring to active pens that provide rich input data and are used primarily for precise ink and pointing interactions.\n\nDevice support\n-   Phones and phablets\n-   Tablet\n-   PCs and laptops\n-   Surface Hub\n-   IoT\n\n![pen](images/input-interactions/icons-pen01.png)\n\nTypical usage\nThe Windows ink platform, together with a pen, provides a natural way to create handwritten notes, drawings, and annotations. The platform supports capturing ink data from digitizer input, generating ink data, rendering that data as ink strokes on the output device, managing the ink data, and performing handwriting recognition. In addition to capturing the spatial movements of the pen as the user writes or draws, your app can also collect info such as pressure, shape, color, and opacity, to offer user experiences that closely resemble drawing on paper with a pen, pencil, or brush.\n\nWhere pen and touch input diverge is the ability for touch to emulate direct manipulation of UI elements on the screen through physical gestures performed on those objects (such as swiping, sliding, dragging, rotating, and so on).\n\nYou should provide pen-specific UI commands, or affordances, to support these interactions. For example, use previous and next (or + and -) buttons to let users flip through pages of content, or rotate, resize, and zoom objects.\n\nMore info\n[Pen design guidelines](https://msdn.microsoft.com/library/windows/apps/dn456352)\n \n\n## <span id=\"Touch\"></span><span id=\"touch\"></span><span id=\"TOUCH\"></span>Touch\n\n\nWith touch, physical gestures from one or more fingers can be used to either emulate the direct manipulation of UI elements (such as panning, rotating, resizing, or moving), as an alternative input method (similar to mouse or pen), or as a complementary input method (to modify aspects of other input, such as smudging an ink stroke drawn with a pen). Tactile experiences such as this can provide more natural, real-world sensations for users as they interact with elements on a screen.\n\nDevice support\n-   Phones and phablets\n-   Tablet\n-   PCs and laptops\n-   Surface Hub\n-   IoT\n\n![touch](images/input-interactions/icons-touch01.png)\n\nTypical usage\nSupport for touch input can vary significantly, depending on the device.\n\nSome devices don't support touch at all, some devices support a single touch contact, while others support multi-touch (two or more contacts).\n\nMost devices that support multi-touch input, typically recognize ten unique, concurrent contacts.\n\nSurface Hub devices recognize 100 unique, concurrent touch contacts.\n\nIn general, touch is:\n\n-   Single user, unless being used with a Microsoft Team device like Surface Hub, where collaboration is emphasized.\n-   Not constrained to device orientation.\n-   Used for all interactions, including text input (touch keyboard) and inking (app-configured).\n\nMore info\n[Touch design guidelines](https://msdn.microsoft.com/library/windows/apps/hh465370)\n \n\n## <span id=\"Touchpad\"></span><span id=\"touchpad\"></span><span id=\"TOUCHPAD\"></span>Touchpad\n\n\nA touchpad combines both indirect multi-touch input with the precision input of a pointing device, such as a mouse. This combination makes the touchpad suited to both a touch-optimized UI and the smaller targets of productivity apps.\n\nDevice support\n-   PCs and laptops\n-   IoT\n\n![touchpad](images/input-interactions/icons-touchpad01.png)\n\nTypical usage\nTouchpads typically support a set of touch gestures that provide support similar to touch for direct manipulation of objects and UI.\n\nBecause of this convergence of interaction experiences supported by touchpads, we recommend also providing mouse-style UI commands or affordances rather than relying solely on support for touch input. Provide touchpad-specific UI commands, or affordances, to support these interactions.\n\nYou should provide mouse-specific UI commands, or affordances, to support these interactions. For example, use previous and next (or + and -) buttons to let users flip through pages of content, or rotate, resize, and zoom objects.\n\nMore info\n[Touchpad design guidelines](https://msdn.microsoft.com/library/windows/apps/dn456353)\n \n\n## <span id=\"Keyboard\"></span><span id=\"keyboard\"></span><span id=\"KEYBOARD\"></span>Keyboard\n\n\nA keyboard is the primary input device for text, and is often indispensable to people with certain disabilities or users who consider it a faster and more efficient way to interact with an app.\n\nWith [Continuum for Phone](http://go.microsoft.com/fwlink/p/?LinkID=699431), a new experience for compatible Windows 10 mobile devices, users can connect their phones to a mouse and keyboard to make their phones work like a laptop.\n\nDevice support\n-   Phones and phablets\n-   Tablet\n-   PCs and laptops\n-   Surface Hub\n-   IoT\n-   Xbox\n-   HoloLens\n\n![keyboard](images/input-interactions/icons-keyboard01.png)\n\nTypical usage\nUsers can interact with Universal Windows apps through a hardware keyboard and two software keyboards: the On-Screen Keyboard (OSK) and the touch keyboard.\n\nThe OSK is a visual, software keyboard that you can use instead of the physical keyboard to type and enter data using touch, mouse, pen/stylus or other pointing device (a touch screen is not required). The OSK is provided for systems that don't have a physical keyboard, or for users whose mobility impairments prevent them from using traditional physical input devices. The OSK emulates most, if not all, the functionality of a hardware keyboard.\n\nThe touch keyboard is a visual, software keyboard used for text entry with touch input. The touch keyboard is not a replacement for the OSK as it is used for text input only (it doesn't emulate the hardware keyboard) and appears only when a text field or other editable text control gets focus. The touch keyboard does not support app or system commands.\n\n**Note**  The OSK has priority over the touch keyboard, which won't be shown if the OSK is present.\n\n \n\nIn general, a keyboard is:\n\n-   Single user.\n-   Not constrained to device orientation.\n-   Used for text input, navigation, gameplay, and accessibility.\n-   Always available, either proactively or reactively.\n\nMore info\n[Keyboard design guidelines](https://msdn.microsoft.com/library/windows/apps/hh972345)\n \n\n## <span id=\"Mouse\"></span><span id=\"mouse\"></span><span id=\"MOUSE\"></span>Mouse\n\n\nA mouse is best suited for productivity apps and high-density UI where user interactions require pixel-level precision for targeting and commanding.\n\nDevice support\n-   Phones and phablets\n-   Tablet\n-   PCs and laptops\n-   Surface Hub\n-   IoT\n\n![mouse](images/input-interactions/icons-mouse01.png)\n\nTypical usage\nMouse input can be modified with the addition of various keyboard keys (Ctrl, Shift, Alt, and so on). These keys can be combined with the left mouse button, the right mouse button, the wheel button, and the X buttons for an expanded mouse-optimized command set. (Some Microsoft mouse devices have two additional buttons, referred to as X buttons, typically used to navigate back and forward in Web browsers).\n\nSimilar to pen, where mouse and touch input diverge is the ability for touch to emulate direct manipulation of UI elements on the screen through physical gestures performed on those objects (such as swiping, sliding, dragging, rotating, and so on).\n\nYou should provide mouse-specific UI commands, or affordances, to support these interactions. For example, use previous and next (or + and -) buttons to let users flip through pages of content, or rotate, resize, and zoom objects.\n\nMore info\n[Mouse design guidelines](https://msdn.microsoft.com/library/windows/apps/dn456351)\n \n\n## <span id=\"Gesture\"></span><span id=\"gesture\"></span><span id=\"GESTURE\"></span>Gesture\n\n\nA gesture is any form of user movement that is recognized as input for controlling or interacting with an application. Gestures take many forms, from simply using a hand to target something on the screen, to specific, learned patterns of movement, to long stretches of continuous movement using the entire body. Be careful when designing custom gestures, as their meaning can vary depending on locale and culture.\n\nDevice support\n-   PCs and laptops\n-   IoT\n-   Xbox\n-   HoloLens\n\n![gesture](images/input-interactions/icons-gesture01.png)\n\nTypical usage\nTouchpads typically support a set of touch gestures that provide support similar to touch for direct manipulation of objects and UI.\n\nBecause of this convergence of interaction experiences supported by touchpads, we recommend also providing mouse-style UI commands or affordances rather than relying solely on support for touch input. Provide touchpad-specific UI commands, or affordances, to support these interactions.\n\nYou should provide mouse-specific UI commands, or affordances, to support these interactions. For example, use previous and next (or + and -) buttons to let users flip through pages of content, or rotate, resize, and zoom objects.\n\n \n\n## <span id=\"Gamepad_Controller\"></span><span id=\"gamepad_controller\"></span><span id=\"GAMEPAD_CONTROLLER\"></span>Gamepad/Controller\n\n\nThe gamepad/controller is a highly specialized device typically dedicated to playing games. However, it is also used for to emulate basic keyboard input and provides a UI navigation experience very similar to the keyboard.\n\nDevice support\n-   PCs and laptops\n-   IoT\n-   Xbox\n\n![controller](images/input-interactions/icons-controller01.png)\n\nTypical usage\nTouchpads typically support a set of touch gestures that provide support similar to touch for direct manipulation of objects and UI.\n\nBecause of this convergence of interaction experiences supported by touchpads, we recommend also providing mouse-style UI commands or affordances rather than relying solely on support for touch input. Provide touchpad-specific UI commands, or affordances, to support these interactions.\n\nYou should provide mouse-specific UI commands, or affordances, to support these interactions. For example, use previous and next (or + and -) buttons to let users flip through pages of content, or rotate, resize, and zoom objects.\n\n \n\n## <span id=\"Multiple_inputs\"></span><span id=\"multiple_inputs\"></span><span id=\"MULTIPLE_INPUTS\"></span>Multiple inputs\n\n\nAccommodating as many users and devices as possible and designing your apps to work with as many input types (gesture, speech, touch, touchpad, mouse, and keyboard) as possible maximizes flexibility, usability, and accessibility.\n\nDevice support\n-   Phones and phablets\n-   Tablet\n-   PCs and laptops\n-   Surface Hub\n-   IoT\n-   Xbox\n-   HoloLens\n\n![multiple inputs](images/input-interactions/icons-inputdevices03-vertical.png)\n\nTypical usage\nJust as people use a combination of voice and gesture when communicating with each other, multiple types and modes of input can also be useful when interacting with an app. However, these combined interactions need to be as intuitive and natural as possible as they can also create a very confusing experience.\n\n\n\n\n\n \n\n \n\n\n\n\n"}