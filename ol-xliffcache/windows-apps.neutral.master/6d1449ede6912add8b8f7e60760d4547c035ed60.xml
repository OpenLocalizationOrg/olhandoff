{"nodes":[{"content":"Use speech recognition to provide input, specify an action or command, and accomplish tasks.","pos":[47,139]},{"content":"Speech recognition","pos":[147,165]},{"content":"Speech recognition","pos":[269,287]},{"content":"Use speech recognition to provide input, specify an action or command, and accomplish tasks.","pos":[290,382]},{"content":"Important APIs","pos":[386,400]},{"content":"Windows.Media.SpeechRecognition","pos":[411,442]},{"content":"Speech recognition is made up of a speech runtime, recognition APIs for programming the runtime, ready-to-use grammars for dictation and web search, and a default system UI that helps users discover and use speech recognition features.","pos":[507,742]},{"pos":[868,889],"content":"Set up the audio feed"},{"content":"Ensure that your device has a microphone or the equivalent.","pos":[892,951]},{"content":"Set the <bpt id=\"p1\">**</bpt>Microphone<ept id=\"p1\">**</ept> device capability (<bpt id=\"p2\">[</bpt><bpt id=\"p3\">**</bpt>DeviceCapability<ept id=\"p3\">**</ept><ept id=\"p2\">](https://msdn.microsoft.com/library/windows/apps/br211430)</ept>) in the <bpt id=\"p4\">[</bpt>App package manifest<ept id=\"p4\">](https://msdn.microsoft.com/library/windows/apps/br211474)</ept> (<bpt id=\"p5\">**</bpt>package.appxmanifest<ept id=\"p5\">**</ept> file) to get access to the microphone’s audio feed.","pos":[953,1242]},{"content":"This allows the app to record audio from connected microphones.","pos":[1243,1306]},{"pos":[1308,1400],"content":"See <bpt id=\"p1\">[</bpt>App capability declarations<ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/mt270968)</ept>."},{"pos":[1528,1550],"content":"Recognize speech input"},{"content":"A <bpt id=\"p1\">*</bpt>constraint<ept id=\"p1\">*</ept> defines the words and phrases (vocabulary) that an app recognizes in speech input.","pos":[1553,1650]},{"content":"Constraints are at the core of speech recognition and give your app great over the accuracy of speech recognition.","pos":[1651,1765]},{"content":"You can use various types of constraints when performing speech recognition:","pos":[1767,1843]},{"pos":[1849,1972],"content":"<bpt id=\"p1\">**</bpt>Predefined grammars<ept id=\"p1\">**</ept> (<bpt id=\"p2\">[</bpt><bpt id=\"p3\">**</bpt>SpeechRecognitionTopicConstraint<ept id=\"p3\">**</ept><ept id=\"p2\">](https://msdn.microsoft.com/library/windows/apps/dn631446)</ept>)."},{"content":"Predefined dictation and web-search grammars provide speech recognition for your app without requiring you to author a grammar.","pos":[1978,2105]},{"content":"When using these grammars, speech recognition is performed by a remote web service and the results are returned to the device.","pos":[2106,2232]},{"content":"The default free-text dictation grammar can recognize most words and phrases that a user can say in a particular language, and is optimized to recognize short phrases.","pos":[2238,2405]},{"content":"The predefined dictation grammar is used if you don't specify any constraints for your <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>SpeechRecognizer<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn653226)</ept> object.","pos":[2406,2581]},{"content":"Free-text dictation is useful when you don't want to limit the kinds of things a user can say.","pos":[2582,2676]},{"content":"Typical uses include creating notes or dictating the content for a message.","pos":[2677,2752]},{"content":"The web-search grammar, like a dictation grammar, contains a large number of words and phrases that a user might say.","pos":[2758,2875]},{"content":"However, it is optimized to recognize terms that people typically use when searching the web.","pos":[2876,2969]},{"pos":[2975,3183],"content":"<bpt id=\"p1\">**</bpt>Note<ept id=\"p1\">**</ept>  Because predefined dictation and web-search grammars can be large, and because they are online (not on the device), performance might not be as fast as with a custom grammar installed on the device."},{"content":"These predefined grammars can be used to recognize up to 10 seconds of speech input and require no authoring effort on your part.","pos":[3196,3325]},{"content":"However, they do require a connection to a network.","pos":[3326,3377]},{"pos":[3383,3592],"content":"To use web-service constraints, speech input and dictation support must be enabled in <bpt id=\"p1\">**</bpt>Settings<ept id=\"p1\">**</ept> by turning on the \"Get to know me\" option in the Settings -<ph id=\"ph1\">&amp;gt;</ph> Privacy -<ph id=\"ph2\">&amp;gt;</ph> Speech, inking, and typing page."},{"content":"Here, we show how to test whether speech input is enabled and open the Settings -<ph id=\"ph1\">&amp;gt;</ph> Privacy -<ph id=\"ph2\">&amp;gt;</ph> Speech, inking, and typing page, if not.","pos":[3598,3738]},{"content":"First, we initialize a global variable (HResultPrivacyStatementDeclined) to the HResult value of 0x80045509.","pos":[3744,3852]},{"content":"See <bpt id=\"p1\">[</bpt>Exception handling for in C<ph id=\"ph1\">\\#</ph> or Visual Basic<ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn532194)</ept>.","pos":[3853,3963]},{"pos":[5523,5655],"content":"<bpt id=\"p1\">**</bpt>Programmatic list constraints<ept id=\"p1\">**</ept> (<bpt id=\"p2\">[</bpt><bpt id=\"p3\">**</bpt>SpeechRecognitionListConstraint<ept id=\"p3\">**</ept><ept id=\"p2\">](https://msdn.microsoft.com/library/windows/apps/dn631421)</ept>)."},{"content":"Programmatic list constraints provide a lightweight approach to creating simple grammars using a list of words or phrases.","pos":[5661,5783]},{"content":"A list constraint works well for recognizing short, distinct phrases.","pos":[5784,5853]},{"content":"Explicitly specifying all words in a grammar also improves recognition accuracy, as the speech recognition engine must only process speech to confirm a match.","pos":[5854,6012]},{"content":"The list can also be programmatically updated.","pos":[6013,6059]},{"content":"A list constraint consists of an array of strings that represents speech input that your app will accept for a recognition operation.","pos":[6065,6198]},{"content":"You can create a list constraint in your app by creating a speech-recognition list-constraint object and passing an array of strings.","pos":[6199,6332]},{"content":"Then, add that object to the constraints collection of the recognizer.","pos":[6333,6403]},{"content":"Recognition is successful when the speech recognizer recognizes any one of the strings in the array.","pos":[6404,6504]},{"pos":[6510,6633],"content":"<bpt id=\"p1\">**</bpt>SRGS grammars<ept id=\"p1\">**</ept> (<bpt id=\"p2\">[</bpt><bpt id=\"p3\">**</bpt>SpeechRecognitionGrammarFileConstraint<ept id=\"p3\">**</ept><ept id=\"p2\">](https://msdn.microsoft.com/library/windows/apps/dn631412)</ept>)."},{"content":"An Speech Recognition Grammar Specification (SRGS) grammar is a static document that, unlike a programmatic list constraint, uses the XML format defined by the <bpt id=\"p1\">[</bpt>SRGS Version 1.0<ept id=\"p1\">](http://go.microsoft.com/fwlink/p/?LinkID=262302)</ept>.","pos":[6639,6867]},{"content":"An SRGS grammar provides the greatest control over the speech recognition experience by letting you capture multiple semantic meanings in a single recognition.","pos":[6868,7027]},{"pos":[7033,7178],"content":"<bpt id=\"p1\">**</bpt>Voice command constraints<ept id=\"p1\">**</ept> (<bpt id=\"p2\">[</bpt><bpt id=\"p3\">**</bpt>SpeechRecognitionVoiceCommandDefinitionConstraint<ept id=\"p3\">**</ept><ept id=\"p2\">](https://msdn.microsoft.com/library/windows/apps/dn653220)</ept>)"},{"content":"Use a Voice Command Definition (VCD) XML file to define the commands that the user can say to initiate actions when activating your app.","pos":[7184,7320]},{"content":"For more detail, see <bpt id=\"p1\">[</bpt>Launch a foreground app with voice commands in Cortana<ept id=\"p1\">](launch-a-foreground-app-with-voice-commands-in-cortana.md)</ept>.","pos":[7321,7458]},{"content":"<bpt id=\"p1\">**</bpt>Note<ept id=\"p1\">**</ept>  Which type of constraint type you use depends on the complexity of the recognition experience you want to create.","pos":[7460,7583]},{"content":"Any could be the best choice for a specific recognition task, and you might find uses for all types of constraints in your app.","pos":[7584,7711]},{"content":"To get started with constraints, see <bpt id=\"p1\">[</bpt>Define custom recognition constraints<ept id=\"p1\">](define-custom-recognition-constraints.md)</ept>.","pos":[7712,7831]},{"content":"The predefined Universal Windows app dictation grammar recognizes most words and short phrases in a language.","pos":[7836,7945]},{"content":"It is activated by default when a speech recognizer object is instantiated without custom constraints.","pos":[7946,8048]},{"content":"In this example, we show how to:","pos":[8050,8082]},{"content":"Create a speech recognizer.","pos":[8088,8115]},{"content":"Compile the default Universal Windows app constraints (no grammars have been added to the speech recognizer's grammar set).","pos":[8120,8243]},{"content":"Start listening for speech by using the basic recognition UI and TTS feedback provided by the <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>RecognizeWithUIAsync<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn653245)</ept> method.","pos":[8248,8434]},{"content":"Use the <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>RecognizeAsync<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn653244)</ept> method if the default UI is not required.","pos":[8435,8563]},{"pos":[9391,9419],"content":"Customize the recognition UI"},{"pos":[9422,9627],"content":"When your app attempts speech recognition by calling <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>SpeechRecognizer.RecognizeWithUIAsync<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn653245)</ept>, several screens are shown in the following order."},{"content":"If you're using a constraint based on a predefined grammar (dictation or web search):","pos":[9629,9714]},{"pos":[9720,9745],"content":"The <bpt id=\"p1\">**</bpt>Listening<ept id=\"p1\">**</ept> screen."},{"pos":[9750,9774],"content":"The <bpt id=\"p1\">**</bpt>Thinking<ept id=\"p1\">**</ept> screen."},{"pos":[9779,9828],"content":"The <bpt id=\"p1\">**</bpt>Heard you say<ept id=\"p1\">**</ept> screen or the error screen."},{"content":"If you're using a constraint based on a list of words or phrases, or a constraint based on a SRGS grammar file:","pos":[9830,9941]},{"pos":[9947,9972],"content":"The <bpt id=\"p1\">**</bpt>Listening<ept id=\"p1\">**</ept> screen."},{"pos":[9977,10082],"content":"The <bpt id=\"p1\">**</bpt>Did you say<ept id=\"p1\">**</ept> screen, if what the user said could be interpreted as more than one potential result."},{"pos":[10087,10136],"content":"The <bpt id=\"p1\">**</bpt>Heard you say<ept id=\"p1\">**</ept> screen or the error screen."},{"content":"The following image shows an example of the flow between screens for a speech recognizer that uses a constraint based on a SRGS grammar file.","pos":[10138,10279]},{"content":"In this example, speech recognition was successful.","pos":[10280,10331]},{"content":"initial recognition screen for a constraint based on a sgrs grammar file","pos":[10335,10407]},{"content":"intermediate recognition screen for a constraint based on a sgrs grammar file","pos":[10449,10526]},{"content":"final recognition screen for a constraint based on a sgrs grammar file","pos":[10573,10643]},{"content":"The <bpt id=\"p1\">**</bpt>Listening<ept id=\"p1\">**</ept> screen can provide examples of words or phrases that the app can recognize.","pos":[10684,10777]},{"content":"Here, we show how to use the properties of the <bpt id=\"p1\">[</bpt><bpt id=\"p2\">**</bpt>SpeechRecognizerUIOptions<ept id=\"p2\">**</ept><ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn653234)</ept> class (obtained by calling the <bpt id=\"p3\">[</bpt><bpt id=\"p4\">**</bpt>SpeechRecognizer.UIOptions<ept id=\"p4\">**</ept><ept id=\"p3\">](https://msdn.microsoft.com/library/windows/apps/dn653254)</ept> property) to customize content on the <bpt id=\"p5\">**</bpt>Listening<ept id=\"p5\">**</ept> screen.","pos":[10778,11096]},{"pos":[12441,12457],"content":"Related articles"},{"content":"Developers","pos":[12462,12472]},{"pos":[12477,12536],"content":"<bpt id=\"p1\">[</bpt>Speech interactions<ept id=\"p1\">](speech-interactions.md)</ept><ph id=\"ph1\">\n</ph><bpt id=\"p2\">**</bpt>Designers<ept id=\"p2\">**</ept>"},{"pos":[12539,12635],"content":"<bpt id=\"p1\">[</bpt>Speech design guidelines<ept id=\"p1\">](https://msdn.microsoft.com/library/windows/apps/dn596121)</ept><ph id=\"ph1\">\n</ph><bpt id=\"p2\">**</bpt>Samples<ept id=\"p2\">**</ept>"},{"content":"Speech recognition and speech synthesis sample","pos":[12639,12685]}],"content":"---\nauthor: Karl-Bridge-Microsoft\nDescription: Use speech recognition to provide input, specify an action or command, and accomplish tasks.\ntitle: Speech recognition\nms.assetid: 553C0FB7-35BC-4894-9EF1-906139E17552\nlabel: Speech recognition\ntemplate: detail.hbs\n---\n\n# Speech recognition\n\n\nUse speech recognition to provide input, specify an action or command, and accomplish tasks.\n\n**Important APIs**\n\n-   [**Windows.Media.SpeechRecognition**](https://msdn.microsoft.com/library/windows/apps/dn653262)\n\n\n\nSpeech recognition is made up of a speech runtime, recognition APIs for programming the runtime, ready-to-use grammars for dictation and web search, and a default system UI that helps users discover and use speech recognition features.\n\n\n## <span id=\"Set_up_the_audio_feed\"></span><span id=\"set_up_the_audio_feed\"></span><span id=\"SET_UP_THE_AUDIO_FEED\"></span>Set up the audio feed\n\n\nEnsure that your device has a microphone or the equivalent.\n\nSet the **Microphone** device capability ([**DeviceCapability**](https://msdn.microsoft.com/library/windows/apps/br211430)) in the [App package manifest](https://msdn.microsoft.com/library/windows/apps/br211474) (**package.appxmanifest** file) to get access to the microphone’s audio feed. This allows the app to record audio from connected microphones.\n\nSee [App capability declarations](https://msdn.microsoft.com/library/windows/apps/mt270968).\n\n## <span id=\"Recognize_speech_input\"></span><span id=\"recognize_speech_input\"></span><span id=\"RECOGNIZE_SPEECH_INPUT\"></span>Recognize speech input\n\n\nA *constraint* defines the words and phrases (vocabulary) that an app recognizes in speech input. Constraints are at the core of speech recognition and give your app great over the accuracy of speech recognition.\n\nYou can use various types of constraints when performing speech recognition:\n\n1.  **Predefined grammars** ([**SpeechRecognitionTopicConstraint**](https://msdn.microsoft.com/library/windows/apps/dn631446)).\n\n    Predefined dictation and web-search grammars provide speech recognition for your app without requiring you to author a grammar. When using these grammars, speech recognition is performed by a remote web service and the results are returned to the device.\n\n    The default free-text dictation grammar can recognize most words and phrases that a user can say in a particular language, and is optimized to recognize short phrases. The predefined dictation grammar is used if you don't specify any constraints for your [**SpeechRecognizer**](https://msdn.microsoft.com/library/windows/apps/dn653226) object. Free-text dictation is useful when you don't want to limit the kinds of things a user can say. Typical uses include creating notes or dictating the content for a message.\n\n    The web-search grammar, like a dictation grammar, contains a large number of words and phrases that a user might say. However, it is optimized to recognize terms that people typically use when searching the web.\n\n    **Note**  Because predefined dictation and web-search grammars can be large, and because they are online (not on the device), performance might not be as fast as with a custom grammar installed on the device.\n\n     \n\n    These predefined grammars can be used to recognize up to 10 seconds of speech input and require no authoring effort on your part. However, they do require a connection to a network.\n\n    To use web-service constraints, speech input and dictation support must be enabled in **Settings** by turning on the \"Get to know me\" option in the Settings -&gt; Privacy -&gt; Speech, inking, and typing page.\n\n    Here, we show how to test whether speech input is enabled and open the Settings -&gt; Privacy -&gt; Speech, inking, and typing page, if not.\n\n    First, we initialize a global variable (HResultPrivacyStatementDeclined) to the HResult value of 0x80045509. See [Exception handling for in C\\# or Visual Basic](https://msdn.microsoft.com/library/windows/apps/dn532194).\n\n```    CSharp\nprivate static uint HResultPrivacyStatementDeclined = 0x80045509;</code></pre></td>\n    </tr>\n    </tbody>\n    </table>\n```\n\n    We then catch any standard exceptions during recogntion and test if the [**HResult**](https://msdn.microsoft.com/library/windows/apps/br206579) value is equal to the value of the HResultPrivacyStatementDeclined variable. If so, we display a warning and call `await Windows.System.Launcher.LaunchUriAsync(new Uri(\"ms-settings:privacy-accounts\"));` to open the Settings page.\n\n    <span codelanguage=\"CSharp\"></span>\n```    CSharp\n    <colgroup>\n    <col width=\"100%\" />\n    </colgroup>\n    <thead>\n    <tr class=\"header\">\n    <th align=\"left\">C#</th>\n    </tr>\n    </thead>\n    <tbody>\n    <tr class=\"odd\">\ncatch (Exception exception)\n    {\n      // Handle the speech privacy policy error.\n      if ((uint)exception.HResult == HResultPrivacyStatementDeclined)\n      {\n        resultTextBlock.Visibility = Visibility.Visible;\n        resultTextBlock.Text = \"The privacy statement was declined. \n          Go to Settings -> Privacy -> Speech, inking and typing, and ensure you \n          have viewed the privacy policy, and &#39;Get To Know You&#39; is enabled.\";\n        // Open the privacy/speech, inking, and typing settings page.\n        await Windows.System.Launcher.LaunchUriAsync(new Uri(\"ms-settings:privacy-accounts\")); \n      }\n      else\n      {\n        var messageDialog = new Windows.UI.Popups.MessageDialog(exception.Message, \"Exception\");\n        await messageDialog.ShowAsync();\n      }\n    }\n```\n\n2.  **Programmatic list constraints** ([**SpeechRecognitionListConstraint**](https://msdn.microsoft.com/library/windows/apps/dn631421)).\n\n    Programmatic list constraints provide a lightweight approach to creating simple grammars using a list of words or phrases. A list constraint works well for recognizing short, distinct phrases. Explicitly specifying all words in a grammar also improves recognition accuracy, as the speech recognition engine must only process speech to confirm a match. The list can also be programmatically updated.\n\n    A list constraint consists of an array of strings that represents speech input that your app will accept for a recognition operation. You can create a list constraint in your app by creating a speech-recognition list-constraint object and passing an array of strings. Then, add that object to the constraints collection of the recognizer. Recognition is successful when the speech recognizer recognizes any one of the strings in the array.\n\n3.  **SRGS grammars** ([**SpeechRecognitionGrammarFileConstraint**](https://msdn.microsoft.com/library/windows/apps/dn631412)).\n\n    An Speech Recognition Grammar Specification (SRGS) grammar is a static document that, unlike a programmatic list constraint, uses the XML format defined by the [SRGS Version 1.0](http://go.microsoft.com/fwlink/p/?LinkID=262302). An SRGS grammar provides the greatest control over the speech recognition experience by letting you capture multiple semantic meanings in a single recognition.\n\n4.  **Voice command constraints** ([**SpeechRecognitionVoiceCommandDefinitionConstraint**](https://msdn.microsoft.com/library/windows/apps/dn653220))\n\n    Use a Voice Command Definition (VCD) XML file to define the commands that the user can say to initiate actions when activating your app. For more detail, see [Launch a foreground app with voice commands in Cortana](launch-a-foreground-app-with-voice-commands-in-cortana.md).\n\n**Note**  Which type of constraint type you use depends on the complexity of the recognition experience you want to create. Any could be the best choice for a specific recognition task, and you might find uses for all types of constraints in your app.\nTo get started with constraints, see [Define custom recognition constraints](define-custom-recognition-constraints.md).\n\n \n\nThe predefined Universal Windows app dictation grammar recognizes most words and short phrases in a language. It is activated by default when a speech recognizer object is instantiated without custom constraints.\n\nIn this example, we show how to:\n\n-   Create a speech recognizer.\n-   Compile the default Universal Windows app constraints (no grammars have been added to the speech recognizer's grammar set).\n-   Start listening for speech by using the basic recognition UI and TTS feedback provided by the [**RecognizeWithUIAsync**](https://msdn.microsoft.com/library/windows/apps/dn653245) method. Use the [**RecognizeAsync**](https://msdn.microsoft.com/library/windows/apps/dn653244) method if the default UI is not required.\n\n```CSharp\nprivate async void StartRecognizing_Click(object sender, RoutedEventArgs e)\n{\n    // Create an instance of SpeechRecognizer.\n    var speechRecognizer = new Windows.Media.SpeechRecognition.SpeechRecognizer();\n\n    // Compile the dictation grammar by default.\n    await speechRecognizer.CompileConstraintsAsync();\n\n    // Start recognition.\n    Windows.Media.SpeechRecognition.SpeechRecognitionResult speechRecognitionResult = await speechRecognizer.RecognizeWithUIAsync();\n\n    // Do something with the recognition result.\n    var messageDialog = new Windows.UI.Popups.MessageDialog(speechRecognitionResult.Text, \"Text spoken\");\n    await messageDialog.ShowAsync();\n}\n```\n\n## <span id=\"Customize_the_recognition_UI\"></span><span id=\"customize_the_recognition_ui\"></span><span id=\"CUSTOMIZE_THE_RECOGNITION_UI\"></span>Customize the recognition UI\n\n\nWhen your app attempts speech recognition by calling [**SpeechRecognizer.RecognizeWithUIAsync**](https://msdn.microsoft.com/library/windows/apps/dn653245), several screens are shown in the following order.\n\nIf you're using a constraint based on a predefined grammar (dictation or web search):\n\n-   The **Listening** screen.\n-   The **Thinking** screen.\n-   The **Heard you say** screen or the error screen.\n\nIf you're using a constraint based on a list of words or phrases, or a constraint based on a SRGS grammar file:\n\n-   The **Listening** screen.\n-   The **Did you say** screen, if what the user said could be interpreted as more than one potential result.\n-   The **Heard you say** screen or the error screen.\n\nThe following image shows an example of the flow between screens for a speech recognizer that uses a constraint based on a SRGS grammar file. In this example, speech recognition was successful.\n\n![initial recognition screen for a constraint based on a sgrs grammar file](images/speech-listening-initial.png)\n\n![intermediate recognition screen for a constraint based on a sgrs grammar file](images/speech-listening-intermediate.png)\n\n![final recognition screen for a constraint based on a sgrs grammar file](images/speech-listening-complete.png)\n\nThe **Listening** screen can provide examples of words or phrases that the app can recognize. Here, we show how to use the properties of the [**SpeechRecognizerUIOptions**](https://msdn.microsoft.com/library/windows/apps/dn653234) class (obtained by calling the [**SpeechRecognizer.UIOptions**](https://msdn.microsoft.com/library/windows/apps/dn653254) property) to customize content on the **Listening** screen.\n\n```CSharp\nprivate async void WeatherSearch_Click(object sender, RoutedEventArgs e)\n{\n    // Create an instance of SpeechRecognizer.\n    var speechRecognizer = new Windows.Media.SpeechRecognition.SpeechRecognizer();\n\n    // Listen for audio input issues.\n    speechRecognizer.RecognitionQualityDegrading += speechRecognizer_RecognitionQualityDegrading;\n\n    // Add a web search grammar to the recognizer.\n    var webSearchGrammar = new Windows.Media.SpeechRecognition.SpeechRecognitionTopicConstraint(Windows.Media.SpeechRecognition.SpeechRecognitionScenario.WebSearch, \"webSearch\");\n\n\n    speechRecognizer.UIOptions.AudiblePrompt = \"Say what you want to search for...\";\n    speechRecognizer.UIOptions.ExampleText = @\"Ex. &#39;weather for London&#39;\";\n    speechRecognizer.Constraints.Add(webSearchGrammar);\n\n    // Compile the constraint.\n    await speechRecognizer.CompileConstraintsAsync();\n\n    // Start recognition.\n    Windows.Media.SpeechRecognition.SpeechRecognitionResult speechRecognitionResult = await speechRecognizer.RecognizeWithUIAsync();\n    //await speechRecognizer.RecognizeWithUIAsync();\n\n    // Do something with the recognition result.\n    var messageDialog = new Windows.UI.Popups.MessageDialog(speechRecognitionResult.Text, \"Text spoken\");\n    await messageDialog.ShowAsync();\n}\n```\n\n## <span id=\"related_topics\"></span>Related articles\n\n\n**Developers**\n* [Speech interactions](speech-interactions.md)\n**Designers**\n* [Speech design guidelines](https://msdn.microsoft.com/library/windows/apps/dn596121)\n**Samples**\n* [Speech recognition and speech synthesis sample](http://go.microsoft.com/fwlink/p/?LinkID=619897)\n \n\n \n\n\n\n\n"}