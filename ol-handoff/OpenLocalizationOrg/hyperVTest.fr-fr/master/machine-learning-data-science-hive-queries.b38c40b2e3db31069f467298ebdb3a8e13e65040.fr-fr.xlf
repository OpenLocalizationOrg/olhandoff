<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" source-language="en-US" target-language="fr-fr">
    <body>
      <group id="main" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Submit Hive Queries to Hadoop clusters in the Cortana Analytics Process  | Microsoft Azure</source>
          <target state="new">Submit Hive Queries to Hadoop clusters in the Cortana Analytics Process  | Microsoft Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Process Data from Hive Tables</source>
          <target state="new">Process Data from Hive Tables</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>Submit Hive Queries to HDInsight Hadoop clusters in the Cortana Analytics Process</source>
          <target state="new">Submit Hive Queries to HDInsight Hadoop clusters in the Cortana Analytics Process</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>This document describes various ways of submitting Hive queries to Hadoop clusters that are managed by an HDInsight service in Azure.</source>
          <target state="new">This document describes various ways of submitting Hive queries to Hadoop clusters that are managed by an HDInsight service in Azure.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>Hive queries can be submitted by using:</source>
          <target state="new">Hive queries can be submitted by using:</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>the Hadoop Command Line on the headnode of the cluster</source>
          <target state="new">the Hadoop Command Line on the headnode of the cluster</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>the IPython Notebook</source>
          <target state="new">the IPython Notebook</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>the Hive Editor</source>
          <target state="new">the Hive Editor</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>Azure PowerShell scripts</source>
          <target state="new">Azure PowerShell scripts</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>Generic Hive queries that can be used to explore the data or to generate features that use embedded Hive User Defined Functions (UDFs) are provided.</source>
          <target state="new">Generic Hive queries that can be used to explore the data or to generate features that use embedded Hive User Defined Functions (UDFs) are provided.</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>Examples of queries the specific to <bpt id="p1">[</bpt>NYC Taxi Trip Data<ept id="p1">](http://chriswhong.com/open-data/foil_nyc_taxi/)</ept><ph id="ph2" /> scenarios are also provided in <bpt id="p2">[</bpt>Github repository<ept id="p2">](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/DataScienceProcess/DataScienceScripts)</ept>.</source>
          <target state="new">Examples of queries the specific to <bpt id="p1">[</bpt>NYC Taxi Trip Data<ept id="p1">](http://chriswhong.com/open-data/foil_nyc_taxi/)</ept><ph id="ph2" /> scenarios are also provided in <bpt id="p2">[</bpt>Github repository<ept id="p2">](https://github.com/Azure/Azure-MachineLearning-DataScience/tree/master/Misc/DataScienceProcess/DataScienceScripts)</ept>.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>These queries already have data schema specified and are ready to be submitted to run for this scenario.</source>
          <target state="new">These queries already have data schema specified and are ready to be submitted to run for this scenario.</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>In the final section, parameters that users can tune to improve the performance of Hive queries are discussed.</source>
          <target state="new">In the final section, parameters that users can tune to improve the performance of Hive queries are discussed.</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>Prerequisites</source>
          <target state="new">Prerequisites</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>This article assumes that you have:</source>
          <target state="new">This article assumes that you have:</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>Created an Azure storage account.</source>
          <target state="new">Created an Azure storage account.</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>If you need instructions for this task, see <bpt id="p3">[</bpt>Create an Azure Storage account<ept id="p3">](../hdinsight-get-started.md#storage)</ept></source>
          <target state="new">If you need instructions for this task, see <bpt id="p3">[</bpt>Create an Azure Storage account<ept id="p3">](../hdinsight-get-started.md#storage)</ept></target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>Provisioned an Hadoop cluster with the HDInsight service.</source>
          <target state="new">Provisioned an Hadoop cluster with the HDInsight service.</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>If you need instructions, see <bpt id="p4">[</bpt>Provision an HDInsight cluster<ept id="p4">](../hdinsight-get-started.md#provision)</ept>.</source>
          <target state="new">If you need instructions, see <bpt id="p4">[</bpt>Provision an HDInsight cluster<ept id="p4">](../hdinsight-get-started.md#provision)</ept>.</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>Uploaded the data to Hive tables in Azure HDInsight Hadoop clusters.</source>
          <target state="new">Uploaded the data to Hive tables in Azure HDInsight Hadoop clusters.</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>If it has not, please follow the instructions provided at <bpt id="p5">[</bpt>Create and load data to Hive tables<ept id="p5">](machine-learning-data-science-hive-tables.md)</ept><ph id="ph3" /> to upload data to Hive tables first.</source>
          <target state="new">If it has not, please follow the instructions provided at <bpt id="p5">[</bpt>Create and load data to Hive tables<ept id="p5">](machine-learning-data-science-hive-tables.md)</ept><ph id="ph3" /> to upload data to Hive tables first.</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>Enabled remote access to the cluster.</source>
          <target state="new">Enabled remote access to the cluster.</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>If you need instructions, see <bpt id="p6">[</bpt>Access the Head Node of Hadoop Cluster<ept id="p6">](machine-learning-data-science-customize-hadoop-cluster.md#remoteaccess)</ept>.</source>
          <target state="new">If you need instructions, see <bpt id="p6">[</bpt>Access the Head Node of Hadoop Cluster<ept id="p6">](machine-learning-data-science-customize-hadoop-cluster.md#remoteaccess)</ept>.</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>How to submit Hive queries</source>
          <target state="new">How to submit Hive queries</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source><bpt id="p7">[</bpt>Submit Hive queries through Hadoop Command Line in headnode of Hadoop cluster<ept id="p7">](#headnode)</ept></source>
          <target state="new"><bpt id="p7">[</bpt>Submit Hive queries through Hadoop Command Line in headnode of Hadoop cluster<ept id="p7">](#headnode)</ept></target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source><bpt id="p8">[</bpt>Submit Hive queries with the Hive Editor<ept id="p8">](#hive-editor)</ept></source>
          <target state="new"><bpt id="p8">[</bpt>Submit Hive queries with the Hive Editor<ept id="p8">](#hive-editor)</ept></target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source><bpt id="p9">[</bpt>Submit Hive queries with Azure PowerShell Commands<ept id="p9">](#ps)</ept></source>
          <target state="new"><bpt id="p9">[</bpt>Submit Hive queries with Azure PowerShell Commands<ept id="p9">](#ps)</ept></target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>1. Submit Hive queries through Hadoop Command Line in headnode of Hadoop cluster</source>
          <target state="new">1. Submit Hive queries through Hadoop Command Line in headnode of Hadoop cluster</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>If the Hive query is complex, submitting it directly in the head node of the Hadoop cluster typically leads to faster turn around than submitting it with a Hive Editor or Azure PowerShell scripts.</source>
          <target state="new">If the Hive query is complex, submitting it directly in the head node of the Hadoop cluster typically leads to faster turn around than submitting it with a Hive Editor or Azure PowerShell scripts.</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>Log in to the head node of the Hadoop cluster, open the Hadoop Command Line on the desktop of the head node, and enter command <ph id="ph4">`cd %hive_home%\bin`</ph>.</source>
          <target state="new">Log in to the head node of the Hadoop cluster, open the Hadoop Command Line on the desktop of the head node, and enter command <ph id="ph4">`cd %hive_home%\bin`</ph>.</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>Users have three ways to submit Hive queries in the Hadoop Command Line:</source>
          <target state="new">Users have three ways to submit Hive queries in the Hadoop Command Line:</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>directly</source>
          <target state="new">directly</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>using .hql files</source>
          <target state="new">using .hql files</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>with the Hive command console</source>
          <target state="new">with the Hive command console</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>Submit Hive queries directly in Hadoop Command Line.</source>
          <target state="new">Submit Hive queries directly in Hadoop Command Line.</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>Users can run command like <ph id="ph5">`hive -e "&lt;your hive query&gt;;`</ph><ph id="ph6" /> to submit simple Hive queries directly in Hadoop Command Line.</source>
          <target state="new">Users can run command like <ph id="ph5">`hive -e "&lt;your hive query&gt;;`</ph><ph id="ph6" /> to submit simple Hive queries directly in Hadoop Command Line.</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>Here is an example, where the red box outlines the command that submits the Hive query, and the green box outlines the output from the Hive query.</source>
          <target state="new">Here is an example, where the red box outlines the command that submits the Hive query, and the green box outlines the output from the Hive query.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source><ph id="ph7">![</ph>Create workspace<ph id="ph8">][10]</ph></source>
          <target state="new"><ph id="ph7">![</ph>Create workspace<ph id="ph8">][10]</ph></target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>Submit Hive queries in .hql files</source>
          <target state="new">Submit Hive queries in .hql files</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>When the Hive query is more complicated and has multiple lines, editing queries in command line or Hive command console is not practical.</source>
          <target state="new">When the Hive query is more complicated and has multiple lines, editing queries in command line or Hive command console is not practical.</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>An alternative is to use a text editor in the head node of the Hadoop cluster to save the Hive queries in a .hql file in a local directory of the head node.</source>
          <target state="new">An alternative is to use a text editor in the head node of the Hadoop cluster to save the Hive queries in a .hql file in a local directory of the head node.</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>Then the Hive query in the .hql file can be submitted by using the <ph id="ph9">`-f`</ph><ph id="ph10" /> argument as follows:</source>
          <target state="new">Then the Hive query in the .hql file can be submitted by using the <ph id="ph9">`-f`</ph><ph id="ph10" /> argument as follows:</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source><ph id="ph11">![</ph>Create workspace<ph id="ph12">][15]</ph></source>
          <target state="new"><ph id="ph11">![</ph>Create workspace<ph id="ph12">][15]</ph></target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source><bpt id="p10">**</bpt>Suppress progress status screen print of Hive queries<ept id="p10">**</ept></source>
          <target state="new"><bpt id="p10">**</bpt>Suppress progress status screen print of Hive queries<ept id="p10">**</ept></target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>By default, after Hive query is submitted in Hadoop Command Line, the progress of the Map/Reduce job will be printed out on screen.</source>
          <target state="new">By default, after Hive query is submitted in Hadoop Command Line, the progress of the Map/Reduce job will be printed out on screen.</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>To suppress the screen print of the Map/Reduce job progress, you can use an argument <ph id="ph13">`-S`</ph><ph id="ph14" /> ("S" in upper case) in the command line as follows:</source>
          <target state="new">To suppress the screen print of the Map/Reduce job progress, you can use an argument <ph id="ph13">`-S`</ph><ph id="ph14" /> ("S" in upper case) in the command line as follows:</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>Submit Hive queries in Hive command console.</source>
          <target state="new">Submit Hive queries in Hive command console.</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>Users can also first enter the Hive command console by running command <ph id="ph15">`hive`</ph><ph id="ph16" /> in Hadoop Command Line, and then submit Hive queries in Hive command console.</source>
          <target state="new">Users can also first enter the Hive command console by running command <ph id="ph15">`hive`</ph><ph id="ph16" /> in Hadoop Command Line, and then submit Hive queries in Hive command console.</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>Here is an example.</source>
          <target state="new">Here is an example.</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>In this example, the two red boxes highlight the commands used to enter the Hive command console, and the Hive query submitted in Hive command console, respectively.</source>
          <target state="new">In this example, the two red boxes highlight the commands used to enter the Hive command console, and the Hive query submitted in Hive command console, respectively.</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>The green box highlights the output from the Hive query.</source>
          <target state="new">The green box highlights the output from the Hive query.</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source><ph id="ph17">![</ph>Create workspace<ph id="ph18">][11]</ph></source>
          <target state="new"><ph id="ph17">![</ph>Create workspace<ph id="ph18">][11]</ph></target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source>The previous examples directly output the Hive query results on screen.</source>
          <target state="new">The previous examples directly output the Hive query results on screen.</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>Users can also write the output to a local file on the head node, or to an Azure blob.</source>
          <target state="new">Users can also write the output to a local file on the head node, or to an Azure blob.</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>Then, users can use other tools to further analyze the output of Hive queries.</source>
          <target state="new">Then, users can use other tools to further analyze the output of Hive queries.</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source><bpt id="p11">**</bpt>Output Hive query results to a local file.<ept id="p11">**</ept></source>
          <target state="new"><bpt id="p11">**</bpt>Output Hive query results to a local file.<ept id="p11">**</ept></target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>To output Hive query results to a local directory on the head node, users have to submit the Hive query in the Hadoop Command Line as follows:</source>
          <target state="new">To output Hive query results to a local directory on the head node, users have to submit the Hive query in the Hadoop Command Line as follows:</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source>In the following example, the output of Hive query is written into a file <ph id="ph19">`hivequeryoutput.txt`</ph><ph id="ph20" /> in directory <ph id="ph21">`C:\apps\temp`</ph>.</source>
          <target state="new">In the following example, the output of Hive query is written into a file <ph id="ph19">`hivequeryoutput.txt`</ph><ph id="ph20" /> in directory <ph id="ph21">`C:\apps\temp`</ph>.</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source><ph id="ph22">![</ph>Create workspace<ph id="ph23">][12]</ph></source>
          <target state="new"><ph id="ph22">![</ph>Create workspace<ph id="ph23">][12]</ph></target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source><bpt id="p12">**</bpt>Output Hive query results to an Azure blob<ept id="p12">**</ept></source>
          <target state="new"><bpt id="p12">**</bpt>Output Hive query results to an Azure blob<ept id="p12">**</ept></target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>Users can also output the Hive query results to an Azure blob, within the default container of the Hadoop cluster.</source>
          <target state="new">Users can also output the Hive query results to an Azure blob, within the default container of the Hadoop cluster.</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>The Hive query has to be like this:</source>
          <target state="new">The Hive query has to be like this:</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>In the following example, the output of Hive query is written to a blob directory <ph id="ph24">`queryoutputdir`</ph><ph id="ph25" /> within the default container of the Hadoop cluster.</source>
          <target state="new">In the following example, the output of Hive query is written to a blob directory <ph id="ph24">`queryoutputdir`</ph><ph id="ph25" /> within the default container of the Hadoop cluster.</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>Here, you only need to provide the directory name, without the blob name.</source>
          <target state="new">Here, you only need to provide the directory name, without the blob name.</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source>An error will be thrown out if you provide both directory and blob names, such as <ph id="ph26">`wasb:///queryoutputdir/queryoutput.txt`</ph>.</source>
          <target state="new">An error will be thrown out if you provide both directory and blob names, such as <ph id="ph26">`wasb:///queryoutputdir/queryoutput.txt`</ph>.</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source><ph id="ph27">![</ph>Create workspace<ph id="ph28">][13]</ph></source>
          <target state="new"><ph id="ph27">![</ph>Create workspace<ph id="ph28">][13]</ph></target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source>If you open the default container of the Hadoop cluster using tools like Azure Storage Explorer, you will see the output of the Hive query as follows.</source>
          <target state="new">If you open the default container of the Hadoop cluster using tools like Azure Storage Explorer, you will see the output of the Hive query as follows.</target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>You can apply the filter (highlighted by red box) to only retrieve the blob with specified letters in names.</source>
          <target state="new">You can apply the filter (highlighted by red box) to only retrieve the blob with specified letters in names.</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source><ph id="ph29">![</ph>Create workspace<ph id="ph30">][14]</ph></source>
          <target state="new"><ph id="ph29">![</ph>Create workspace<ph id="ph30">][14]</ph></target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source>2. Submit Hive queries with the Hive Editor</source>
          <target state="new">2. Submit Hive queries with the Hive Editor</target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source>Users can also use Query Console (Hive Editor) by entering the URL in a web browser <ph id="ph31">`https://&lt;Hadoop cluster name&gt;.azurehdinsight.net/Home/HiveEditor`</ph><ph id="ph32" /> (you will be asked to input the Hadoop cluster credentials to log in),</source>
          <target state="new">Users can also use Query Console (Hive Editor) by entering the URL in a web browser <ph id="ph31">`https://&lt;Hadoop cluster name&gt;.azurehdinsight.net/Home/HiveEditor`</ph><ph id="ph32" /> (you will be asked to input the Hadoop cluster credentials to log in),</target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source>3. Submit Hive queries with Azure PowerShell Commands</source>
          <target state="new">3. Submit Hive queries with Azure PowerShell Commands</target>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source>Users can also us PowerShell to submit Hive queries.</source>
          <target state="new">Users can also us PowerShell to submit Hive queries.</target>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source>For instructions, see <bpt id="p13">[</bpt>Submit Hive jobs using PowerShell<ept id="p13">](../hdinsight/hdinsight-submit-hadoop-jobs-programmatically.md#hive-powershell)</ept>.</source>
          <target state="new">For instructions, see <bpt id="p13">[</bpt>Submit Hive jobs using PowerShell<ept id="p13">](../hdinsight/hdinsight-submit-hadoop-jobs-programmatically.md#hive-powershell)</ept>.</target>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source>Data Exploration, Feature Engineering and Hive Parameter Tuning</source>
          <target state="new">Data Exploration, Feature Engineering and Hive Parameter Tuning</target>
        </trans-unit>
        <trans-unit id="180" translate="yes" xml:space="preserve">
          <source>We describe the following data wrangling tasks in this section using Hive in Azure HDInsight Hadoop clusters:</source>
          <target state="new">We describe the following data wrangling tasks in this section using Hive in Azure HDInsight Hadoop clusters:</target>
        </trans-unit>
        <trans-unit id="181" translate="yes" xml:space="preserve">
          <source><bpt id="p14">[</bpt>Data Exploration<ept id="p14">](#hive-dataexploration)</ept></source>
          <target state="new"><bpt id="p14">[</bpt>Data Exploration<ept id="p14">](#hive-dataexploration)</ept></target>
        </trans-unit>
        <trans-unit id="182" translate="yes" xml:space="preserve">
          <source><bpt id="p15">[</bpt>Feature Generation<ept id="p15">](#hive-featureengineering)</ept></source>
          <target state="new"><bpt id="p15">[</bpt>Feature Generation<ept id="p15">](#hive-featureengineering)</ept></target>
        </trans-unit>
        <trans-unit id="183" translate="yes" xml:space="preserve">
          <source><ph id="ph33">[AZURE.NOTE]</ph><ph id="ph34" /> The sample Hive queries assume that the data has been uploaded to Hive tables in Azure HDInsight Hadoop clusters.</source>
          <target state="new"><ph id="ph33">[AZURE.NOTE]</ph><ph id="ph34" /> The sample Hive queries assume that the data has been uploaded to Hive tables in Azure HDInsight Hadoop clusters.</target>
        </trans-unit>
        <trans-unit id="184" translate="yes" xml:space="preserve">
          <source>If it has not, please follow <bpt id="p16">[</bpt>Create and load data to Hive tables<ept id="p16">](machine-learning-data-science-hive-tables.md)</ept><ph id="ph35" /> to upload data to Hive tables first.</source>
          <target state="new">If it has not, please follow <bpt id="p16">[</bpt>Create and load data to Hive tables<ept id="p16">](machine-learning-data-science-hive-tables.md)</ept><ph id="ph35" /> to upload data to Hive tables first.</target>
        </trans-unit>
        <trans-unit id="186" translate="yes" xml:space="preserve">
          <source>Data Exploration</source>
          <target state="new">Data Exploration</target>
        </trans-unit>
        <trans-unit id="187" translate="yes" xml:space="preserve">
          <source>Here are a few sample Hive scripts that can be used to explore data in Hive tables.</source>
          <target state="new">Here are a few sample Hive scripts that can be used to explore data in Hive tables.</target>
        </trans-unit>
        <trans-unit id="188" translate="yes" xml:space="preserve">
          <source>Get the count of observations per partition
 <ph id="ph36">`SELECT &lt;partitionfieldname&gt;, count(*) from &lt;databasename&gt;.&lt;tablename&gt; group by &lt;partitionfieldname&gt;;`</ph></source>
          <target state="new">Get the count of observations per partition
 <ph id="ph36">`SELECT &lt;partitionfieldname&gt;, count(*) from &lt;databasename&gt;.&lt;tablename&gt; group by &lt;partitionfieldname&gt;;`</ph></target>
        </trans-unit>
        <trans-unit id="189" translate="yes" xml:space="preserve">
          <source>Get the count of observations per day
 <ph id="ph37">`SELECT to_date(&lt;date_columnname&gt;), count(*) from &lt;databasename&gt;.&lt;tablename&gt; group by to_date(&lt;date_columnname&gt;);`</ph></source>
          <target state="new">Get the count of observations per day
 <ph id="ph37">`SELECT to_date(&lt;date_columnname&gt;), count(*) from &lt;databasename&gt;.&lt;tablename&gt; group by to_date(&lt;date_columnname&gt;);`</ph></target>
        </trans-unit>
        <trans-unit id="190" translate="yes" xml:space="preserve">
          <source>Get the levels in a categorical column  
 <ph id="ph38">`SELECT  distinct &lt;column_name&gt; from &lt;databasename&gt;.&lt;tablename&gt;`</ph></source>
          <target state="new">Get the levels in a categorical column  
 <ph id="ph38">`SELECT  distinct &lt;column_name&gt; from &lt;databasename&gt;.&lt;tablename&gt;`</ph></target>
        </trans-unit>
        <trans-unit id="191" translate="yes" xml:space="preserve">
          <source>Get the number of levels in combination of two categorical columns 
 <ph id="ph39">`SELECT &lt;column_a&gt;, &lt;column_b&gt;, count(*) from &lt;databasename&gt;.&lt;tablename&gt; group by &lt;column_a&gt;, &lt;column_b&gt;`</ph></source>
          <target state="new">Get the number of levels in combination of two categorical columns 
 <ph id="ph39">`SELECT &lt;column_a&gt;, &lt;column_b&gt;, count(*) from &lt;databasename&gt;.&lt;tablename&gt; group by &lt;column_a&gt;, &lt;column_b&gt;`</ph></target>
        </trans-unit>
        <trans-unit id="192" translate="yes" xml:space="preserve">
          <source>Get the distribution for numerical columns  
 <ph id="ph40">`SELECT &lt;column_name&gt;, count(*) from &lt;databasename&gt;.&lt;tablename&gt; group by &lt;column_name&gt;`</ph></source>
          <target state="new">Get the distribution for numerical columns  
 <ph id="ph40">`SELECT &lt;column_name&gt;, count(*) from &lt;databasename&gt;.&lt;tablename&gt; group by &lt;column_name&gt;`</ph></target>
        </trans-unit>
        <trans-unit id="193" translate="yes" xml:space="preserve">
          <source>Extract records from joining two tables</source>
          <target state="new">Extract records from joining two tables</target>
        </trans-unit>
        <trans-unit id="195" translate="yes" xml:space="preserve">
          <source>Feature Generation</source>
          <target state="new">Feature Generation</target>
        </trans-unit>
        <trans-unit id="196" translate="yes" xml:space="preserve">
          <source>In this section, we describe ways of generating features using Hive queries:</source>
          <target state="new">In this section, we describe ways of generating features using Hive queries:</target>
        </trans-unit>
        <trans-unit id="197" translate="yes" xml:space="preserve">
          <source><bpt id="p17">[</bpt>Frequency based Feature Generation<ept id="p17">](#hive-frequencyfeature)</ept></source>
          <target state="new"><bpt id="p17">[</bpt>Frequency based Feature Generation<ept id="p17">](#hive-frequencyfeature)</ept></target>
        </trans-unit>
        <trans-unit id="198" translate="yes" xml:space="preserve">
          <source><bpt id="p18">[</bpt>Risks of Categorical Variables in Binary Classification<ept id="p18">](#hive-riskfeature)</ept></source>
          <target state="new"><bpt id="p18">[</bpt>Risks of Categorical Variables in Binary Classification<ept id="p18">](#hive-riskfeature)</ept></target>
        </trans-unit>
        <trans-unit id="199" translate="yes" xml:space="preserve">
          <source><bpt id="p19">[</bpt>Extract features from Datetime Field<ept id="p19">](#hive-datefeature)</ept></source>
          <target state="new"><bpt id="p19">[</bpt>Extract features from Datetime Field<ept id="p19">](#hive-datefeature)</ept></target>
        </trans-unit>
        <trans-unit id="200" translate="yes" xml:space="preserve">
          <source><bpt id="p20">[</bpt>Extract features from Text Field<ept id="p20">](#hive-textfeature)</ept></source>
          <target state="new"><bpt id="p20">[</bpt>Extract features from Text Field<ept id="p20">](#hive-textfeature)</ept></target>
        </trans-unit>
        <trans-unit id="201" translate="yes" xml:space="preserve">
          <source><bpt id="p21">[</bpt>Calculate distance between GPS coordinates<ept id="p21">](#hive-gpsdistance)</ept></source>
          <target state="new"><bpt id="p21">[</bpt>Calculate distance between GPS coordinates<ept id="p21">](#hive-gpsdistance)</ept></target>
        </trans-unit>
        <trans-unit id="202" translate="yes" xml:space="preserve">
          <source><ph id="ph41">[AZURE.NOTE]</ph><ph id="ph42" /> Once you generate additional features, you can either add them as columns to the existing table or create a new table with the additional features and primary key, which can then be joined with the original table.</source>
          <target state="new"><ph id="ph41">[AZURE.NOTE]</ph><ph id="ph42" /> Once you generate additional features, you can either add them as columns to the existing table or create a new table with the additional features and primary key, which can then be joined with the original table.</target>
        </trans-unit>
        <trans-unit id="204" translate="yes" xml:space="preserve">
          <source>Frequency based Feature Generation</source>
          <target state="new">Frequency based Feature Generation</target>
        </trans-unit>
        <trans-unit id="205" translate="yes" xml:space="preserve">
          <source>Sometimes, it is valuable to calculate the frequencies of the levels of a categorical variable, or the frequencies of level combinations of multiple categorical variables.</source>
          <target state="new">Sometimes, it is valuable to calculate the frequencies of the levels of a categorical variable, or the frequencies of level combinations of multiple categorical variables.</target>
        </trans-unit>
        <trans-unit id="206" translate="yes" xml:space="preserve">
          <source>Users can use the following script to calculate the frequencies:</source>
          <target state="new">Users can use the following script to calculate the frequencies:</target>
        </trans-unit>
        <trans-unit id="208" translate="yes" xml:space="preserve">
          <source>Risks of Categorical Variables in Binary Classification</source>
          <target state="new">Risks of Categorical Variables in Binary Classification</target>
        </trans-unit>
        <trans-unit id="209" translate="yes" xml:space="preserve">
          <source>In binary classification, sometimes we need to convert non-numeric categorical variables into numeric features by replacing the non-numeric levels with numeric risks, since some models might only take numeric features.</source>
          <target state="new">In binary classification, sometimes we need to convert non-numeric categorical variables into numeric features by replacing the non-numeric levels with numeric risks, since some models might only take numeric features.</target>
        </trans-unit>
        <trans-unit id="210" translate="yes" xml:space="preserve">
          <source>In this section, we show some generic Hive queries of calculating the risk values (log odds) of a categorical variable.</source>
          <target state="new">In this section, we show some generic Hive queries of calculating the risk values (log odds) of a categorical variable.</target>
        </trans-unit>
        <trans-unit id="211" translate="yes" xml:space="preserve">
          <source>In this example, variables <ph id="ph43">`smooth_param1`</ph><ph id="ph44" /> and <ph id="ph45">`smooth_param2`</ph><ph id="ph46" /> are set to smooth the risk values calculated from data. Ri</source>
          <target state="new">In this example, variables <ph id="ph43">`smooth_param1`</ph><ph id="ph44" /> and <ph id="ph45">`smooth_param2`</ph><ph id="ph46" /> are set to smooth the risk values calculated from data. Ri</target>
        </trans-unit>
        <trans-unit id="212" translate="yes" xml:space="preserve">
          <source>sks are ranged between -Inf and Inf. Ri</source>
          <target state="new">sks are ranged between -Inf and Inf. Ri</target>
        </trans-unit>
        <trans-unit id="213" translate="yes" xml:space="preserve">
          <source>sks&gt;0 stands for the probability that the target equals 1 is greater than 0.5.</source>
          <target state="new">sks&gt;0 stands for the probability that the target equals 1 is greater than 0.5.</target>
        </trans-unit>
        <trans-unit id="214" translate="yes" xml:space="preserve">
          <source>After the risk table is calculated, users can assign risk values to a table by joining it with the risk table.</source>
          <target state="new">After the risk table is calculated, users can assign risk values to a table by joining it with the risk table.</target>
        </trans-unit>
        <trans-unit id="215" translate="yes" xml:space="preserve">
          <source>The Hive joining query has been given in previous section.</source>
          <target state="new">The Hive joining query has been given in previous section.</target>
        </trans-unit>
        <trans-unit id="217" translate="yes" xml:space="preserve">
          <source>Extract features from Datetime Fields</source>
          <target state="new">Extract features from Datetime Fields</target>
        </trans-unit>
        <trans-unit id="218" translate="yes" xml:space="preserve">
          <source>Hive comes along with a set of UDFs for processing datetime fields.</source>
          <target state="new">Hive comes along with a set of UDFs for processing datetime fields.</target>
        </trans-unit>
        <trans-unit id="219" translate="yes" xml:space="preserve">
          <source>In Hive, the default datetime format is 'yyyy-MM-dd 00:00:00' (like '1970-01-01 12:21:32').</source>
          <target state="new">In Hive, the default datetime format is 'yyyy-MM-dd 00:00:00' (like '1970-01-01 12:21:32').</target>
        </trans-unit>
        <trans-unit id="220" translate="yes" xml:space="preserve">
          <source>In this section, we show examples of extracting the day of a month, and the month from a datetime field, and examples of converting a datetime string in a format other than the default format to a datetime string in default format.</source>
          <target state="new">In this section, we show examples of extracting the day of a month, and the month from a datetime field, and examples of converting a datetime string in a format other than the default format to a datetime string in default format.</target>
        </trans-unit>
        <trans-unit id="221" translate="yes" xml:space="preserve">
          <source>This Hive query assumes that the <ph id="ph47">`&lt;datetime field&gt;`</ph><ph id="ph48" /> is in the default datetime format.</source>
          <target state="new">This Hive query assumes that the <ph id="ph47">`&lt;datetime field&gt;`</ph><ph id="ph48" /> is in the default datetime format.</target>
        </trans-unit>
        <trans-unit id="222" translate="yes" xml:space="preserve">
          <source>If a datetime field is not in the default format, we need to first convert the datetile field into Unix time stamp, and then convert the Unix time stamp to a datetime string in the default format.</source>
          <target state="new">If a datetime field is not in the default format, we need to first convert the datetile field into Unix time stamp, and then convert the Unix time stamp to a datetime string in the default format.</target>
        </trans-unit>
        <trans-unit id="223" translate="yes" xml:space="preserve">
          <source>After the datetime is in default format, users can apply the embedded datetime UDFs to extract features.</source>
          <target state="new">After the datetime is in default format, users can apply the embedded datetime UDFs to extract features.</target>
        </trans-unit>
        <trans-unit id="224" translate="yes" xml:space="preserve">
          <source>In this query, if the <ph id="ph49">`&lt;datetime field&gt;`</ph><ph id="ph50" /> has the pattern like <ph id="ph51">`03/26/2015 12:04:39`</ph>, the <ph id="ph52">`'&lt;pattern of the datetime field&gt;'`</ph><ph id="ph53" /> should be <ph id="ph54">`'MM/dd/yyyy HH:mm:ss'`</ph>.</source>
          <target state="new">In this query, if the <ph id="ph49">`&lt;datetime field&gt;`</ph><ph id="ph50" /> has the pattern like <ph id="ph51">`03/26/2015 12:04:39`</ph>, the <ph id="ph52">`'&lt;pattern of the datetime field&gt;'`</ph><ph id="ph53" /> should be <ph id="ph54">`'MM/dd/yyyy HH:mm:ss'`</ph>.</target>
        </trans-unit>
        <trans-unit id="225" translate="yes" xml:space="preserve">
          <source>To test it, users can run</source>
          <target state="new">To test it, users can run</target>
        </trans-unit>
        <trans-unit id="226" translate="yes" xml:space="preserve">
          <source>In this query, <ph id="ph55">`hivesampletable`</ph><ph id="ph56" /> comes with all Azure HDInsight Hadoop clusters by default when the clusters are provisioned.</source>
          <target state="new">In this query, <ph id="ph55">`hivesampletable`</ph><ph id="ph56" /> comes with all Azure HDInsight Hadoop clusters by default when the clusters are provisioned.</target>
        </trans-unit>
        <trans-unit id="228" translate="yes" xml:space="preserve">
          <source>Extract features from Text Fields</source>
          <target state="new">Extract features from Text Fields</target>
        </trans-unit>
        <trans-unit id="229" translate="yes" xml:space="preserve">
          <source>Assume that the Hive table has a text field, which is a string of words separated by space, the following query extract the length of the string, and the number of words in the string.</source>
          <target state="new">Assume that the Hive table has a text field, which is a string of words separated by space, the following query extract the length of the string, and the number of words in the string.</target>
        </trans-unit>
        <trans-unit id="231" translate="yes" xml:space="preserve">
          <source>Calculate distance between GPS coordinates</source>
          <target state="new">Calculate distance between GPS coordinates</target>
        </trans-unit>
        <trans-unit id="232" translate="yes" xml:space="preserve">
          <source>The query given in this section can be directly applied on the NYC Taxi Trip Data.</source>
          <target state="new">The query given in this section can be directly applied on the NYC Taxi Trip Data.</target>
        </trans-unit>
        <trans-unit id="233" translate="yes" xml:space="preserve">
          <source>The purpose of this query is to show how to apply the embedded mathematical functions in Hive to generate features.</source>
          <target state="new">The purpose of this query is to show how to apply the embedded mathematical functions in Hive to generate features.</target>
        </trans-unit>
        <trans-unit id="234" translate="yes" xml:space="preserve">
          <source>The fields that are used in this query are GPS coordinates of pickup and dropoff locations, named pickup\_longitude, pickup\_latitude, dropoff\_longitude, and dropoff\_latitude.</source>
          <target state="new">The fields that are used in this query are GPS coordinates of pickup and dropoff locations, named pickup\_longitude, pickup\_latitude, dropoff\_longitude, and dropoff\_latitude.</target>
        </trans-unit>
        <trans-unit id="235" translate="yes" xml:space="preserve">
          <source>The queries to calculate the direct distance between the pickup and dropoff coordinates are:</source>
          <target state="new">The queries to calculate the direct distance between the pickup and dropoff coordinates are:</target>
        </trans-unit>
        <trans-unit id="236" translate="yes" xml:space="preserve">
          <source>The mathematical equations of calculating distance between two GPS coordinates can be found at <bpt id="p22">[</bpt>Movable Type Scripts<ept id="p22">](http://www.movable-type.co.uk/scripts/latlong.html)</ept>, authored by Peter Lapisu.</source>
          <target state="new">The mathematical equations of calculating distance between two GPS coordinates can be found at <bpt id="p22">[</bpt>Movable Type Scripts<ept id="p22">](http://www.movable-type.co.uk/scripts/latlong.html)</ept>, authored by Peter Lapisu.</target>
        </trans-unit>
        <trans-unit id="237" translate="yes" xml:space="preserve">
          <source>In his Javascript, the function toRad() is just <ph id="ph57">`lat_or_lon*pi/180`</ph>, which converts degrees to radians.</source>
          <target state="new">In his Javascript, the function toRad() is just <ph id="ph57">`lat_or_lon*pi/180`</ph>, which converts degrees to radians.</target>
        </trans-unit>
        <trans-unit id="238" translate="yes" xml:space="preserve">
          <source>Here, <ph id="ph58">`lat_or_lon`</ph><ph id="ph59" /> is the latitude or longitude.</source>
          <target state="new">Here, <ph id="ph58">`lat_or_lon`</ph><ph id="ph59" /> is the latitude or longitude.</target>
        </trans-unit>
        <trans-unit id="239" translate="yes" xml:space="preserve">
          <source>Since Hive does not provide function <ph id="ph60">`atan2`</ph>, but provides function <ph id="ph61">`atan`</ph>, the <ph id="ph62">`atan2`</ph><ph id="ph63" /> function is implemented by <ph id="ph64">`atan`</ph><ph id="ph65" /> function in the above Hive query, based on its definition in <bpt id="p23">[</bpt>Wikipedia<ept id="p23">](http://en.wikipedia.org/wiki/Atan2)</ept>.</source>
          <target state="new">Since Hive does not provide function <ph id="ph60">`atan2`</ph>, but provides function <ph id="ph61">`atan`</ph>, the <ph id="ph62">`atan2`</ph><ph id="ph63" /> function is implemented by <ph id="ph64">`atan`</ph><ph id="ph65" /> function in the above Hive query, based on its definition in <bpt id="p23">[</bpt>Wikipedia<ept id="p23">](http://en.wikipedia.org/wiki/Atan2)</ept>.</target>
        </trans-unit>
        <trans-unit id="240" translate="yes" xml:space="preserve">
          <source><ph id="ph66">![</ph>Create workspace<ph id="ph67">][1]</ph></source>
          <target state="new"><ph id="ph66">![</ph>Create workspace<ph id="ph67">][1]</ph></target>
        </trans-unit>
        <trans-unit id="241" translate="yes" xml:space="preserve">
          <source>A full list of Hive embedded UDFs can be found in the <bpt id="p24">[</bpt>UDF Language Manual<ept id="p24">](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-MathematicalFunctions)</ept>.</source>
          <target state="new">A full list of Hive embedded UDFs can be found in the <bpt id="p24">[</bpt>UDF Language Manual<ept id="p24">](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-MathematicalFunctions)</ept>.</target>
        </trans-unit>
        <trans-unit id="242" translate="yes" xml:space="preserve">
          <source>Advanced topics: Tune Hive Parameters to Improve Query Speed</source>
          <target state="new">Advanced topics: Tune Hive Parameters to Improve Query Speed</target>
        </trans-unit>
        <trans-unit id="243" translate="yes" xml:space="preserve">
          <source>The default parameter settings of Hive cluster might not be suitable for the Hive queries and the data the queries are processing.</source>
          <target state="new">The default parameter settings of Hive cluster might not be suitable for the Hive queries and the data the queries are processing.</target>
        </trans-unit>
        <trans-unit id="244" translate="yes" xml:space="preserve">
          <source>In this section, we discuss some parameters that users can tune so that the performance of Hive queries can be improved.</source>
          <target state="new">In this section, we discuss some parameters that users can tune so that the performance of Hive queries can be improved.</target>
        </trans-unit>
        <trans-unit id="245" translate="yes" xml:space="preserve">
          <source>Users need to add the parameter tuning queries before the queries of processing data.</source>
          <target state="new">Users need to add the parameter tuning queries before the queries of processing data.</target>
        </trans-unit>
        <trans-unit id="246" translate="yes" xml:space="preserve">
          <source>Java heap space : For queries involving joining large datasets, or processing long records, a typical error is <bpt id="p25">**</bpt>running out of heap space<ept id="p25">**</ept>.</source>
          <target state="new">Java heap space : For queries involving joining large datasets, or processing long records, a typical error is <bpt id="p25">**</bpt>running out of heap space<ept id="p25">**</ept>.</target>
        </trans-unit>
        <trans-unit id="247" translate="yes" xml:space="preserve">
          <source>This can be tuned by setting parameters <ph id="ph68">`mapreduce.map.java.opts`</ph><ph id="ph69" /> and <ph id="ph70">`mapreduce.task.io.sort.mb`</ph><ph id="ph71" /> to desired values.</source>
          <target state="new">This can be tuned by setting parameters <ph id="ph68">`mapreduce.map.java.opts`</ph><ph id="ph69" /> and <ph id="ph70">`mapreduce.task.io.sort.mb`</ph><ph id="ph71" /> to desired values.</target>
        </trans-unit>
        <trans-unit id="248" translate="yes" xml:space="preserve">
          <source>Here is an example:</source>
          <target state="new">Here is an example:</target>
        </trans-unit>
        <trans-unit id="249" translate="yes" xml:space="preserve">
          <source>This parameter allocates 4GB memory to Java heap space and also makes sorting more efficient by allocating more memory for it.</source>
          <target state="new">This parameter allocates 4GB memory to Java heap space and also makes sorting more efficient by allocating more memory for it.</target>
        </trans-unit>
        <trans-unit id="250" translate="yes" xml:space="preserve">
          <source>It is a good idea to play with it if there are any job failure errors related to heap space.</source>
          <target state="new">It is a good idea to play with it if there are any job failure errors related to heap space.</target>
        </trans-unit>
        <trans-unit id="251" translate="yes" xml:space="preserve">
          <source>DFS block size : This parameter sets the smallest unit of data that the file system stores.</source>
          <target state="new">DFS block size : This parameter sets the smallest unit of data that the file system stores.</target>
        </trans-unit>
        <trans-unit id="252" translate="yes" xml:space="preserve">
          <source>As an example, if the DFS block size is 128MB, then any data of size less than and up to 128MB is stored in a single block, while data that is larger than 128MB is allotted extra blocks.</source>
          <target state="new">As an example, if the DFS block size is 128MB, then any data of size less than and up to 128MB is stored in a single block, while data that is larger than 128MB is allotted extra blocks.</target>
        </trans-unit>
        <trans-unit id="253" translate="yes" xml:space="preserve">
          <source>Choosing a very small block size causes large overheads in Hadoop since the name node has to process many more requests to find the relevant block pertaining to the file.</source>
          <target state="new">Choosing a very small block size causes large overheads in Hadoop since the name node has to process many more requests to find the relevant block pertaining to the file.</target>
        </trans-unit>
        <trans-unit id="254" translate="yes" xml:space="preserve">
          <source>A recommended setting when dealing with gigabytes (or larger) data is :</source>
          <target state="new">A recommended setting when dealing with gigabytes (or larger) data is :</target>
        </trans-unit>
        <trans-unit id="255" translate="yes" xml:space="preserve">
          <source>Optimizing join operation in Hive : While join operations in the map/reduce framework typically take place in the reduce phase, some times, enormous gains can be achieved by scheduling joins in the map phase (also called "mapjoins").</source>
          <target state="new">Optimizing join operation in Hive : While join operations in the map/reduce framework typically take place in the reduce phase, some times, enormous gains can be achieved by scheduling joins in the map phase (also called "mapjoins").</target>
        </trans-unit>
        <trans-unit id="256" translate="yes" xml:space="preserve">
          <source>To direct Hive to do this whenever possible, we can set :</source>
          <target state="new">To direct Hive to do this whenever possible, we can set :</target>
        </trans-unit>
        <trans-unit id="257" translate="yes" xml:space="preserve">
          <source>Specifying the number of mappers to Hive : While Hadoop allows the user to set the number of reducers, the number of mappers may typically not be set by the user.</source>
          <target state="new">Specifying the number of mappers to Hive : While Hadoop allows the user to set the number of reducers, the number of mappers may typically not be set by the user.</target>
        </trans-unit>
        <trans-unit id="258" translate="yes" xml:space="preserve">
          <source>A trick that allows some degree of control on this number is to choose the hadoop variables, mapred.min.split.size and mapred.max.split.size.</source>
          <target state="new">A trick that allows some degree of control on this number is to choose the hadoop variables, mapred.min.split.size and mapred.max.split.size.</target>
        </trans-unit>
        <trans-unit id="259" translate="yes" xml:space="preserve">
          <source>The size of each map task is determined by :</source>
          <target state="new">The size of each map task is determined by :</target>
        </trans-unit>
        <trans-unit id="260" translate="yes" xml:space="preserve">
          <source>Typically, the default value of mapred.min.split.size is 0, that of mapred.max.split.size is Long.MAX and that of dfs.block.size is 64MB.</source>
          <target state="new">Typically, the default value of mapred.min.split.size is 0, that of mapred.max.split.size is Long.MAX and that of dfs.block.size is 64MB.</target>
        </trans-unit>
        <trans-unit id="261" translate="yes" xml:space="preserve">
          <source>As we can see, given the data size, tuning these parameters by "setting" them allows us to tune the number of mappers used.</source>
          <target state="new">As we can see, given the data size, tuning these parameters by "setting" them allows us to tune the number of mappers used.</target>
        </trans-unit>
        <trans-unit id="262" translate="yes" xml:space="preserve">
          <source>A few other more advanced options for optimizing Hive performance are mentioned below.</source>
          <target state="new">A few other more advanced options for optimizing Hive performance are mentioned below.</target>
        </trans-unit>
        <trans-unit id="263" translate="yes" xml:space="preserve">
          <source>These allow for the setting of memory allocated to map and reduce tasks, and can be useful in tweaking performance.</source>
          <target state="new">These allow for the setting of memory allocated to map and reduce tasks, and can be useful in tweaking performance.</target>
        </trans-unit>
        <trans-unit id="264" translate="yes" xml:space="preserve">
          <source>Please keep in mind that the <ph id="ph72">`mapreduce.reduce.memory.mb`</ph><ph id="ph73" /> cannot be greater than the physical memory size of each worker node in the Hadoop cluster.</source>
          <target state="new">Please keep in mind that the <ph id="ph72">`mapreduce.reduce.memory.mb`</ph><ph id="ph73" /> cannot be greater than the physical memory size of each worker node in the Hadoop cluster.</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">e25dd8654c7e0cbcbfc6aed70d5c43a0f1599a9f</xliffext:olfilehash>
  </header>
</xliff>