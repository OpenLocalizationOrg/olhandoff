<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" source-language="en-US" target-language="fr-fr">
    <body>
      <group id="main" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Feature selection in the Cortana Analytics Process | Microsoft Azure</source>
          <target state="new">Feature selection in the Cortana Analytics Process | Microsoft Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Explains the purpose of feature selection and provides examples of their role in the data enhancement process of machine learning.</source>
          <target state="new">Explains the purpose of feature selection and provides examples of their role in the data enhancement process of machine learning.</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Feature selection in the Cortana Analytics Process</source>
          <target state="new">Feature selection in the Cortana Analytics Process</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>This topic explains the purposes of feature selection and provides examples of its role in the data enhancement process of machine learning.</source>
          <target state="new">This topic explains the purposes of feature selection and provides examples of its role in the data enhancement process of machine learning.</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>These examples are drawn from Azure Machine Learning Studio.</source>
          <target state="new">These examples are drawn from Azure Machine Learning Studio.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>Introduction</source>
          <target state="new">Introduction</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>This topic explains the purpose of feature selection and provides examples of its role in the data enhancement process of machine learning.</source>
          <target state="new">This topic explains the purpose of feature selection and provides examples of its role in the data enhancement process of machine learning.</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>These examples are drawn from Azure Machine Learning Studio.</source>
          <target state="new">These examples are drawn from Azure Machine Learning Studio.</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>The engineering and selection of features is one part of the CAP process outlined in the <bpt id="p1">[</bpt>What is the Cortana Analytics Process?<ept id="p1">](machine-learning-data-science-the-cortana-analytics-process.md)</ept><ph id="ph3" /> Feature engineering and selection are parts of the <bpt id="p2">**</bpt>Develop features<ept id="p2">**</ept><ph id="ph4" /> step of the CAP.</source>
          <target state="new">The engineering and selection of features is one part of the CAP process outlined in the <bpt id="p1">[</bpt>What is the Cortana Analytics Process?<ept id="p1">](machine-learning-data-science-the-cortana-analytics-process.md)</ept><ph id="ph3" /> Feature engineering and selection are parts of the <bpt id="p2">**</bpt>Develop features<ept id="p2">**</ept><ph id="ph4" /> step of the CAP.</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source><bpt id="p3">**</bpt>feature engineering<ept id="p3">**</ept>: This process attempts to create additional relevant features from the existing raw features in the data, and to increase predictive power to the learning algorithm.</source>
          <target state="new"><bpt id="p3">**</bpt>feature engineering<ept id="p3">**</ept>: This process attempts to create additional relevant features from the existing raw features in the data, and to increase predictive power to the learning algorithm.</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source><bpt id="p4">**</bpt>feature selection<ept id="p4">**</ept>: This process selects the key subset of original data features in an attempt to reduce the dimensionality of the training problem.</source>
          <target state="new"><bpt id="p4">**</bpt>feature selection<ept id="p4">**</ept>: This process selects the key subset of original data features in an attempt to reduce the dimensionality of the training problem.</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>Normally <bpt id="p5">**</bpt>feature engineering<ept id="p5">**</ept><ph id="ph5" /> is applied first to generate additional features, and then the <bpt id="p6">**</bpt>feature selection<ept id="p6">**</ept><ph id="ph6" /> step is performed to eliminate irrelevant, redundant, or highly correlated features.</source>
          <target state="new">Normally <bpt id="p5">**</bpt>feature engineering<ept id="p5">**</ept><ph id="ph5" /> is applied first to generate additional features, and then the <bpt id="p6">**</bpt>feature selection<ept id="p6">**</ept><ph id="ph6" /> step is performed to eliminate irrelevant, redundant, or highly correlated features.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>Filtering Features from Your Data - Feature Selection</source>
          <target state="new">Filtering Features from Your Data - Feature Selection</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>Feature selection is a process that is commonly applied for the construction of training datasets for predictive modeling tasks such as classification or regression tasks.</source>
          <target state="new">Feature selection is a process that is commonly applied for the construction of training datasets for predictive modeling tasks such as classification or regression tasks.</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>The goal is to select a subset of the features from the original dataset that reduce its dimensions by using a minimal set of features to represent the maximum amount of variance in the data.</source>
          <target state="new">The goal is to select a subset of the features from the original dataset that reduce its dimensions by using a minimal set of features to represent the maximum amount of variance in the data.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>This subset of features are, then, the only features to be included to train the model.</source>
          <target state="new">This subset of features are, then, the only features to be included to train the model.</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>Feature selection serves two main purposes.</source>
          <target state="new">Feature selection serves two main purposes.</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>First, feature selection often increases classification accuracy by eliminating irrelevant, redundant, or highly correlated features.</source>
          <target state="new">First, feature selection often increases classification accuracy by eliminating irrelevant, redundant, or highly correlated features.</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>Second, it decreases the number of features which makes model training process more efficient.</source>
          <target state="new">Second, it decreases the number of features which makes model training process more efficient.</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>This is particularly important for learners that are expensive to train such as support vector machines.</source>
          <target state="new">This is particularly important for learners that are expensive to train such as support vector machines.</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>Although feature selection does seek to reduce the number of features in the dataset used to train the model, it is not usually referred to by the term "dimensionality reduction".</source>
          <target state="new">Although feature selection does seek to reduce the number of features in the dataset used to train the model, it is not usually referred to by the term "dimensionality reduction".</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>Feature selection methods extract a subset of original features in the data without changing them.</source>
          <target state="new">Feature selection methods extract a subset of original features in the data without changing them.</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>Dimensionality reduction methods employ engineered features that can transform the original features and thus modify them.</source>
          <target state="new">Dimensionality reduction methods employ engineered features that can transform the original features and thus modify them.</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>Examples of dimensionality reduction methods include Principal Component Analysis, canonical correlation analysis, and Singular Value Decomposition.</source>
          <target state="new">Examples of dimensionality reduction methods include Principal Component Analysis, canonical correlation analysis, and Singular Value Decomposition.</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>Among others, one widely applied category of feature selection methods in a supervised context is called "filter based feature selection".</source>
          <target state="new">Among others, one widely applied category of feature selection methods in a supervised context is called "filter based feature selection".</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>By evaluating the correlation between each feature and the target attribute, these methods apply a statistical measure to assign a score to each feature.</source>
          <target state="new">By evaluating the correlation between each feature and the target attribute, these methods apply a statistical measure to assign a score to each feature.</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>The features are then ranked by the score, which may be used to help set the threshold for keeping or eliminating a specific feature.</source>
          <target state="new">The features are then ranked by the score, which may be used to help set the threshold for keeping or eliminating a specific feature.</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>Examples of the statistical measures used in these methods include Person correlation, mutual information, and the Chi squared test.</source>
          <target state="new">Examples of the statistical measures used in these methods include Person correlation, mutual information, and the Chi squared test.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>In Azure Machine Learning Studio, there are modules provided for feature selection.</source>
          <target state="new">In Azure Machine Learning Studio, there are modules provided for feature selection.</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>As shown in the following figure, these modules include [Filter-Based Feature Selection][filter-based-feature-selection] and [Fisher Linear Discriminant Analysis][fisher-linear-discriminant-analysis].</source>
          <target state="new">As shown in the following figure, these modules include [Filter-Based Feature Selection][filter-based-feature-selection] and [Fisher Linear Discriminant Analysis][fisher-linear-discriminant-analysis].</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source><ph id="ph7">![</ph>Feature selection example<ph id="ph8">](./media/machine-learning-data-science-select-features/feature-Selection.png)</ph></source>
          <target state="new"><ph id="ph7">![</ph>Feature selection example<ph id="ph8">](./media/machine-learning-data-science-select-features/feature-Selection.png)</ph></target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>Consider, for example, the use of the [Filter-Based Feature Selection][filter-based-feature-selection] module.</source>
          <target state="new">Consider, for example, the use of the [Filter-Based Feature Selection][filter-based-feature-selection] module.</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>For the purpose of convenience, we continue to use the text mining example outlined above.</source>
          <target state="new">For the purpose of convenience, we continue to use the text mining example outlined above.</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>Assume that we want to build a regression model after a set of 256 features are created through the [Feature Hashing][feature-hashing] module, and that the response variable is the "Col1" and represents a book review ratings ranging from 1 to 5.</source>
          <target state="new">Assume that we want to build a regression model after a set of 256 features are created through the [Feature Hashing][feature-hashing] module, and that the response variable is the "Col1" and represents a book review ratings ranging from 1 to 5.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>By setting "Feature scoring method" to be "Pearson Correlation", the "Target column" to be "Col1", and the "Number of desired features" to 50.</source>
          <target state="new">By setting "Feature scoring method" to be "Pearson Correlation", the "Target column" to be "Col1", and the "Number of desired features" to 50.</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>Then the module [Filter-Based Feature Selection][filter-based-feature-selection] will produce a dataset containing 50 features together with the target attribute "Col1".</source>
          <target state="new">Then the module [Filter-Based Feature Selection][filter-based-feature-selection] will produce a dataset containing 50 features together with the target attribute "Col1".</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>The following figure shows the flow of this experiment and the input parameters we just described.</source>
          <target state="new">The following figure shows the flow of this experiment and the input parameters we just described.</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source><ph id="ph9">![</ph>Feature selection example<ph id="ph10">](./media/machine-learning-data-science-select-features/feature-Selection1.png)</ph></source>
          <target state="new"><ph id="ph9">![</ph>Feature selection example<ph id="ph10">](./media/machine-learning-data-science-select-features/feature-Selection1.png)</ph></target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>The following figure shows the resulting datasets.</source>
          <target state="new">The following figure shows the resulting datasets.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>Each feature is scored based on the Pearson Correlation between itself and the target attribute "Col1".</source>
          <target state="new">Each feature is scored based on the Pearson Correlation between itself and the target attribute "Col1".</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>The features with top scores are kept.</source>
          <target state="new">The features with top scores are kept.</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source><ph id="ph11">![</ph>Feature selection example<ph id="ph12">](./media/machine-learning-data-science-select-features/feature-Selection2.png)</ph></source>
          <target state="new"><ph id="ph11">![</ph>Feature selection example<ph id="ph12">](./media/machine-learning-data-science-select-features/feature-Selection2.png)</ph></target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>The corresponding scores of the selected features are shown in the following figure.</source>
          <target state="new">The corresponding scores of the selected features are shown in the following figure.</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source><ph id="ph13">![</ph>Feature selection example<ph id="ph14">](./media/machine-learning-data-science-select-features/feature-Selection3.png)</ph></source>
          <target state="new"><ph id="ph13">![</ph>Feature selection example<ph id="ph14">](./media/machine-learning-data-science-select-features/feature-Selection3.png)</ph></target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>By applying this [Filter-Based Feature Selection][filter-based-feature-selection] module, 50 out of 256 features are selected because they have the most correlated features with the target variable "Col1", based on the scoring method "Pearson Correlation".</source>
          <target state="new">By applying this [Filter-Based Feature Selection][filter-based-feature-selection] module, 50 out of 256 features are selected because they have the most correlated features with the target variable "Col1", based on the scoring method "Pearson Correlation".</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>Conclusion</source>
          <target state="new">Conclusion</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>Feature engineering and feature selection are two commonly Engineered and selected features increase the efficiency of the training process which attempts to extract the key information contained in the data.</source>
          <target state="new">Feature engineering and feature selection are two commonly Engineered and selected features increase the efficiency of the training process which attempts to extract the key information contained in the data.</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>They also improve the power of these models to classify the input data accurately and to predict outcomes of interest more robustly.</source>
          <target state="new">They also improve the power of these models to classify the input data accurately and to predict outcomes of interest more robustly.</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>Feature engineering and selection can also combine to make the learning more computationally tractable.</source>
          <target state="new">Feature engineering and selection can also combine to make the learning more computationally tractable.</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>It does so by enhancing and then reducing the number of features needed to calibrate or train a model.</source>
          <target state="new">It does so by enhancing and then reducing the number of features needed to calibrate or train a model.</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>Mathematically speaking, the features selected to train the model are a minimal set of independent variables that explain the patterns in the data and then predict outcomes successfully.</source>
          <target state="new">Mathematically speaking, the features selected to train the model are a minimal set of independent variables that explain the patterns in the data and then predict outcomes successfully.</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>Note that it is not always necessarily to perform feature engineering or feature selection.</source>
          <target state="new">Note that it is not always necessarily to perform feature engineering or feature selection.</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>Whether it is needed or not depends on the data we have or collect, the algorithm we pick, and the objective of the experiment.</source>
          <target state="new">Whether it is needed or not depends on the data we have or collect, the algorithm we pick, and the objective of the experiment.</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>[feature-hashing]: https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/
[filter-based-feature-selection]: https://msdn.microsoft.com/library/azure/918b356b-045c-412b-aa12-94a1d2dad90f/
[fisher-linear-discriminant-analysis]: https://msdn.microsoft.com/library/azure/dcaab0b2-59ca-4bec-bb66-79fd23540080/</source>
          <target state="new">[feature-hashing]: https://msdn.microsoft.com/library/azure/c9a82660-2d9c-411d-8122-4d9e0b3ce92a/
[filter-based-feature-selection]: https://msdn.microsoft.com/library/azure/918b356b-045c-412b-aa12-94a1d2dad90f/
[fisher-linear-discriminant-analysis]: https://msdn.microsoft.com/library/azure/dcaab0b2-59ca-4bec-bb66-79fd23540080/</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">28024d0d883fe56dd4cf95c7cd6795d5910f0a70</xliffext:olfilehash>
  </header>
</xliff>