<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" source-language="en-US" target-language="fr-fr">
    <body>
      <group id="main" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Move data from an on-premise SQL Server to SQL Azure with Azure Data Factory | Azure</source>
          <target state="new">Move data from an on-premise SQL Server to SQL Azure with Azure Data Factory | Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Set up an ADF pipeline that composes two data migration activities that together move data on a daily basis between databases on-premise and in the cloud.</source>
          <target state="new">Set up an ADF pipeline that composes two data migration activities that together move data on a daily basis between databases on-premise and in the cloud.</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Move data from an on-premise SQL server to SQL Azure with Azure Data Factory</source>
          <target state="new">Move data from an on-premise SQL server to SQL Azure with Azure Data Factory</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>This topic shows how to move data from an on-premise SQL Server Database to a SQL Azure Database via Azure Blob Storage using the Azure Data Factory (ADF).</source>
          <target state="new">This topic shows how to move data from an on-premise SQL Server Database to a SQL Azure Database via Azure Blob Storage using the Azure Data Factory (ADF).</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>menu<ept id="p1">**</ept><ph id="ph2" /> below links to topics that describe how to ingest data into other target environments where the data can be stored and processed during the Cortana Analytics Process (CAPS).</source>
          <target state="new">The <bpt id="p1">**</bpt>menu<ept id="p1">**</ept><ph id="ph2" /> below links to topics that describe how to ingest data into other target environments where the data can be stored and processed during the Cortana Analytics Process (CAPS).</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>Introduction: What is ADF and when should it be used to migrate data?</source>
          <target state="new">Introduction: What is ADF and when should it be used to migrate data?</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>Azure Data Factory is a fully managed cloud-based data integration service that orchestrates and automates the movement and transformation of data.</source>
          <target state="new">Azure Data Factory is a fully managed cloud-based data integration service that orchestrates and automates the movement and transformation of data.</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>The key concept in the ADF model is pipeline.</source>
          <target state="new">The key concept in the ADF model is pipeline.</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source>A pipelines is a logical groupings of Activities, each of which defines the actions to perform on the data contained in Datasets.</source>
          <target state="new">A pipelines is a logical groupings of Activities, each of which defines the actions to perform on the data contained in Datasets.</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>Linked services are used to define the information needed for Data Factory to connect to the data resources.</source>
          <target state="new">Linked services are used to define the information needed for Data Factory to connect to the data resources.</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source>With ADF, existing data processing services can be composed into data pipelines that are highly available and managed in the cloud.</source>
          <target state="new">With ADF, existing data processing services can be composed into data pipelines that are highly available and managed in the cloud.</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>These data pipelines can be scheduled to ingest, prepare, transform, analyze, and publish data, and ADF will manage and orchestrate all of the complex data and processing dependencies.</source>
          <target state="new">These data pipelines can be scheduled to ingest, prepare, transform, analyze, and publish data, and ADF will manage and orchestrate all of the complex data and processing dependencies.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>Solutions can be quickly built and deployed in the cloud, connecting a growing number of on-premises and cloud data sources.</source>
          <target state="new">Solutions can be quickly built and deployed in the cloud, connecting a growing number of on-premises and cloud data sources.</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>Consider using ADF when data needs to be continually migrated in a hybrid scenario that accesses both on-premise and cloud resources, and when the data is transacted or needs to be modified or have business logic added to it in the course of being migrated.</source>
          <target state="new">Consider using ADF when data needs to be continually migrated in a hybrid scenario that accesses both on-premise and cloud resources, and when the data is transacted or needs to be modified or have business logic added to it in the course of being migrated.</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>ADF allows for the scheduling and monitoring of jobs using simple JSON scripts that manage the movement of data on a periodic basis.</source>
          <target state="new">ADF allows for the scheduling and monitoring of jobs using simple JSON scripts that manage the movement of data on a periodic basis.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>ADF also has other capabilities such as support for complex operations.</source>
          <target state="new">ADF also has other capabilities such as support for complex operations.</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>For more information on ADF, see the documentation at <bpt id="p2">[</bpt>Azure Data Factory (ADF)<ept id="p2">](https://azure.microsoft.com/services/data-factory/)</ept>.</source>
          <target state="new">For more information on ADF, see the documentation at <bpt id="p2">[</bpt>Azure Data Factory (ADF)<ept id="p2">](https://azure.microsoft.com/services/data-factory/)</ept>.</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>The Scenario</source>
          <target state="new">The Scenario</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>We set up an ADF pipeline that composes two data migration activities that together move data on a daily basis between an on-premise SQL database and an Azure SQL Database in the cloud.</source>
          <target state="new">We set up an ADF pipeline that composes two data migration activities that together move data on a daily basis between an on-premise SQL database and an Azure SQL Database in the cloud.</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>The two activities are:</source>
          <target state="new">The two activities are:</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>copy data from an on-premise SQL Server database to an Azure Blob Storage account</source>
          <target state="new">copy data from an on-premise SQL Server database to an Azure Blob Storage account</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>copy data from the Azure Blob Storage account to an Azure SQL Database.</source>
          <target state="new">copy data from the Azure Blob Storage account to an Azure SQL Database.</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source><bpt id="p3">**</bpt>Reference<ept id="p3">**</ept>: The steps shown here have been adapted from the more detailed tutorial <bpt id="p4">[</bpt>Enable your pipelines to work with on-premises data<ept id="p4">](data-factory-use-onpremises-datasources.md)</ept><ph id="ph4" /> provided by the ADF team and references to the relevant sections of that topic are provided when appropriate.</source>
          <target state="new"><bpt id="p3">**</bpt>Reference<ept id="p3">**</ept>: The steps shown here have been adapted from the more detailed tutorial <bpt id="p4">[</bpt>Enable your pipelines to work with on-premises data<ept id="p4">](data-factory-use-onpremises-datasources.md)</ept><ph id="ph4" /> provided by the ADF team and references to the relevant sections of that topic are provided when appropriate.</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>Prerequisites</source>
          <target state="new">Prerequisites</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>This tutorial assumes you have:</source>
          <target state="new">This tutorial assumes you have:</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>An <bpt id="p5">**</bpt>Azure subscription<ept id="p5">**</ept>.</source>
          <target state="new">An <bpt id="p5">**</bpt>Azure subscription<ept id="p5">**</ept>.</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>If you do not have a subscription, you can sign up for a <bpt id="p6">[</bpt>free trial<ept id="p6">](https://azure.microsoft.com/pricing/free-trial/)</ept>.</source>
          <target state="new">If you do not have a subscription, you can sign up for a <bpt id="p6">[</bpt>free trial<ept id="p6">](https://azure.microsoft.com/pricing/free-trial/)</ept>.</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>An <bpt id="p7">**</bpt>Azure storage account<ept id="p7">**</ept>.</source>
          <target state="new">An <bpt id="p7">**</bpt>Azure storage account<ept id="p7">**</ept>.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>You will use an Azure storage account for storing the data in this tutorial.</source>
          <target state="new">You will use an Azure storage account for storing the data in this tutorial.</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>If you don't have an Azure storage account, see the <bpt id="p8">[</bpt>Create a storage account<ept id="p8">](storage-create-storage-account.md#create-a-storage-account)</ept><ph id="ph5" /> article.</source>
          <target state="new">If you don't have an Azure storage account, see the <bpt id="p8">[</bpt>Create a storage account<ept id="p8">](storage-create-storage-account.md#create-a-storage-account)</ept><ph id="ph5" /> article.</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>After you have created the storage account, you will need to obtain the account key used to access the storage.</source>
          <target state="new">After you have created the storage account, you will need to obtain the account key used to access the storage.</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>See <bpt id="p9">[</bpt>View, copy and regenerate storage access keys<ept id="p9">](storage-create-storage-account.md#view-copy-and-regenerate-storage-access-keys)</ept>.</source>
          <target state="new">See <bpt id="p9">[</bpt>View, copy and regenerate storage access keys<ept id="p9">](storage-create-storage-account.md#view-copy-and-regenerate-storage-access-keys)</ept>.</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>Access to an <bpt id="p10">**</bpt>Azure SQL Database<ept id="p10">**</ept>.</source>
          <target state="new">Access to an <bpt id="p10">**</bpt>Azure SQL Database<ept id="p10">**</ept>.</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>If you must setup an Azure SQL Database, <bpt id="p11">[</bpt>Getting Started with Microsoft Azure SQL Database <ept id="p11">](sql-database-get-started.md)</ept><ph id="ph6" /> provides information on how to provision a new instance of a Azure SQL Database.</source>
          <target state="new">If you must setup an Azure SQL Database, <bpt id="p11">[</bpt>Getting Started with Microsoft Azure SQL Database <ept id="p11">](sql-database-get-started.md)</ept><ph id="ph6" /> provides information on how to provision a new instance of a Azure SQL Database.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>Installed and configured <bpt id="p12">**</bpt>Azure PowerShell<ept id="p12">**</ept><ph id="ph7" /> locally.</source>
          <target state="new">Installed and configured <bpt id="p12">**</bpt>Azure PowerShell<ept id="p12">**</ept><ph id="ph7" /> locally.</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>For instructions, see <bpt id="p13">[</bpt>How to install and configure Azure PowerShell<ept id="p13">](powershell-install-configure.md)</ept>.</source>
          <target state="new">For instructions, see <bpt id="p13">[</bpt>How to install and configure Azure PowerShell<ept id="p13">](powershell-install-configure.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source><ph id="ph8">[AZURE.NOTE]</ph><ph id="ph9" /> This procedure uses the <bpt id="p14">[</bpt>Azure Portal<ept id="p14">](https://ms.portal.azure.com/)</ept>.</source>
          <target state="new"><ph id="ph8">[AZURE.NOTE]</ph><ph id="ph9" /> This procedure uses the <bpt id="p14">[</bpt>Azure Portal<ept id="p14">](https://ms.portal.azure.com/)</ept>.</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>Upload the data to your on-premise SQL Server</source>
          <target state="new">Upload the data to your on-premise SQL Server</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>We use the <bpt id="p15">[</bpt>NYC Taxi dataset<ept id="p15">](http://chriswhong.com/open-data/foil_nyc_taxi/)</ept><ph id="ph10" /> to demonstrate the migration process.</source>
          <target state="new">We use the <bpt id="p15">[</bpt>NYC Taxi dataset<ept id="p15">](http://chriswhong.com/open-data/foil_nyc_taxi/)</ept><ph id="ph10" /> to demonstrate the migration process.</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>The NYC Taxi dataset is available, as noted that post, on Azure blob storage <bpt id="p16">[</bpt>NYC Taxi Data<ept id="p16">](http://www.andresmh.com/nyctaxitrips/)</ept>.</source>
          <target state="new">The NYC Taxi dataset is available, as noted that post, on Azure blob storage <bpt id="p16">[</bpt>NYC Taxi Data<ept id="p16">](http://www.andresmh.com/nyctaxitrips/)</ept>.</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>The data has two files, the trip_data.csv file which contains trip details and the  trip_far.csv file which contains details of the fare paid for each trip.</source>
          <target state="new">The data has two files, the trip_data.csv file which contains trip details and the  trip_far.csv file which contains details of the fare paid for each trip.</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>A sample and description of these files are provided in <bpt id="p17">[</bpt>NYC Taxi Trips Dataset Description<ept id="p17">](machine-learning-data-science-process-sql-walkthrough.md#dataset)</ept>.</source>
          <target state="new">A sample and description of these files are provided in <bpt id="p17">[</bpt>NYC Taxi Trips Dataset Description<ept id="p17">](machine-learning-data-science-process-sql-walkthrough.md#dataset)</ept>.</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>You can either adapt the procedure provided here to a set of your own data or follow the steps as described by using the NYC Taxi dataset.</source>
          <target state="new">You can either adapt the procedure provided here to a set of your own data or follow the steps as described by using the NYC Taxi dataset.</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>To upload the NYC Taxi dataset into your on-premise SQL Server database, follow the procedure outlined in <bpt id="p18">[</bpt>Bulk Import Data into SQL Server Database<ept id="p18">](machine-learning-data-science-process-sql-walkthrough.md#dbload)</ept>.</source>
          <target state="new">To upload the NYC Taxi dataset into your on-premise SQL Server database, follow the procedure outlined in <bpt id="p18">[</bpt>Bulk Import Data into SQL Server Database<ept id="p18">](machine-learning-data-science-process-sql-walkthrough.md#dbload)</ept>.</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>These instructions are for a SQL Server on an Azure Virtual Machine, but the procedure for uploading to the on-premise SQL Server is the same.</source>
          <target state="new">These instructions are for a SQL Server on an Azure Virtual Machine, but the procedure for uploading to the on-premise SQL Server is the same.</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>Create an Azure Data Factory</source>
          <target state="new">Create an Azure Data Factory</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>The instructions for creating a new Azure Data Factory and a resource group in the <bpt id="p19">[</bpt>Azure Portal<ept id="p19">](https://ms.portal.azure.com/)</ept><ph id="ph11" /> are provided <bpt id="p20">[</bpt>Create an Azure Data Factory<ept id="p20">](data-factory-build-your-first-pipeline-using-editor.md#step-1-creating-the-data-factory)</ept>.</source>
          <target state="new">The instructions for creating a new Azure Data Factory and a resource group in the <bpt id="p19">[</bpt>Azure Portal<ept id="p19">](https://ms.portal.azure.com/)</ept><ph id="ph11" /> are provided <bpt id="p20">[</bpt>Create an Azure Data Factory<ept id="p20">](data-factory-build-your-first-pipeline-using-editor.md#step-1-creating-the-data-factory)</ept>.</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>Name the new ADF instance <bpt id="p21">*</bpt>adfdsp<ept id="p21">*</ept><ph id="ph12" /> and name the resource group created <bpt id="p22">*</bpt>adfdsprg<ept id="p22">*</ept>.</source>
          <target state="new">Name the new ADF instance <bpt id="p21">*</bpt>adfdsp<ept id="p21">*</ept><ph id="ph12" /> and name the resource group created <bpt id="p22">*</bpt>adfdsprg<ept id="p22">*</ept>.</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>Install and configure up the Data Management Gateway</source>
          <target state="new">Install and configure up the Data Management Gateway</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>To enable your pipelines in an Azure data factory to work with an on-premise SQL Server, you need to add it as a linked service to the data factory.</source>
          <target state="new">To enable your pipelines in an Azure data factory to work with an on-premise SQL Server, you need to add it as a linked service to the data factory.</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>To create a Linked Service for the on-premise SQL Server, you must first download and install Microsoft Data Management Gateway onto the on-premise computer and configure the linked service for the on-premises data source to use the gateway.</source>
          <target state="new">To create a Linked Service for the on-premise SQL Server, you must first download and install Microsoft Data Management Gateway onto the on-premise computer and configure the linked service for the on-premises data source to use the gateway.</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>The Data Management Gateway serializes and deserializes the source and sink data on the computer where it is hosted.</source>
          <target state="new">The Data Management Gateway serializes and deserializes the source and sink data on the computer where it is hosted.</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source>For set-up instructions and details on Data Management Gateway, see <bpt id="p23">[</bpt>Enable your pipelines to work with on-premises data<ept id="p23">](data-factory-use-onpremises-datasources.md)</ept></source>
          <target state="new">For set-up instructions and details on Data Management Gateway, see <bpt id="p23">[</bpt>Enable your pipelines to work with on-premises data<ept id="p23">](data-factory-use-onpremises-datasources.md)</ept></target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>Create linked services to connect to the data resources</source>
          <target state="new">Create linked services to connect to the data resources</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>A linked service defines the information needed for Azure Data Factory to connect to a data resource.</source>
          <target state="new">A linked service defines the information needed for Azure Data Factory to connect to a data resource.</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>The step-by-step procedure for creating linked services is provided in <bpt id="p24">[</bpt>Create linked services<ept id="p24">](data-factory-use-onpremises-datasources.md#step-2-create-linked-services)</ept>.</source>
          <target state="new">The step-by-step procedure for creating linked services is provided in <bpt id="p24">[</bpt>Create linked services<ept id="p24">](data-factory-use-onpremises-datasources.md#step-2-create-linked-services)</ept>.</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>We have three resources in this scenario for which linked services are needed.</source>
          <target state="new">We have three resources in this scenario for which linked services are needed.</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source><bpt id="p25">[</bpt>Linked service for on-premise SQL Server<ept id="p25">](#adf-linked-service-onprem-sql)</ept></source>
          <target state="new"><bpt id="p25">[</bpt>Linked service for on-premise SQL Server<ept id="p25">](#adf-linked-service-onprem-sql)</ept></target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source><bpt id="p26">[</bpt>Linked service for Azure Blob Storage<ept id="p26">](#adf-linked-service-blob-store)</ept></source>
          <target state="new"><bpt id="p26">[</bpt>Linked service for Azure Blob Storage<ept id="p26">](#adf-linked-service-blob-store)</ept></target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source><bpt id="p27">[</bpt>Linked service for Azure SQL database<ept id="p27">](#adf-linked-service-azure-sql)</ept></source>
          <target state="new"><bpt id="p27">[</bpt>Linked service for Azure SQL database<ept id="p27">](#adf-linked-service-azure-sql)</ept></target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source><ph id="ph13">&lt;a name="adf-linked-service-onprem-sql"&gt;</ph><ph id="ph14">&lt;/a&gt;</ph>Linked service for on-premise SQL Server database
To create the linked service for the on-premise SQL Server, click on the <bpt id="p28">**</bpt>Data Store<ept id="p28">**</ept><ph id="ph15" /> in the ADF landing page on Azure Classic Portal, select <bpt id="p29">*</bpt>SQL<ept id="p29">*</ept><ph id="ph16" /> and enter the credentials for the <bpt id="p30">*</bpt>username<ept id="p30">*</ept><ph id="ph17" /> and <bpt id="p31">*</bpt>password<ept id="p31">*</ept><ph id="ph18" /> for the on-premise SQL Server.</source>
          <target state="new"><ph id="ph13">&lt;a name="adf-linked-service-onprem-sql"&gt;</ph><ph id="ph14">&lt;/a&gt;</ph>Linked service for on-premise SQL Server database
To create the linked service for the on-premise SQL Server, click on the <bpt id="p28">**</bpt>Data Store<ept id="p28">**</ept><ph id="ph15" /> in the ADF landing page on Azure Classic Portal, select <bpt id="p29">*</bpt>SQL<ept id="p29">*</ept><ph id="ph16" /> and enter the credentials for the <bpt id="p30">*</bpt>username<ept id="p30">*</ept><ph id="ph17" /> and <bpt id="p31">*</bpt>password<ept id="p31">*</ept><ph id="ph18" /> for the on-premise SQL Server.</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>You need to enter the servername as a <bpt id="p32">**</bpt>fully qualified servername backslash instance name (servername\instancename)<ept id="p32">**</ept>.</source>
          <target state="new">You need to enter the servername as a <bpt id="p32">**</bpt>fully qualified servername backslash instance name (servername\instancename)<ept id="p32">**</ept>.</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>Name the linked service <bpt id="p33">*</bpt>adfonpremsql<ept id="p33">*</ept>.</source>
          <target state="new">Name the linked service <bpt id="p33">*</bpt>adfonpremsql<ept id="p33">*</ept>.</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source><ph id="ph19">&lt;a name="adf-linked-service-blob-store"&gt;</ph><ph id="ph20">&lt;/a&gt;</ph>Linked service for Blob
To create the linked service for the Azure Blob Storage account, click on the <bpt id="p34">**</bpt>Data Store<ept id="p34">**</ept><ph id="ph21" /> in the ADF landing page on Azure Classic Portal, select <bpt id="p35">*</bpt>Azure Storage Account<ept id="p35">*</ept><ph id="ph22" /> and enter the Azure Blob Storage account key and container name.</source>
          <target state="new"><ph id="ph19">&lt;a name="adf-linked-service-blob-store"&gt;</ph><ph id="ph20">&lt;/a&gt;</ph>Linked service for Blob
To create the linked service for the Azure Blob Storage account, click on the <bpt id="p34">**</bpt>Data Store<ept id="p34">**</ept><ph id="ph21" /> in the ADF landing page on Azure Classic Portal, select <bpt id="p35">*</bpt>Azure Storage Account<ept id="p35">*</ept><ph id="ph22" /> and enter the Azure Blob Storage account key and container name.</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source>Name the link service <bpt id="p36">*</bpt>adfds<ept id="p36">*</ept>.</source>
          <target state="new">Name the link service <bpt id="p36">*</bpt>adfds<ept id="p36">*</ept>.</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source><ph id="ph23">&lt;a name="adf-linked-service-azure-sql"&gt;</ph><ph id="ph24">&lt;/a&gt;</ph>Linked service for Azure SQL database
To create the linked service for the Azure SQL Database, click on the <bpt id="p37">**</bpt>Data Store<ept id="p37">**</ept><ph id="ph25" /> in the ADF landing page on Azure Classic Portal, select <bpt id="p38">*</bpt>Azure SQL<ept id="p38">*</ept><ph id="ph26" /> and enter the credentials for the <bpt id="p39">*</bpt>username<ept id="p39">*</ept><ph id="ph27" /> and <bpt id="p40">*</bpt>password<ept id="p40">*</ept><ph id="ph28" /> for the Azure SQL Database.</source>
          <target state="new"><ph id="ph23">&lt;a name="adf-linked-service-azure-sql"&gt;</ph><ph id="ph24">&lt;/a&gt;</ph>Linked service for Azure SQL database
To create the linked service for the Azure SQL Database, click on the <bpt id="p37">**</bpt>Data Store<ept id="p37">**</ept><ph id="ph25" /> in the ADF landing page on Azure Classic Portal, select <bpt id="p38">*</bpt>Azure SQL<ept id="p38">*</ept><ph id="ph26" /> and enter the credentials for the <bpt id="p39">*</bpt>username<ept id="p39">*</ept><ph id="ph27" /> and <bpt id="p40">*</bpt>password<ept id="p40">*</ept><ph id="ph28" /> for the Azure SQL Database.</target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source>The <bpt id="p41">*</bpt>username<ept id="p41">*</ept><ph id="ph29" /> must be specified as <bpt id="p42">*</bpt>user@servername<ept id="p42">*</ept>.</source>
          <target state="new">The <bpt id="p41">*</bpt>username<ept id="p41">*</ept><ph id="ph29" /> must be specified as <bpt id="p42">*</bpt>user@servername<ept id="p42">*</ept>.</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>Define and create tables to specify how to access the datasets</source>
          <target state="new">Define and create tables to specify how to access the datasets</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source>Create tables that specify the structure, location and availability of the datasets with the following script-based procedures.</source>
          <target state="new">Create tables that specify the structure, location and availability of the datasets with the following script-based procedures.</target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source>JSON files are used to define the tables.</source>
          <target state="new">JSON files are used to define the tables.</target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source>For more information on the structure of these files, see <bpt id="p43">[</bpt>Datasets<ept id="p43">](data-factory-create-datasets.md)</ept>.</source>
          <target state="new">For more information on the structure of these files, see <bpt id="p43">[</bpt>Datasets<ept id="p43">](data-factory-create-datasets.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source><ph id="ph30">[AZURE.NOTE]</ph><ph id="ph31" />  You should execute the <ph id="ph32">`Add-AzureAccount`</ph><ph id="ph33" /> cmdlet prior to executing the <bpt id="p44">[</bpt>New-AzureDataFactoryTable<ept id="p44">](https://msdn.microsoft.com/library/azure/dn835096.aspx)</ept><ph id="ph34" /> cmdlet to confirm that the right Azure subscription is selected for the command execution.</source>
          <target state="new"><ph id="ph30">[AZURE.NOTE]</ph><ph id="ph31" />  You should execute the <ph id="ph32">`Add-AzureAccount`</ph><ph id="ph33" /> cmdlet prior to executing the <bpt id="p44">[</bpt>New-AzureDataFactoryTable<ept id="p44">](https://msdn.microsoft.com/library/azure/dn835096.aspx)</ept><ph id="ph34" /> cmdlet to confirm that the right Azure subscription is selected for the command execution.</target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source>For documentation of this cmdlet, see <bpt id="p45">[</bpt>Add-AzureAccount<ept id="p45">](https://msdn.microsoft.com/library/azure/dn790372.aspx)</ept>.</source>
          <target state="new">For documentation of this cmdlet, see <bpt id="p45">[</bpt>Add-AzureAccount<ept id="p45">](https://msdn.microsoft.com/library/azure/dn790372.aspx)</ept>.</target>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source>The JSON-based definitions in the tables use the following names:</source>
          <target state="new">The JSON-based definitions in the tables use the following names:</target>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source>the <bpt id="p46">**</bpt>table name<ept id="p46">**</ept><ph id="ph35" /> in the on-premise SQL server is <bpt id="p47">*</bpt>nyctaxi_data<ept id="p47">*</ept></source>
          <target state="new">the <bpt id="p46">**</bpt>table name<ept id="p46">**</ept><ph id="ph35" /> in the on-premise SQL server is <bpt id="p47">*</bpt>nyctaxi_data<ept id="p47">*</ept></target>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source>the <bpt id="p48">**</bpt>container name<ept id="p48">**</ept><ph id="ph36" /> in the Azure Blob Storage account is <bpt id="p49">*</bpt>containername<ept id="p49">*</ept></source>
          <target state="new">the <bpt id="p48">**</bpt>container name<ept id="p48">**</ept><ph id="ph36" /> in the Azure Blob Storage account is <bpt id="p49">*</bpt>containername<ept id="p49">*</ept></target>
        </trans-unit>
        <trans-unit id="180" translate="yes" xml:space="preserve">
          <source>Three table definitions are needed for this ADF pipeline:</source>
          <target state="new">Three table definitions are needed for this ADF pipeline:</target>
        </trans-unit>
        <trans-unit id="181" translate="yes" xml:space="preserve">
          <source><bpt id="p50">[</bpt>SQL on-premise Table<ept id="p50">](#adf-table-onprem-sql)</ept></source>
          <target state="new"><bpt id="p50">[</bpt>SQL on-premise Table<ept id="p50">](#adf-table-onprem-sql)</ept></target>
        </trans-unit>
        <trans-unit id="182" translate="yes" xml:space="preserve">
          <source><bpt id="p51">[</bpt>Blob Table <ept id="p51">](#adf-table-blob-store)</ept></source>
          <target state="new"><bpt id="p51">[</bpt>Blob Table <ept id="p51">](#adf-table-blob-store)</ept></target>
        </trans-unit>
        <trans-unit id="183" translate="yes" xml:space="preserve">
          <source><bpt id="p52">[</bpt>SQL Azure Table<ept id="p52">](#adf-table-azure-sql)</ept></source>
          <target state="new"><bpt id="p52">[</bpt>SQL Azure Table<ept id="p52">](#adf-table-azure-sql)</ept></target>
        </trans-unit>
        <trans-unit id="184" translate="yes" xml:space="preserve">
          <source><ph id="ph37">[AZURE.NOTE]</ph><ph id="ph38" />  The following procedures use Azure PowerShell to define and create the ADF activities.</source>
          <target state="new"><ph id="ph37">[AZURE.NOTE]</ph><ph id="ph38" />  The following procedures use Azure PowerShell to define and create the ADF activities.</target>
        </trans-unit>
        <trans-unit id="185" translate="yes" xml:space="preserve">
          <source>But these tasks can also be accomplished using the Azure Portal.</source>
          <target state="new">But these tasks can also be accomplished using the Azure Portal.</target>
        </trans-unit>
        <trans-unit id="186" translate="yes" xml:space="preserve">
          <source>For details, see <bpt id="p53">[</bpt>Create input and output datasets<ept id="p53">](data-factory-use-onpremises-datasources.md#step-3-create-input-and-output-datasets)</ept>.</source>
          <target state="new">For details, see <bpt id="p53">[</bpt>Create input and output datasets<ept id="p53">](data-factory-use-onpremises-datasources.md#step-3-create-input-and-output-datasets)</ept>.</target>
        </trans-unit>
        <trans-unit id="188" translate="yes" xml:space="preserve">
          <source>SQL on-premise Table</source>
          <target state="new">SQL on-premise Table</target>
        </trans-unit>
        <trans-unit id="189" translate="yes" xml:space="preserve">
          <source>The table definition for the on-premise SQL Server is specified in the following JSON file.</source>
          <target state="new">The table definition for the on-premise SQL Server is specified in the following JSON file.</target>
        </trans-unit>
        <trans-unit id="190" translate="yes" xml:space="preserve">
          <source>Note that the column names were not included here, you can sub-select on the column names by including them here (for details please check the <bpt id="p54">[</bpt>ADF documentation<ept id="p54">](data-factory-data-movement-activities.md )</ept>).</source>
          <target state="new">Note that the column names were not included here, you can sub-select on the column names by including them here (for details please check the <bpt id="p54">[</bpt>ADF documentation<ept id="p54">](data-factory-data-movement-activities.md )</ept>).</target>
        </trans-unit>
        <trans-unit id="191" translate="yes" xml:space="preserve">
          <source>Copy the JSON definition of the table into a file called <bpt id="p55">*</bpt>onpremtabledef.json<ept id="p55">*</ept><ph id="ph39" /> file and save it to a known location (here assumed to be <bpt id="p56">*</bpt>C:\temp\onpremtabledef.json<ept id="p56">*</ept>).</source>
          <target state="new">Copy the JSON definition of the table into a file called <bpt id="p55">*</bpt>onpremtabledef.json<ept id="p55">*</ept><ph id="ph39" /> file and save it to a known location (here assumed to be <bpt id="p56">*</bpt>C:\temp\onpremtabledef.json<ept id="p56">*</ept>).</target>
        </trans-unit>
        <trans-unit id="192" translate="yes" xml:space="preserve">
          <source>Create the table in ADF with the following Azure PowerShell cmdlet.</source>
          <target state="new">Create the table in ADF with the following Azure PowerShell cmdlet.</target>
        </trans-unit>
        <trans-unit id="194" translate="yes" xml:space="preserve">
          <source>Blob Table</source>
          <target state="new">Blob Table</target>
        </trans-unit>
        <trans-unit id="195" translate="yes" xml:space="preserve">
          <source>Definition for the table for the output blob location is in the following (this maps the ingested data from on-premise to Azure blob):</source>
          <target state="new">Definition for the table for the output blob location is in the following (this maps the ingested data from on-premise to Azure blob):</target>
        </trans-unit>
        <trans-unit id="196" translate="yes" xml:space="preserve">
          <source>Copy the JSON definition of the table into a file called <bpt id="p57">*</bpt>bloboutputtabledef.json<ept id="p57">*</ept><ph id="ph40" /> file and save it to a known location (here assumed to be <bpt id="p58">*</bpt>C:\temp\bloboutputtabledef.json<ept id="p58">*</ept>).</source>
          <target state="new">Copy the JSON definition of the table into a file called <bpt id="p57">*</bpt>bloboutputtabledef.json<ept id="p57">*</ept><ph id="ph40" /> file and save it to a known location (here assumed to be <bpt id="p58">*</bpt>C:\temp\bloboutputtabledef.json<ept id="p58">*</ept>).</target>
        </trans-unit>
        <trans-unit id="197" translate="yes" xml:space="preserve">
          <source>Create the table in ADF with the following Azure PowerShell cmdlet.</source>
          <target state="new">Create the table in ADF with the following Azure PowerShell cmdlet.</target>
        </trans-unit>
        <trans-unit id="199" translate="yes" xml:space="preserve">
          <source>SQL Azure Table</source>
          <target state="new">SQL Azure Table</target>
        </trans-unit>
        <trans-unit id="200" translate="yes" xml:space="preserve">
          <source>Definition for the table for the SQL Azure output is in the following (this schema maps the data coming from the blob):</source>
          <target state="new">Definition for the table for the SQL Azure output is in the following (this schema maps the data coming from the blob):</target>
        </trans-unit>
        <trans-unit id="201" translate="yes" xml:space="preserve">
          <source>Copy the JSON definition of the table into a file called <bpt id="p59">*</bpt>AzureSqlTable.json<ept id="p59">*</ept><ph id="ph41" /> file and save it to a known location (here assumed to be <bpt id="p60">*</bpt>C:\temp\AzureSqlTable.json<ept id="p60">*</ept>).</source>
          <target state="new">Copy the JSON definition of the table into a file called <bpt id="p59">*</bpt>AzureSqlTable.json<ept id="p59">*</ept><ph id="ph41" /> file and save it to a known location (here assumed to be <bpt id="p60">*</bpt>C:\temp\AzureSqlTable.json<ept id="p60">*</ept>).</target>
        </trans-unit>
        <trans-unit id="202" translate="yes" xml:space="preserve">
          <source>Create the table in ADF with the following Azure PowerShell cmdlet.</source>
          <target state="new">Create the table in ADF with the following Azure PowerShell cmdlet.</target>
        </trans-unit>
        <trans-unit id="204" translate="yes" xml:space="preserve">
          <source>Define and create the pipeline</source>
          <target state="new">Define and create the pipeline</target>
        </trans-unit>
        <trans-unit id="205" translate="yes" xml:space="preserve">
          <source>Specify the activities that belong to the pipeline and create the pipeline with the following script-based procedures.</source>
          <target state="new">Specify the activities that belong to the pipeline and create the pipeline with the following script-based procedures.</target>
        </trans-unit>
        <trans-unit id="206" translate="yes" xml:space="preserve">
          <source>A JSON file is used to define the pipeline properties.</source>
          <target state="new">A JSON file is used to define the pipeline properties.</target>
        </trans-unit>
        <trans-unit id="207" translate="yes" xml:space="preserve">
          <source>The script assumes that the <bpt id="p61">**</bpt>pipeline name<ept id="p61">**</ept><ph id="ph42" /> is <bpt id="p62">*</bpt>AMLDSProcessPipeline<ept id="p62">*</ept>.</source>
          <target state="new">The script assumes that the <bpt id="p61">**</bpt>pipeline name<ept id="p61">**</ept><ph id="ph42" /> is <bpt id="p62">*</bpt>AMLDSProcessPipeline<ept id="p62">*</ept>.</target>
        </trans-unit>
        <trans-unit id="208" translate="yes" xml:space="preserve">
          <source>Also note that we set the periodicity of the pipeline to be executed on daily basis and use the default execution time for the job (12 am UTC).</source>
          <target state="new">Also note that we set the periodicity of the pipeline to be executed on daily basis and use the default execution time for the job (12 am UTC).</target>
        </trans-unit>
        <trans-unit id="209" translate="yes" xml:space="preserve">
          <source><ph id="ph43">[AZURE.NOTE]</ph><ph id="ph44" />  The following procedures use Azure PowerShell to define and create the ADF pipeline.</source>
          <target state="new"><ph id="ph43">[AZURE.NOTE]</ph><ph id="ph44" />  The following procedures use Azure PowerShell to define and create the ADF pipeline.</target>
        </trans-unit>
        <trans-unit id="210" translate="yes" xml:space="preserve">
          <source>But this task can also be accomplished using the Azure Portal.</source>
          <target state="new">But this task can also be accomplished using the Azure Portal.</target>
        </trans-unit>
        <trans-unit id="211" translate="yes" xml:space="preserve">
          <source>For details, see <bpt id="p63">[</bpt>Create and run a pipeline<ept id="p63">](../data-factory/data-factory-use-onpremises-datasources.md#step-4-create-and-run-a-pipeline)</ept>.</source>
          <target state="new">For details, see <bpt id="p63">[</bpt>Create and run a pipeline<ept id="p63">](../data-factory/data-factory-use-onpremises-datasources.md#step-4-create-and-run-a-pipeline)</ept>.</target>
        </trans-unit>
        <trans-unit id="212" translate="yes" xml:space="preserve">
          <source>Using the table definitions provided above, the pipeline definition for the ADF is specified as follows:</source>
          <target state="new">Using the table definitions provided above, the pipeline definition for the ADF is specified as follows:</target>
        </trans-unit>
        <trans-unit id="213" translate="yes" xml:space="preserve">
          <source>Copy this JSON definition of the pipeline into a file called <bpt id="p64">*</bpt>pipelinedef.json<ept id="p64">*</ept><ph id="ph45" /> file and save it to a known location (here assumed to be <bpt id="p65">*</bpt>C:\temp\pipelinedef.json<ept id="p65">*</ept>).</source>
          <target state="new">Copy this JSON definition of the pipeline into a file called <bpt id="p64">*</bpt>pipelinedef.json<ept id="p64">*</ept><ph id="ph45" /> file and save it to a known location (here assumed to be <bpt id="p65">*</bpt>C:\temp\pipelinedef.json<ept id="p65">*</ept>).</target>
        </trans-unit>
        <trans-unit id="214" translate="yes" xml:space="preserve">
          <source>Create the pipeline in ADF with the following Azure PowerShell cmdlet.</source>
          <target state="new">Create the pipeline in ADF with the following Azure PowerShell cmdlet.</target>
        </trans-unit>
        <trans-unit id="215" translate="yes" xml:space="preserve">
          <source>Confirm that you can see the pipeline on the ADF in the Azure Classic Portal show up as following (when you click on the diagram)</source>
          <target state="new">Confirm that you can see the pipeline on the ADF in the Azure Classic Portal show up as following (when you click on the diagram)</target>
        </trans-unit>
        <trans-unit id="217" translate="yes" xml:space="preserve">
          <source>Start the Pipeline</source>
          <target state="new">Start the Pipeline</target>
        </trans-unit>
        <trans-unit id="218" translate="yes" xml:space="preserve">
          <source>The pipeline can now be run using the following command:</source>
          <target state="new">The pipeline can now be run using the following command:</target>
        </trans-unit>
        <trans-unit id="219" translate="yes" xml:space="preserve">
          <source>The <bpt id="p66">*</bpt>startdate<ept id="p66">*</ept><ph id="ph47" /> and <bpt id="p67">*</bpt>enddate<ept id="p67">*</ept><ph id="ph48" /> parameter values need to be replaced with the actual dates between which you want the pipeline to run.</source>
          <target state="new">The <bpt id="p66">*</bpt>startdate<ept id="p66">*</ept><ph id="ph47" /> and <bpt id="p67">*</bpt>enddate<ept id="p67">*</ept><ph id="ph48" /> parameter values need to be replaced with the actual dates between which you want the pipeline to run.</target>
        </trans-unit>
        <trans-unit id="220" translate="yes" xml:space="preserve">
          <source>Once the pipeline executes, you should be able to see the data show up in the container selected for the blob, one file per day.</source>
          <target state="new">Once the pipeline executes, you should be able to see the data show up in the container selected for the blob, one file per day.</target>
        </trans-unit>
        <trans-unit id="221" translate="yes" xml:space="preserve">
          <source>Note that we have not leveraged the functionality provided by ADF to pipe data incrementally.</source>
          <target state="new">Note that we have not leveraged the functionality provided by ADF to pipe data incrementally.</target>
        </trans-unit>
        <trans-unit id="222" translate="yes" xml:space="preserve">
          <source>For more details on how to do this and other capabilities provided by ADF, see the <bpt id="p68">[</bpt>ADF documentation<ept id="p68">](https://azure.microsoft.com/services/data-factory/)</ept>.</source>
          <target state="new">For more details on how to do this and other capabilities provided by ADF, see the <bpt id="p68">[</bpt>ADF documentation<ept id="p68">](https://azure.microsoft.com/services/data-factory/)</ept>.</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">3552e1462607989b78ff53474d7cbae5b16e374f</xliffext:olfilehash>
  </header>
</xliff>