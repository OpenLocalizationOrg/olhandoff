<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" original="markdown" source-language="en-US" target-language="fr-fr">
    <header>
      <tool tool-id="mdxliff" tool-name="mdxliff" tool-version="1.0-48076a9" tool-company="Microsoft" />
      <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">defebf0926bfdc9a7eb84c8c1cd6997124ad7288</xliffext:olfilehash>
      <xliffext:olfilepath xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">wdg-cpub-test\ndolci2\input-and-devices\speech-interactions.md</xliffext:olfilepath>
      <xliffext:oltranslationpriority xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">
      </xliffext:oltranslationpriority>
      <xliffext:oltranslationtype xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">Human Translation</xliffext:oltranslationtype>
    </header>
    <body>
      <group id="content" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Incorporate speech into your apps using Cortana voice commands, speech recognition, and speech synthesis.</source>
          <target state="new">Incorporate speech into your apps using Cortana voice commands, speech recognition, and speech synthesis.</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Speech interactions</source>
          <target state="new">Speech interactions</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Speech interactions</source>
          <target state="new">Speech interactions</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>\[ Updated for UWP apps on Windows 10.</source>
          <target state="new">\[ Updated for UWP apps on Windows 10.</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>For Windows 8.x articles, see the <bpt id="p1">[</bpt>archive<ept id="p1">](http://go.microsoft.com/fwlink/p/?linkid=619132)</ept> \]</source>
          <target state="new">For Windows 8.x articles, see the <bpt id="p1">[</bpt>archive<ept id="p1">](http://go.microsoft.com/fwlink/p/?linkid=619132)</ept> \]</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>Integrate speech recognition and text-to-speech (also known as TTS, or speech synthesis) directly into the user experience of your app.</source>
          <target state="new">Integrate speech recognition and text-to-speech (also known as TTS, or speech synthesis) directly into the user experience of your app.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>Other speech components</source>
          <target state="new">Other speech components</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source>See the <bpt id="p1">[</bpt>Cortana design guidelines<ept id="p1">](cortana-interactions.md)</ept> if you are exposing app functionality in the <bpt id="p2">**</bpt>Cortana<ept id="p2">**</ept> UI.</source>
          <target state="new">See the <bpt id="p1">[</bpt>Cortana design guidelines<ept id="p1">](cortana-interactions.md)</ept> if you are exposing app functionality in the <bpt id="p2">**</bpt>Cortana<ept id="p2">**</ept> UI.</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Speech recognition:  <ept id="p1">**</ept>converts words spoken by the user into text for form input, for text dictation, to specify an action or command, and to accomplish tasks.</source>
          <target state="new"><bpt id="p1">**</bpt>Speech recognition:  <ept id="p1">**</ept>converts words spoken by the user into text for form input, for text dictation, to specify an action or command, and to accomplish tasks.</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source>Both pre-defined grammars for free-text dictation and web search, and custom grammars authored using Speech Recognition Grammar Specification (SRGS) Version 1.0 are supported.</source>
          <target state="new">Both pre-defined grammars for free-text dictation and web search, and custom grammars authored using Speech Recognition Grammar Specification (SRGS) Version 1.0 are supported.</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>TTS:  <ept id="p1">**</ept>uses a speech synthesis engine (voice) to convert a text string into spoken words.</source>
          <target state="new"><bpt id="p1">**</bpt>TTS:  <ept id="p1">**</ept>uses a speech synthesis engine (voice) to convert a text string into spoken words.</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>The input string can be either basic, unadorned text or more complex Speech Synthesis Markup Language (SSML).</source>
          <target state="new">The input string can be either basic, unadorned text or more complex Speech Synthesis Markup Language (SSML).</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source>SSML provides a standard way to control characteristics of speech output, such as pronunciation, volume, pitch, rate or speed, and emphasis.</source>
          <target state="new">SSML provides a standard way to control characteristics of speech output, such as pronunciation, volume, pitch, rate or speed, and emphasis.</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Note<ept id="p1">**</ept>  Using <bpt id="p2">**</bpt>Cortana<ept id="p2">**</ept> and customized voice commands, your app can be launched in the foreground (the app takes focus, just as if it was launched from the Start menu) or activated as a background service (<bpt id="p3">**</bpt>Cortana<ept id="p3">**</ept> retains focus but provides results from the app).</source>
          <target state="new"><bpt id="p1">**</bpt>Note<ept id="p1">**</ept>  Using <bpt id="p2">**</bpt>Cortana<ept id="p2">**</ept> and customized voice commands, your app can be launched in the foreground (the app takes focus, just as if it was launched from the Start menu) or activated as a background service (<bpt id="p3">**</bpt>Cortana<ept id="p3">**</ept> retains focus but provides results from the app).</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>Commands that require additional context or user input (such as sending a message to a specific contact) are best handled in a foreground app, while basic commands can be handled in <bpt id="p1">**</bpt>Cortana<ept id="p1">**</ept> through a background app.</source>
          <target state="new">Commands that require additional context or user input (such as sending a message to a specific contact) are best handled in a foreground app, while basic commands can be handled in <bpt id="p1">**</bpt>Cortana<ept id="p1">**</ept> through a background app.</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source>If you are exposing functionality as a background service through voice commands in the <bpt id="p1">**</bpt>Cortana<ept id="p1">**</ept> UI, see the <bpt id="p2">[</bpt>Cortana design guidelines<ept id="p2">](cortana-design-guidelines.md)</ept>.</source>
          <target state="new">If you are exposing functionality as a background service through voice commands in the <bpt id="p1">**</bpt>Cortana<ept id="p1">**</ept> UI, see the <bpt id="p2">[</bpt>Cortana design guidelines<ept id="p2">](cortana-design-guidelines.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source>Designed and implemented thoughtfully, speech can be a robust and enjoyable way for people to interact with your app, complementing, or even replacing, keyboard, mouse, touch, and gestures.</source>
          <target state="new">Designed and implemented thoughtfully, speech can be a robust and enjoyable way for people to interact with your app, complementing, or even replacing, keyboard, mouse, touch, and gestures.</target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source>Speech interaction design</source>
          <target state="new">Speech interaction design</target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source>These guidelines and recommendations describe how to best integrate both speech recognition and TTS into the interaction experience of your app.</source>
          <target state="new">These guidelines and recommendations describe how to best integrate both speech recognition and TTS into the interaction experience of your app.</target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>If you are considering supporting speech interactions in your app:</source>
          <target state="new">If you are considering supporting speech interactions in your app:</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>What actions can be taken through speech?</source>
          <target state="new">What actions can be taken through speech?</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source>Can a user navigate between pages, invoke commands, or enter data as text fields, brief notes, or long messages?</source>
          <target state="new">Can a user navigate between pages, invoke commands, or enter data as text fields, brief notes, or long messages?</target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source>Is speech input a good option for completing a task?</source>
          <target state="new">Is speech input a good option for completing a task?</target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source>How does a user know when speech input is available?</source>
          <target state="new">How does a user know when speech input is available?</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source>Is the app always listening, or does the user need to take an action for the app to enter listening mode?</source>
          <target state="new">Is the app always listening, or does the user need to take an action for the app to enter listening mode?</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>What phrases initiate an action or behavior?</source>
          <target state="new">What phrases initiate an action or behavior?</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source>Do the phrases and actions need to be enumerated on screen?</source>
          <target state="new">Do the phrases and actions need to be enumerated on screen?</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>Are prompt, confirmation, and disambiguation screens or TTS required?</source>
          <target state="new">Are prompt, confirmation, and disambiguation screens or TTS required?</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source>What is the interaction dialog between app and user?</source>
          <target state="new">What is the interaction dialog between app and user?</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>Is a custom or constrained vocabulary required (such as medicine, science, or locale) for the context of your app?</source>
          <target state="new">Is a custom or constrained vocabulary required (such as medicine, science, or locale) for the context of your app?</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>Is network connectivity required?</source>
          <target state="new">Is network connectivity required?</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>Text input</source>
          <target state="new">Text input</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>Speech for text input can range from short form (single word or phrase) to long form (continuous dictation).</source>
          <target state="new">Speech for text input can range from short form (single word or phrase) to long form (continuous dictation).</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>Short form input must be less than 10 seconds in length, while long form input session can be up to two minutes in length.</source>
          <target state="new">Short form input must be less than 10 seconds in length, while long form input session can be up to two minutes in length.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>(Long form input can be restarted without user intervention to give the impression of continuous dictation.)</source>
          <target state="new">(Long form input can be restarted without user intervention to give the impression of continuous dictation.)</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>You should provide a visual cue to indicate that speech recognition is supported and available to the user and whether the user needs to turn it on.</source>
          <target state="new">You should provide a visual cue to indicate that speech recognition is supported and available to the user and whether the user needs to turn it on.</target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source>For example, a command bar button with a microphone glyph (see <bpt id="p1">[</bpt>Command bars<ept id="p1">](../controls-and-patterns/app-bars.md)</ept>) can be used to show both availability and state.</source>
          <target state="new">For example, a command bar button with a microphone glyph (see <bpt id="p1">[</bpt>Command bars<ept id="p1">](../controls-and-patterns/app-bars.md)</ept>) can be used to show both availability and state.</target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>Provide ongoing recognition feedback to minimize any apparent lack of response while recognition is being performed.</source>
          <target state="new">Provide ongoing recognition feedback to minimize any apparent lack of response while recognition is being performed.</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>Let users revise recognition text using keyboard input, disambiguation prompts, suggestions, or additional speech recognition.</source>
          <target state="new">Let users revise recognition text using keyboard input, disambiguation prompts, suggestions, or additional speech recognition.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>Stop recognition if input is detected from a device other than speech recognition, such as touch or keyboard.</source>
          <target state="new">Stop recognition if input is detected from a device other than speech recognition, such as touch or keyboard.</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>This probably indicates that the user has moved onto another task, such as correcting the recognition text or interacting with other form fields.</source>
          <target state="new">This probably indicates that the user has moved onto another task, such as correcting the recognition text or interacting with other form fields.</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>Specify the length of time for which no speech input indicates that recognition is over.</source>
          <target state="new">Specify the length of time for which no speech input indicates that recognition is over.</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>Do not automatically restart recognition after this period of time as it typically indicates the user has stopped engaging with your app.</source>
          <target state="new">Do not automatically restart recognition after this period of time as it typically indicates the user has stopped engaging with your app.</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>Disable all continuous recognition UI and terminate the recognition session if a network connection is not available.</source>
          <target state="new">Disable all continuous recognition UI and terminate the recognition session if a network connection is not available.</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>Continuous recogntion requires a network connection.</source>
          <target state="new">Continuous recogntion requires a network connection.</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source>Commanding</source>
          <target state="new">Commanding</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>Speech input can initiate actions, invoke commands, and accomplish tasks.</source>
          <target state="new">Speech input can initiate actions, invoke commands, and accomplish tasks.</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>If space permits, consider displaying the supported responses for the current app context, with examples of valid input.</source>
          <target state="new">If space permits, consider displaying the supported responses for the current app context, with examples of valid input.</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>This reduces the potential responses your app has to process and also eliminates confusion for the user.</source>
          <target state="new">This reduces the potential responses your app has to process and also eliminates confusion for the user.</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>Try to frame your questions such that they elicit as specific a response as possible.</source>
          <target state="new">Try to frame your questions such that they elicit as specific a response as possible.</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>For example, "What do you want to do today?"</source>
          <target state="new">For example, "What do you want to do today?"</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>is very open ended and would require a very large grammar definition due to how varied the responses could be.</source>
          <target state="new">is very open ended and would require a very large grammar definition due to how varied the responses could be.</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>Alternatively, "Would you like to play a game or listen to music?"</source>
          <target state="new">Alternatively, "Would you like to play a game or listen to music?"</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>constrains the response to one of two valid answers with a correspondingly small grammar definition.</source>
          <target state="new">constrains the response to one of two valid answers with a correspondingly small grammar definition.</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source>A small grammar is much easier to author and results in much more accurate recognition results.</source>
          <target state="new">A small grammar is much easier to author and results in much more accurate recognition results.</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>Request confirmation from the user when speech recognition confidence is low.</source>
          <target state="new">Request confirmation from the user when speech recognition confidence is low.</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>If the user's intent is unclear, it's better to get clarification than to initiate an unintended action.</source>
          <target state="new">If the user's intent is unclear, it's better to get clarification than to initiate an unintended action.</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>You should provide a visual cue to indicate that speech recognition is supported and available to the user and whether the user needs to turn it on.</source>
          <target state="new">You should provide a visual cue to indicate that speech recognition is supported and available to the user and whether the user needs to turn it on.</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>For example, a command bar button with a microphone glyph (see <bpt id="p1">[</bpt>Guidelines for command bars<ept id="p1">](../controls-and-patterns/app-bars.md)</ept>) can be used to show both availability and state.</source>
          <target state="new">For example, a command bar button with a microphone glyph (see <bpt id="p1">[</bpt>Guidelines for command bars<ept id="p1">](../controls-and-patterns/app-bars.md)</ept>) can be used to show both availability and state.</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source>If the speech recognition switch is typically out of view, consider displaying a state indicator in the content area of the app.</source>
          <target state="new">If the speech recognition switch is typically out of view, consider displaying a state indicator in the content area of the app.</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>If recognition is initiated by the user, consider using the built-in recognition experience for consistency.</source>
          <target state="new">If recognition is initiated by the user, consider using the built-in recognition experience for consistency.</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source>The built-in experience includes customizable screens with prompts, examples, disambiguations, confirmations, and errors.</source>
          <target state="new">The built-in experience includes customizable screens with prompts, examples, disambiguations, confirmations, and errors.</target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>The screens vary depending on the specified constraints:</source>
          <target state="new">The screens vary depending on the specified constraints:</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>Pre-defined grammar (dictation or web search)</source>
          <target state="new">Pre-defined grammar (dictation or web search)</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>Listening<ept id="p1">**</ept> screen.</source>
          <target state="new">The <bpt id="p1">**</bpt>Listening<ept id="p1">**</ept> screen.</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>Thinking<ept id="p1">**</ept> screen.</source>
          <target state="new">The <bpt id="p1">**</bpt>Thinking<ept id="p1">**</ept> screen.</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>Heard you say<ept id="p1">**</ept> screen or the error screen.</source>
          <target state="new">The <bpt id="p1">**</bpt>Heard you say<ept id="p1">**</ept> screen or the error screen.</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source>List of words or phrases, or a SRGS grammar file</source>
          <target state="new">List of words or phrases, or a SRGS grammar file</target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>Listening<ept id="p1">**</ept> screen.</source>
          <target state="new">The <bpt id="p1">**</bpt>Listening<ept id="p1">**</ept> screen.</target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>Did you say<ept id="p1">**</ept> screen, if what the user said could be interpreted as more than one potential result.</source>
          <target state="new">The <bpt id="p1">**</bpt>Did you say<ept id="p1">**</ept> screen, if what the user said could be interpreted as more than one potential result.</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>The <bpt id="p1">**</bpt>Heard you say<ept id="p1">**</ept> screen or the error screen.</source>
          <target state="new">The <bpt id="p1">**</bpt>Heard you say<ept id="p1">**</ept> screen or the error screen.</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source>On the <bpt id="p1">**</bpt>Listening<ept id="p1">**</ept> screen you can:</source>
          <target state="new">On the <bpt id="p1">**</bpt>Listening<ept id="p1">**</ept> screen you can:</target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source>Customize the heading text.</source>
          <target state="new">Customize the heading text.</target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source>Provide example text of what the user can say.</source>
          <target state="new">Provide example text of what the user can say.</target>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source>Specify whether the <bpt id="p1">**</bpt>Heard you say<ept id="p1">**</ept> screen is shown.</source>
          <target state="new">Specify whether the <bpt id="p1">**</bpt>Heard you say<ept id="p1">**</ept> screen is shown.</target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source>Read the recognized string back to the user on the <bpt id="p1">**</bpt>Heard you say<ept id="p1">**</ept> screen.</source>
          <target state="new">Read the recognized string back to the user on the <bpt id="p1">**</bpt>Heard you say<ept id="p1">**</ept> screen.</target>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source>Here is an example of the built-in recognition flow for a speech recognizer that uses a SRGS-defined constraint.</source>
          <target state="new">Here is an example of the built-in recognition flow for a speech recognizer that uses a SRGS-defined constraint.</target>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source>In this example, speech recognition is successful.</source>
          <target state="new">In this example, speech recognition is successful.</target>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source>initial recognition screen for a constraint based on a sgrs grammar file</source>
          <target state="new">initial recognition screen for a constraint based on a sgrs grammar file</target>
        </trans-unit>
        <trans-unit id="180" translate="yes" xml:space="preserve">
          <source>intermediate recognition screen for a constraint based on a sgrs grammar file</source>
          <target state="new">intermediate recognition screen for a constraint based on a sgrs grammar file</target>
        </trans-unit>
        <trans-unit id="181" translate="yes" xml:space="preserve">
          <source>final recognition screen for a constraint based on a sgrs grammar file</source>
          <target state="new">final recognition screen for a constraint based on a sgrs grammar file</target>
        </trans-unit>
        <trans-unit id="182" translate="yes" xml:space="preserve">
          <source>Always listening</source>
          <target state="new">Always listening</target>
        </trans-unit>
        <trans-unit id="183" translate="yes" xml:space="preserve">
          <source>Your app can listen for and recognize speech input as soon as the app is launched, without user intervention.</source>
          <target state="new">Your app can listen for and recognize speech input as soon as the app is launched, without user intervention.</target>
        </trans-unit>
        <trans-unit id="184" translate="yes" xml:space="preserve">
          <source>You should customize the grammar constraints based on the app context.</source>
          <target state="new">You should customize the grammar constraints based on the app context.</target>
        </trans-unit>
        <trans-unit id="185" translate="yes" xml:space="preserve">
          <source>This keeps the speech recognition experience very targeted and relevant to the current task, and minimizes errors.</source>
          <target state="new">This keeps the speech recognition experience very targeted and relevant to the current task, and minimizes errors.</target>
        </trans-unit>
        <trans-unit id="186" translate="yes" xml:space="preserve">
          <source>"What can I say?"</source>
          <target state="new">"What can I say?"</target>
        </trans-unit>
        <trans-unit id="187" translate="yes" xml:space="preserve">
          <source>When speech input is enabled, it's important to help users discover what exactly can be understood and what actions can be performed.</source>
          <target state="new">When speech input is enabled, it's important to help users discover what exactly can be understood and what actions can be performed.</target>
        </trans-unit>
        <trans-unit id="188" translate="yes" xml:space="preserve">
          <source>If speech recognition is user enabled, consider using the command bar or a menu command to show all words and phrases supported in the current context.</source>
          <target state="new">If speech recognition is user enabled, consider using the command bar or a menu command to show all words and phrases supported in the current context.</target>
        </trans-unit>
        <trans-unit id="189" translate="yes" xml:space="preserve">
          <source>If speech recognition is always on, consider adding the phrase "What can I say?"</source>
          <target state="new">If speech recognition is always on, consider adding the phrase "What can I say?"</target>
        </trans-unit>
        <trans-unit id="190" translate="yes" xml:space="preserve">
          <source>to every page.</source>
          <target state="new">to every page.</target>
        </trans-unit>
        <trans-unit id="191" translate="yes" xml:space="preserve">
          <source>When the user says this phrase, display all words and phrases supported in the current context.</source>
          <target state="new">When the user says this phrase, display all words and phrases supported in the current context.</target>
        </trans-unit>
        <trans-unit id="192" translate="yes" xml:space="preserve">
          <source>Using this phrase provides a consistent way for users to discover speech capabilities across the system.</source>
          <target state="new">Using this phrase provides a consistent way for users to discover speech capabilities across the system.</target>
        </trans-unit>
        <trans-unit id="193" translate="yes" xml:space="preserve">
          <source>Recognition failures</source>
          <target state="new">Recognition failures</target>
        </trans-unit>
        <trans-unit id="194" translate="yes" xml:space="preserve">
          <source>Speech recognition will fail.</source>
          <target state="new">Speech recognition will fail.</target>
        </trans-unit>
        <trans-unit id="195" translate="yes" xml:space="preserve">
          <source>Failures happen when audio quality is poor, when only part of a phrase is recognized, or when no input is detected at all.</source>
          <target state="new">Failures happen when audio quality is poor, when only part of a phrase is recognized, or when no input is detected at all.</target>
        </trans-unit>
        <trans-unit id="196" translate="yes" xml:space="preserve">
          <source>Handle failure gracefully, help a user understand why recognition failed, and recover.</source>
          <target state="new">Handle failure gracefully, help a user understand why recognition failed, and recover.</target>
        </trans-unit>
        <trans-unit id="197" translate="yes" xml:space="preserve">
          <source>Your app should inform the user that they weren't understood and that they need to try again.</source>
          <target state="new">Your app should inform the user that they weren't understood and that they need to try again.</target>
        </trans-unit>
        <trans-unit id="198" translate="yes" xml:space="preserve">
          <source>Consider providing examples of one or more supported phrases.</source>
          <target state="new">Consider providing examples of one or more supported phrases.</target>
        </trans-unit>
        <trans-unit id="199" translate="yes" xml:space="preserve">
          <source>The user is likely to repeat a suggested phrase, which increases recognition success.</source>
          <target state="new">The user is likely to repeat a suggested phrase, which increases recognition success.</target>
        </trans-unit>
        <trans-unit id="200" translate="yes" xml:space="preserve">
          <source>You should display a list of potential matches for a user to select from.</source>
          <target state="new">You should display a list of potential matches for a user to select from.</target>
        </trans-unit>
        <trans-unit id="201" translate="yes" xml:space="preserve">
          <source>This can be far more efficient than going through the recognition process again.</source>
          <target state="new">This can be far more efficient than going through the recognition process again.</target>
        </trans-unit>
        <trans-unit id="202" translate="yes" xml:space="preserve">
          <source>You should always support alternative input types, which is especially helpful for handling repeated recognition failures.</source>
          <target state="new">You should always support alternative input types, which is especially helpful for handling repeated recognition failures.</target>
        </trans-unit>
        <trans-unit id="203" translate="yes" xml:space="preserve">
          <source>For example, you could suggest that the user try to use a keyboard, or use touch or a mouse to select from a list of potential matches.</source>
          <target state="new">For example, you could suggest that the user try to use a keyboard, or use touch or a mouse to select from a list of potential matches.</target>
        </trans-unit>
        <trans-unit id="204" translate="yes" xml:space="preserve">
          <source>Use the built-in speech recognition experience as it includes screens that inform the user that recognition was not successful and lets the user make another recognition attempt.</source>
          <target state="new">Use the built-in speech recognition experience as it includes screens that inform the user that recognition was not successful and lets the user make another recognition attempt.</target>
        </trans-unit>
        <trans-unit id="205" translate="yes" xml:space="preserve">
          <source>Listen for and try to correct issues in the audio input.</source>
          <target state="new">Listen for and try to correct issues in the audio input.</target>
        </trans-unit>
        <trans-unit id="206" translate="yes" xml:space="preserve">
          <source>The speech recognizer can detect issues with the audio quality that might adversely affect speech recognition accuracy.</source>
          <target state="new">The speech recognizer can detect issues with the audio quality that might adversely affect speech recognition accuracy.</target>
        </trans-unit>
        <trans-unit id="207" translate="yes" xml:space="preserve">
          <source>You can use the information provided by the speech recognizer to inform the user of the issue and let them take corrective action, if possible.</source>
          <target state="new">You can use the information provided by the speech recognizer to inform the user of the issue and let them take corrective action, if possible.</target>
        </trans-unit>
        <trans-unit id="208" translate="yes" xml:space="preserve">
          <source>For example, if the volume setting on the microphone is too low, you can prompt the user to speak louder or turn the volume up.</source>
          <target state="new">For example, if the volume setting on the microphone is too low, you can prompt the user to speak louder or turn the volume up.</target>
        </trans-unit>
        <trans-unit id="209" translate="yes" xml:space="preserve">
          <source>Constraints</source>
          <target state="new">Constraints</target>
        </trans-unit>
        <trans-unit id="210" translate="yes" xml:space="preserve">
          <source>Constraints, or grammars, define the spoken words and phrases that can be matched by the speech recognizer.</source>
          <target state="new">Constraints, or grammars, define the spoken words and phrases that can be matched by the speech recognizer.</target>
        </trans-unit>
        <trans-unit id="211" translate="yes" xml:space="preserve">
          <source>You can specify one of the pre-defined web service grammars or you can create a custom grammar that is installed with your app.</source>
          <target state="new">You can specify one of the pre-defined web service grammars or you can create a custom grammar that is installed with your app.</target>
        </trans-unit>
        <trans-unit id="212" translate="yes" xml:space="preserve">
          <source>Predefined grammars</source>
          <target state="new">Predefined grammars</target>
        </trans-unit>
        <trans-unit id="213" translate="yes" xml:space="preserve">
          <source>Predefined dictation and web-search grammars provide speech recognition for your app without requiring you to author a grammar.</source>
          <target state="new">Predefined dictation and web-search grammars provide speech recognition for your app without requiring you to author a grammar.</target>
        </trans-unit>
        <trans-unit id="214" translate="yes" xml:space="preserve">
          <source>When using these grammars, speech recognition is performed by a remote web service and the results are returned to the device</source>
          <target state="new">When using these grammars, speech recognition is performed by a remote web service and the results are returned to the device</target>
        </trans-unit>
        <trans-unit id="215" translate="yes" xml:space="preserve">
          <source>The default free-text dictation grammar can recognize most words and phrases that a user can say in a particular language, and is optimized to recognize short phrases.</source>
          <target state="new">The default free-text dictation grammar can recognize most words and phrases that a user can say in a particular language, and is optimized to recognize short phrases.</target>
        </trans-unit>
        <trans-unit id="216" translate="yes" xml:space="preserve">
          <source>Free-text dictation is useful when you don't want to limit the kinds of things a user can say.</source>
          <target state="new">Free-text dictation is useful when you don't want to limit the kinds of things a user can say.</target>
        </trans-unit>
        <trans-unit id="217" translate="yes" xml:space="preserve">
          <source>Typical uses include creating notes or dictating the content for a message.</source>
          <target state="new">Typical uses include creating notes or dictating the content for a message.</target>
        </trans-unit>
        <trans-unit id="218" translate="yes" xml:space="preserve">
          <source>The web-search grammar, like a dictation grammar, contains a large number of words and phrases that a user might say.</source>
          <target state="new">The web-search grammar, like a dictation grammar, contains a large number of words and phrases that a user might say.</target>
        </trans-unit>
        <trans-unit id="219" translate="yes" xml:space="preserve">
          <source>However, it is optimized to recognize terms that people typically use when searching the web.</source>
          <target state="new">However, it is optimized to recognize terms that people typically use when searching the web.</target>
        </trans-unit>
        <trans-unit id="220" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Note<ept id="p1">**</ept>  Because predefined dictation and web-search grammars can be large, and because they are online (not on the device), performance might not be as fast as with a custom grammar installed on the device.</source>
          <target state="new"><bpt id="p1">**</bpt>Note<ept id="p1">**</ept>  Because predefined dictation and web-search grammars can be large, and because they are online (not on the device), performance might not be as fast as with a custom grammar installed on the device.</target>
        </trans-unit>
        <trans-unit id="221" translate="yes" xml:space="preserve">
          <source>These predefined grammars can be used to recognize up to 10 seconds of speech input and require no authoring effort on your part.</source>
          <target state="new">These predefined grammars can be used to recognize up to 10 seconds of speech input and require no authoring effort on your part.</target>
        </trans-unit>
        <trans-unit id="222" translate="yes" xml:space="preserve">
          <source>However, they do require connection to a network.</source>
          <target state="new">However, they do require connection to a network.</target>
        </trans-unit>
        <trans-unit id="223" translate="yes" xml:space="preserve">
          <source>Custom grammars</source>
          <target state="new">Custom grammars</target>
        </trans-unit>
        <trans-unit id="224" translate="yes" xml:space="preserve">
          <source>A custom grammar is designed and authored by you and is installed with your app.</source>
          <target state="new">A custom grammar is designed and authored by you and is installed with your app.</target>
        </trans-unit>
        <trans-unit id="225" translate="yes" xml:space="preserve">
          <source>Speech recognition using a custom constraint is performed on the device.</source>
          <target state="new">Speech recognition using a custom constraint is performed on the device.</target>
        </trans-unit>
        <trans-unit id="226" translate="yes" xml:space="preserve">
          <source>Programmatic list constraints provide a lightweight approach to creating simple grammars using a list of words or phrases.</source>
          <target state="new">Programmatic list constraints provide a lightweight approach to creating simple grammars using a list of words or phrases.</target>
        </trans-unit>
        <trans-unit id="227" translate="yes" xml:space="preserve">
          <source>A list constraint works well for recognizing short, distinct phrases.</source>
          <target state="new">A list constraint works well for recognizing short, distinct phrases.</target>
        </trans-unit>
        <trans-unit id="228" translate="yes" xml:space="preserve">
          <source>Explicitly specifying all words in a grammar also improves recognition accuracy, as the speech recognition engine must only process speech to confirm a match.</source>
          <target state="new">Explicitly specifying all words in a grammar also improves recognition accuracy, as the speech recognition engine must only process speech to confirm a match.</target>
        </trans-unit>
        <trans-unit id="229" translate="yes" xml:space="preserve">
          <source>The list can also be programmatically updated.</source>
          <target state="new">The list can also be programmatically updated.</target>
        </trans-unit>
        <trans-unit id="230" translate="yes" xml:space="preserve">
          <source>An SRGS grammar is a static document that, unlike a programmatic list constraint, uses the XML format defined by the <bpt id="p1">[</bpt>SRGS Version 1.0<ept id="p1">](http://go.microsoft.com/fwlink/p/?LinkID=262302)</ept>.</source>
          <target state="new">An SRGS grammar is a static document that, unlike a programmatic list constraint, uses the XML format defined by the <bpt id="p1">[</bpt>SRGS Version 1.0<ept id="p1">](http://go.microsoft.com/fwlink/p/?LinkID=262302)</ept>.</target>
        </trans-unit>
        <trans-unit id="231" translate="yes" xml:space="preserve">
          <source>An SRGS grammar provides the greatest control over the speech recognition experience by letting you capture multiple semantic meanings in a single recognition.</source>
          <target state="new">An SRGS grammar provides the greatest control over the speech recognition experience by letting you capture multiple semantic meanings in a single recognition.</target>
        </trans-unit>
        <trans-unit id="232" translate="yes" xml:space="preserve">
          <source>Here are some tips for authoring SRGS grammars:</source>
          <target state="new">Here are some tips for authoring SRGS grammars:</target>
        </trans-unit>
        <trans-unit id="233" translate="yes" xml:space="preserve">
          <source>Keep each grammar small.</source>
          <target state="new">Keep each grammar small.</target>
        </trans-unit>
        <trans-unit id="234" translate="yes" xml:space="preserve">
          <source>Grammars that contain fewer phrases tend to provide more accurate recognition than larger grammars that contain many phrases.</source>
          <target state="new">Grammars that contain fewer phrases tend to provide more accurate recognition than larger grammars that contain many phrases.</target>
        </trans-unit>
        <trans-unit id="235" translate="yes" xml:space="preserve">
          <source>It's better to have several smaller grammars for specific scenarios than to have a single grammar for your entire app.</source>
          <target state="new">It's better to have several smaller grammars for specific scenarios than to have a single grammar for your entire app.</target>
        </trans-unit>
        <trans-unit id="236" translate="yes" xml:space="preserve">
          <source>Let users know what to say for each app context and enable and disable grammars as needed.</source>
          <target state="new">Let users know what to say for each app context and enable and disable grammars as needed.</target>
        </trans-unit>
        <trans-unit id="237" translate="yes" xml:space="preserve">
          <source>Design each grammar so users can speak a command in a variety of ways.</source>
          <target state="new">Design each grammar so users can speak a command in a variety of ways.</target>
        </trans-unit>
        <trans-unit id="238" translate="yes" xml:space="preserve">
          <source>For example, you can use the <bpt id="p1">**</bpt>GARBAGE<ept id="p1">**</ept> rule to match speech input that your grammar does not define.</source>
          <target state="new">For example, you can use the <bpt id="p1">**</bpt>GARBAGE<ept id="p1">**</ept> rule to match speech input that your grammar does not define.</target>
        </trans-unit>
        <trans-unit id="239" translate="yes" xml:space="preserve">
          <source>This lets users speak additional words that have no meaning to your app.</source>
          <target state="new">This lets users speak additional words that have no meaning to your app.</target>
        </trans-unit>
        <trans-unit id="240" translate="yes" xml:space="preserve">
          <source>For example, "give me", "and", "uh", "maybe", and so on.</source>
          <target state="new">For example, "give me", "and", "uh", "maybe", and so on.</target>
        </trans-unit>
        <trans-unit id="241" translate="yes" xml:space="preserve">
          <source>Use the <bpt id="p1">[</bpt>sapi:subset<ept id="p1">](http://msdn.microsoft.com/library/windowsphone/design/jj572474.aspx)</ept> element to help match speech input.</source>
          <target state="new">Use the <bpt id="p1">[</bpt>sapi:subset<ept id="p1">](http://msdn.microsoft.com/library/windowsphone/design/jj572474.aspx)</ept> element to help match speech input.</target>
        </trans-unit>
        <trans-unit id="242" translate="yes" xml:space="preserve">
          <source>This is a Microsoft extension to the SRGS specification to help match partial phrases.</source>
          <target state="new">This is a Microsoft extension to the SRGS specification to help match partial phrases.</target>
        </trans-unit>
        <trans-unit id="243" translate="yes" xml:space="preserve">
          <source>Try to avoid defining phrases in your grammar that contain only one syllable.</source>
          <target state="new">Try to avoid defining phrases in your grammar that contain only one syllable.</target>
        </trans-unit>
        <trans-unit id="244" translate="yes" xml:space="preserve">
          <source>Recognition tends to be more accurate for phrases containing two or more syllables.</source>
          <target state="new">Recognition tends to be more accurate for phrases containing two or more syllables.</target>
        </trans-unit>
        <trans-unit id="245" translate="yes" xml:space="preserve">
          <source>Avoid using phrases that sound similar.</source>
          <target state="new">Avoid using phrases that sound similar.</target>
        </trans-unit>
        <trans-unit id="246" translate="yes" xml:space="preserve">
          <source>For example, phrases such as "hello", "bellow", and "fellow" can confuse the recognition engine and result in poor recognition accuracy.</source>
          <target state="new">For example, phrases such as "hello", "bellow", and "fellow" can confuse the recognition engine and result in poor recognition accuracy.</target>
        </trans-unit>
        <trans-unit id="247" translate="yes" xml:space="preserve">
          <source><bpt id="p1">**</bpt>Note<ept id="p1">**</ept>  Which type of constraint type you use depends on the complexity of the recognition experience you want to create.</source>
          <target state="new"><bpt id="p1">**</bpt>Note<ept id="p1">**</ept>  Which type of constraint type you use depends on the complexity of the recognition experience you want to create.</target>
        </trans-unit>
        <trans-unit id="248" translate="yes" xml:space="preserve">
          <source>Any could be the best choice for a specific recognition task, and you might find uses for all types of constraints in your app.</source>
          <target state="new">Any could be the best choice for a specific recognition task, and you might find uses for all types of constraints in your app.</target>
        </trans-unit>
        <trans-unit id="249" translate="yes" xml:space="preserve">
          <source>Custom pronunciations</source>
          <target state="new">Custom pronunciations</target>
        </trans-unit>
        <trans-unit id="250" translate="yes" xml:space="preserve">
          <source>If your app contains specialized vocabulary with unusual or fictional words, or words with uncommon pronunciations, you might be able to improve recognition performance for those words by defining custom pronunciations.</source>
          <target state="new">If your app contains specialized vocabulary with unusual or fictional words, or words with uncommon pronunciations, you might be able to improve recognition performance for those words by defining custom pronunciations.</target>
        </trans-unit>
        <trans-unit id="251" translate="yes" xml:space="preserve">
          <source>For a small list of words and phrases, or a list of infrequently used words and phrases, you can create custom pronunciations in a SRGS grammar.</source>
          <target state="new">For a small list of words and phrases, or a list of infrequently used words and phrases, you can create custom pronunciations in a SRGS grammar.</target>
        </trans-unit>
        <trans-unit id="252" translate="yes" xml:space="preserve">
          <source>See <bpt id="p1">[</bpt>token Element<ept id="p1">](http://msdn.microsoft.com/library/windowsphone/design/hh361600.aspx)</ept> for more info.</source>
          <target state="new">See <bpt id="p1">[</bpt>token Element<ept id="p1">](http://msdn.microsoft.com/library/windowsphone/design/hh361600.aspx)</ept> for more info.</target>
        </trans-unit>
        <trans-unit id="253" translate="yes" xml:space="preserve">
          <source>For larger lists of words and phrases, or frequently used words and phrases, you can create separate pronunciation lexicon documents.</source>
          <target state="new">For larger lists of words and phrases, or frequently used words and phrases, you can create separate pronunciation lexicon documents.</target>
        </trans-unit>
        <trans-unit id="254" translate="yes" xml:space="preserve">
          <source>See <bpt id="p1">[</bpt>About Lexicons and Phonetic Alphabets<ept id="p1">](http://msdn.microsoft.com/library/windowsphone/design/hh361646.aspx)</ept> for more info.</source>
          <target state="new">See <bpt id="p1">[</bpt>About Lexicons and Phonetic Alphabets<ept id="p1">](http://msdn.microsoft.com/library/windowsphone/design/hh361646.aspx)</ept> for more info.</target>
        </trans-unit>
        <trans-unit id="255" translate="yes" xml:space="preserve">
          <source>Testing</source>
          <target state="new">Testing</target>
        </trans-unit>
        <trans-unit id="256" translate="yes" xml:space="preserve">
          <source>Test speech recognition accuracy and any supporting UI with your app's target audience.</source>
          <target state="new">Test speech recognition accuracy and any supporting UI with your app's target audience.</target>
        </trans-unit>
        <trans-unit id="257" translate="yes" xml:space="preserve">
          <source>This is the best way to determine the effectiveness of the speech interaction experience in your app.</source>
          <target state="new">This is the best way to determine the effectiveness of the speech interaction experience in your app.</target>
        </trans-unit>
        <trans-unit id="258" translate="yes" xml:space="preserve">
          <source>For example, are users getting poor recognition results because your app isn't listening for a common phrase?</source>
          <target state="new">For example, are users getting poor recognition results because your app isn't listening for a common phrase?</target>
        </trans-unit>
        <trans-unit id="259" translate="yes" xml:space="preserve">
          <source>Either modify the grammar to support this phrase or provide users with a list of supported phrases.</source>
          <target state="new">Either modify the grammar to support this phrase or provide users with a list of supported phrases.</target>
        </trans-unit>
        <trans-unit id="260" translate="yes" xml:space="preserve">
          <source>If you already provide the list of supported phrases, ensure it is easily discoverable.</source>
          <target state="new">If you already provide the list of supported phrases, ensure it is easily discoverable.</target>
        </trans-unit>
        <trans-unit id="261" translate="yes" xml:space="preserve">
          <source>Text-to-speech (TTS)</source>
          <target state="new">Text-to-speech (TTS)</target>
        </trans-unit>
        <trans-unit id="262" translate="yes" xml:space="preserve">
          <source>TTS generates speech output from plain text or SSML.</source>
          <target state="new">TTS generates speech output from plain text or SSML.</target>
        </trans-unit>
        <trans-unit id="263" translate="yes" xml:space="preserve">
          <source>Try to design prompts that are polite and encouraging.</source>
          <target state="new">Try to design prompts that are polite and encouraging.</target>
        </trans-unit>
        <trans-unit id="264" translate="yes" xml:space="preserve">
          <source>Consider whether you should read long strings of text.</source>
          <target state="new">Consider whether you should read long strings of text.</target>
        </trans-unit>
        <trans-unit id="265" translate="yes" xml:space="preserve">
          <source>It's one thing to listen to a text message, but quite another to listen to a long list of search results that are difficult to remember.</source>
          <target state="new">It's one thing to listen to a text message, but quite another to listen to a long list of search results that are difficult to remember.</target>
        </trans-unit>
        <trans-unit id="266" translate="yes" xml:space="preserve">
          <source>You should provide media controls to let users pause, or stop, TTS.</source>
          <target state="new">You should provide media controls to let users pause, or stop, TTS.</target>
        </trans-unit>
        <trans-unit id="267" translate="yes" xml:space="preserve">
          <source>You should listen to all TTS strings to ensure they are intelligible and sound natural.</source>
          <target state="new">You should listen to all TTS strings to ensure they are intelligible and sound natural.</target>
        </trans-unit>
        <trans-unit id="268" translate="yes" xml:space="preserve">
          <source>Stringing together an unusual sequence of words or speaking part numbers or punctuation might cause a phrase to become unintelligible.</source>
          <target state="new">Stringing together an unusual sequence of words or speaking part numbers or punctuation might cause a phrase to become unintelligible.</target>
        </trans-unit>
        <trans-unit id="269" translate="yes" xml:space="preserve">
          <source>Speech can sound unnatural when the prosody or cadence is different from how a native speaker would say a phrase.</source>
          <target state="new">Speech can sound unnatural when the prosody or cadence is different from how a native speaker would say a phrase.</target>
        </trans-unit>
        <trans-unit id="270" translate="yes" xml:space="preserve">
          <source>Both issues can be addressed bu using SSML instead of plain text as input to the speech synthesizer.</source>
          <target state="new">Both issues can be addressed bu using SSML instead of plain text as input to the speech synthesizer.</target>
        </trans-unit>
        <trans-unit id="271" translate="yes" xml:space="preserve">
          <source>For more info about SSML, see <bpt id="p1">[</bpt>Use SSML to Control Synthesized Speech<ept id="p1">](http://msdn.microsoft.com/library/windowsphone/design/hh378454.aspx)</ept> and <bpt id="p2">[</bpt>Speech Synthesis Markup Language Reference<ept id="p2">](http://msdn.microsoft.com/library/windowsphone/design/hh378377.aspx)</ept>.</source>
          <target state="new">For more info about SSML, see <bpt id="p1">[</bpt>Use SSML to Control Synthesized Speech<ept id="p1">](http://msdn.microsoft.com/library/windowsphone/design/hh378454.aspx)</ept> and <bpt id="p2">[</bpt>Speech Synthesis Markup Language Reference<ept id="p2">](http://msdn.microsoft.com/library/windowsphone/design/hh378377.aspx)</ept>.</target>
        </trans-unit>
        <trans-unit id="272" translate="yes" xml:space="preserve">
          <source>Other articles in this section</source>
          <target state="new">Other articles in this section</target>
        </trans-unit>
        <trans-unit id="273" translate="yes" xml:space="preserve">
          <source>Topic</source>
          <target state="new">Topic</target>
        </trans-unit>
        <trans-unit id="274" translate="yes" xml:space="preserve">
          <source>Description</source>
          <target state="new">Description</target>
        </trans-unit>
        <trans-unit id="275" translate="yes" xml:space="preserve">
          <source>Speech recognition</source>
          <target state="new">Speech recognition</target>
        </trans-unit>
        <trans-unit id="276" translate="yes" xml:space="preserve">
          <source>Use speech recognition to provide input, specify an action or command, and accomplish tasks.</source>
          <target state="new">Use speech recognition to provide input, specify an action or command, and accomplish tasks.</target>
        </trans-unit>
        <trans-unit id="277" translate="yes" xml:space="preserve">
          <source>Specify the speech recognizer language</source>
          <target state="new">Specify the speech recognizer language</target>
        </trans-unit>
        <trans-unit id="278" translate="yes" xml:space="preserve">
          <source>Learn how to select an installed language to use for speech recognition.</source>
          <target state="new">Learn how to select an installed language to use for speech recognition.</target>
        </trans-unit>
        <trans-unit id="279" translate="yes" xml:space="preserve">
          <source>Define custom recognition constraints</source>
          <target state="new">Define custom recognition constraints</target>
        </trans-unit>
        <trans-unit id="280" translate="yes" xml:space="preserve">
          <source>Learn how to define and use custom constraints for speech recognition.</source>
          <target state="new">Learn how to define and use custom constraints for speech recognition.</target>
        </trans-unit>
        <trans-unit id="281" translate="yes" xml:space="preserve">
          <source>Enable continuous dictation</source>
          <target state="new">Enable continuous dictation</target>
        </trans-unit>
        <trans-unit id="282" translate="yes" xml:space="preserve">
          <source>Learn how to capture and recognize long-form, continuous dictation speech input.</source>
          <target state="new">Learn how to capture and recognize long-form, continuous dictation speech input.</target>
        </trans-unit>
        <trans-unit id="283" translate="yes" xml:space="preserve">
          <source>Manage issues with audio input</source>
          <target state="new">Manage issues with audio input</target>
        </trans-unit>
        <trans-unit id="284" translate="yes" xml:space="preserve">
          <source>Learn how to manage issues with speech-recognition accuracy caused by audio-input quality.</source>
          <target state="new">Learn how to manage issues with speech-recognition accuracy caused by audio-input quality.</target>
        </trans-unit>
        <trans-unit id="285" translate="yes" xml:space="preserve">
          <source>Set speech recognition timeouts</source>
          <target state="new">Set speech recognition timeouts</target>
        </trans-unit>
        <trans-unit id="286" translate="yes" xml:space="preserve">
          <source>Set how long a speech recognizer ignores silence or unrecognizable sounds (babble) and continues listening for speech input.</source>
          <target state="new">Set how long a speech recognizer ignores silence or unrecognizable sounds (babble) and continues listening for speech input.</target>
        </trans-unit>
        <trans-unit id="287" translate="yes" xml:space="preserve">
          <source>Related articles</source>
          <target state="new">Related articles</target>
        </trans-unit>
        <trans-unit id="288" translate="yes" xml:space="preserve">
          <source>Speech interactions</source>
          <target state="new">Speech interactions</target>
        </trans-unit>
        <trans-unit id="289" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt>Cortana interactions<ept id="p1">](https://msdn.microsoft.com/library/windows/apps/mt185598)</ept><ph id="ph1">
</ph><bpt id="p2">**</bpt>Samples<ept id="p2">**</ept></source>
          <target state="new"><bpt id="p1">[</bpt>Cortana interactions<ept id="p1">](https://msdn.microsoft.com/library/windows/apps/mt185598)</ept><ph id="ph1">
</ph><bpt id="p2">**</bpt>Samples<ept id="p2">**</ept></target>
        </trans-unit>
        <trans-unit id="290" translate="yes" xml:space="preserve">
          <source>Speech recognition and speech synthesis sample</source>
          <target state="new">Speech recognition and speech synthesis sample</target>
        </trans-unit>
      </group>
    </body>
  </file>
</xliff>