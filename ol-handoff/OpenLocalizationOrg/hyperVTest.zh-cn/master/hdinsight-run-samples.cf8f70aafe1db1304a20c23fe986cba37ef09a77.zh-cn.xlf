<?xml version="1.0"?>
<xliff version="1.2" xmlns="urn:oasis:names:tc:xliff:document:1.2" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="urn:oasis:names:tc:xliff:document:1.2 xliff-core-1.2-transitional.xsd">
  <file datatype="xml" source-language="en-US" target-language="zh-cn">
    <body>
      <group id="main" extype="content">
        <trans-unit id="101" translate="yes" xml:space="preserve">
          <source>Run the Hadoop samples in HDInsight | Microsoft Azure</source>
          <target state="new">Run the Hadoop samples in HDInsight | Microsoft Azure</target>
        </trans-unit>
        <trans-unit id="102" translate="yes" xml:space="preserve">
          <source>Get started using the Azure HDInsight service with the samples provided.</source>
          <target state="new">Get started using the Azure HDInsight service with the samples provided.</target>
        </trans-unit>
        <trans-unit id="103" translate="yes" xml:space="preserve">
          <source>Use PowerShell scripts that run MapReduce programs on data clusters.</source>
          <target state="new">Use PowerShell scripts that run MapReduce programs on data clusters.</target>
        </trans-unit>
        <trans-unit id="104" translate="yes" xml:space="preserve">
          <source>Run Hadoop MapReduce samples in Windows-based HDInsight</source>
          <target state="new">Run Hadoop MapReduce samples in Windows-based HDInsight</target>
        </trans-unit>
        <trans-unit id="105" translate="yes" xml:space="preserve">
          <source>A set of samples are provided to help you get started running MapReduce jobs on Hadoop clusters using Azure HDInsight.</source>
          <target state="new">A set of samples are provided to help you get started running MapReduce jobs on Hadoop clusters using Azure HDInsight.</target>
        </trans-unit>
        <trans-unit id="106" translate="yes" xml:space="preserve">
          <source>These samples are made available on each of the HDInsight managed clusters that you create.</source>
          <target state="new">These samples are made available on each of the HDInsight managed clusters that you create.</target>
        </trans-unit>
        <trans-unit id="107" translate="yes" xml:space="preserve">
          <source>Running these samples will familiarize you with using Azure PowerShell cmdlets to run jobs on Hadoop clusters.</source>
          <target state="new">Running these samples will familiarize you with using Azure PowerShell cmdlets to run jobs on Hadoop clusters.</target>
        </trans-unit>
        <trans-unit id="108" translate="yes" xml:space="preserve">
          <source><bpt id="p1">[</bpt><bpt id="p2">**</bpt>Word count<ept id="p2">**</ept><ept id="p1">][hdinsight-sample-wordcount]</ept>: Counts word occurrences in a text file.</source>
          <target state="new"><bpt id="p1">[</bpt><bpt id="p2">**</bpt>Word count<ept id="p2">**</ept><ept id="p1">][hdinsight-sample-wordcount]</ept>: Counts word occurrences in a text file.</target>
        </trans-unit>
        <trans-unit id="109" translate="yes" xml:space="preserve">
          <source><bpt id="p3">[</bpt><bpt id="p4">**</bpt>C# streaming word count<ept id="p4">**</ept><ept id="p3">][hdinsight-sample-csharp-streaming]</ept>: Counts word occurrences in a text file using the Hadoop streaming interface.</source>
          <target state="new"><bpt id="p3">[</bpt><bpt id="p4">**</bpt>C# streaming word count<ept id="p4">**</ept><ept id="p3">][hdinsight-sample-csharp-streaming]</ept>: Counts word occurrences in a text file using the Hadoop streaming interface.</target>
        </trans-unit>
        <trans-unit id="110" translate="yes" xml:space="preserve">
          <source><bpt id="p5">[</bpt><bpt id="p6">**</bpt>Pi estimator<ept id="p6">**</ept><ept id="p5">][hdinsight-sample-pi-estimator]</ept>: Uses a statistical (quasi-Monte Carlo) method to estimate the value of pi.</source>
          <target state="new"><bpt id="p5">[</bpt><bpt id="p6">**</bpt>Pi estimator<ept id="p6">**</ept><ept id="p5">][hdinsight-sample-pi-estimator]</ept>: Uses a statistical (quasi-Monte Carlo) method to estimate the value of pi.</target>
        </trans-unit>
        <trans-unit id="111" translate="yes" xml:space="preserve">
          <source><bpt id="p7">[</bpt><bpt id="p8">**</bpt>10-GB Graysort<ept id="p8">**</ept><ept id="p7">][hdinsight-sample-10gb-graysort]</ept>: Run a general purpose GraySort on a 10 GB file by using HDInsight.</source>
          <target state="new"><bpt id="p7">[</bpt><bpt id="p8">**</bpt>10-GB Graysort<ept id="p8">**</ept><ept id="p7">][hdinsight-sample-10gb-graysort]</ept>: Run a general purpose GraySort on a 10 GB file by using HDInsight.</target>
        </trans-unit>
        <trans-unit id="112" translate="yes" xml:space="preserve">
          <source>There are three jobs to run: Teragen to generate the data, Terasort to sort the data, and Teravalidate to confirm that the data has been properly sorted.</source>
          <target state="new">There are three jobs to run: Teragen to generate the data, Terasort to sort the data, and Teravalidate to confirm that the data has been properly sorted.</target>
        </trans-unit>
        <trans-unit id="113" translate="yes" xml:space="preserve">
          <source><ph id="ph3">[AZURE.NOTE]</ph><ph id="ph4" /> The source code can be found in the Appendix.</source>
          <target state="new"><ph id="ph3">[AZURE.NOTE]</ph><ph id="ph4" /> The source code can be found in the Appendix.</target>
        </trans-unit>
        <trans-unit id="114" translate="yes" xml:space="preserve">
          <source>Much additional documentation exists on the web for Hadoop-related technologies, such as Java-based MapReduce programming and streaming, and documentation about the cmdlets that are used in Windows PowerShell scripting.</source>
          <target state="new">Much additional documentation exists on the web for Hadoop-related technologies, such as Java-based MapReduce programming and streaming, and documentation about the cmdlets that are used in Windows PowerShell scripting.</target>
        </trans-unit>
        <trans-unit id="115" translate="yes" xml:space="preserve">
          <source>For more information about these resources, see:</source>
          <target state="new">For more information about these resources, see:</target>
        </trans-unit>
        <trans-unit id="116" translate="yes" xml:space="preserve">
          <source><bpt id="p9">[</bpt>Develop Java MapReduce programs for Hadoop in HDInsight<ept id="p9">](hdinsight-develop-deploy-java-mapreduce.md)</ept></source>
          <target state="new"><bpt id="p9">[</bpt>Develop Java MapReduce programs for Hadoop in HDInsight<ept id="p9">](hdinsight-develop-deploy-java-mapreduce.md)</ept></target>
        </trans-unit>
        <trans-unit id="117" translate="yes" xml:space="preserve">
          <source><bpt id="p10">[</bpt>Develop C# Hadoop streaming programs for HDInsight<ept id="p10">](hdinsight-hadoop-develop-deploy-streaming-jobs.md)</ept></source>
          <target state="new"><bpt id="p10">[</bpt>Develop C# Hadoop streaming programs for HDInsight<ept id="p10">](hdinsight-hadoop-develop-deploy-streaming-jobs.md)</ept></target>
        </trans-unit>
        <trans-unit id="118" translate="yes" xml:space="preserve">
          <source><bpt id="p11">[</bpt>Submit Hadoop jobs in HDInsight<ept id="p11">](hdinsight-submit-hadoop-jobs-programmatically.md)</ept></source>
          <target state="new"><bpt id="p11">[</bpt>Submit Hadoop jobs in HDInsight<ept id="p11">](hdinsight-submit-hadoop-jobs-programmatically.md)</ept></target>
        </trans-unit>
        <trans-unit id="119" translate="yes" xml:space="preserve">
          <source><bpt id="p12">[</bpt>Introduction to Azure HDInsight<ept id="p12">][hdinsight-introduction]</ept></source>
          <target state="new"><bpt id="p12">[</bpt>Introduction to Azure HDInsight<ept id="p12">][hdinsight-introduction]</ept></target>
        </trans-unit>
        <trans-unit id="120" translate="yes" xml:space="preserve">
          <source>Nowadays, a lot of people choose Hive and Pig over MapReduce.</source>
          <target state="new">Nowadays, a lot of people choose Hive and Pig over MapReduce.</target>
        </trans-unit>
        <trans-unit id="121" translate="yes" xml:space="preserve">
          <source>For more information, see :</source>
          <target state="new">For more information, see :</target>
        </trans-unit>
        <trans-unit id="122" translate="yes" xml:space="preserve">
          <source><bpt id="p13">[</bpt>Use Hive in HDInsight<ept id="p13">](hdinsight-use-hive.md)</ept></source>
          <target state="new"><bpt id="p13">[</bpt>Use Hive in HDInsight<ept id="p13">](hdinsight-use-hive.md)</ept></target>
        </trans-unit>
        <trans-unit id="123" translate="yes" xml:space="preserve">
          <source><bpt id="p14">[</bpt>Use Pig in HDInsight<ept id="p14">](hdinsight-use-pig.md)</ept></source>
          <target state="new"><bpt id="p14">[</bpt>Use Pig in HDInsight<ept id="p14">](hdinsight-use-pig.md)</ept></target>
        </trans-unit>
        <trans-unit id="124" translate="yes" xml:space="preserve">
          <source><bpt id="p15">**</bpt>Prerequisites<ept id="p15">**</ept>:</source>
          <target state="new"><bpt id="p15">**</bpt>Prerequisites<ept id="p15">**</ept>:</target>
        </trans-unit>
        <trans-unit id="125" translate="yes" xml:space="preserve">
          <source><bpt id="p16">**</bpt>An Azure subscription<ept id="p16">**</ept>.</source>
          <target state="new"><bpt id="p16">**</bpt>An Azure subscription<ept id="p16">**</ept>.</target>
        </trans-unit>
        <trans-unit id="126" translate="yes" xml:space="preserve">
          <source>See <bpt id="p17">[</bpt>Get Azure free trial<ept id="p17">](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/)</ept>.</source>
          <target state="new">See <bpt id="p17">[</bpt>Get Azure free trial<ept id="p17">](https://azure.microsoft.com/documentation/videos/get-azure-free-trial-for-testing-hadoop-in-hdinsight/)</ept>.</target>
        </trans-unit>
        <trans-unit id="127" translate="yes" xml:space="preserve">
          <source><bpt id="p18">**</bpt>an HDInsight cluster<ept id="p18">**</ept>.</source>
          <target state="new"><bpt id="p18">**</bpt>an HDInsight cluster<ept id="p18">**</ept>.</target>
        </trans-unit>
        <trans-unit id="128" translate="yes" xml:space="preserve">
          <source>For instructions on the various ways in which such clusters can be created, see <bpt id="p19">[</bpt>Create Hadoop clusters in HDInsight<ept id="p19">](hdinsight-provision-clusters.md)</ept>.</source>
          <target state="new">For instructions on the various ways in which such clusters can be created, see <bpt id="p19">[</bpt>Create Hadoop clusters in HDInsight<ept id="p19">](hdinsight-provision-clusters.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="129" translate="yes" xml:space="preserve">
          <source><bpt id="p20">**</bpt>A workstation with Azure PowerShell<ept id="p20">**</ept>.</source>
          <target state="new"><bpt id="p20">**</bpt>A workstation with Azure PowerShell<ept id="p20">**</ept>.</target>
        </trans-unit>
        <trans-unit id="130" translate="yes" xml:space="preserve">
          <source>See <bpt id="p21">[</bpt>Install Azure PowerShell 1.0 and greater<ept id="p21">](hdinsight-administer-use-powershell.md#install-azure-powershell-10-and-greater)</ept>.</source>
          <target state="new">See <bpt id="p21">[</bpt>Install Azure PowerShell 1.0 and greater<ept id="p21">](hdinsight-administer-use-powershell.md#install-azure-powershell-10-and-greater)</ept>.</target>
        </trans-unit>
        <trans-unit id="131" translate="yes" xml:space="preserve">
          <source>Word count - Java</source>
          <target state="new">Word count - Java</target>
        </trans-unit>
        <trans-unit id="132" translate="yes" xml:space="preserve">
          <source>To submit a MapReduce project, you first create a MapReduce job definition.</source>
          <target state="new">To submit a MapReduce project, you first create a MapReduce job definition.</target>
        </trans-unit>
        <trans-unit id="133" translate="yes" xml:space="preserve">
          <source>In the job definition, you specify the MapReduce program jar file and the location of the jar file, which is <bpt id="p22">**</bpt>wasb:///example/jars/hadoop-mapreduce-examples.jar<ept id="p22">**</ept>, the class name, and the arguments.</source>
          <target state="new">In the job definition, you specify the MapReduce program jar file and the location of the jar file, which is <bpt id="p22">**</bpt>wasb:///example/jars/hadoop-mapreduce-examples.jar<ept id="p22">**</ept>, the class name, and the arguments.</target>
        </trans-unit>
        <trans-unit id="134" translate="yes" xml:space="preserve">
          <source>The wordcount MapReduce program takes two arguments: the source file that will be used to count words, and the location for output.</source>
          <target state="new">The wordcount MapReduce program takes two arguments: the source file that will be used to count words, and the location for output.</target>
        </trans-unit>
        <trans-unit id="135" translate="yes" xml:space="preserve">
          <source>The source code can be found in the <bpt id="p23">[</bpt>Appendix A<ept id="p23">](#apendix-a---the-word-count-MapReduce-program-in-java)</ept>.</source>
          <target state="new">The source code can be found in the <bpt id="p23">[</bpt>Appendix A<ept id="p23">](#apendix-a---the-word-count-MapReduce-program-in-java)</ept>.</target>
        </trans-unit>
        <trans-unit id="136" translate="yes" xml:space="preserve">
          <source>For the procedure of developing a Java MapReduce program, see - <bpt id="p24">[</bpt>Develop Java MapReduce programs for Hadoop in HDInsight<ept id="p24">](hdinsight-develop-deploy-java-mapreduce.md)</ept></source>
          <target state="new">For the procedure of developing a Java MapReduce program, see - <bpt id="p24">[</bpt>Develop Java MapReduce programs for Hadoop in HDInsight<ept id="p24">](hdinsight-develop-deploy-java-mapreduce.md)</ept></target>
        </trans-unit>
        <trans-unit id="137" translate="yes" xml:space="preserve">
          <source><bpt id="p25">**</bpt>To submit a word count MapReduce job<ept id="p25">**</ept></source>
          <target state="new"><bpt id="p25">**</bpt>To submit a word count MapReduce job<ept id="p25">**</ept></target>
        </trans-unit>
        <trans-unit id="138" translate="yes" xml:space="preserve">
          <source>Open <bpt id="p26">**</bpt>Windows PowerShell ISE<ept id="p26">**</ept>.</source>
          <target state="new">Open <bpt id="p26">**</bpt>Windows PowerShell ISE<ept id="p26">**</ept>.</target>
        </trans-unit>
        <trans-unit id="139" translate="yes" xml:space="preserve">
          <source>For instructions, see <bpt id="p27">[</bpt>Install and configure Azure PowerShell<ept id="p27">][powershell-install-configure]</ept>.</source>
          <target state="new">For instructions, see <bpt id="p27">[</bpt>Install and configure Azure PowerShell<ept id="p27">][powershell-install-configure]</ept>.</target>
        </trans-unit>
        <trans-unit id="140" translate="yes" xml:space="preserve">
          <source>Paste the following PowerShell script:</source>
          <target state="new">Paste the following PowerShell script:</target>
        </trans-unit>
        <trans-unit id="141" translate="yes" xml:space="preserve">
          <source>The MapReduce job produces a file named <bpt id="p28">*</bpt>part-r-00000<ept id="p28">*</ept>, which contains words and the counts.</source>
          <target state="new">The MapReduce job produces a file named <bpt id="p28">*</bpt>part-r-00000<ept id="p28">*</ept>, which contains words and the counts.</target>
        </trans-unit>
        <trans-unit id="142" translate="yes" xml:space="preserve">
          <source>The script uses the <bpt id="p29">**</bpt>findstr<ept id="p29">**</ept><ph id="ph5" /> command to list all of the words that contains <bpt id="p30">*</bpt>"there"<ept id="p30">*</ept>.</source>
          <target state="new">The script uses the <bpt id="p29">**</bpt>findstr<ept id="p29">**</ept><ph id="ph5" /> command to list all of the words that contains <bpt id="p30">*</bpt>"there"<ept id="p30">*</ept>.</target>
        </trans-unit>
        <trans-unit id="143" translate="yes" xml:space="preserve">
          <source>Set the first 3 variables, and run the script.</source>
          <target state="new">Set the first 3 variables, and run the script.</target>
        </trans-unit>
        <trans-unit id="144" translate="yes" xml:space="preserve">
          <source>Word count - C# streaming</source>
          <target state="new">Word count - C# streaming</target>
        </trans-unit>
        <trans-unit id="145" translate="yes" xml:space="preserve">
          <source>Hadoop provides a streaming API to MapReduce, which enables you to write map and reduce functions in languages other than Java.</source>
          <target state="new">Hadoop provides a streaming API to MapReduce, which enables you to write map and reduce functions in languages other than Java.</target>
        </trans-unit>
        <trans-unit id="146" translate="yes" xml:space="preserve">
          <source><ph id="ph6">[AZURE.NOTE]</ph><ph id="ph7" /> The steps in this tutorial apply only to Windows-based HDInsight clusters.</source>
          <target state="new"><ph id="ph6">[AZURE.NOTE]</ph><ph id="ph7" /> The steps in this tutorial apply only to Windows-based HDInsight clusters.</target>
        </trans-unit>
        <trans-unit id="147" translate="yes" xml:space="preserve">
          <source>For an example of streaming for Linux-based HDInsight clusters, see <bpt id="p31">[</bpt>Develop Python streaming programs for HDInsight<ept id="p31">](hdinsight-hadoop-streaming-python.md)</ept>.</source>
          <target state="new">For an example of streaming for Linux-based HDInsight clusters, see <bpt id="p31">[</bpt>Develop Python streaming programs for HDInsight<ept id="p31">](hdinsight-hadoop-streaming-python.md)</ept>.</target>
        </trans-unit>
        <trans-unit id="148" translate="yes" xml:space="preserve">
          <source>In the example, the mapper and the reducer are executables that read the input from [stdin][stdin-stdout-stderr] (line-by-line) and emit the output to [stdout][stdin-stdout-stderr].</source>
          <target state="new">In the example, the mapper and the reducer are executables that read the input from [stdin][stdin-stdout-stderr] (line-by-line) and emit the output to [stdout][stdin-stdout-stderr].</target>
        </trans-unit>
        <trans-unit id="149" translate="yes" xml:space="preserve">
          <source>The program counts all of the words in the text.</source>
          <target state="new">The program counts all of the words in the text.</target>
        </trans-unit>
        <trans-unit id="150" translate="yes" xml:space="preserve">
          <source>When an executable is specified for <bpt id="p32">**</bpt>mappers<ept id="p32">**</ept>, each mapper task launches the executable as a separate process when the mapper is initialized.</source>
          <target state="new">When an executable is specified for <bpt id="p32">**</bpt>mappers<ept id="p32">**</ept>, each mapper task launches the executable as a separate process when the mapper is initialized.</target>
        </trans-unit>
        <trans-unit id="151" translate="yes" xml:space="preserve">
          <source>As the mapper task runs, it converts its input into lines, and feeds the lines to the [stdin][stdin-stdout-stderr] of the process.</source>
          <target state="new">As the mapper task runs, it converts its input into lines, and feeds the lines to the [stdin][stdin-stdout-stderr] of the process.</target>
        </trans-unit>
        <trans-unit id="152" translate="yes" xml:space="preserve">
          <source>In the meantime, the mapper collects the line-oriented output from the stdout of the process.</source>
          <target state="new">In the meantime, the mapper collects the line-oriented output from the stdout of the process.</target>
        </trans-unit>
        <trans-unit id="153" translate="yes" xml:space="preserve">
          <source>It converts each line into a key/value pair, which is collected as the output of the mapper.</source>
          <target state="new">It converts each line into a key/value pair, which is collected as the output of the mapper.</target>
        </trans-unit>
        <trans-unit id="154" translate="yes" xml:space="preserve">
          <source>By default, the prefix of a line up to the first Tab character is the key, and the remainder of the line (excluding the Tab character) is the value.</source>
          <target state="new">By default, the prefix of a line up to the first Tab character is the key, and the remainder of the line (excluding the Tab character) is the value.</target>
        </trans-unit>
        <trans-unit id="155" translate="yes" xml:space="preserve">
          <source>If there is no Tab character in the line, entire line is considered as the key, and the value is null.</source>
          <target state="new">If there is no Tab character in the line, entire line is considered as the key, and the value is null.</target>
        </trans-unit>
        <trans-unit id="156" translate="yes" xml:space="preserve">
          <source>When an executable is specified for <bpt id="p33">**</bpt>reducers<ept id="p33">**</ept>, each reducer task launches the executable as a separate process when the reducer is initialized.</source>
          <target state="new">When an executable is specified for <bpt id="p33">**</bpt>reducers<ept id="p33">**</ept>, each reducer task launches the executable as a separate process when the reducer is initialized.</target>
        </trans-unit>
        <trans-unit id="157" translate="yes" xml:space="preserve">
          <source>As the reducer task runs, it converts its input key/values pairs into lines, and it feeds the lines to the [stdin][stdin-stdout-stderr] of the process.</source>
          <target state="new">As the reducer task runs, it converts its input key/values pairs into lines, and it feeds the lines to the [stdin][stdin-stdout-stderr] of the process.</target>
        </trans-unit>
        <trans-unit id="158" translate="yes" xml:space="preserve">
          <source>In the meantime, the reducer collects the line-oriented output from the [stdout][stdin-stdout-stderr] of the process.</source>
          <target state="new">In the meantime, the reducer collects the line-oriented output from the [stdout][stdin-stdout-stderr] of the process.</target>
        </trans-unit>
        <trans-unit id="159" translate="yes" xml:space="preserve">
          <source>It converts each line to a key/value pair, which is collected as the output of the reducer.</source>
          <target state="new">It converts each line to a key/value pair, which is collected as the output of the reducer.</target>
        </trans-unit>
        <trans-unit id="160" translate="yes" xml:space="preserve">
          <source>By default, the prefix of a line up to the first Tab character is the key, and the remainder of the line (excluding the Tab character) is the value.</source>
          <target state="new">By default, the prefix of a line up to the first Tab character is the key, and the remainder of the line (excluding the Tab character) is the value.</target>
        </trans-unit>
        <trans-unit id="161" translate="yes" xml:space="preserve">
          <source>For more information about the Hadoop Streaming interface, see [Hadoop Streaming][hadoop-streaming].</source>
          <target state="new">For more information about the Hadoop Streaming interface, see [Hadoop Streaming][hadoop-streaming].</target>
        </trans-unit>
        <trans-unit id="162" translate="yes" xml:space="preserve">
          <source><bpt id="p34">**</bpt>To submit a C# streaming word count job<ept id="p34">**</ept></source>
          <target state="new"><bpt id="p34">**</bpt>To submit a C# streaming word count job<ept id="p34">**</ept></target>
        </trans-unit>
        <trans-unit id="163" translate="yes" xml:space="preserve">
          <source>Follow the procdure in <bpt id="p35">[</bpt>Word count - Java<ept id="p35">](#word-count-java)</ept>, and replace the job definition with the following:</source>
          <target state="new">Follow the procdure in <bpt id="p35">[</bpt>Word count - Java<ept id="p35">](#word-count-java)</ept>, and replace the job definition with the following:</target>
        </trans-unit>
        <trans-unit id="164" translate="yes" xml:space="preserve">
          <source>The output file shall be:</source>
          <target state="new">The output file shall be:</target>
        </trans-unit>
        <trans-unit id="165" translate="yes" xml:space="preserve">
          <source>PI estimator</source>
          <target state="new">PI estimator</target>
        </trans-unit>
        <trans-unit id="166" translate="yes" xml:space="preserve">
          <source>The pi estimator uses a statistical (quasi-Monte Carlo) method to estimate the value of pi.</source>
          <target state="new">The pi estimator uses a statistical (quasi-Monte Carlo) method to estimate the value of pi.</target>
        </trans-unit>
        <trans-unit id="167" translate="yes" xml:space="preserve">
          <source>Points placed at random inside of a unit square also fall within a circle inscribed within that square with a probability equal to the area of the circle, pi/4.</source>
          <target state="new">Points placed at random inside of a unit square also fall within a circle inscribed within that square with a probability equal to the area of the circle, pi/4.</target>
        </trans-unit>
        <trans-unit id="168" translate="yes" xml:space="preserve">
          <source>The value of pi can be estimated from the value of 4R, where R is the ratio of the number of points that are inside the circle to the total number of points that are within the square.</source>
          <target state="new">The value of pi can be estimated from the value of 4R, where R is the ratio of the number of points that are inside the circle to the total number of points that are within the square.</target>
        </trans-unit>
        <trans-unit id="169" translate="yes" xml:space="preserve">
          <source>The larger the sample of points used, the better the estimate is.</source>
          <target state="new">The larger the sample of points used, the better the estimate is.</target>
        </trans-unit>
        <trans-unit id="170" translate="yes" xml:space="preserve">
          <source>The script provided for this sample submits a Hadoop jar job and is set up to run with a value 16 maps, each of which is required to compute 10 million sample points by the parameter values.</source>
          <target state="new">The script provided for this sample submits a Hadoop jar job and is set up to run with a value 16 maps, each of which is required to compute 10 million sample points by the parameter values.</target>
        </trans-unit>
        <trans-unit id="171" translate="yes" xml:space="preserve">
          <source>These parameter values can be changed to improve the estimated value of pi.</source>
          <target state="new">These parameter values can be changed to improve the estimated value of pi.</target>
        </trans-unit>
        <trans-unit id="172" translate="yes" xml:space="preserve">
          <source>For reference, the first 10 decimal places of pi are 3.1415926535.</source>
          <target state="new">For reference, the first 10 decimal places of pi are 3.1415926535.</target>
        </trans-unit>
        <trans-unit id="173" translate="yes" xml:space="preserve">
          <source><bpt id="p36">**</bpt>To submit a pi estimator job<ept id="p36">**</ept></source>
          <target state="new"><bpt id="p36">**</bpt>To submit a pi estimator job<ept id="p36">**</ept></target>
        </trans-unit>
        <trans-unit id="174" translate="yes" xml:space="preserve">
          <source>Follow the procdure in <bpt id="p37">[</bpt>Word count - Java<ept id="p37">](#word-count-java)</ept>, and replace the job definition with the following:</source>
          <target state="new">Follow the procdure in <bpt id="p37">[</bpt>Word count - Java<ept id="p37">](#word-count-java)</ept>, and replace the job definition with the following:</target>
        </trans-unit>
        <trans-unit id="175" translate="yes" xml:space="preserve">
          <source>10-GB Graysort</source>
          <target state="new">10-GB Graysort</target>
        </trans-unit>
        <trans-unit id="176" translate="yes" xml:space="preserve">
          <source>This sample uses a modest 10GB of data so that it can be run relatively quickly.</source>
          <target state="new">This sample uses a modest 10GB of data so that it can be run relatively quickly.</target>
        </trans-unit>
        <trans-unit id="177" translate="yes" xml:space="preserve">
          <source>It uses the MapReduce applications developed by Owen O'Malley and Arun Murthy that won the annual general-purpose ("daytona") terabyte sort benchmark in 2009 with a rate of 0.578TB/min (100TB in 173 minutes).</source>
          <target state="new">It uses the MapReduce applications developed by Owen O'Malley and Arun Murthy that won the annual general-purpose ("daytona") terabyte sort benchmark in 2009 with a rate of 0.578TB/min (100TB in 173 minutes).</target>
        </trans-unit>
        <trans-unit id="178" translate="yes" xml:space="preserve">
          <source>For more information on this and other sorting benchmarks, see the <bpt id="p38">[</bpt>Sortbenchmark<ept id="p38">](http://sortbenchmark.org/)</ept><ph id="ph8" /> site.</source>
          <target state="new">For more information on this and other sorting benchmarks, see the <bpt id="p38">[</bpt>Sortbenchmark<ept id="p38">](http://sortbenchmark.org/)</ept><ph id="ph8" /> site.</target>
        </trans-unit>
        <trans-unit id="179" translate="yes" xml:space="preserve">
          <source>This sample uses three sets of MapReduce programs:</source>
          <target state="new">This sample uses three sets of MapReduce programs:</target>
        </trans-unit>
        <trans-unit id="180" translate="yes" xml:space="preserve">
          <source><bpt id="p39">**</bpt>TeraGen<ept id="p39">**</ept><ph id="ph9" /> is a MapReduce program that you can use to generate the rows of data to sort.</source>
          <target state="new"><bpt id="p39">**</bpt>TeraGen<ept id="p39">**</ept><ph id="ph9" /> is a MapReduce program that you can use to generate the rows of data to sort.</target>
        </trans-unit>
        <trans-unit id="181" translate="yes" xml:space="preserve">
          <source><bpt id="p40">**</bpt>TeraSort<ept id="p40">**</ept><ph id="ph10" /> samples the input data and uses MapReduce to sort the data into a total order. TeraS</source>
          <target state="new"><bpt id="p40">**</bpt>TeraSort<ept id="p40">**</ept><ph id="ph10" /> samples the input data and uses MapReduce to sort the data into a total order. TeraS</target>
        </trans-unit>
        <trans-unit id="182" translate="yes" xml:space="preserve">
          <source>ort is a standard sort of MapReduce functions, except for a custom partitioner that uses a sorted list of N-1 sampled keys that define the key range for each reduce. In pa</source>
          <target state="new">ort is a standard sort of MapReduce functions, except for a custom partitioner that uses a sorted list of N-1 sampled keys that define the key range for each reduce. In pa</target>
        </trans-unit>
        <trans-unit id="183" translate="yes" xml:space="preserve">
          <source>rticular, all keys such that sample[i-1] &lt;= key &lt; sample[i] are sent to reduce i.</source>
          <target state="new">rticular, all keys such that sample[i-1] &lt;= key &lt; sample[i] are sent to reduce i.</target>
        </trans-unit>
        <trans-unit id="184" translate="yes" xml:space="preserve">
          <source>This guarantees that the outputs of reduce i are all less than the output of reduce i+1.</source>
          <target state="new">This guarantees that the outputs of reduce i are all less than the output of reduce i+1.</target>
        </trans-unit>
        <trans-unit id="185" translate="yes" xml:space="preserve">
          <source><bpt id="p41">**</bpt>TeraValidate<ept id="p41">**</ept><ph id="ph11" /> is a MapReduce program that validates that the output is globally sorted.</source>
          <target state="new"><bpt id="p41">**</bpt>TeraValidate<ept id="p41">**</ept><ph id="ph11" /> is a MapReduce program that validates that the output is globally sorted.</target>
        </trans-unit>
        <trans-unit id="186" translate="yes" xml:space="preserve">
          <source>It creates one map per file in the output directory, and each map ensures that each key is less than or equal to the previous one.</source>
          <target state="new">It creates one map per file in the output directory, and each map ensures that each key is less than or equal to the previous one.</target>
        </trans-unit>
        <trans-unit id="187" translate="yes" xml:space="preserve">
          <source>The map function also generates records of the first and last keys of each file, and the reduce function ensures that the first key of file i is greater than the last key of file i-1.</source>
          <target state="new">The map function also generates records of the first and last keys of each file, and the reduce function ensures that the first key of file i is greater than the last key of file i-1.</target>
        </trans-unit>
        <trans-unit id="188" translate="yes" xml:space="preserve">
          <source>Any problems are reported as an output of the reduce with the keys that are out of order.</source>
          <target state="new">Any problems are reported as an output of the reduce with the keys that are out of order.</target>
        </trans-unit>
        <trans-unit id="189" translate="yes" xml:space="preserve">
          <source>The input and output format, used by all three applications, reads and writes the text files in the right format.</source>
          <target state="new">The input and output format, used by all three applications, reads and writes the text files in the right format.</target>
        </trans-unit>
        <trans-unit id="190" translate="yes" xml:space="preserve">
          <source>The output of the reduce has replication set to 1, instead of the default 3, because the benchmark contest does not require that the output data be replicated on to multiple nodes.</source>
          <target state="new">The output of the reduce has replication set to 1, instead of the default 3, because the benchmark contest does not require that the output data be replicated on to multiple nodes.</target>
        </trans-unit>
        <trans-unit id="191" translate="yes" xml:space="preserve">
          <source>Three tasks are required by the sample, each corresponding to one of the MapReduce programs described in the introduction:</source>
          <target state="new">Three tasks are required by the sample, each corresponding to one of the MapReduce programs described in the introduction:</target>
        </trans-unit>
        <trans-unit id="192" translate="yes" xml:space="preserve">
          <source>Generate the data for sorting by running the <bpt id="p42">**</bpt>TeraGen<ept id="p42">**</ept><ph id="ph12" /> MapReduce job.</source>
          <target state="new">Generate the data for sorting by running the <bpt id="p42">**</bpt>TeraGen<ept id="p42">**</ept><ph id="ph12" /> MapReduce job.</target>
        </trans-unit>
        <trans-unit id="193" translate="yes" xml:space="preserve">
          <source>Sort the data by running the <bpt id="p43">**</bpt>TeraSort<ept id="p43">**</ept><ph id="ph13" /> MapReduce job.</source>
          <target state="new">Sort the data by running the <bpt id="p43">**</bpt>TeraSort<ept id="p43">**</ept><ph id="ph13" /> MapReduce job.</target>
        </trans-unit>
        <trans-unit id="194" translate="yes" xml:space="preserve">
          <source>Confirm that the data has been correctly sorted by running the <bpt id="p44">**</bpt>TeraValidate<ept id="p44">**</ept><ph id="ph14" /> MapReduce job.</source>
          <target state="new">Confirm that the data has been correctly sorted by running the <bpt id="p44">**</bpt>TeraValidate<ept id="p44">**</ept><ph id="ph14" /> MapReduce job.</target>
        </trans-unit>
        <trans-unit id="195" translate="yes" xml:space="preserve">
          <source><bpt id="p45">**</bpt>To submit the jobs<ept id="p45">**</ept></source>
          <target state="new"><bpt id="p45">**</bpt>To submit the jobs<ept id="p45">**</ept></target>
        </trans-unit>
        <trans-unit id="196" translate="yes" xml:space="preserve">
          <source>Follow the procdure in <bpt id="p46">[</bpt>Word count - Java<ept id="p46">](#word-count-java)</ept>, and use the following job definitions:</source>
          <target state="new">Follow the procdure in <bpt id="p46">[</bpt>Word count - Java<ept id="p46">](#word-count-java)</ept>, and use the following job definitions:</target>
        </trans-unit>
        <trans-unit id="197" translate="yes" xml:space="preserve">
          <source>$teragen = New-AzureRmHDInsightMapReduceJobDefinition <ph id="ph15">`
                              -JarFile "/example/jars/hadoop-mapreduce-examples.jar" `</ph><ph id="ph16" />
                              -ClassName "teragen" `
                              <ph id="ph17" />-Arguments "-Dmapred.map.tasks=50", "100000000", "/example/data/10GB-sort-input"</source>
          <target state="new">$teragen = New-AzureRmHDInsightMapReduceJobDefinition <ph id="ph15">`
                              -JarFile "/example/jars/hadoop-mapreduce-examples.jar" `</ph><ph id="ph16" />
                              -ClassName "teragen" `
                              <ph id="ph17" />-Arguments "-Dmapred.map.tasks=50", "100000000", "/example/data/10GB-sort-input"</target>
        </trans-unit>
        <trans-unit id="198" translate="yes" xml:space="preserve">
          <source>$terasort = New-AzureRmHDInsightMapReduceJobDefinition <ph id="ph18">`
                              -JarFile "/example/jars/hadoop-mapreduce-examples.jar" `</ph><ph id="ph19" />
                              -ClassName "terasort" `
                              <ph id="ph20" />-Arguments "-Dmapred.map.tasks=50", "-Dmapred.reduce.tasks=25", "/example/data/10GB-sort-input", "/example/data/10GB-sort-output"</source>
          <target state="new">$terasort = New-AzureRmHDInsightMapReduceJobDefinition <ph id="ph18">`
                              -JarFile "/example/jars/hadoop-mapreduce-examples.jar" `</ph><ph id="ph19" />
                              -ClassName "terasort" `
                              <ph id="ph20" />-Arguments "-Dmapred.map.tasks=50", "-Dmapred.reduce.tasks=25", "/example/data/10GB-sort-input", "/example/data/10GB-sort-output"</target>
        </trans-unit>
        <trans-unit id="199" translate="yes" xml:space="preserve">
          <source>$teravalidate = New-AzureRmHDInsightMapReduceJobDefinition <ph id="ph21">`
                              -JarFile "/example/jars/hadoop-mapreduce-examples.jar" `</ph><ph id="ph22" />
                              -ClassName "teravalidate" `
                              <ph id="ph23" />-Arguments "-Dmapred.map.tasks=50", "-Dmapred.reduce.tasks=25", "/example/data/10GB-sort-output", "/example/data/10GB-sort-validate"</source>
          <target state="new">$teravalidate = New-AzureRmHDInsightMapReduceJobDefinition <ph id="ph21">`
                              -JarFile "/example/jars/hadoop-mapreduce-examples.jar" `</ph><ph id="ph22" />
                              -ClassName "teravalidate" `
                              <ph id="ph23" />-Arguments "-Dmapred.map.tasks=50", "-Dmapred.reduce.tasks=25", "/example/data/10GB-sort-output", "/example/data/10GB-sort-validate"</target>
        </trans-unit>
        <trans-unit id="200" translate="yes" xml:space="preserve">
          <source>Next steps</source>
          <target state="new">Next steps</target>
        </trans-unit>
        <trans-unit id="201" translate="yes" xml:space="preserve">
          <source>From this article and the articles in each of the samples, you learned how to run the samples included with the HDInsight clusters by using Azure PowerShell.</source>
          <target state="new">From this article and the articles in each of the samples, you learned how to run the samples included with the HDInsight clusters by using Azure PowerShell.</target>
        </trans-unit>
        <trans-unit id="202" translate="yes" xml:space="preserve">
          <source>For tutorials about using Pig, Hive, and MapReduce with HDInsight, see the following topics:</source>
          <target state="new">For tutorials about using Pig, Hive, and MapReduce with HDInsight, see the following topics:</target>
        </trans-unit>
        <trans-unit id="203" translate="yes" xml:space="preserve">
          <source><bpt id="p47">[</bpt>Get started using Hadoop with Hive in HDInsight to analyze mobile handset use<ept id="p47">][hdinsight-get-started]</ept></source>
          <target state="new"><bpt id="p47">[</bpt>Get started using Hadoop with Hive in HDInsight to analyze mobile handset use<ept id="p47">][hdinsight-get-started]</ept></target>
        </trans-unit>
        <trans-unit id="204" translate="yes" xml:space="preserve">
          <source><bpt id="p48">[</bpt>Use Pig with Hadoop on HDInsight<ept id="p48">][hdinsight-use-pig]</ept></source>
          <target state="new"><bpt id="p48">[</bpt>Use Pig with Hadoop on HDInsight<ept id="p48">][hdinsight-use-pig]</ept></target>
        </trans-unit>
        <trans-unit id="205" translate="yes" xml:space="preserve">
          <source><bpt id="p49">[</bpt>Use Hive with Hadoop on HDInsight<ept id="p49">][hdinsight-use-hive]</ept></source>
          <target state="new"><bpt id="p49">[</bpt>Use Hive with Hadoop on HDInsight<ept id="p49">][hdinsight-use-hive]</ept></target>
        </trans-unit>
        <trans-unit id="206" translate="yes" xml:space="preserve">
          <source><bpt id="p50">[</bpt>Submit Hadoop Jobs in HDInsight<ept id="p50">] [hdinsight-submit-jobs]</ept></source>
          <target state="new"><bpt id="p50">[</bpt>Submit Hadoop Jobs in HDInsight<ept id="p50">] [hdinsight-submit-jobs]</ept></target>
        </trans-unit>
        <trans-unit id="207" translate="yes" xml:space="preserve">
          <source><bpt id="p51">[</bpt>Azure HDInsight SDK documentation<ept id="p51">][hdinsight-sdk-documentation]</ept></source>
          <target state="new"><bpt id="p51">[</bpt>Azure HDInsight SDK documentation<ept id="p51">][hdinsight-sdk-documentation]</ept></target>
        </trans-unit>
        <trans-unit id="208" translate="yes" xml:space="preserve">
          <source><bpt id="p52">[</bpt>Debug Hadoop in HDInsight: Error messages<ept id="p52">] [hdinsight-errors]</ept></source>
          <target state="new"><bpt id="p52">[</bpt>Debug Hadoop in HDInsight: Error messages<ept id="p52">] [hdinsight-errors]</ept></target>
        </trans-unit>
        <trans-unit id="209" translate="yes" xml:space="preserve">
          <source>Appendix A - The Word count source code</source>
          <target state="new">Appendix A - The Word count source code</target>
        </trans-unit>
        <trans-unit id="210" translate="yes" xml:space="preserve">
          <source>Appendix B - The word count streaming source code</source>
          <target state="new">Appendix B - The word count streaming source code</target>
        </trans-unit>
        <trans-unit id="211" translate="yes" xml:space="preserve">
          <source>The MapReduce program uses the cat.exe application as a mapping interface to stream the text into the console and the wc.exe application as the reduce interface to count the number of words that are streamed from a document.</source>
          <target state="new">The MapReduce program uses the cat.exe application as a mapping interface to stream the text into the console and the wc.exe application as the reduce interface to count the number of words that are streamed from a document.</target>
        </trans-unit>
        <trans-unit id="212" translate="yes" xml:space="preserve">
          <source>Both the mapper and reducer read characters, line-by-line, from the standard input stream (stdin) and write to the standard output stream (stdout).</source>
          <target state="new">Both the mapper and reducer read characters, line-by-line, from the standard input stream (stdin) and write to the standard output stream (stdout).</target>
        </trans-unit>
        <trans-unit id="213" translate="yes" xml:space="preserve">
          <source>The mapper code in the cat.cs file uses a <bpt id="p53">[</bpt>StreamReader<ept id="p53">][streamreader]</ept><ph id="ph24" /> object to read the characters of the incoming stream to the console, which then writes the stream to the standard output stream with the static <bpt id="p54">[</bpt>Console.Writeline<ept id="p54">][console-writeline]</ept><ph id="ph25" /> method.</source>
          <target state="new">The mapper code in the cat.cs file uses a <bpt id="p53">[</bpt>StreamReader<ept id="p53">][streamreader]</ept><ph id="ph24" /> object to read the characters of the incoming stream to the console, which then writes the stream to the standard output stream with the static <bpt id="p54">[</bpt>Console.Writeline<ept id="p54">][console-writeline]</ept><ph id="ph25" /> method.</target>
        </trans-unit>
        <trans-unit id="214" translate="yes" xml:space="preserve">
          <source>The reducer code in the wc.cs file uses a <bpt id="p55">[</bpt>StreamReader<ept id="p55">][streamreader]</ept><ph id="ph26" />   object to read characters from the standard input stream that have been output by the cat.exe mapper.</source>
          <target state="new">The reducer code in the wc.cs file uses a <bpt id="p55">[</bpt>StreamReader<ept id="p55">][streamreader]</ept><ph id="ph26" />   object to read characters from the standard input stream that have been output by the cat.exe mapper.</target>
        </trans-unit>
        <trans-unit id="215" translate="yes" xml:space="preserve">
          <source>As it reads the characters with the <bpt id="p56">[</bpt>Console.Writeline<ept id="p56">][console-writeline]</ept><ph id="ph27" /> method, it counts the words by counting spaces and end-of-line characters at the end of each word.</source>
          <target state="new">As it reads the characters with the <bpt id="p56">[</bpt>Console.Writeline<ept id="p56">][console-writeline]</ept><ph id="ph27" /> method, it counts the words by counting spaces and end-of-line characters at the end of each word.</target>
        </trans-unit>
        <trans-unit id="216" translate="yes" xml:space="preserve">
          <source>It then writes the total to the standard output stream with the <bpt id="p57">[</bpt>Console.Writeline<ept id="p57">][console-writeline]</ept><ph id="ph28" /> method.</source>
          <target state="new">It then writes the total to the standard output stream with the <bpt id="p57">[</bpt>Console.Writeline<ept id="p57">][console-writeline]</ept><ph id="ph28" /> method.</target>
        </trans-unit>
        <trans-unit id="217" translate="yes" xml:space="preserve">
          <source>Appendix C - The Pi estimator source code</source>
          <target state="new">Appendix C - The Pi estimator source code</target>
        </trans-unit>
        <trans-unit id="218" translate="yes" xml:space="preserve">
          <source>The pi estimator Java code that contains the mapper and reducer functions is available for inspection below.</source>
          <target state="new">The pi estimator Java code that contains the mapper and reducer functions is available for inspection below.</target>
        </trans-unit>
        <trans-unit id="219" translate="yes" xml:space="preserve">
          <source>The mapper program generates a specified number of points placed at random inside of a unit square and then counts the number of those points that are inside the circle.</source>
          <target state="new">The mapper program generates a specified number of points placed at random inside of a unit square and then counts the number of those points that are inside the circle.</target>
        </trans-unit>
        <trans-unit id="220" translate="yes" xml:space="preserve">
          <source>The reducer program accumulates points counted by the mappers and then estimates the value of pi from the formula 4R, where R is the ratio of the number of points counted inside the circle to the total number of points that are within the square.</source>
          <target state="new">The reducer program accumulates points counted by the mappers and then estimates the value of pi from the formula 4R, where R is the ratio of the number of points counted inside the circle to the total number of points that are within the square.</target>
        </trans-unit>
        <trans-unit id="221" translate="yes" xml:space="preserve">
          <source>Appendix D - The 10gb graysort source code</source>
          <target state="new">Appendix D - The 10gb graysort source code</target>
        </trans-unit>
        <trans-unit id="222" translate="yes" xml:space="preserve">
          <source>The code for the TeraSort MapReduce program is presented for inspection in this section.</source>
          <target state="new">The code for the TeraSort MapReduce program is presented for inspection in this section.</target>
        </trans-unit>
      </group>
    </body>
  </file>
  <header xmlns="">
    <xliffext:olfilehash xmlns:xliffext="urn:microsoft:content:schema:xliffextensions">c8bbef30177b56c1c32df8ed3495cd68ffbbf53a</xliffext:olfilehash>
  </header>
</xliff>